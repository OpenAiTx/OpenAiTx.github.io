<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - Read DeepSeek-V3 documentation in Spanish. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read DeepSeek-V3 documentation in Spanish. This project has 0 stars on GitHub.">
    <meta name="keywords" content="DeepSeek-V3, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "DeepSeek-V3",
  "description": "Read DeepSeek-V3 documentation in Spanish. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "deepseek-ai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/deepseek-ai/DeepSeek-V3/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    DeepSeek-V3
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Spanish</span>
                <span>by deepseek-ai</span>
            </div>
        </div>
        
        <div class="content">
            <p><!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header --></p><p><div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div></p><h2>Tabla de Contenidos</h2></p><ul><li><a href="#1-introducción" target="_blank" rel="noopener noreferrer">Introducción</a></li>
<li><a href="#2-resumen-del-modelo" target="_blank" rel="noopener noreferrer">Resumen del Modelo</a></li>
<li><a href="#3-descarga-del-modelo" target="_blank" rel="noopener noreferrer">Descarga del Modelo</a></li>
<li><a href="#4-resultados-de-evaluación" target="_blank" rel="noopener noreferrer">Resultados de Evaluación</a></li>
<li><a href="#5-sitio-web-de-chat-y-plataforma-api" target="_blank" rel="noopener noreferrer">Sitio Web de Chat y Plataforma API</a></li>
<li><a href="#6-cómo-ejecutar-localmente" target="_blank" rel="noopener noreferrer">Cómo Ejecutar Localmente</a></li>
<li><a href="#7-licencia" target="_blank" rel="noopener noreferrer">Licencia</a></li>
<li><a href="#8-citación" target="_blank" rel="noopener noreferrer">Citación</a></li>
<li><a href="#9-contacto" target="_blank" rel="noopener noreferrer">Contacto</a></li></p><p>
</ul><h2>1. Introducción</h2></p><p>Presentamos DeepSeek-V3, un potente modelo de lenguaje Mixture-of-Experts (MoE) con 671B parámetros totales y 37B activados por cada token.  
Para lograr una inferencia eficiente y un entrenamiento rentable, DeepSeek-V3 adopta las arquitecturas Multi-head Latent Attention (MLA) y DeepSeekMoE, las cuales fueron validadas exhaustivamente en DeepSeek-V2.  
Además, DeepSeek-V3 es pionero en una estrategia de balanceo de carga sin pérdida auxiliar y establece un objetivo de entrenamiento de predicción multi-token para un rendimiento superior.  
Preentrenamos DeepSeek-V3 en 14.8 billones de tokens diversos y de alta calidad, seguido de etapas de Fine-Tuning Supervisado y Aprendizaje por Refuerzo para aprovechar al máximo sus capacidades.  
Evaluaciones exhaustivas revelan que DeepSeek-V3 supera a otros modelos open-source y alcanza un rendimiento comparable a los modelos líderes de código cerrado.  
A pesar de su excelente rendimiento, DeepSeek-V3 requiere solo 2.788M horas de GPU H800 para su entrenamiento completo.  
Además, su proceso de entrenamiento es notablemente estable.  
Durante todo el proceso de entrenamiento, no experimentamos picos de pérdida irrecuperables ni realizamos retrocesos.  
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p></p><h2>2. Resumen del Modelo</h2></p><hr></p><p><strong>Arquitectura: Estrategia Innovadora de Balanceo de Carga y Objetivo de Entrenamiento</strong></p><ul><li>Sobre la arquitectura eficiente de DeepSeek-V2, desarrollamos una estrategia de balanceo de carga sin pérdida auxiliar, que minimiza la degradación de rendimiento que surge al incentivar el balanceo de carga.</li>
<li> Investigamos un objetivo de Predicción Multi-Token (MTP) y demostramos que es beneficioso para el rendimiento del modelo.  </li>
    </ul>También puede usarse para decodificación especulativa para acelerar la inferencia. </p><hr></p><p><strong>Preentrenamiento: Hacia la Eficiencia Máxima de Entrenamiento</strong></p><ul><li>Diseñamos un marco de entrenamiento de precisión mixta FP8 y, por primera vez, validamos la viabilidad y efectividad del entrenamiento en FP8 en un modelo de escala extremadamente grande.  </li>
<li>A través del co-diseño de algoritmos, marcos y hardware, superamos el cuello de botella de comunicación en el entrenamiento MoE entre nodos, logrando casi una superposición completa entre computación y comunicación.  </li>
  </ul>Esto mejora significativamente nuestra eficiencia de entrenamiento y reduce los costes, permitiéndonos escalar aún más el tamaño del modelo sin gastos adicionales.  
<ul><li>A un costo económico de solo 2.664M horas de GPU H800, completamos el preentrenamiento de DeepSeek-V3 en 14.8T tokens, produciendo el modelo base open-source más fuerte hasta la fecha. Las etapas subsiguientes tras el preentrenamiento requieren solo 0.1M horas GPU.</li></p><p></ul>---</p><p><strong>Post-Entrenamiento: Destilación de Conocimientos desde DeepSeek-R1</strong></p><ul><li>  Introducimos una metodología innovadora para destilar capacidades de razonamiento desde el modelo Chain-of-Thought (CoT) largo, específicamente de uno de los modelos DeepSeek R1, hacia LLMs estándar, en particular DeepSeek-V3. Nuestra canalización incorpora elegantemente los patrones de verificación y reflexión de R1 en DeepSeek-V3 y mejora notablemente su rendimiento en razonamiento. Al mismo tiempo, también mantenemos el control sobre el estilo y la longitud de salida de DeepSeek-V3.</li></p><p></ul>---</p><h2>3. Descarga del Modelo</h2></p><p><div align="center"></p><p>| <strong>Modelo</strong> | <strong>#Parámetros Totales</strong> | <strong>#Parámetros Activados</strong> | <strong>Longitud de Contexto</strong> | <strong>Descarga</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |</p><p></div></p><blockquote>[!NOTA]</blockquote>
<blockquote>El tamaño total de los modelos DeepSeek-V3 en Hugging Face es 685B, lo que incluye 671B de los pesos del Modelo Principal y 14B de los pesos del Módulo de Predicción Multi-Token (MTP).</blockquote></p><p>Para garantizar un rendimiento óptimo y flexibilidad, nos hemos asociado con comunidades open-source y proveedores de hardware para proporcionar múltiples formas de ejecutar el modelo localmente. Para una guía paso a paso, consulte la Sección 6: <a href="#6-cómo-ejecutar-localmente" target="_blank" rel="noopener noreferrer">Cómo Ejecutar Localmente</a>.</p><p>Para desarrolladores interesados en profundizar, recomendamos explorar <a href="./README_WEIGHTS.md" target="_blank" rel="noopener noreferrer">README_WEIGHTS.md</a> para detalles sobre los pesos del Modelo Principal y los Módulos de Predicción Multi-Token (MTP). Tenga en cuenta que el soporte para MTP está actualmente en desarrollo activo dentro de la comunidad, y agradecemos sus contribuciones y comentarios.</p><h2>4. Resultados de Evaluación</h2>
<h3>Modelo Base</h3>
#### Benchmarks Estándar</p><p><div align="center"></p><p>
|  | Benchmark (Métrica) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Arquitectura | - | MoE | Denso | Denso | MoE |
| | # Parámetros Activados | - | 21B | 72B | 405B | 37B |
| | # Parámetros Totales | - | 236B | 72B | 405B | 671B |
| Inglés | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Prec.) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Prec.) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Prec.) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Prec.) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Prec.) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Prec.) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Prec.) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Prec.) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Prec.) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Prec.) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Prec.) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Código | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Prec.) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Prec.) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Matemáticas | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Chino | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Prec.) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Prec.) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Prec.) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Prec.) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Multilingüe | MMMLU-no-Inglés (Prec.) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p><p></div></p><blockquote>[!NOTA]</blockquote>
<blockquote>Los mejores resultados se muestran en negrita. Las puntuaciones con una diferencia no superior a 0.3 se consideran del mismo nivel. DeepSeek-V3 logra el mejor rendimiento en la mayoría de los benchmarks, especialmente en tareas de matemáticas y código.</blockquote>
<blockquote>Para más detalles de evaluación, consulte nuestro paper. </blockquote></p><p>#### Ventana de Contexto
<p align="center">
  <img width="80%" src="figures/niah.png">
</p></p><p>Resultados de evaluación en las pruebas `<code>Needle In A Haystack</code><code> (NIAH). DeepSeek-V3 rinde bien en todas las longitudes de ventana de contexto hasta <strong>128K</strong>. </p><h3>Modelo de Chat</h3>
#### Benchmarks Estándar (Modelos mayores a 67B)
<div align="center"></p><p>| | <strong>Benchmark (Métrica)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Arquitectura | MoE | MoE | Denso | Denso | - | - | MoE |
| | # Parámetros Activados | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Parámetros Totales | 236B | 236B | 72B | 405B | - | - | 671B |
| Inglés | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Estricto) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Correcto) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Prec.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Prec.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Código | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Percentil) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Resuelto) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Prec.) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Prec.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Matemáticas | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Chino | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Correcto) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p><p></div></p><blockquote>[!NOTA]</blockquote>
<blockquote>Todos los modelos se evalúan en una configuración que limita la longitud de salida a 8K. Los benchmarks con menos de 1000 muestras se prueban varias veces usando diferentes temperaturas para obtener resultados robustos. DeepSeek-V3 es el modelo open-source con mejor rendimiento, y también muestra un rendimiento competitivo frente a modelos de código cerrado de vanguardia.</blockquote></p><p>
####  Evaluación de Generación Abierta</p><p><div align="center"></p><p>| Modelo | Arena-Hard | AlpacaEval 2.0 |
|--------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p><p></div></p><blockquote>[!NOTA]</blockquote>
<blockquote>Evaluaciones de conversación abierta en inglés. Para AlpacaEval 2.0, usamos la tasa de victoria controlada por longitud como métrica.</blockquote></p><h2>5. Sitio Web de Chat y Plataforma API</h2></p><p>Puede chatear con DeepSeek-V3 en el sitio web oficial de DeepSeek: <a href="https://chat.deepseek.com/sign_in" target="_blank" rel="noopener noreferrer">chat.deepseek.com</a></p><p>También proporcionamos una API compatible con OpenAI en DeepSeek Platform: <a href="https://platform.deepseek.com/" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a></p><h2>6. Cómo Ejecutar Localmente</h2></p><p>DeepSeek-V3 puede desplegarse localmente utilizando el siguiente hardware y software de la comunidad open-source:</p><ul><li><strong>DeepSeek-Infer Demo</strong>: Ofrecemos una demo simple y ligera para inferencia FP8 y BF16.</li>
<li><strong>SGLang</strong>: Soporta completamente el modelo DeepSeek-V3 en modos de inferencia BF16 y FP8, con Predicción Multi-Token <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">próximamente</a>.</li>
<li><strong>LMDeploy</strong>: Permite inferencia eficiente FP8 y BF16 para despliegue local y en la nube.</li>
<li><strong>TensorRT-LLM</strong>: Actualmente soporta inferencia BF16 y cuantización INT4/8, con soporte FP8 próximamente.</li>
<li><strong>vLLM</strong>: Soporta el modelo DeepSeek-V3 en modos FP8 y BF16 para paralelismo tensorial y de pipeline.</li>
<li><strong>LightLLM</strong>: Permite un despliegue eficiente en uno o varios nodos para FP8 y BF16.</li>
<li><strong>AMD GPU</strong>: Permite ejecutar el modelo DeepSeek-V3 en GPUs AMD a través de SGLang en modos BF16 y FP8.</li>
<li><strong>Huawei Ascend NPU</strong>: Permite ejecutar DeepSeek-V3 en dispositivos Huawei Ascend.</li></p><p></ul>Dado que el entrenamiento FP8 es adoptado nativamente en nuestro framework, solo proporcionamos pesos FP8. Si requiere pesos BF16 para experimentación, puede usar el script de conversión proporcionado para realizar la transformación.</p><p>Aquí un ejemplo de conversión de pesos FP8 a BF16:</p><pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /ruta/a/pesos_fp8 --output-bf16-hf-path /ruta/a/pesos_bf16</code></pre></p><blockquote>[!NOTA]</blockquote>
<blockquote>Hugging Face Transformers aún no está directamente soportado.</blockquote></p><h3>6.1 Inferencia con DeepSeek-Infer Demo (solo ejemplo)</h3></p><p>#### Requisitos del Sistema</p><blockquote>[!NOTA] </blockquote>
<blockquote>Solo Linux con Python 3.10. Mac y Windows no están soportados.</blockquote></p><p>Dependencias:
</code>`<code>pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
<pre><code class="language-">#### Preparación de Pesos del Modelo y Código Demo</p><p>Primero, clone nuestro repositorio de DeepSeek-V3 en GitHub:
</code></pre>shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
<pre><code class="language-">
Navegue a la carpeta </code>inference<code> e instale las dependencias listadas en </code>requirements.txt<code>. La forma más fácil es usar un gestor de paquetes como </code>conda<code> o </code>uv<code> para crear un nuevo entorno virtual e instalar las dependencias.
</code></pre>shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
<pre><code class="language-">
Descargue los pesos del modelo desde Hugging Face y colóquelos en la carpeta </code>/ruta/a/DeepSeek-V3<code>.</p><p>#### Conversión de Pesos del Modelo</p><p>Convierta los pesos del modelo Hugging Face a un formato específico:
</code></pre>shell
python convert.py --hf-ckpt-path /ruta/a/DeepSeek-V3 --save-path /ruta/a/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
<pre><code class="language-">
#### Ejecución</p><p>Ahora puede chatear con DeepSeek-V3:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /ruta/a/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
<pre><code class="language-">
O inferencia en lote sobre un archivo dado:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /ruta/a/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
<pre><code class="language-">
<h3>6.2 Inferencia con SGLang (recomendado)</h3></p><p><a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a> soporta actualmente <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations" target="_blank" rel="noopener noreferrer">optimizaciones MLA</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models" target="_blank" rel="noopener noreferrer">DP Attention</a>, FP8 (W8A8), FP8 KV Cache y Torch Compile, ofreciendo latencia y rendimiento de vanguardia entre los frameworks open-source.</p><p>En particular, <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1" target="_blank" rel="noopener noreferrer">SGLang v0.4.1</a> soporta completamente la ejecución de DeepSeek-V3 tanto en <strong>GPUs NVIDIA como AMD</strong>, haciéndolo una solución altamente versátil y robusta.</p><p>SGLang también soporta <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208" target="_blank" rel="noopener noreferrer">paralelismo tensorial multi-nodo</a>, permitiéndole ejecutar el modelo en varias máquinas conectadas en red.</p><p>La Predicción Multi-Token (MTP) está en desarrollo, y el progreso puede seguirse en el <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">plan de optimización</a>.</p><p>Aquí las instrucciones de lanzamiento del equipo SGLang: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p><h3>6.3 Inferencia con LMDeploy (recomendado)</h3>
<a href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener noreferrer">LMDeploy</a>, un framework flexible y de alto rendimiento para inferencia y servicio de grandes modelos de lenguaje, ahora soporta DeepSeek-V3. Ofrece procesamiento offline en pipeline y despliegue online, integrándose perfectamente con flujos de trabajo basados en PyTorch.</p><p>Para instrucciones paso a paso sobre cómo ejecutar DeepSeek-V3 con LMDeploy, consulte: https://github.com/InternLM/lmdeploy/issues/2960</p><h3>6.4 Inferencia con TRT-LLM (recomendado)</h3></p><p><a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener noreferrer">TensorRT-LLM</a> ahora soporta el modelo DeepSeek-V3, ofreciendo opciones de precisión como BF16 e INT4/INT8 solo-pesos. El soporte para FP8 está en desarrollo y será lanzado próximamente. Puede acceder a la rama personalizada de TRTLLM específicamente para DeepSeek-V3 aquí: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. </p><h3>6.5 Inferencia con vLLM (recomendado)</h3></p><p><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> v0.6.6 soporta inferencia DeepSeek-V3 para modos FP8 y BF16 tanto en GPUs NVIDIA como AMD. Además de técnicas estándar, vLLM ofrece _paralelismo de pipeline_ permitiéndole ejecutar el modelo en varias máquinas conectadas en red. Para una guía detallada, consulte las <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html" target="_blank" rel="noopener noreferrer">instrucciones vLLM</a>. Puede seguir el <a href="https://github.com/vllm-project/vllm/issues/11539" target="_blank" rel="noopener noreferrer">plan de mejora</a> también.</p><h3>6.6 Inferencia con LightLLM (recomendado)</h3></p><p><a href="https://github.com/ModelTC/lightllm/tree/main" target="_blank" rel="noopener noreferrer">LightLLM</a> v1.0.1 soporta despliegue tensorial en máquina única y multi-máquina para DeepSeek-R1 (FP8/BF16) y proporciona despliegue de precisión mixta, con más modos de cuantización integrándose continuamente. Para más detalles, consulte las <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html" target="_blank" rel="noopener noreferrer">instrucciones LightLLM</a>. Además, LightLLM ofrece despliegue PD-disaggregation para DeepSeek-V2, y la implementación para DeepSeek-V3 está en desarrollo.</p><h3>6.7 Inferencia Recomendada con GPUs AMD</h3></p><p>En colaboración con el equipo AMD, hemos logrado soporte Día-Uno para GPUs AMD usando SGLang, con compatibilidad total para precisión FP8 y BF16. Para una guía detallada, consulte las <a href="#63-inferencia-con-lmdeploy-recomendado" target="_blank" rel="noopener noreferrer">instrucciones SGLang</a>.</p><h3>6.8 Inferencia Recomendada con NPUs Huawei Ascend</h3>
El framework <a href="https://www.hiascend.com/en/software/mindie" target="_blank" rel="noopener noreferrer">MindIE</a> de la comunidad Huawei Ascend ha adaptado exitosamente la versión BF16 de DeepSeek-V3. Para una guía paso a paso en NPUs Ascend, siga las <a href="https://modelers.cn/models/MindIE/deepseekv3" target="_blank" rel="noopener noreferrer">instrucciones aquí</a>.</p><h2>7. Licencia</h2>
Este repositorio de código está licenciado bajo <a href="LICENSE-CODE" target="_blank" rel="noopener noreferrer">Licencia MIT</a>. El uso de los modelos DeepSeek-V3 Base/Chat está sujeto a la <a href="LICENSE-MODEL" target="_blank" rel="noopener noreferrer">Licencia del Modelo</a>. La serie DeepSeek-V3 (incluyendo Base y Chat) soporta uso comercial.</p><h2>8. Citación</code></pre></h2>
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code>``</p><h2>9. Contacto</h2>
Si tiene alguna pregunta, por favor abra un issue o contáctenos en <a href="service@deepseek.com" target="_blank" rel="noopener noreferrer">service@deepseek.com</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>