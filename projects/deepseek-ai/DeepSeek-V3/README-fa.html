<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - Read DeepSeek-V3 documentation in Persian. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read DeepSeek-V3 documentation in Persian. This project has 0 stars on GitHub.">
    <meta name="keywords" content="DeepSeek-V3, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "DeepSeek-V3",
  "description": "Read DeepSeek-V3 documentation in Persian. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "deepseek-ai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/deepseek-ai/DeepSeek-V3/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    DeepSeek-V3
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Persian</span>
                <span>by deepseek-ai</span>
            </div>
        </div>
        
        <div class="content">
            <p><!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header --></p><p><div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div></p><h2>فهرست مطالب</h2></p><ul><li><a href="#1-مقدمه" target="_blank" rel="noopener noreferrer">مقدمه</a></li>
<li><a href="#2-خلاصه-مدل" target="_blank" rel="noopener noreferrer">خلاصه مدل</a></li>
<li><a href="#3-دریافت-مدلها" target="_blank" rel="noopener noreferrer">دریافت مدل‌ها</a></li>
<li><a href="#4-نتایج-ارزیابی" target="_blank" rel="noopener noreferrer">نتایج ارزیابی</a></li>
<li><a href="#5-وبسایت-چت--بستر-api" target="_blank" rel="noopener noreferrer">وبسایت چت و بستر API</a></li>
<li><a href="#6-نحوه-اجرای-محلی" target="_blank" rel="noopener noreferrer">نحوه اجرای محلی</a></li>
<li><a href="#7-مجوز" target="_blank" rel="noopener noreferrer">مجوز</a></li>
<li><a href="#8-ارجاع" target="_blank" rel="noopener noreferrer">ارجاع</a></li>
<li><a href="#9-تماس" target="_blank" rel="noopener noreferrer">تماس</a></li></p><p>
</ul><h2>1. مقدمه</h2></p><p>ما DeepSeek-V3 را معرفی می‌کنیم، یک مدل زبانی Mixture-of-Experts (MoE) قدرتمند با ۶۷۱ میلیارد پارامتر کل و ۳۷ میلیارد پارامتر فعال برای هر توکن.  
برای دستیابی به استنتاج کارآمد و آموزش مقرون‌به‌صرفه، DeepSeek-V3 از معماری‌های Multi-head Latent Attention (MLA) و DeepSeekMoE بهره می‌گیرد که در DeepSeek-V2 به طور کامل اعتبارسنجی شده‌اند.  
علاوه بر این، DeepSeek-V3 برای اولین بار یک استراتژی توازن بار بدون زیان کمکی (auxiliary-loss-free) را پیاده‌سازی کرده و هدف آموزشی پیش‌بینی چندتوکنی را برای عملکرد بهتر اتخاذ می‌کند.  
ما DeepSeek-V3 را بر روی ۱۴.۸ تریلیون توکن متنوع و با کیفیت بالا پیش‌آموزش داده‌ایم و سپس مراحل Fine-Tuning تحت نظارت و یادگیری تقویتی را برای بهره‌برداری کامل از توانمندی‌های مدل انجام داده‌ایم.  
ارزیابی‌های جامع نشان می‌دهد که DeepSeek-V3 از سایر مدل‌های متن‌باز پیشی می‌گیرد و به عملکرد مدل‌های پیشتاز متن‌بسته نزدیک می‌شود.  
علی‌رغم عملکرد عالی، آموزش کامل DeepSeek-V3 تنها به ۲.۷۸۸ میلیون ساعت GPU مدل H800 نیاز دارد.  
افزون بر این، فرایند آموزش آن به طرز چشمگیری پایدار است.  
در طول فرایند آموزش، هیچ جهش غیرقابل بازیابی در زیان رخ نداد و هیچ بازگشتی انجام نشد.
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p></p><h2>2. خلاصه مدل</h2></p><hr></p><p><strong>معماری: استراتژی نوآورانه توازن بار و هدف آموزشی</strong></p><ul><li>بر پایه معماری کارآمد DeepSeek-V2، ما استراتژی بدون زیان کمکی را برای توازن بار ابداع کردیم که افت عملکرد ناشی از توازن بار را به حداقل می‌رساند.</li>
<li>ما هدف پیش‌بینی چندتوکنی (MTP) را بررسی و اثبات کردیم که برای عملکرد مدل سودمند است.  </li>
    </ul>همچنین می‌تواند برای رمزگشایی پیش‌بینانه و تسریع استنتاج به کار رود.</p><hr></p><p><strong>پیش‌آموزش: به سوی کارایی نهایی آموزش</strong></p><ul><li>ما چارچوب آموزش با دقت ترکیبی FP8 را طراحی کردیم و برای اولین بار امکان‌پذیری و کارایی آموزش FP8 را بر روی مدلی در این مقیاس بسیار بزرگ اعتبارسنجی کردیم.  </li>
<li>از طریق هم‌طراحی الگوریتم، چارچوب و سخت‌افزار، گلوگاه ارتباطی در آموزش MoE بین نودها را رفع کردیم و تقریباً همپوشانی کامل محاسبه-ارتباط را به دست آوردیم.  </li>
  </ul>این امر به طور قابل توجهی کارایی آموزش ما را افزایش داده و هزینه آموزش را کاهش داده است، به طوری که می‌توانیم اندازه مدل را بدون سربار اضافی افزایش دهیم.  
<ul><li>با هزینه اقتصادی تنها ۲.۶۶۴ میلیون ساعت GPU مدل H800، پیش‌آموزش DeepSeek-V3 را بر روی ۱۴.۸ تریلیون توکن کامل کردیم و قوی‌ترین مدل پایه متن‌باز فعلی را تولید نمودیم. مراحل آموزشی پس از پیش‌آموزش تنها به ۰.۱ میلیون ساعت GPU نیاز دارند.</li></p><p></ul>---</p><p><strong>پس‌آموزش: تقطیر دانش از DeepSeek-R1</strong></p><ul><li>  ما روشی نوآورانه برای تقطیر قابلیت‌های استدلالی از مدل زنجیره تفکر (CoT) بلند، به‌ویژه یکی از مدل‌های سری DeepSeek R1، به LLMهای استاندارد، به ویژه DeepSeek-V3، معرفی کردیم. خط لوله ما به طور زیبا الگوهای تأیید و بازتاب R1 را در DeepSeek-V3 ادغام کرده و عملکرد استدلالی آن را به طور قابل توجهی بهبود می‌بخشد. در عین حال، کنترل بر سبک و طول خروجی DeepSeek-V3 را نیز حفظ می‌کنیم.</li></p><p></ul>---</p><h2>3. دریافت مدل‌ها</h2></p><p><div align="center"></p><p>| <strong>مدل</strong> | <strong>تعداد کل پارامترها</strong> | <strong>تعداد پارامتر فعال</strong> | <strong>طول کانتکست</strong> | <strong>دریافت</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | ۶۷۱B | ۳۷B | ۱۲۸K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |
| DeepSeek-V3   | ۶۷۱B | ۳۷B | ۱۲۸K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>مجموع حجم مدل‌های DeepSeek-V3 در Hugging Face معادل ۶۸۵ میلیارد پارامتر است که شامل ۶۷۱ میلیارد پارامتر وزن مدل اصلی و ۱۴ میلیارد پارامتر وزن ماژول پیش‌بینی چندتوکنی (MTP) می‌باشد.</blockquote></p><p>برای اطمینان از عملکرد و انعطاف‌پذیری بهینه، ما با جوامع متن‌باز و فروشندگان سخت‌افزار همکاری کرده‌ایم تا چندین روش برای اجرای مدل به صورت محلی فراهم کنیم. برای راهنمایی گام به گام، به بخش ۶: <a href="#6-نحوه-اجرای-محلی" target="_blank" rel="noopener noreferrer">نحوه اجرای محلی</a> مراجعه نمایید.</p><p>برای توسعه‌دهندگانی که به دنبال جزئیات بیشتر هستند، توصیه می‌کنیم <a href="./README_WEIGHTS.md" target="_blank" rel="noopener noreferrer">README_WEIGHTS.md</a> را برای اطلاعات مربوط به وزن‌های مدل اصلی و ماژول‌های پیش‌بینی چندتوکنی (MTP) مطالعه کنید. لطفاً توجه داشته باشید که پشتیبانی از MTP در حال توسعه فعال توسط جامعه است و از مشارکت‌ها و بازخورد شما استقبال می‌شود.</p><h2>4. نتایج ارزیابی</h2>
<h3>مدل پایه</h3>
#### معیارهای استاندارد</p><p><div align="center"></p><p>
|  | معیار (متریک) | تعداد نمونه | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | معماری | - | MoE | Dense | Dense | MoE |
| | تعداد پارامتر فعال | - | ۲۱B | ۷۲B | ۴۰۵B | ۳۷B |
| | تعداد کل پارامتر | - | ۲۳۶B | ۷۲B | ۴۰۵B | ۶۷۱B |
| انگلیسی | Pile-test (BPB) | - | ۰.۶۰۶ | ۰.۶۳۸ | <strong>۰.۵۴۲</strong> | ۰.۵۴۸ |
| | BBH (EM) | ۳ نمونه | ۷۸.۸ | ۷۹.۸ | ۸۲.۹ | <strong>۸۷.۵</strong> |
| | MMLU (دقت) | ۵ نمونه | ۷۸.۴ | ۸۵.۰ | ۸۴.۴ | <strong>۸۷.۱</strong> |
| | MMLU-Redux (دقت) | ۵ نمونه | ۷۵.۶ | ۸۳.۲ | ۸۱.۳ | <strong>۸۶.۲</strong> |
| | MMLU-Pro (دقت) | ۵ نمونه | ۵۱.۴ | ۵۸.۳ | ۵۲.۸ | <strong>۶۴.۴</strong> |
| | DROP (F1) | ۳ نمونه | ۸۰.۴ | ۸۰.۶ | ۸۶.۰ | <strong>۸۹.۰</strong> |
| | ARC-Easy (دقت) | ۲۵ نمونه | ۹۷.۶ | ۹۸.۴ | ۹۸.۴ | <strong>۹۸.۹</strong> |
| | ARC-Challenge (دقت) | ۲۵ نمونه | ۹۲.۲ | ۹۴.۵ | <strong>۹۵.۳</strong> | <strong>۹۵.۳</strong> |
| | HellaSwag (دقت) | ۱۰ نمونه | ۸۷.۱ | ۸۴.۸ | <strong>۸۹.۲</strong> | ۸۸.۹ |
| | PIQA (دقت) | بدون نمونه | ۸۳.۹ | ۸۲.۶ | <strong>۸۵.۹</strong> | ۸۴.۷ |
| | WinoGrande (دقت) | ۵ نمونه | <strong>۸۶.۳</strong> | ۸۲.۳ | ۸۵.۲ | ۸۴.۹ |
| | RACE-Middle (دقت) | ۵ نمونه | ۷۳.۱ | ۶۸.۱ | <strong>۷۴.۲</strong> | ۶۷.۱ |
| | RACE-High (دقت) | ۵ نمونه | ۵۲.۶ | ۵۰.۳ | <strong>۵۶.۸</strong> | ۵۱.۳ |
| | TriviaQA (EM) | ۵ نمونه | ۸۰.۰ | ۷۱.۹ | ۸۲.۷ | <strong>۸۲.۹</strong> |
| | NaturalQuestions (EM) | ۵ نمونه | ۳۸.۶ | ۳۳.۲ | <strong>۴۱.۵</strong> | ۴۰.۰ |
| | AGIEval (دقت) | بدون نمونه | ۵۷.۵ | ۷۵.۸ | ۶۰.۶ | <strong>۷۹.۶</strong> |
| کد | HumanEval (Pass@1) | بدون نمونه | ۴۳.۳ | ۵۳.۰ | ۵۴.۹ | <strong>۶۵.۲</strong> |
| | MBPP (Pass@1) | ۳ نمونه | ۶۵.۰ | ۷۲.۶ | ۶۸.۴ | <strong>۷۵.۴</strong> |
| | LiveCodeBench-Base (Pass@1) | ۳ نمونه | ۱۱.۶ | ۱۲.۹ | ۱۵.۵ | <strong>۱۹.۴</strong> |
| | CRUXEval-I (دقت) | ۲ نمونه | ۵۲.۵ | ۵۹.۱ | ۵۸.۵ | <strong>۶۷.۳</strong> |
| | CRUXEval-O (دقت) | ۲ نمونه | ۴۹.۸ | ۵۹.۹ | ۵۹.۹ | <strong>۶۹.۸</strong> |
| ریاضی | GSM8K (EM) | ۸ نمونه | ۸۱.۶ | ۸۸.۳ | ۸۳.۵ | <strong>۸۹.۳</strong> |
| | MATH (EM) | ۴ نمونه | ۴۳.۴ | ۵۴.۴ | ۴۹.۰ | <strong>۶۱.۶</strong> |
| | MGSM (EM) | ۸ نمونه | ۶۳.۶ | ۷۶.۲ | ۶۹.۹ | <strong>۷۹.۸</strong> |
| | CMath (EM) | ۳ نمونه | ۷۸.۷ | ۸۴.۵ | ۷۷.۳ | <strong>۹۰.۷</strong> |
| چینی | CLUEWSC (EM) | ۵ نمونه | ۸۲.۰ | ۸۲.۵ | <strong>۸۳.۰</strong> | ۸۲.۷ |
| | C-Eval (دقت) | ۵ نمونه | ۸۱.۴ | ۸۹.۲ | ۷۲.۵ | <strong>۹۰.۱</strong> |
| | CMMLU (دقت) | ۵ نمونه | ۸۴.۰ | <strong>۸۹.۵</strong> | ۷۳.۷ | ۸۸.۸ |
| | CMRC (EM) | ۱ نمونه | <strong>۷۷.۴</strong> | ۷۵.۸ | ۷۶.۰ | ۷۶.۳ |
| | C3 (دقت) | بدون نمونه | ۷۷.۴ | ۷۶.۷ | <strong>۷۹.۷</strong> | ۷۸.۶ |
| | CCPM (دقت) | بدون نمونه | <strong>۹۳.۰</strong> | ۸۸.۵ | ۷۸.۶ | ۹۲.۰ |
| چندزبانه | MMMLU-non-English (دقت) | ۵ نمونه | ۶۴.۰ | ۷۴.۸ | ۷۳.۸ | <strong>۷۹.۴</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>بهترین نتایج به صورت بولد نشان داده شده‌اند. امتیازهایی که اختلاف آن‌ها بیش از ۰.۳ نباشد در یک سطح در نظر گرفته می‌شوند. DeepSeek-V3 در اغلب معیارها، به ویژه در ریاضیات و کدنویسی بهترین عملکرد را دارد.</blockquote>
<blockquote>برای جزئیات بیشتر ارزیابی، مقاله ما را مطالعه نمایید.</blockquote></p><p>#### پنجره کانتکست
<p align="center">
  <img width="80%" src="figures/niah.png">
</p></p><p>نتایج ارزیابی در تست «سوزن در انبار کاه» (NIAH). DeepSeek-V3 در تمام طول‌های پنجره کانتکست تا <strong>۱۲۸K</strong> عملکرد خوبی دارد.</p><h3>مدل چت</h3>
#### معیارهای استاندارد (مدل‌های بزرگ‌تر از ۶۷B)
<div align="center"></p><p>| | <strong>معیار (متریک)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | معماری | MoE | MoE | Dense | Dense | - | - | MoE |
| | تعداد پارامتر فعال | ۲۱B | ۲۱B | ۷۲B | ۴۰۵B | - | - | ۳۷B |
| | تعداد کل پارامتر | ۲۳۶B | ۲۳۶B | ۷۲B | ۴۰۵B | - | - | ۶۷۱B |
| انگلیسی | MMLU (EM) | ۷۸.۲ | ۸۰.۶ | ۸۵.۳ | <strong>۸۸.۶</strong> | <strong>۸۸.۳</strong> | ۸۷.۲ | <strong>۸۸.۵</strong> |
| | MMLU-Redux (EM) | ۷۷.۹ | ۸۰.۳ | ۸۵.۶ | ۸۶.۲ | <strong>۸۸.۹</strong> | ۸۸.۰ | <strong>۸۹.۱</strong> |
| | MMLU-Pro (EM) | ۵۸.۵ | ۶۶.۲ | ۷۱.۶ | ۷۳.۳ | <strong>۷۸.۰</strong> | ۷۲.۶ | ۷۵.۹ |
| | DROP (۳ نمونه F1) | ۸۳.۰ | ۸۷.۸ | ۷۶.۷ | ۸۸.۷ | ۸۸.۳ | ۸۳.۷ | <strong>۹۱.۶</strong> |
| | IF-Eval (Prompt Strict) | ۵۷.۷ | ۸۰.۶ | ۸۴.۱ | ۸۶.۰ | <strong>۸۶.۵</strong> | ۸۴.۳ | ۸۶.۱ |
| | GPQA-Diamond (Pass@1) | ۳۵.۳ | ۴۱.۳ | ۴۹.۰ | ۵۱.۱ | <strong>۶۵.۰</strong> | ۴۹.۹ | ۵۹.۱ |
| | SimpleQA (Correct) | ۹.۰ | ۱۰.۲ | ۹.۱ | ۱۷.۱ | ۲۸.۴ | <strong>۳۸.۲</strong> | ۲۴.۹ |
| | FRAMES (دقت) | ۶۶.۹ | ۶۵.۴ | ۶۹.۸ | ۷۰.۰ | ۷۲.۵ | <strong>۸۰.۵</strong> | ۷۳.۳ |
| | LongBench v2 (دقت) | ۳۱.۶ | ۳۵.۴ | ۳۹.۴ | ۳۶.۱ | ۴۱.۰ | ۴۸.۱ | <strong>۴۸.۷</strong> |
| کد | HumanEval-Mul (Pass@1) | ۶۹.۳ | ۷۷.۴ | ۷۷.۳ | ۷۷.۲ | ۸۱.۷ | ۸۰.۵ | <strong>۸۲.۶</strong> |
| | LiveCodeBench (Pass@1-COT) | ۱۸.۸ | ۲۹.۲ | ۳۱.۱ | ۲۸.۴ | ۳۶.۳ | ۳۳.۴ | <strong>۴۰.۵</strong> |
| | LiveCodeBench (Pass@1) | ۲۰.۳ | ۲۸.۴ | ۲۸.۷ | ۳۰.۱ | ۳۲.۸ | ۳۴.۲ | <strong>۳۷.۶</strong> |
| | Codeforces (درصد) | ۱۷.۵ | ۳۵.۶ | ۲۴.۸ | ۲۵.۳ | ۲۰.۳ | ۲۳.۶ | <strong>۵۱.۶</strong> |
| | SWE Verified (Resolved) | - | ۲۲.۶ | ۲۳.۸ | ۲۴.۵ | <strong>۵۰.۸</strong> | ۳۸.۸ | ۴۲.۰ |
| | Aider-Edit (دقت) | ۶۰.۳ | ۷۱.۶ | ۶۵.۴ | ۶۳.۹ | <strong>۸۴.۲</strong> | ۷۲.۹ | ۷۹.۷ |
| | Aider-Polyglot (دقت) | - | ۱۸.۲ | ۷.۶ | ۵.۸ | ۴۵.۳ | ۱۶.۰ | <strong>۴۹.۶</strong> |
| ریاضی | AIME 2024 (Pass@1) | ۴.۶ | ۱۶.۷ | ۲۳.۳ | ۲۳.۳ | ۱۶.۰ | ۹.۳ | <strong>۳۹.۲</strong> |
| | MATH-500 (EM) | ۵۶.۳ | ۷۴.۷ | ۸۰.۰ | ۷۳.۸ | ۷۸.۳ | ۷۴.۶ | <strong>۹۰.۲</strong> |
| | CNMO 2024 (Pass@1) | ۲.۸ | ۱۰.۸ | ۱۵.۹ | ۶.۸ | ۱۳.۱ | ۱۰.۸ | <strong>۴۳.۲</strong> |
| چینی | CLUEWSC (EM) | ۸۹.۹ | ۹۰.۴ | <strong>۹۱.۴</strong> | ۸۴.۷ | ۸۵.۴ | ۸۷.۹ | ۹۰.۹ |
| | C-Eval (EM) | ۷۸.۶ | ۷۹.۵ | ۸۶.۱ | ۶۱.۵ | ۷۶.۷ | ۷۶.۰ | <strong>۸۶.۵</strong> |
| | C-SimpleQA (درست) | ۴۸.۵ | ۵۴.۱ | ۴۸.۴ | ۵۰.۴ | ۵۱.۳ | ۵۹.۳ | <strong>۶۴.۸</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>تمام مدل‌ها در پیکربندی‌ای ارزیابی شده‌اند که طول خروجی را به ۸K محدود می‌کند. معیارهایی با کمتر از ۱۰۰۰ نمونه چندین بار با تنظیمات دمای مختلف آزمایش شده‌اند تا نتایج نهایی مقاوم حاصل شود. DeepSeek-V3 بهترین مدل متن‌باز است و همچنین عملکرد رقابتی با مدل‌های پیشتاز متن‌بسته دارد.</blockquote></p><p>
####  ارزیابی تولید بازمتن</p><p><div align="center"></p><p>| مدل | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | ۷۶.۲ | ۵۰.۵ |
| Qwen2.5-72B-Instruct | ۸۱.۲ | ۴۹.۱ |
| LLaMA-3.1 405B | ۶۹.۳ | ۴۰.۵ |
| GPT-4o-051۳ | ۸۰.۴ | ۵۱.۱ |
| Claude-Sonnet-3.5-1022 | ۸۵.۲ | ۵۲.۰ |
| DeepSeek-V3 | <strong>۸۵.۵</strong> | <strong>۷۰.۰</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>ارزیابی مکالمه بازمتن به زبان انگلیسی. برای AlpacaEval 2.0 از نرخ پیروزی کنترل‌شده بر اساس طول به عنوان معیار استفاده می‌شود.</blockquote></p><h2>5. وبسایت چت و بستر API</h2>
شما می‌توانید با DeepSeek-V3 در وبسایت رسمی DeepSeek چت کنید: <a href="https://chat.deepseek.com/sign_in" target="_blank" rel="noopener noreferrer">chat.deepseek.com</a></p><p>ما همچنین API سازگار با OpenAI را در بستر DeepSeek ارائه می‌کنیم: <a href="https://platform.deepseek.com/" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a></p><h2>6. نحوه اجرای محلی</h2></p><p>DeepSeek-V3 را می‌توان به صورت محلی با استفاده از سخت‌افزارها و نرم‌افزارهای متن‌باز زیر پیاده‌سازی کرد:</p><ul><li><strong>دموی DeepSeek-Infer</strong>: دموی ساده و سبک برای استنتاج FP8 و BF16 فراهم کرده‌ایم.</li>
<li><strong>SGLang</strong>: پشتیبانی کامل از مدل DeepSeek-V3 در هر دو حالت استنتاج BF16 و FP8، با پیش‌بینی چندتوکنی <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">به زودی</a>.</li>
<li><strong>LMDeploy</strong>: استنتاج کارآمد FP8 و BF16 برای پیاده‌سازی محلی و ابری را فراهم می‌کند.</li>
<li><strong>TensorRT-LLM</strong>: در حال حاضر از استنتاج BF16 و کوانتیزاسیون INT4/8 پشتیبانی می‌کند، پشتیبانی از FP8 به زودی ارائه می‌شود.</li>
<li><strong>vLLM</strong>: پشتیبانی از مدل DeepSeek-V3 با حالت‌های FP8 و BF16 برای موازی‌سازی تانسوری و خط لوله‌ای.</li>
<li><strong>LightLLM</strong>: پشتیبانی از پیاده‌سازی کارآمد تک نود یا چند نود برای FP8 و BF16.</li>
<li><strong>AMD GPU</strong>: اجرای مدل DeepSeek-V3 روی GPUهای AMD از طریق SGLang در هر دو حالت BF16 و FP8.</li>
<li><strong>Huawei Ascend NPU</strong>: اجرای DeepSeek-V3 روی دستگاه‌های Huawei Ascend پشتیبانی می‌شود.</li></p><p></ul>از آنجا که آموزش FP8 به طور بومی در چارچوب ما پیاده‌سازی شده است، تنها وزن‌های FP8 ارائه می‌شوند. اگر به وزن‌های BF16 برای آزمایش نیاز دارید، می‌توانید از اسکریپت تبدیل ارائه شده جهت انجام تبدیل استفاده نمایید.</p><p>نمونه تبدیل وزن‌های FP8 به BF16:</p><pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights</code></pre></p><blockquote>[!NOTE]</blockquote>
<blockquote>Transformers هاجینگ‌فیس هنوز به طور مستقیم پشتیبانی نمی‌شود.</blockquote></p><h3>6.1 استنتاج با دموی DeepSeek-Infer (فقط نمونه)</h3></p><p>#### نیازمندی‌های سیستم</p><blockquote>[!NOTE] </blockquote>
<blockquote>فقط لینوکس با پایتون ۳.۱۰. مک و ویندوز پشتیبانی نمی‌شوند.</blockquote></p><p>وابستگی‌ها:
``<code>pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
<pre><code class="language-">#### آماده‌سازی وزن‌های مدل و کد دمو</p><p>ابتدا مخزن گیت‌هاب DeepSeek-V3 را کلون کنید:
</code></pre>shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
<pre><code class="language-">
به پوشه </code>inference<code> رفته و وابستگی‌های موجود در </code>requirements.txt<code> را نصب کنید. بهترین روش استفاده از یک مدیر بسته مانند </code>conda<code> یا </code>uv<code> برای ایجاد محیط مجازی جدید و نصب وابستگی‌هاست.
</code></pre>shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
<pre><code class="language-">
وزن‌های مدل را از Hugging Face دانلود کرده و در پوشه </code>/path/to/DeepSeek-V3<code> قرار دهید.</p><p>#### تبدیل وزن‌های مدل</p><p>وزن‌های مدل Hugging Face را به فرمت مخصوص تبدیل کنید:
</code></pre>shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
<pre><code class="language-">
#### اجرا</p><p>اکنون می‌توانید با DeepSeek-V3 چت کنید:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
<pre><code class="language-">
یا استنتاج دسته‌ای روی یک فایل داده شده:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
<pre><code class="language-">
<h3>6.2 استنتاج با SGLang (توصیه‌شده)</h3></p><p><a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a> هم‌اکنون از <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations" target="_blank" rel="noopener noreferrer">بهینه‌سازی‌های MLA</a>، <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models" target="_blank" rel="noopener noreferrer">DP Attention</a>، FP8 (W8A8)، کش FP8 KV و Torch Compile پشتیبانی می‌کند و بهترین عملکرد تأخیر و توان عملیاتی را در میان چارچوب‌های متن‌باز ارائه می‌دهد.</p><p>قابل توجه است که <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1" target="_blank" rel="noopener noreferrer">SGLang v0.4.1</a> به طور کامل اجرای DeepSeek-V3 را روی <strong>کارت‌های گرافیک NVIDIA و AMD</strong> پشتیبانی می‌کند و آن را به راهکاری همه‌کاره و قوی بدل می‌سازد.</p><p>SGLang همچنین از <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208" target="_blank" rel="noopener noreferrer">موازی‌سازی تانسوری چند نودی</a> پشتیبانی می‌کند و امکان اجرای مدل روی چندین ماشین شبکه‌ای را می‌دهد.</p><p>پیش‌بینی چندتوکنی (MTP) در دست توسعه است و پیشرفت آن را می‌توانید در <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">برنامه بهینه‌سازی</a> پیگیری کنید.</p><p>دستورالعمل‌های راه‌اندازی تیم SGLang در اینجا: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p><h3>6.3 استنتاج با LMDeploy (توصیه‌شده)</h3>
<a href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener noreferrer">LMDeploy</a>، چارچوبی منعطف و با عملکرد بالا برای استنتاج و سرویس‌دهی مدل‌های زبانی بزرگ، هم‌اکنون از DeepSeek-V3 پشتیبانی می‌کند. این چارچوب هر دو قابلیت پردازش آفلاین خط لوله‌ای و استقرار آنلاین را فراهم کرده و با گردش کاری مبتنی بر PyTorch یکپارچه می‌شود.</p><p>برای راهنمایی گام به گام جهت اجرای DeepSeek-V3 با LMDeploy به اینجا مراجعه نمایید: https://github.com/InternLM/lmdeploy/issues/2960</p><h3>6.4 استنتاج با TRT-LLM (توصیه‌شده)</h3></p><p><a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener noreferrer">TensorRT-LLM</a> اکنون از مدل DeepSeek-V3 پشتیبانی می‌کند و گزینه‌هایی مانند BF16 و INT4/INT8 را برای وزن‌ها ارائه می‌دهد. پشتیبانی از FP8 در حال توسعه است و به‌زودی منتشر خواهد شد. برای تجربه ویژگی‌های جدید، به شاخه اختصاصی TRTLLM برای DeepSeek-V3 مراجعه کنید: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. </p><h3>6.5 استنتاج با vLLM (توصیه‌شده)</h3></p><p><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> نسخه ۰.۶.۶ از استنتاج DeepSeek-V3 در حالت‌های FP8 و BF16 روی کارت‌های NVIDIA و AMD پشتیبانی می‌کند. علاوه بر تکنیک‌های استاندارد، vLLM موازی‌سازی خط لوله‌ای را ارائه می‌کند تا بتوانید مدل را روی چندین ماشین شبکه‌ای اجرا کنید. برای راهنمایی دقیق، به <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html" target="_blank" rel="noopener noreferrer">راهنمای vLLM</a> مراجعه نمایید. همچنین می‌توانید <a href="https://github.com/vllm-project/vllm/issues/11539" target="_blank" rel="noopener noreferrer">برنامه بهبود</a> را دنبال کنید.</p><h3>6.6 استنتاج با LightLLM (توصیه‌شده)</h3></p><p><a href="https://github.com/ModelTC/lightllm/tree/main" target="_blank" rel="noopener noreferrer">LightLLM</a> نسخه ۱.۰.۱ از پیاده‌سازی موازی‌سازی تانسوری تک‌ماشینه و چندماشینه برای DeepSeek-R1 (FP8/BF16) پشتیبانی می‌کند و پیاده‌سازی با دقت ترکیبی و حالت‌های کوانتیزاسیون بیشتر در حال ادغام است. برای جزئیات بیشتر به <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html" target="_blank" rel="noopener noreferrer">راهنمای LightLLM</a> مراجعه کنید. همچنین، LightLLM پیاده‌سازی PD-disaggregation را برای DeepSeek-V2 ارائه داده و پیاده‌سازی آن برای DeepSeek-V3 در دست توسعه است.</p><h3>6.7 استنتاج توصیه‌شده با کارت‌های گرافیک AMD</h3></p><p>در همکاری با تیم AMD، پشتیبانی روز اول برای کارت‌های گرافیک AMD با استفاده از SGLang و سازگاری کامل برای هر دو دقت FP8 و BF16 فراهم شده است. برای راهنمایی دقیق، به <a href="#63-inference-with-lmdeploy-recommended" target="_blank" rel="noopener noreferrer">راهنمای SGLang</a> مراجعه نمایید.</p><h3>6.8 استنتاج توصیه‌شده با NPUهای Huawei Ascend</h3>
چارچوب <a href="https://www.hiascend.com/en/software/mindie" target="_blank" rel="noopener noreferrer">MindIE</a> از جامعه Huawei Ascend نسخه BF16 مدل DeepSeek-V3 را با موفقیت تطبیق داده است. برای راهنمایی گام به گام روی Ascend NPU به <a href="https://modelers.cn/models/MindIE/deepseekv3" target="_blank" rel="noopener noreferrer">اینجا</a> مراجعه نمایید.</p><h2>7. مجوز</h2>
این مخزن کد تحت <a href="LICENSE-CODE" target="_blank" rel="noopener noreferrer">مجوز MIT</a> منتشر شده است. استفاده از مدل‌های پایه/چت DeepSeek-V3 مشمول <a href="LICENSE-MODEL" target="_blank" rel="noopener noreferrer">مجوز مدل</a> می‌باشد. مجموعه DeepSeek-V3 (شامل Base و Chat) از استفاده تجاری پشتیبانی می‌کند.</p><h2>8. ارجاع</code></pre></h2>
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code>``</p><h2>9. تماس</h2>
در صورت داشتن هرگونه سؤال، لطفاً issue ثبت کنید یا با ما در <a href="service@deepseek.com" target="_blank" rel="noopener noreferrer">service@deepseek.com</a> تماس بگیرید.</p><p>
---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>