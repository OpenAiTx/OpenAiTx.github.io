<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - Read DeepSeek-V3 documentation in Dutch. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read DeepSeek-V3 documentation in Dutch. This project has 0 stars on GitHub.">
    <meta name="keywords" content="DeepSeek-V3, Dutch, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "DeepSeek-V3",
  "description": "Read DeepSeek-V3 documentation in Dutch. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "deepseek-ai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/deepseek-ai/DeepSeek-V3/README-nl.html",
  "sameAs": "https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    DeepSeek-V3
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Dutch</span>
                <span>by deepseek-ai</span>
            </div>
        </div>
        
        <div class="content">
            <p><!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header --></p><p><div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div></p><h2>Inhoudsopgave</h2></p><ul><li><a href="#1-introductie" target="_blank" rel="noopener noreferrer">Introductie</a></li>
<li><a href="#2-modeloverzicht" target="_blank" rel="noopener noreferrer">Modeloverzicht</a></li>
<li><a href="#3-modeldownloads" target="_blank" rel="noopener noreferrer">Modeldownloads</a></li>
<li><a href="#4-evaluatieresultaten" target="_blank" rel="noopener noreferrer">Evaluatieresultaten</a></li>
<li><a href="#5-chatwebsite--api-platform" target="_blank" rel="noopener noreferrer">Chatwebsite & API Platform</a></li>
<li><a href="#6-lokaal-uitvoeren" target="_blank" rel="noopener noreferrer">Lokaal uitvoeren</a></li>
<li><a href="#7-licentie" target="_blank" rel="noopener noreferrer">Licentie</a></li>
<li><a href="#8-citeren" target="_blank" rel="noopener noreferrer">Citeren</a></li>
<li><a href="#9-contact" target="_blank" rel="noopener noreferrer">Contact</a></li></p><p>
</ul><h2>1. Introductie</h2></p><p>We presenteren DeepSeek-V3, een krachtig Mixture-of-Experts (MoE) taalmodel met 671B totale parameters waarvan 37B geactiveerd per token.
Om efficiënte inferentie en kosteneffectieve training te bereiken, hanteert DeepSeek-V3 Multi-head Latent Attention (MLA) en DeepSeekMoE-architecturen, die grondig zijn gevalideerd in DeepSeek-V2.
Bovendien introduceert DeepSeek-V3 als pionier een auxiliary-loss-vrije strategie voor load balancing en stelt het een multi-token voorspellingstrainingsdoelstelling vast voor betere prestaties.
We pre-trainen DeepSeek-V3 op 14,8 biljoen diverse en hoogwaardige tokens, gevolgd door Supervised Fine-Tuning en Reinforcement Learning stadia om het volledige potentieel te benutten.
Uitgebreide evaluaties tonen aan dat DeepSeek-V3 beter presteert dan andere open-source modellen en vergelijkbare prestaties behaalt als toonaangevende gesloten modellen.
Ondanks de uitstekende prestaties vereist DeepSeek-V3 slechts 2.788M H800 GPU-uren voor de volledige training.
Daarnaast is het trainingsproces opmerkelijk stabiel.
Gedurende het gehele trainingsproces hebben we geen onherstelbare verliespieken ervaren of rollbacks hoeven uitvoeren.
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p></p><h2>2. Modeloverzicht</h2></p><hr></p><p><strong>Architectuur: Innovatieve Load Balancing Strategie en Trainingsdoelstelling</strong></p><ul><li>Bovenop de efficiënte architectuur van DeepSeek-V2 introduceren we een auxiliary-loss-vrije strategie voor load balancing, die de prestatievermindering minimaliseert die ontstaat door het stimuleren van load balancing.</li>
<li>We onderzoeken een Multi-Token Prediction (MTP) doelstelling en bewijzen het voordeel voor de modelprestaties.</li>
    </ul>Het kan ook gebruikt worden voor speculative decoding om inferentie te versnellen.</p><hr></p><p><strong>Pre-Training: Naar Ultieme Trainingsefficiëntie</strong></p><ul><li>We ontwerpen een FP8 mixed precision trainingsframework en valideren voor het eerst de haalbaarheid en effectiviteit van FP8-training op een extreem grootschalig model.</li>
<li>Door co-design van algoritmes, frameworks en hardware overwinnen we de communicatiestuwing in cross-node MoE-training, en bereiken we vrijwel volledige overlap tussen berekening en communicatie.</li>
  </ul>Dit verhoogt onze trainingsefficiëntie aanzienlijk en vermindert de trainingskosten, waardoor we het model verder kunnen opschalen zonder extra overhead.
<ul><li>Tegen een economische kost van slechts 2.664M H800 GPU-uren voltooien we de pre-training van DeepSeek-V3 op 14,8T tokens, waarmee we het momenteel krachtigste open-source basismodel produceren. De daaropvolgende trainingsstadia na pre-training vereisen slechts 0,1M GPU-uren.</li></p><p></ul>---</p><p><strong>Post-Training: Kennisdistillatie van DeepSeek-R1</strong></p><ul><li>  We introduceren een innovatieve methodologie om redeneercapaciteiten te distilleren van het long-Chain-of-Thought (CoT) model, specifiek van een van de DeepSeek R1-serie modellen, naar standaard LLM's, in het bijzonder DeepSeek-V3. Onze pipeline integreert op elegante wijze de verificatie- en reflectiepatronen van R1 in DeepSeek-V3 en verbetert het redeneervermogen aanzienlijk. Ondertussen behouden we controle over de outputstijl en lengte van DeepSeek-V3.</li></p><p></ul>---</p><h2>3. Modeldownloads</h2></p><p><div align="center"></p><p>| <strong>Model</strong> | <strong>#Totale Params</strong> | <strong>#Geactiveerde Params</strong> | <strong>Contextlengte</strong> | <strong>Download</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |</p><p></div></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>De totale omvang van de DeepSeek-V3 modellen op Hugging Face is 685B, wat 671B aan hoofdmodelgewichten en 14B aan Multi-Token Prediction (MTP) Module gewichten omvat.</blockquote></p><p>Om optimale prestaties en flexibiliteit te garanderen, werken we samen met open-source gemeenschappen en hardwareleveranciers om meerdere manieren te bieden om het model lokaal uit te voeren. Raadpleeg sectie 6: <a href="#6-lokaal-uitvoeren" target="_blank" rel="noopener noreferrer">Lokaal uitvoeren</a> voor stapsgewijze begeleiding.</p><p>Voor ontwikkelaars die dieper willen duiken, raden we aan <a href="./README_WEIGHTS.md" target="_blank" rel="noopener noreferrer">README_WEIGHTS.md</a> te bekijken voor details over de hoofdmodelgewichten en de Multi-Token Prediction (MTP) Modules. Let op: MTP-ondersteuning wordt momenteel actief ontwikkeld binnen de community, en we verwelkomen je bijdragen en feedback.</p><h2>4. Evaluatieresultaten</h2>
<h3>Basismodel</h3>
#### Standaard Benchmarks</p><p><div align="center"></p><p>
|  | Benchmark (Metriek) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architectuur | - | MoE | Dense | Dense | MoE |
| | # Geactiveerde Params | - | 21B | 72B | 405B | 37B |
| | # Totale Params | - | 236B | 72B | 405B | 671B |
| Engels | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Acc.) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Wiskunde | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Chinees | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Acc.) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Acc.) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Meertalig | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p><p></div></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Beste resultaten zijn vetgedrukt. Scores met een verschil van niet meer dan 0,3 worden als gelijkwaardig beschouwd. DeepSeek-V3 behaalt de beste prestaties op de meeste benchmarks, vooral bij wiskunde- en codeertaken.</blockquote>
<blockquote>Voor meer evaluatiedetails, zie ons paper.</blockquote></p><p>#### Contextvenster
<p align="center">
  <img width="80%" src="figures/niah.png">
</p></p><p>Evaluatieresultaten op de `<code>Needle In A Haystack</code><code> (NIAH) tests. DeepSeek-V3 presteert goed over alle contextvensterlengtes tot <strong>128K</strong>.</p><h3>Chatmodel</h3>
#### Standaard Benchmarks (Modellen groter dan 67B)
<div align="center"></p><p>| | <strong>Benchmark (Metriek)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architectuur | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Geactiveerde Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Totale Params | 236B | 236B | 72B | 405B | - | - | 671B |
| Engels | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Percentiel) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Opgelost) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Wiskunde | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Chinees | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p><p></div></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Alle modellen worden geëvalueerd in een configuratie die de outputlengte beperkt tot 8K. Benchmarks met minder dan 1000 samples worden meerdere keren getest met verschillende temperatuurinstellingen om robuuste eindresultaten te verkrijgen. DeepSeek-V3 is het best presterende open-source model en vertoont ook concurrerende prestaties ten opzichte van vooraanstaande gesloten modellen.</blockquote></p><p>
####  Open Ended Generation Evaluatie</p><p><div align="center"></p><p>| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p><p></div></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Engelse open-ended gespreksbeoordelingen. Voor AlpacaEval 2.0 gebruiken we het lengte-gecontroleerde winpercentage als metriek.</blockquote></p><h2>5. Chatwebsite & API Platform</h2>
Je kunt chatten met DeepSeek-V3 op de officiële website van DeepSeek: <a href="https://chat.deepseek.com/sign_in" target="_blank" rel="noopener noreferrer">chat.deepseek.com</a></p><p>We bieden ook een OpenAI-compatibele API op het DeepSeek Platform: <a href="https://platform.deepseek.com/" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a></p><h2>6. Lokaal uitvoeren</h2></p><p>DeepSeek-V3 kan lokaal worden ingezet met behulp van de volgende hardware en open-source community software:</p><ul><li><strong>DeepSeek-Infer Demo</strong>: We bieden een eenvoudige en lichte demo voor FP8 en BF16 inferentie.</li>
<li><strong>SGLang</strong>: Volledige ondersteuning voor het DeepSeek-V3 model in zowel BF16- als FP8-inferentiemodi, met Multi-Token Prediction <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">binnenkort beschikbaar</a>.</li>
<li><strong>LMDeploy</strong>: Maakt efficiënte FP8 en BF16 inferentie mogelijk voor lokale en cloudimplementatie.</li>
<li><strong>TensorRT-LLM</strong>: Ondersteunt momenteel BF16-inferentie en INT4/8 kwantisatie, met FP8-ondersteuning binnenkort beschikbaar.</li>
<li><strong>vLLM</strong>: Ondersteunt DeepSeek-V3 model met FP8- en BF16-modi voor tensor parallelisme en pipeline parallelisme.</li>
<li><strong>LightLLM</strong>: Ondersteunt efficiënte single-node of multi-node implementatie voor FP8 en BF16.</li>
<li><strong>AMD GPU</strong>: Maakt het mogelijk om het DeepSeek-V3 model op AMD GPU's uit te voeren via SGLang in zowel BF16- als FP8-modi.</li>
<li><strong>Huawei Ascend NPU</strong>: Ondersteunt het draaien van DeepSeek-V3 op Huawei Ascend apparaten.</li></p><p></ul>Aangezien FP8-training standaard is geïmplementeerd in ons framework, leveren we alleen FP8-gewichten. Als je BF16-gewichten nodig hebt voor experimenten, kun je het bijgeleverde conversiescript gebruiken om de transformatie uit te voeren.</p><p>Hier is een voorbeeld van het converteren van FP8-gewichten naar BF16:</p><pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights</code></pre></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Hugging Face's Transformers wordt nog niet direct ondersteund.</blockquote></p><h3>6.1 Inferentie met DeepSeek-Infer Demo (alleen voorbeeld)</h3></p><p>#### Systeemeisen</p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Alleen Linux met Python 3.10. Mac en Windows worden niet ondersteund.</blockquote></p><p>Vereiste afhankelijkheden:
</code>`<code>pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
<pre><code class="language-">#### Modelgewichten & Demo Code Voorbereiding</p><p>Kloon eerst onze DeepSeek-V3 GitHub repository:
</code></pre>shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
<pre><code class="language-">
Navigeer naar de </code>inference<code> map en installeer de afhankelijkheden uit </code>requirements.txt<code>. De makkelijkste manier is om een pakketbeheerder zoals </code>conda<code> of </code>uv<code> te gebruiken om een nieuwe virtuele omgeving te maken en de afhankelijkheden te installeren.
</code></pre>shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
<pre><code class="language-">
Download de modelgewichten van Hugging Face en plaats deze in de </code>/path/to/DeepSeek-V3<code> map.</p><p>#### Modelgewichten Converteren</p><p>Converteer Hugging Face modelgewichten naar een specifiek formaat:
</code></pre>shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
<pre><code class="language-">
#### Uitvoeren</p><p>Daarna kun je chatten met DeepSeek-V3:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
<pre><code class="language-">
Of batch-inferentie op een opgegeven bestand:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
<pre><code class="language-">
<h3>6.2 Inferentie met SGLang (aanbevolen)</h3></p><p><a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a> ondersteunt momenteel <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations" target="_blank" rel="noopener noreferrer">MLA optimalisaties</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models" target="_blank" rel="noopener noreferrer">DP Attention</a>, FP8 (W8A8), FP8 KV Cache en Torch Compile, en levert state-of-the-art latency en throughput prestaties onder open-source frameworks.</p><p>Opmerkelijk is dat <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1" target="_blank" rel="noopener noreferrer">SGLang v0.4.1</a> volledige ondersteuning biedt voor DeepSeek-V3 op zowel <strong>NVIDIA als AMD GPU's</strong>, waardoor het een veelzijdige en robuuste oplossing is.</p><p>SGLang ondersteunt ook <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208" target="_blank" rel="noopener noreferrer">multi-node tensorparallelisme</a>, waarmee je dit model op meerdere met elkaar verbonden machines kunt uitvoeren.</p><p>Multi-Token Prediction (MTP) is in ontwikkeling en de voortgang is te volgen in het <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">optimalisatieplan</a>.</p><p>Hier zijn de startinstructies van het SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p><h3>6.3 Inferentie met LMDeploy (aanbevolen)</h3>
<a href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener noreferrer">LMDeploy</a>, een flexibel en high-performance inferentie- en serveerframework voor grote taalmodellen, ondersteunt nu DeepSeek-V3. Het biedt zowel offline pipeline verwerking als online implementatie, en integreert naadloos met PyTorch-gebaseerde workflows.</p><p>Voor uitgebreide stapsgewijze instructies voor het uitvoeren van DeepSeek-V3 met LMDeploy, zie hier: https://github.com/InternLM/lmdeploy/issues/2960</p><h3>6.4 Inferentie met TRT-LLM (aanbevolen)</h3></p><p><a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener noreferrer">TensorRT-LLM</a> ondersteunt nu het DeepSeek-V3 model, met precisieopties zoals BF16 en INT4/INT8 weight-only. Ondersteuning voor FP8 is momenteel in ontwikkeling en zal binnenkort worden uitgebracht. Je kunt de aangepaste branch van TRTLLM, specifiek voor DeepSeek-V3, direct hier vinden: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3.</p><h3>6.5 Inferentie met vLLM (aanbevolen)</h3></p><p><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> v0.6.6 ondersteunt DeepSeek-V3 inferentie voor FP8- en BF16-modi op zowel NVIDIA als AMD GPU's. Naast standaardtechnieken biedt vLLM _pipeline parallelism_, waarmee je dit model op meerdere via netwerken verbonden machines kunt uitvoeren. Zie de <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html" target="_blank" rel="noopener noreferrer">vLLM instructies</a> voor gedetailleerde begeleiding. Volg ook gerust <a href="https://github.com/vllm-project/vllm/issues/11539" target="_blank" rel="noopener noreferrer">het verbeterplan</a>.</p><h3>6.6 Inferentie met LightLLM (aanbevolen)</h3></p><p><a href="https://github.com/ModelTC/lightllm/tree/main" target="_blank" rel="noopener noreferrer">LightLLM</a> v1.0.1 ondersteunt single-machine en multi-machine tensor parallel implementatie voor DeepSeek-R1 (FP8/BF16) en biedt mixed-precision implementatie, met steeds meer kwantisatiemodi in integratie. Zie voor meer details de <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html" target="_blank" rel="noopener noreferrer">LightLLM instructies</a>. Daarnaast biedt LightLLM PD-disaggregatie voor DeepSeek-V2, en de implementatie van PD-disaggregatie voor DeepSeek-V3 is in ontwikkeling.</p><h3>6.7 Aanbevolen inferentiefuncties met AMD GPU's</h3></p><p>In samenwerking met het AMD-team hebben we Day-One ondersteuning voor AMD GPU's bereikt met SGLang, met volledige compatibiliteit voor zowel FP8 als BF16 precisie. Raadpleeg de <a href="#63-inferentie-met-lmdeploy-aanbevolen" target="_blank" rel="noopener noreferrer">SGLang instructies</a> voor meer details.</p><h3>6.8 Aanbevolen inferentiefuncties met Huawei Ascend NPU's</h3>
Het <a href="https://www.hiascend.com/en/software/mindie" target="_blank" rel="noopener noreferrer">MindIE</a> framework van de Huawei Ascend-community heeft met succes de BF16-versie van DeepSeek-V3 aangepast. Voor stapsgewijze begeleiding op Ascend NPU's, volg de <a href="https://modelers.cn/models/MindIE/deepseekv3" target="_blank" rel="noopener noreferrer">instructies hier</a>.</p><h2>7. Licentie</h2>
Deze coderepository is gelicentieerd onder <a href="LICENSE-CODE" target="_blank" rel="noopener noreferrer">de MIT-licentie</a>. Het gebruik van DeepSeek-V3 Base/Chat modellen is onderworpen aan <a href="LICENSE-MODEL" target="_blank" rel="noopener noreferrer">de Model Licentie</a>. DeepSeek-V3-serie (inclusief Base en Chat) ondersteunt commercieel gebruik.</p><h2>8. Citeren</code></pre></h2>
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code>``</p><h2>9. Contact</h2>
Als je vragen hebt, maak dan een issue aan of neem contact met ons op via <a href="service@deepseek.com" target="_blank" rel="noopener noreferrer">service@deepseek.com</a>.</p><p>
---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>