<!DOCTYPE html>
<html lang="id">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-V3 - Read DeepSeek-V3 documentation in Indonesian. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read DeepSeek-V3 documentation in Indonesian. This project has 0 stars on GitHub.">
    <meta name="keywords" content="DeepSeek-V3, Indonesian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "DeepSeek-V3",
  "description": "Read DeepSeek-V3 documentation in Indonesian. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "deepseek-ai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/deepseek-ai/DeepSeek-V3/README-id.html",
  "sameAs": "https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    DeepSeek-V3
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Indonesian</span>
                <span>by deepseek-ai</span>
            </div>
        </div>
        
        <div class="content">
            <p><!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header --></p><p><div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"
    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"
    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>👁️</a>
</div></p><h2>Daftar Isi</h2></p><ul><li><a href="#1-pendahuluan" target="_blank" rel="noopener noreferrer">Pendahuluan</a></li>
<li><a href="#2-ringkasan-model" target="_blank" rel="noopener noreferrer">Ringkasan Model</a></li>
<li><a href="#3-unduh-model" target="_blank" rel="noopener noreferrer">Unduh Model</a></li>
<li><a href="#4-hasil-evaluasi" target="_blank" rel="noopener noreferrer">Hasil Evaluasi</a></li>
<li><a href="#5-situs-chat--platform-api" target="_blank" rel="noopener noreferrer">Situs Chat & Platform API</a></li>
<li><a href="#6-cara-menjalankan-secara-lokal" target="_blank" rel="noopener noreferrer">Cara Menjalankan Secara Lokal</a></li>
<li><a href="#7-lisensi" target="_blank" rel="noopener noreferrer">Lisensi</a></li>
<li><a href="#8-sitasi" target="_blank" rel="noopener noreferrer">Sitasi</a></li>
<li><a href="#9-kontak" target="_blank" rel="noopener noreferrer">Kontak</a></li></p><p>
</ul><h2>1. Pendahuluan</h2></p><p>Kami mempersembahkan DeepSeek-V3, model bahasa Mixture-of-Experts (MoE) yang kuat dengan total 671M parameter dan 37M parameter yang diaktifkan untuk setiap token. 
Untuk mencapai inferensi yang efisien dan pelatihan yang hemat biaya, DeepSeek-V3 mengadopsi arsitektur Multi-head Latent Attention (MLA) dan DeepSeekMoE, yang telah divalidasi secara menyeluruh pada DeepSeek-V2. 
Selain itu, DeepSeek-V3 menjadi pelopor strategi load balancing tanpa auxiliary-loss dan menetapkan tujuan pelatihan prediksi multi-token untuk performa yang lebih kuat. 
Kami melakukan pre-training DeepSeek-V3 pada 14,8 triliun token yang beragam dan berkualitas tinggi, diikuti tahap Fine-Tuning Terawasi dan Pembelajaran Penguatan untuk memaksimalkan kemampuannya. 
Evaluasi komprehensif mengungkapkan bahwa DeepSeek-V3 melampaui model open-source lain dan mencapai performa sebanding dengan model closed-source terdepan.
Terlepas dari performanya yang luar biasa, DeepSeek-V3 hanya membutuhkan 2,788M jam GPU H800 untuk pelatihan penuh.
Selain itu, proses pelatihannya sangat stabil. 
Sepanjang proses pelatihan, kami tidak mengalami lonjakan loss yang tidak dapat dipulihkan atau melakukan rollback apa pun.
<p align="center">
  <img width="80%" src="figures/benchmark.png">
</p></p><h2>2. Ringkasan Model</h2></p><hr></p><p><strong>Arsitektur: Strategi Load Balancing dan Tujuan Pelatihan Inovatif</strong></p><ul><li>Berdasarkan arsitektur efisien DeepSeek-V2, kami mempelopori strategi load balancing tanpa auxiliary-loss, yang meminimalkan degradasi performa akibat dorongan load balancing.</li>
<li> Kami meneliti tujuan Multi-Token Prediction (MTP) dan membuktikan manfaatnya bagi performa model. </li>
    </ul>MTP juga dapat digunakan untuk speculative decoding guna mempercepat inferensi.</p><hr></p><p><strong>Pre-Training: Menuju Efisiensi Pelatihan Tertinggi</strong></p><ul><li>Kami merancang kerangka pelatihan presisi campuran FP8 dan, untuk pertama kalinya, memvalidasi kelayakan serta efektivitas pelatihan FP8 pada model berskala sangat besar.  </li>
<li>Melalui co-design algoritma, kerangka, dan perangkat keras, kami mengatasi bottleneck komunikasi pada pelatihan MoE antar node, hampir mencapai overlap penuh antara komputasi dan komunikasi.  </li>
  </ul>Hal ini secara signifikan meningkatkan efisiensi dan menurunkan biaya pelatihan, memungkinkan kami meningkatkan ukuran model tanpa overhead tambahan.
<ul><li>Dengan biaya hanya 2,664M jam GPU H800, kami menyelesaikan pre-training DeepSeek-V3 pada 14,8T token, menghasilkan model basis open-source terkuat saat ini. Tahap pelatihan lanjutan setelah pre-training hanya membutuhkan 0,1M jam GPU.</li></p><p></ul>---</p><p><strong>Post-Training: Distilasi Pengetahuan dari DeepSeek-R1</strong></p><ul><li>  Kami memperkenalkan metodologi inovatif untuk mendistilasi kemampuan penalaran dari model long-Chain-of-Thought (CoT), khususnya dari salah satu model DeepSeek R1 series, ke LLM standar, terutama DeepSeek-V3. Proses ini secara elegan mengintegrasikan pola verifikasi dan refleksi dari R1 ke DeepSeek-V3 dan secara signifikan meningkatkan performa penalaran. Sementara itu, kami juga tetap mengendalikan gaya dan panjang output DeepSeek-V3.</li></p><p></ul>---</p><h2>3. Unduh Model</h2></p><p><div align="center"></p><p>| <strong>Model</strong> | <strong>#Total Params</strong> | <strong>#Activated Params</strong> | <strong>Context Length</strong> | <strong>Unduh</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |
| DeepSeek-V3   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">🤗 Hugging Face</a>   |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>Total ukuran model DeepSeek-V3 di Hugging Face adalah 685B, termasuk 671B bobot Main Model dan 14B bobot Modul Multi-Token Prediction (MTP).</blockquote></p><p>Untuk memastikan performa optimal dan fleksibilitas, kami telah bermitra dengan komunitas open-source dan vendor perangkat keras untuk menyediakan berbagai cara menjalankan model ini secara lokal. Untuk panduan langkah demi langkah, silakan lihat Bagian 6: <a href="#6-cara-menjalankan-secara-lokal" target="_blank" rel="noopener noreferrer">Cara Menjalankan Secara Lokal</a>.</p><p>Untuk pengembang yang ingin mendalami, kami merekomendasikan untuk mengeksplorasi <a href="./README_WEIGHTS.md" target="_blank" rel="noopener noreferrer">README_WEIGHTS.md</a> untuk detail tentang bobot Main Model dan Modul Multi-Token Prediction (MTP). Perlu dicatat bahwa dukungan MTP saat ini masih dalam pengembangan aktif di komunitas, dan kami menyambut kontribusi serta masukan Anda.</p><h2>4. Hasil Evaluasi</h2>
<h3>Model Basis</h3>
#### Benchmark Standar</p><p><div align="center"></p><p>|  | Benchmark (Metrik) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Arsitektur | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| Inggris | Pile-test (BPB) | - | 0.606 | 0.638 | <strong>0.542</strong> | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | <strong>87.5</strong> |
| | MMLU (Akurasi) | 5-shot | 78.4 | 85.0 | 84.4 | <strong>87.1</strong> |
| | MMLU-Redux (Akurasi) | 5-shot | 75.6 | 83.2 | 81.3 | <strong>86.2</strong> |
| | MMLU-Pro (Akurasi) | 5-shot | 51.4 | 58.3 | 52.8 | <strong>64.4</strong> |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | <strong>89.0</strong> |
| | ARC-Easy (Akurasi) | 25-shot | 97.6 | 98.4 | 98.4 | <strong>98.9</strong> |
| | ARC-Challenge (Akurasi) | 25-shot | 92.2 | 94.5 | <strong>95.3</strong> | <strong>95.3</strong> |
| | HellaSwag (Akurasi) | 10-shot | 87.1 | 84.8 | <strong>89.2</strong> | 88.9 |
| | PIQA (Akurasi) | 0-shot | 83.9 | 82.6 | <strong>85.9</strong> | 84.7 |
| | WinoGrande (Akurasi) | 5-shot | <strong>86.3</strong> | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Akurasi) | 5-shot | 73.1 | 68.1 | <strong>74.2</strong> | 67.1 |
| | RACE-High (Akurasi) | 5-shot | 52.6 | 50.3 | <strong>56.8</strong> | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | <strong>82.9</strong> |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | <strong>41.5</strong> | 40.0 |
| | AGIEval (Akurasi) | 0-shot | 57.5 | 75.8 | 60.6 | <strong>79.6</strong> |
| Kode | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | <strong>65.2</strong> |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | <strong>75.4</strong> |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | <strong>19.4</strong> |
| | CRUXEval-I (Akurasi) | 2-shot | 52.5 | 59.1 | 58.5 | <strong>67.3</strong> |
| | CRUXEval-O (Akurasi) | 2-shot | 49.8 | 59.9 | 59.9 | <strong>69.8</strong> |
| Matematika | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | <strong>89.3</strong> |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | <strong>61.6</strong> |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | <strong>79.8</strong> |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | <strong>90.7</strong> |
| Tionghoa | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | <strong>83.0</strong> | 82.7 |
| | C-Eval (Akurasi) | 5-shot | 81.4 | 89.2 | 72.5 | <strong>90.1</strong> |
| | CMMLU (Akurasi) | 5-shot | 84.0 | <strong>89.5</strong> | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | <strong>77.4</strong> | 75.8 | 76.0 | 76.3 |
| | C3 (Akurasi) | 0-shot | 77.4 | 76.7 | <strong>79.7</strong> | 78.6 |
| | CCPM (Akurasi) | 0-shot | <strong>93.0</strong> | 88.5 | 78.6 | 92.0 |
| Multibahasa | MMMLU-non-English (Akurasi) | 5-shot | 64.0 | 74.8 | 73.8 | <strong>79.4</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>Hasil terbaik dicetak tebal. Skor dengan selisih tidak lebih dari 0,3 dianggap setara. DeepSeek-V3 meraih performa terbaik pada sebagian besar benchmark, khususnya pada tugas matematika dan kode.</blockquote>
<blockquote>Untuk detail evaluasi lebih lanjut, silakan cek makalah kami. </blockquote></p><p>#### Context Window
<p align="center">
  <img width="80%" src="figures/niah.png">
</p></p><p>Hasil evaluasi pada tes `<code>Needle In A Haystack</code><code> (NIAH). DeepSeek-V3 tampil baik di semua panjang context window hingga <strong>128K</strong>.</p><h3>Model Chat</h3>
#### Benchmark Standar (Model lebih besar dari 67B)
<div align="center"></p><p>| | <strong>Benchmark (Metrik)</strong> | <strong>DeepSeek V2-0506</strong> | <strong>DeepSeek V2.5-0905</strong> | <strong>Qwen2.5 72B-Inst.</strong> | <strong>Llama3.1 405B-Inst.</strong> | <strong>Claude-3.5-Sonnet-1022</strong> | <strong>GPT-4o 0513</strong> | <strong>DeepSeek V3</strong> |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Arsitektur | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| Inggris | MMLU (EM) | 78.2 | 80.6 | 85.3 | <strong>88.6</strong> | <strong>88.3</strong> | 87.2 | <strong>88.5</strong> |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | <strong>88.9</strong> | 88.0 | <strong>89.1</strong> |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | <strong>78.0</strong> | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | <strong>91.6</strong> |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | <strong>86.5</strong> | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | <strong>65.0</strong> | 49.9 | 59.1 |
| | SimpleQA (Benar) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | <strong>38.2</strong> | 24.9 |
| | FRAMES (Akurasi) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | <strong>80.5</strong> | 73.3 |
| | LongBench v2 (Akurasi) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | <strong>48.7</strong> |
| Kode | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | <strong>82.6</strong> |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | <strong>40.5</strong> |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | <strong>37.6</strong> |
| | Codeforces (Persentil) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | <strong>51.6</strong> |
| | SWE Verified (Terselesaikan) | - | 22.6 | 23.8 | 24.5 | <strong>50.8</strong> | 38.8 | 42.0 |
| | Aider-Edit (Akurasi) | 60.3 | 71.6 | 65.4 | 63.9 | <strong>84.2</strong> | 72.9 | 79.7 |
| | Aider-Polyglot (Akurasi) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | <strong>49.6</strong> |
| Matematika | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | <strong>39.2</strong> |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | <strong>90.2</strong> |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | <strong>43.2</strong> |
| Tionghoa | CLUEWSC (EM) | 89.9 | 90.4 | <strong>91.4</strong> | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | <strong>86.5</strong> |
| | C-SimpleQA (Benar) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | <strong>64.8</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>Semua model dievaluasi dengan konfigurasi yang membatasi panjang output hingga 8K. Benchmark dengan kurang dari 1000 sampel diuji beberapa kali menggunakan pengaturan suhu berbeda untuk memperoleh hasil yang kuat. DeepSeek-V3 adalah model open-source dengan performa terbaik, dan juga bersaing dengan model closed-source terdepan.</blockquote></p><p>####  Evaluasi Open Ended Generation</p><p><div align="center"></p><p>| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | <strong>85.5</strong> | <strong>70.0</strong> |</p><p></div></p><blockquote>[!NOTE]</blockquote>
<blockquote>Evaluasi percakapan open-ended bahasa Inggris. Untuk AlpacaEval 2.0, digunakan metrik win rate dengan kontrol panjang output.</blockquote></p><h2>5. Situs Chat & Platform API</h2>
Anda dapat berinteraksi dengan DeepSeek-V3 di situs resmi DeepSeek: <a href="https://chat.deepseek.com/sign_in" target="_blank" rel="noopener noreferrer">chat.deepseek.com</a></p><p>Kami juga menyediakan API kompatibel OpenAI di Platform DeepSeek: <a href="https://platform.deepseek.com/" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a></p><h2>6. Cara Menjalankan Secara Lokal</h2></p><p>DeepSeek-V3 dapat dideploy secara lokal menggunakan perangkat keras dan perangkat lunak komunitas open-source berikut:</p><ul><li><strong>DeepSeek-Infer Demo</strong>: Kami menyediakan demo sederhana dan ringan untuk inferensi FP8 dan BF16.</li>
<li><strong>SGLang</strong>: Mendukung penuh model DeepSeek-V3 dalam mode inferensi BF16 dan FP8, dengan Multi-Token Prediction <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">segera hadir</a>.</li>
<li><strong>LMDeploy</strong>: Memungkinkan inferensi FP8 dan BF16 yang efisien untuk deployment lokal maupun cloud.</li>
<li><strong>TensorRT-LLM</strong>: Saat ini mendukung inferensi BF16 dan kuantisasi INT4/8, dengan dukungan FP8 segera hadir.</li>
<li><strong>vLLM</strong>: Mendukung model DeepSeek-V3 dengan mode FP8 dan BF16 untuk tensor parallelism dan pipeline parallelism.</li>
<li><strong>LightLLM</strong>: Mendukung deployment efisien single-node atau multi-node untuk FP8 dan BF16.</li>
<li><strong>AMD GPU</strong>: Memungkinkan menjalankan model DeepSeek-V3 di GPU AMD melalui SGLang baik dalam mode BF16 maupun FP8.</li>
<li><strong>Huawei Ascend NPU</strong>: Mendukung menjalankan DeepSeek-V3 di perangkat Huawei Ascend.</li></p><p></ul>Karena pelatihan FP8 diadopsi secara native dalam kerangka kami, kami hanya menyediakan bobot FP8. Jika Anda membutuhkan bobot BF16 untuk eksperimen, Anda dapat menggunakan skrip konversi yang disediakan untuk melakukan transformasi.</p><p>Berikut contoh konversi bobot FP8 ke BF16:</p><pre><code class="language-shell">cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights</code></pre></p><blockquote>[!NOTE]</blockquote>
<blockquote>Transformers dari Hugging Face belum didukung secara langsung.</blockquote></p><h3>6.1 Inferensi dengan DeepSeek-Infer Demo (contoh saja)</h3></p><p>#### Persyaratan Sistem</p><blockquote>[!NOTE] </blockquote>
<blockquote>Hanya Linux dengan Python 3.10. Mac dan Windows tidak didukung.</blockquote></p><p>Dependensi:
</code>`<code>pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
<pre><code class="language-">#### Persiapan Bobot Model & Kode Demo</p><p>Pertama, kloning repositori GitHub DeepSeek-V3 kami:
</code></pre>shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
<pre><code class="language-">
Masuk ke folder </code>inference<code> dan instal dependensi di </code>requirements.txt<code>. Cara termudah adalah menggunakan package manager seperti </code>conda<code> atau </code>uv<code> untuk membuat environment virtual baru dan menginstal dependensi.
</code></pre>shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
<pre><code class="language-">
Unduh bobot model dari Hugging Face, dan tempatkan ke folder </code>/path/to/DeepSeek-V3<code>.</p><p>#### Konversi Bobot Model</p><p>Konversi bobot model Hugging Face ke format spesifik:
</code></pre>shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
<pre><code class="language-">
#### Jalankan</p><p>Setelah itu Anda dapat melakukan chat dengan DeepSeek-V3:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
<pre><code class="language-">
Atau inferensi batch pada file tertentu:
</code></pre>shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
<pre><code class="language-">
<h3>6.2 Inferensi dengan SGLang (direkomendasikan)</h3></p><p><a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a> saat ini mendukung <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations" target="_blank" rel="noopener noreferrer">optimasi MLA</a>, <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models" target="_blank" rel="noopener noreferrer">DP Attention</a>, FP8 (W8A8), FP8 KV Cache, dan Torch Compile, memberikan latensi dan throughput terbaik di antara framework open-source.</p><p>Secara khusus, <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1" target="_blank" rel="noopener noreferrer">SGLang v0.4.1</a> mendukung penuh DeepSeek-V3 baik di <strong>GPU NVIDIA maupun AMD</strong>, sehingga sangat fleksibel dan tangguh.</p><p>SGLang juga mendukung <a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208" target="_blank" rel="noopener noreferrer">tensor parallelism multi-node</a>, memungkinkan Anda menjalankan model ini di banyak mesin yang terhubung jaringan.</p><p>Multi-Token Prediction (MTP) sedang dalam pengembangan, dan perkembangannya dapat diikuti di <a href="https://github.com/sgl-project/sglang/issues/2591" target="_blank" rel="noopener noreferrer">rencana optimasi</a>.</p><p>Berikut instruksi peluncuran dari tim SGLang: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3</p><h3>6.3 Inferensi dengan LMDeploy (direkomendasikan)</h3>
<a href="https://github.com/InternLM/lmdeploy" target="_blank" rel="noopener noreferrer">LMDeploy</a>, framework inferensi dan serving yang fleksibel serta berperforma tinggi untuk model bahasa besar, kini mendukung DeepSeek-V3. Mendukung proses pipeline offline maupun deployment online, dan terintegrasi mulus dengan workflow berbasis PyTorch.</p><p>Untuk instruksi langkah demi langkah menjalankan DeepSeek-V3 dengan LMDeploy, silakan lihat di sini: https://github.com/InternLM/lmdeploy/issues/2960</p><h3>6.4 Inferensi dengan TRT-LLM (direkomendasikan)</h3></p><p><a href="https://github.com/NVIDIA/TensorRT-LLM" target="_blank" rel="noopener noreferrer">TensorRT-LLM</a> kini mendukung model DeepSeek-V3, menawarkan opsi presisi seperti BF16 dan INT4/INT8 weight-only. Dukungan untuk FP8 sedang dalam pengembangan dan akan segera dirilis. Anda dapat mengakses branch khusus TRTLLM untuk DeepSeek-V3 melalui tautan berikut: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. </p><h3>6.5 Inferensi dengan vLLM (direkomendasikan)</h3></p><p><a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a> v0.6.6 mendukung inferensi DeepSeek-V3 untuk mode FP8 dan BF16 di GPU NVIDIA dan AMD. Selain teknik standar, vLLM menawarkan _pipeline parallelism_ sehingga Anda dapat menjalankan model ini di banyak mesin yang terhubung jaringan. Untuk panduan detail, silakan lihat <a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html" target="_blank" rel="noopener noreferrer">petunjuk vLLM</a>. Silakan ikuti <a href="https://github.com/vllm-project/vllm/issues/11539" target="_blank" rel="noopener noreferrer">rencana pengembangan</a> juga.</p><h3>6.6 Inferensi dengan LightLLM (direkomendasikan)</h3></p><p><a href="https://github.com/ModelTC/lightllm/tree/main" target="_blank" rel="noopener noreferrer">LightLLM</a> v1.0.1 mendukung deployment tensor parallel single-machine dan multi-machine untuk DeepSeek-R1 (FP8/BF16) serta deployment mixed-precision, dengan mode kuantisasi lain terus diintegrasikan. Untuk detail lebih lanjut, silakan lihat <a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html" target="_blank" rel="noopener noreferrer">petunjuk LightLLM</a>. Selain itu, LightLLM menawarkan deployment PD-disaggregation untuk DeepSeek-V2, dan implementasi PD-disaggregation untuk DeepSeek-V3 sedang dikembangkan.</p><h3>6.7 Fungsionalitas Inferensi Rekomendasi dengan GPU AMD</h3></p><p>Bekerja sama dengan tim AMD, kami telah mencapai dukungan Day-One untuk GPU AMD dengan SGLang, dengan kompatibilitas penuh FP8 dan BF16. Untuk panduan lengkap, silakan lihat <a href="#63-inference-with-lmdeploy-recommended" target="_blank" rel="noopener noreferrer">petunjuk SGLang</a>.</p><h3>6.8 Fungsionalitas Inferensi Rekomendasi dengan Huawei Ascend NPU</h3>
Kerangka <a href="https://www.hiascend.com/en/software/mindie" target="_blank" rel="noopener noreferrer">MindIE</a> dari komunitas Huawei Ascend telah berhasil mengadaptasi versi BF16 dari DeepSeek-V3. Untuk panduan langkah demi langkah pada Ascend NPU, silakan ikuti <a href="https://modelers.cn/models/MindIE/deepseekv3" target="_blank" rel="noopener noreferrer">petunjuk di sini</a>.</p><h2>7. Lisensi</h2>
Repositori kode ini dilisensikan di bawah <a href="LICENSE-CODE" target="_blank" rel="noopener noreferrer">Lisensi MIT</a>. Penggunaan model DeepSeek-V3 Base/Chat tunduk pada <a href="LICENSE-MODEL" target="_blank" rel="noopener noreferrer">Lisensi Model</a>. DeepSeek-V3 series (termasuk Base dan Chat) mendukung penggunaan komersial.</p><h2>8. Sitasi</code></pre></h2>
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
</code>``</p><h2>9. Kontak</h2>
Jika Anda memiliki pertanyaan, silakan ajukan issue atau hubungi kami di <a href="service@deepseek.com" target="_blank" rel="noopener noreferrer">service@deepseek.com</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>