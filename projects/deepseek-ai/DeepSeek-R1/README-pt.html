<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-R1 - Read DeepSeek-R1 documentation in Portuguese. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read DeepSeek-R1 documentation in Portuguese. This project has 0 stars on GitHub.">
    <meta name="keywords" content="DeepSeek-R1, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "DeepSeek-R1",
  "description": "Read DeepSeek-R1 documentation in Portuguese. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "deepseek-ai"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/deepseek-ai/DeepSeek-R1/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/deepseek-ai/DeepSeek-R1/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/deepseek-ai/DeepSeek-R1" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    DeepSeek-R1
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Portuguese</span>
                <span>by deepseek-ai</span>
            </div>
        </div>
        
        <div class="content">
            <h1>DeepSeek-R1</h1>
<!-- markdownlint-disable first-line-h1 -->
<!-- markdownlint-disable html -->
<!-- markdownlint-disable no-duplicate-header --></p><p><div align="center">
  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-R1" />
</div>
<hr>
<div align="center" style="line-height: 1;">
  <a href="https://www.deepseek.com/" target="_blank"><img alt="Homepage"
    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>
  <a href="https://chat.deepseek.com/" target="_blank"><img alt="Chat"
    src="https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white"/></a>
  <a href="https://huggingface.co/deepseek-ai" target="_blank"><img alt="Hugging Face"
    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>
  <br>
  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank"><img alt="Discord"
    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>
  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank"><img alt="WeChat"
    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>
  <a href="https://twitter.com/deepseek_ai" target="_blank"><img alt="Twitter Follow"
    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE"><img alt="License"
    src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53"/></a>
  <br>
  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Link do Artigo</b>👁️</a>
</div></p><h2>1. Introdução</h2></p><p>Apresentamos nossos modelos de raciocínio de primeira geração, DeepSeek-R1-Zero e DeepSeek-R1.  
O DeepSeek-R1-Zero, um modelo treinado via aprendizado por reforço em larga escala (RL) sem ajuste fino supervisionado (SFT) como etapa preliminar, demonstrou desempenho notável em raciocínio.  
Com RL, o DeepSeek-R1-Zero emergiu naturalmente com diversos comportamentos poderosos e interessantes de raciocínio.  
No entanto, o DeepSeek-R1-Zero enfrenta desafios como repetição sem fim, baixa legibilidade e mistura de idiomas. Para abordar essas questões e aprimorar ainda mais o desempenho de raciocínio,  
apresentamos o DeepSeek-R1, que incorpora dados de início a frio antes do RL.  
O DeepSeek-R1 alcança desempenho comparável ao OpenAI-o1 em tarefas de matemática, código e raciocínio.  
Para apoiar a comunidade de pesquisa, abrimos o código-fonte do DeepSeek-R1-Zero, DeepSeek-R1 e seis modelos densos destilados do DeepSeek-R1 baseados em Llama e Qwen. O DeepSeek-R1-Distill-Qwen-32B supera o OpenAI-o1-mini em vários benchmarks, alcançando novos resultados de estado da arte para modelos densos.</p><p><strong>NOTA: Antes de executar localmente os modelos da série DeepSeek-R1, recomendamos gentilmente revisar a seção <a href="#usage-recommendations" target="_blank" rel="noopener noreferrer">Recomendações de Uso</a>.</strong></p><p><p align="center">
  <img width="80%" src="figures/benchmark.jpg">
</p></p><h2>2. Resumo do Modelo</h2></p><hr></p><p><strong>Pós-Treinamento: Aprendizado por Reforço em Larga Escala no Modelo Base</strong></p><ul><li>Aplicamos diretamente o aprendizado por reforço (RL) ao modelo base sem depender de ajuste fino supervisionado (SFT) como etapa preliminar. Essa abordagem permite que o modelo explore cadeias de pensamento (CoT) para resolver problemas complexos, resultando no desenvolvimento do DeepSeek-R1-Zero. O DeepSeek-R1-Zero demonstra capacidades como auto-verificação, reflexão e geração de longas cadeias de pensamento, marcando um marco importante para a comunidade de pesquisa. Notavelmente, é a primeira pesquisa aberta a validar que as capacidades de raciocínio de LLMs podem ser incentivadas puramente por RL, sem a necessidade de SFT. Este avanço abre caminho para futuros desenvolvimentos nesta área.</li></p><p><li>Apresentamos nosso pipeline para desenvolver o DeepSeek-R1. O pipeline incorpora dois estágios de RL visando descobrir padrões de raciocínio aprimorados e alinhar com preferências humanas, além de dois estágios de SFT que servem como semente para as capacidades de raciocínio e não raciocínio do modelo.  </li>
  </ul>Acreditamos que o pipeline beneficiará a indústria ao criar melhores modelos.</p><hr></p><p><strong>Destilação: Modelos Menores Também Podem Ser Poderosos</strong></p><ul><li>Demonstramos que os padrões de raciocínio de modelos maiores podem ser destilados em modelos menores, resultando em desempenho superior aos padrões de raciocínio descobertos por RL em modelos pequenos. O DeepSeek-R1 de código aberto, bem como sua API, beneficiarão a comunidade de pesquisa para destilar melhores modelos menores no futuro.  </li>
<li>Usando os dados de raciocínio gerados pelo DeepSeek-R1, ajustamos vários modelos densos amplamente utilizados na comunidade de pesquisa. Os resultados da avaliação demonstram que os modelos densos menores destilados apresentam desempenho excepcional em benchmarks. Abrimos os checkpoints destilados de 1.5B, 7B, 8B, 14B, 32B e 70B baseados nas séries Qwen2.5 e Llama3 para a comunidade.</li></p><p></ul><h2>3. Downloads dos Modelos</h2></p><h3>Modelos DeepSeek-R1</h3></p><p><div align="center"></p><p>| <strong>Modelo</strong> | <strong>#Params Totais</strong> | <strong>#Params Ativados</strong> | <strong>Comprimento do Contexto</strong> | <strong>Download</strong> |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-R1-Zero | 671B | 37B | 128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
| DeepSeek-R1   | 671B | 37B |  128K   | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |</p><p></div></p><p>O DeepSeek-R1-Zero & DeepSeek-R1 são treinados com base no DeepSeek-V3-Base.  
Para mais detalhes sobre a arquitetura do modelo, consulte o repositório <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">DeepSeek-V3</a>.</p><h3>Modelos DeepSeek-R1-Distill</h3></p><p><div align="center"></p><p>| <strong>Modelo</strong> | <strong>Modelo Base</strong> | <strong>Download</strong> |
| :------------: | :------------: | :------------: |
| DeepSeek-R1-Distill-Qwen-1.5B  | <a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B" target="_blank" rel="noopener noreferrer">Qwen2.5-Math-1.5B</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
| DeepSeek-R1-Distill-Qwen-7B  | <a href="https://huggingface.co/Qwen/Qwen2.5-Math-7B" target="_blank" rel="noopener noreferrer">Qwen2.5-Math-7B</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
| DeepSeek-R1-Distill-Llama-8B  | <a href="https://huggingface.co/meta-llama/Llama-3.1-8B" target="_blank" rel="noopener noreferrer">Llama-3.1-8B</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
| DeepSeek-R1-Distill-Qwen-14B   | <a href="https://huggingface.co/Qwen/Qwen2.5-14B" target="_blank" rel="noopener noreferrer">Qwen2.5-14B</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
|DeepSeek-R1-Distill-Qwen-32B  | <a href="https://huggingface.co/Qwen/Qwen2.5-32B" target="_blank" rel="noopener noreferrer">Qwen2.5-32B</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |
| DeepSeek-R1-Distill-Llama-70B  | <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct" target="_blank" rel="noopener noreferrer">Llama-3.3-70B-Instruct</a> | <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B" target="_blank" rel="noopener noreferrer">🤗 HuggingFace</a>   |</p><p></div></p><p>Os modelos DeepSeek-R1-Distill são ajustados com base em modelos de código aberto, utilizando amostras geradas pelo DeepSeek-R1.  
Alteramos levemente suas configurações e tokenizadores. Por favor, use nossa configuração para executar esses modelos.</p><h2>4. Resultados de Avaliação</h2></p><h3>Avaliação DeepSeek-R1</h3>
Para todos os nossos modelos, o comprimento máximo de geração é definido como 32.768 tokens. Para benchmarks que requerem amostragem, usamos temperatura de $0.6$, valor top-p de $0.95$ e geramos 64 respostas por consulta para estimar pass@1.
<div align="center"></p><p>
| Categoria | Benchmark (Métrica) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |
|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|
| | Arquitetura | - | - | MoE | - | - | MoE |
| | # Params Ativados | - | - | 37B | - | - | 37B |
| | # Params Totais | - | - | 671B | - | - | 671B |
| Inglês | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | <strong>91.8</strong> | 90.8 |
| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | <strong>92.9</strong> |
| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | <strong>84.0</strong> |
| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | <strong>92.2</strong> |
| | IF-Eval (Prompt Strict) | <strong>86.5</strong> | 84.3 | 86.1 | 84.8 | - | 83.3 |
| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | <strong>75.7</strong> | 71.5 |
| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | <strong>47.0</strong> | 30.1 |
| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | <strong>82.5</strong> |
| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | <strong>87.6</strong> |
| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | <strong>92.3</strong> |
| Código | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | <strong>65.9</strong> |
| | Codeforces (Percentil) | 20.3 | 23.6 | 58.7 | 93.4 | <strong>96.6</strong> | 96.3 |
| | Codeforces (Nota) | 717 | 759 | 1134 | 1820 | <strong>2061</strong> | 2029 |
| | SWE Verified (Resolvido) | <strong>50.8</strong> | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |
| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | <strong>61.7</strong> | 53.3 |
| Matemática | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | <strong>79.8</strong> |
| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | <strong>97.3</strong> |
| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | <strong>78.8</strong> |
| Chinês | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | <strong>92.8</strong> |
| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | <strong>91.8</strong> |
| | C-SimpleQA (Correct) | 55.4 | 58.7 | <strong>68.0</strong> | 40.3 | - | 63.7 |</p><p></div></p><h3>Avaliação dos Modelos Destilados</h3></p><p><div align="center"></p><p>| Modelo                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces nota |
|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|
| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |
| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |
| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | <strong>1820</strong>          |
| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |
| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |
| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |
| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |
| DeepSeek-R1-Distill-Qwen-32B        | <strong>72.6</strong>         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |
| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |
| DeepSeek-R1-Distill-Llama-70B        | 70.0             | <strong>86.7</strong>          | <strong>94.5</strong>        | <strong>65.2</strong>             | <strong>57.5</strong>             | 1633              |</p><p></div></p><h2>5. Site de Chat & Plataforma de API</h2></p><p>Você pode conversar com o DeepSeek-R1 no site oficial da DeepSeek: <a href="https://chat.deepseek.com" target="_blank" rel="noopener noreferrer">chat.deepseek.com</a>, e ativar o botão "DeepThink".</p><p>Também fornecemos uma API compatível com OpenAI na Plataforma DeepSeek: <a href="https://platform.deepseek.com/" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a></p><h2>6. Como Rodar Localmente</h2></p><h3>Modelos DeepSeek-R1</h3></p><p>Por favor, visite o repositório <a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noopener noreferrer">DeepSeek-V3</a> para mais informações sobre como rodar o DeepSeek-R1 localmente.</p><p><strong>NOTA: O Transformers do Hugging Face ainda não é suportado diretamente.</strong></p><h3>Modelos DeepSeek-R1-Distill</h3></p><p>Os modelos DeepSeek-R1-Distill podem ser utilizados da mesma forma que os modelos Qwen ou Llama.</p><p>Por exemplo, você pode facilmente iniciar um serviço usando o <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">vLLM</a>:</p><pre><code class="language-shell">vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager</code></pre></p><p>Você também pode iniciar facilmente um serviço usando o <a href="https://github.com/sgl-project/sglang" target="_blank" rel="noopener noreferrer">SGLang</a></p><pre><code class="language-bash">python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2</code></pre></p><h3>Recomendações de Uso</h3></p><p><strong>Recomendamos aderir às seguintes configurações ao utilizar os modelos da série DeepSeek-R1, incluindo benchmarking, para obter o desempenho esperado:</strong></p><ul><li>Defina a temperatura dentro da faixa de 0.5-0.7 (0.6 é recomendado) para evitar repetições intermináveis ou saídas incoerentes.</li>
<li><strong>Evite adicionar um prompt de sistema; todas as instruções devem estar contidas no prompt do usuário.</strong></li>
<li>Para problemas matemáticos, é aconselhável incluir uma diretiva no seu prompt, como: "Por favor, raciocine passo a passo e coloque sua resposta final dentro de \boxed{}."</li>
<li>Ao avaliar o desempenho do modelo, recomenda-se realizar vários testes e calcular a média dos resultados.</li></p><p></ul>Além disso, observamos que os modelos da série DeepSeek-R1 tendem a ignorar o padrão de pensamento (ou seja, ao gerar "\<think\>\n\n\</think\>") ao responder a determinadas consultas, o que pode prejudicar o desempenho do modelo.  
<strong>Para garantir que o modelo realize um raciocínio completo, recomendamos forçar o modelo a iniciar sua resposta com "\<think\>\n" no início de cada saída.</strong></p><h3>Prompts Oficiais</h3>
No site/app oficial da DeepSeek, não usamos prompts de sistema, mas projetamos dois prompts específicos para upload de arquivos e busca na web para melhor experiência do usuário. Além disso, a temperatura no site/app é 0.6.</p><p>Para upload de arquivos, siga o template para criar prompts, onde {file_name}, {file_content} e {question} são argumentos. 
<pre><code class="language-">file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""</code></pre></p><p>Para Busca na Web, {search_results}, {cur_date} e {question} são argumentos. </p><p>Para consulta em chinês, usamos o prompt:
<pre><code class="language-">search_answer_zh_template = \
'''# 以下内容是基于用户发送的消息的搜索结果:
{search_results}
在我给你的搜索结果中，每个结果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的数字索引。请在适当的情况下在句子末尾引用上下文。请按照引用编号[citation:X]的格式在答案中对应部分引用上下文。如果一句话源自多个上下文，请列出所有相关的引用编号，例如[citation:3][citation:5]，切记不要将引用集中在最后返回引用编号，而是在答案对应部分列出。
在回答时，请注意以下几点：
<ul><li>今天是{cur_date}。</li>
<li>并非搜索结果的所有内容都与用户的问题密切相关，你需要结合问题，对搜索结果进行甄别、筛选。</li>
<li>对于列举类的问题（如列举所有航班信息），尽量将答案控制在10个要点以内，并告诉用户可以查看搜索来源、获得完整信息。优先提供信息完整、最相关的列举项；如非必要，不要主动告诉用户搜索结果未提供的内容。</li>
<li>对于创作类的问题（如写论文），请务必在正文的段落中引用对应的参考编号，例如[citation:3][citation:5]，不能只在文章末尾引用。你需要解读并概括用户的题目要求，选择合适的格式，充分利用搜索结果并抽取重要信息，生成符合用户要求、极具思想深度、富有创造力与专业性的答案。你的创作篇幅需要尽可能延长，对于每一个要点的论述要推测用户的意图，给出尽可能多角度的回答要点，且务必信息量大、论述详尽。</li>
<li>如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。</li>
<li>对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。</li>
<li>你需要根据用户要求和回答内容选择合适、美观的回答格式，确保可读性强。</li>
<li>你的回答应该综合多个相关网页来回答，不能重复引用一个网页。</li>
<li>除非用户要求，否则你回答的语言需要和用户提问的语言保持一致。</li></p><p></ul><h1>用户消息为：</h1>
{question}'''</code></pre></p><p>Para consulta em inglês, usamos o prompt:
<pre><code class="language-">search_answer_en_template = \
'''# The following contents are the search results related to the user's message:
{search_results}
In the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.
When responding, please keep the following points in mind:
<ul><li>Today is {cur_date}.</li>
<li>Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.</li>
<li>For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.</li>
<li>For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.</li>
<li>If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.</li>
<li>For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.</li>
<li>Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.</li>
<li>Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.</li>
<li>Unless the user requests otherwise, your response should be in the same language as the user's question.</li></p><p></ul><h1>The user's message is:</h1>
{question}'''</code></pre></p><h2>7. Licença</h2></p><p>Este repositório de código e os pesos do modelo estão licenciados sob a <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">Licença MIT</a>.  
A série DeepSeek-R1 suporta uso comercial, permite quaisquer modificações e trabalhos derivados, incluindo, mas não se limitando, à destilação para treinamento de outros LLMs. Observe que:
<ul><li>DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B e DeepSeek-R1-Distill-Qwen-32B são derivados da <a href="https://github.com/QwenLM/Qwen2.5" target="_blank" rel="noopener noreferrer">série Qwen-2.5</a>, originalmente licenciados sob a <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">Licença Apache 2.0</a>, e agora ajustados com 800k amostras curadas com DeepSeek-R1.</li>
<li>DeepSeek-R1-Distill-Llama-8B é derivado de Llama3.1-8B-Base e originalmente licenciado sob a <a href="https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">licença Llama3.1</a>.</li>
<li>DeepSeek-R1-Distill-Llama-70B é derivado de Llama3.3-70B-Instruct e originalmente licenciado sob a <a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">licença Llama3.3</a>.</li></p><p></ul><h2>8. Citação</h2>
<pre><code class="language-bibtex">@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}</code></pre></p><h2>9. Contato</h2>
Se você tiver alguma dúvida, por favor, abra uma issue ou entre em contato pelo <a href="mailto:service@deepseek.com" target="_blank" rel="noopener noreferrer">service@deepseek.com</a>.</p><p>
---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-R1/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>