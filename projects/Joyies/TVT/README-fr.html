<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TVT - [ICCV2025] Code officiel pour la super-r&#233;solution d&#39;images r&#233;elles avec pr&#233;servation de la fine structure via l&#39;entra&#238;nement par transfert VAE</title>
    <meta name="description" content="[ICCV2025] Code officiel pour la super-r&#233;solution d&#39;images r&#233;elles avec pr&#233;servation de la fine structure via l&#39;entra&#238;nement par transfert VAE">
    <meta name="keywords" content="TVT, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TVT",
  "description": "[ICCV2025] Code officiel pour la super-résolution d'images réelles avec préservation de la fine structure via l'entraînement par transfert VAE",
  "author": {
    "@type": "Person",
    "name": "Joyies"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 116
  },
  "url": "https://OpenAiTx.github.io/projects/Joyies/TVT/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/Joyies/TVT/main/README.md",
  "datePublished": "2026-02-22",
  "dateModified": "2026-02-22"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Joyies/TVT" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TVT
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 116 stars</span>
                <span class="language">French</span>
                <span>by Joyies</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center">
<h2>Super-résolution d’images réelles avec préservation de la structure fine via un entraînement VAE par transfert</h2></p><p>🚩 Accepté par ICCV2025</p><p><a href="https://dblp.org/pid/249/8335.html" target="_blank" rel="noopener noreferrer">Qiaosi Yi</a><sup>1,2</sup>
| <a href="https://scholar.google.com/citations?hl=zh-CN&user=Bd73ldQAAAAJ" target="_blank" rel="noopener noreferrer">Shuai Li</a><sup>1</sup>
| <a href="https://scholar.google.com/citations?user=A-U8zE8AAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer">Rongyuan Wu</a><sup>1,2</sup>
| <a href="https://scholar.google.com/citations?hl=zh-CN&tzom=-480&user=ZCDjTn8AAAAJ" target="_blank" rel="noopener noreferrer">Lingchen Sun</a><sup>1,2</sup>
| <a href="https://dblp.org/pid/41/915-1.html" target="_blank" rel="noopener noreferrer">Yuhui Wu</a><sup>1,2</sup>
| <a href="https://www4.comp.polyu.edu.hk/~cslzhang" target="_blank" rel="noopener noreferrer">Lei Zhang</a><sup>1,2</sup></p><p><sup>1</sup>L’Université Polytechnique de Hong Kong, <sup>2</sup>Institut de Recherche OPPO
</div></p><p><a href="https://arxiv.org/pdf/2507.20291" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ArXiv%20-Paper-b31b1b?logo=arxiv&logoColor=red" alt=""></a>&nbsp; <a href="https://huggingface.co/Joypop/TVTSR/tree/main" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-model%20weights-blue" alt="weights"></a></p><h2>⏰ Mise à jour</h2>
<ul><li><strong>2025.7.29</strong> : Article publié sur <a href="https://arxiv.org/pdf/2507.20291" target="_blank" rel="noopener noreferrer">ArXiv</a>.</li>
<li><strong>2025.7.28</strong> : Le code d’entraînement et de test est publié.</li>
<li><strong>2025.7.24</strong> : Le dépôt est publié.</li></p><p></ul>:star: Si TVT vous aide pour vos images ou projets, merci d’étoiler ce dépôt. Merci ! :hugs:</p><h3>À FAIRE </h3>
<ul><li>[x] Publier le code pour l’inférence.</li>
<li>[x] Mettre à jour le code pour l’entraînement.</li>
<li>[ ] VAED4 en fp16.</li></p><p>
</ul><h2>⚙ Dépendances et installation</h2>
<pre><code class="language-shell">## git clone this repository
git clone https://github.com/Joyies/TVT.git
cd TVT</p><h1>create an environment</h1>
conda create -n TVT python=3.10
conda activate TVT
pip install --upgrade pip
pip install -r requirements.txt</code></pre></p><h2>🏂 Inférence rapide</h2></p><h3>Super-résolution d'image réelle</h3></p><p>#### Étape 1 : Télécharger les modèles pré-entraînés
<ul><li>Téléchargez les modèles SD-2.1-base pré-entraînés depuis <a href="https://huggingface.co/Joypop/stable-diffusion-2-1-base" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SD%202.1%20Base-blue" alt="weights"></a>&nbsp;</li>
<li>Téléchargez les poids des modèles (<a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">VAED4</a>, <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">modèle TVT</a>, <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">TVTUNet</a>, <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">DAPE</a>, et <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">RAM</a>) depuis <a href="https://huggingface.co/Joypop/TVTSR/tree/main" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-model%20weights-blue" alt="weights"></a>&nbsp; et placez les poids dans le dossier <code>ckp/</code> :</li></p><p></ul>#### Étape 2 : Préparez les données de test et lancez la commande de test 
Vous pouvez modifier input_path et output_path pour lancer la commande de test. input_path est le chemin de l'image de test et output_path est le chemin où les images de sortie sont sauvegardées.
<pre><code class="language-">python TVT/inferences/inference.py \
--input_image input_path \
--output_dir output_path \
--pretrained_path ckp/model_TVT.pkl \
--pretrained_model_name_or_path stabilityai/stable-diffusion-2-1-base \
--pretrained_unet_path ckp/TVTUNet \
--vae4d_path ckp/vae.ckpt \
--ram_ft_path ckp/DAPE.pth \
--negprompt 'dotted, noise, blur, lowres, smooth' \
--prompt 'clean, high-resolution, 8k' \
--upscale 4 \
--time_step 1</code></pre>
or 
<pre><code class="language-">bash scripts/test/test_realsr.sh</code></pre>
Nous fournissons également le code de tuiles pour économiser la mémoire GPU lors de l'inférence. Vous pouvez exécuter la commande et modifier la taille des tuiles et le pas en fonction de la VRAM de votre appareil.
<pre><code class="language-">python TVT/inferences/inference_tile.py \
--input_image input_path \
--output_dir output_path \
--pretrained_path ckp/model_TVT.pkl \
--pretrained_model_name_or_path stabilityai/stable-diffusion-2-1-base \
--pretrained_unet_path ckp/TVTUNet \
--vae4d_path ckp/vae.ckpt \
--ram_ft_path ckp/DAPE.pth \
--negprompt 'dotted, noise, blur, lowres, smooth' \
--prompt 'clean, high-resolution, 8k' \
--upscale 4 \
--time_step 1 \
--tiled_size 96 \
--tiled_overlap 32</code></pre></p><h2>🚄 Phase d'entraînement</h2></p><h3>Entraîner VAED4 sur les jeux de données OpenImage et LSDIR.</h3>
#### Étape 1 : Préparer les données d'entraînement
Téléchargez le <a href="https://storage.googleapis.com/openimages/web/index.html" target="_blank" rel="noopener noreferrer">jeu de données OpenImage</a> et le <a href="https://github.com/ofsoundof/LSDIR" target="_blank" rel="noopener noreferrer">jeu de données LSIDR</a>. Pour chaque image du jeu de données LSDIR, découpez plusieurs patches d’image de 512×512 en utilisant une fenêtre glissante avec un pas de 64 pixels ;</p><p>#### Étape 2 : Entraîner VAED4
Le <a href="https://github.com/CompVis/latent-diffusion" target="_blank" rel="noopener noreferrer">code LDM</a> est utilisé pour entraîner VAED4. </p><h3>Entraîner TVTSR sur les jeux de données Real-ISR</h3>
#### Étape 1 : Préparer les données d'entraînement</p><p>Téléchargez le <a href="https://github.com/ofsoundof/LSDIR" target="_blank" rel="noopener noreferrer">jeu de données LSIDR</a> et les 10 000 premières images du <a href="https://github.com/NVlabs/ffhq-dataset" target="_blank" rel="noopener noreferrer">jeu de données FFHQ</a>. Ensuite, effectuez une augmentation des données sur le jeu de données d'entraînement. Plus précisément, pour chaque image du jeu LSDIR, découpez plusieurs patches d’image de 512×512 en utilisant une fenêtre glissante avec un pas de 64 pixels ; pour le jeu FFHQ, redimensionnez directement toutes les images à 512×512.</p><p>#### Étape 2 : Entraîner le modèle Real-ISR</p><ul><li>Téléchargez les modèles <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">VAED4</a>, <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">TVTUNet</a> et <a href="https://huggingface.co/Joypop/TVTSR/tree/main/ckp" target="_blank" rel="noopener noreferrer">RAM</a>, et placez-les dans le dossier <code>ckp/</code>. </li></p><p><li>Lancez l’entraînement.</li>
    </ul><pre><code class="language-shell">   accelerate launch --gpu_ids=0,1,2,3, --num_processes=4 TVT/train_TVTSR/train.py \
    --pretrained_model_name_or_path="stabilityai/stable-diffusion-2-1-base" \
    --pretrained_model_name_or_path_vsd="stabilityai/stable-diffusion-2-1-base" \
    --pretrained_unet_path='ckp/TVTUNet' \
    --vae4d_path='ckp/vae.ckpt' \
    --dataset_folder="data_path" \
    --testdataset_folder="test_path" \
    --resolution=512 \
    --learning_rate=5e-5 \
    --train_batch_size=2 \
    --gradient_accumulation_steps=2 \
    --enable_xformers_memory_efficient_attention \
    --eval_freq 500 \
    --checkpointing_steps 500 \
    --mixed_precision='fp16' \
    --report_to "tensorboard" \
    --output_dir="output_path" \
    --lora_rank_unet_vsd=4 \
    --lora_rank_unet=4 \
    --lambda_lpips=2 \
    --lambda_l2=1 \
    --lambda_vsd=1 \
    --lambda_vsd_lora=1 \
    --min_dm_step_ratio=0.25 \
    --max_dm_step_ratio=0.75 \
    --use_vae_encode_lora \
    --align_method="adain" \
    --use_online_deg \
    --deg_file_path="params_TVT.yml" \
    --negative_prompt='painting, oil painting, illustration, drawing, art, sketch, oil painting, cartoon, CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, low quality, frames, watermark, signature, jpeg artifacts, deformed, lowres, over-smooth' \
    --test_image_prep='no_resize' \
    --time_step=1 \
    --tracker_project_name "experiment_track_name"
    ``<code>
    or
    </code>`<code>shell
   bash scripts/train/train.sh
    </code>`<code></p><h2>🔗 Citations</h2>
Si notre code aide votre recherche ou votre travail, veuillez envisager de citer notre article.
Voici les références BibTeX :
</code></pre>
@article{yi2025fine,
  title={Fine-structure Preserved Real-world Image Super-resolution via Transfer VAE Training},
  author={Yi, Qiaosi and Li, Shuai and Wu, Rongyuan and Sun, Lingchen and Wu, Yuhui and Zhang, Lei},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  year={2025}
}
</code>``</p><h2>©️ Licence</h2>
Ce projet est publié sous la <a href="LICENSE" target="_blank" rel="noopener noreferrer">licence Apache 2.0</a>.</p><h2>📧 Contact</h2>
Si vous avez des questions, veuillez contacter : qiaosiyijoyies@gmail.com</p><h2>Remerciements</h2>
Ce projet est basé sur <a href="https://github.com/huggingface/diffusers" target="_blank" rel="noopener noreferrer">diffusers</a>, <a href="https://github.com/CompVis/latent-diffusion" target="_blank" rel="noopener noreferrer">LDM</a>, <a href="https://github.com/cswry/OSEDiff" target="_blank" rel="noopener noreferrer">OSEDiff</a> et <a href="https://github.com/csslc/PiSA-SR" target="_blank" rel="noopener noreferrer">PiSA-SR</a>. Merci pour ce travail remarquable. </p><p><details>
<summary>statistiques</summary></p><p><img src="https://visitor-badge.laobi.icu/badge?page_id=Joyies/TVT" alt="visiteurs"></p><p></details></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2026-02-22

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Joyies/TVT/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2026-02-22 
    </div>
    
</body>
</html>