<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - Read text-generation-inference documentation in Dutch. This project has 10206 stars on GitHub.</title>
    <meta name="description" content="Read text-generation-inference documentation in Dutch. This project has 10206 stars on GitHub.">
    <meta name="keywords" content="text-generation-inference, Dutch, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "text-generation-inference",
  "description": "Read text-generation-inference documentation in Dutch. This project has 10206 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "huggingface"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 10206
  },
  "url": "https://OpenAiTx.github.io/projects/huggingface/text-generation-inference/README-nl.html",
  "sameAs": "https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    text-generation-inference
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 10206 stars</span>
                <span class="language">Dutch</span>
                <span>by huggingface</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center"></p><p><a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png">
</a></p><h1>Tekstgeneratie Inference</h1></p><p><a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API-documentatie" src="https://img.shields.io/badge/API-Swagger-informational">
</a></p><p>Een Rust-, Python- en gRPC-server voor tekstgeneratie-inference. Wordt in productie gebruikt bij <a href="https://huggingface.co" target="_blank" rel="noopener noreferrer">Hugging Face</a>
om Hugging Chat, de Inference API en Inference Endpoints aan te drijven.</p><p></div></p><h2>Inhoudsopgave</h2></p><ul><li><a href="#get-started" target="_blank" rel="noopener noreferrer">Aan de slag</a></li>
    <li><a href="#docker" target="_blank" rel="noopener noreferrer">Docker</a></li>
    <li><a href="#api-documentation" target="_blank" rel="noopener noreferrer">API-documentatie</a></li>
    <li><a href="#using-a-private-or-gated-model" target="_blank" rel="noopener noreferrer">Een privé- of afgeschermd model gebruiken</a></li>
    <li><a href="#a-note-on-shared-memory-shm" target="_blank" rel="noopener noreferrer">Een opmerking over gedeeld geheugen (shm)</a></li>
    <li><a href="#distributed-tracing" target="_blank" rel="noopener noreferrer">Gedistribueerde tracing</a></li>
    <li><a href="#architecture" target="_blank" rel="noopener noreferrer">Architectuur</a></li>
    <li><a href="#local-install" target="_blank" rel="noopener noreferrer">Lokale installatie</a></li>
    <li><a href="#local-install-nix" target="_blank" rel="noopener noreferrer">Lokale installatie (Nix)</a></li>
  <li><a href="#optimized-architectures" target="_blank" rel="noopener noreferrer">Geoptimaliseerde architecturen</a></li>
  <li><a href="#run-locally" target="_blank" rel="noopener noreferrer">Lokaal uitvoeren</a></li>
    <li><a href="#run" target="_blank" rel="noopener noreferrer">Uitvoeren</a></li>
    <li><a href="#quantization" target="_blank" rel="noopener noreferrer">Quantisatie</a></li>
  <li><a href="#develop" target="_blank" rel="noopener noreferrer">Ontwikkelen</a></li>
  <li><a href="#testing" target="_blank" rel="noopener noreferrer">Testen</a></li></p><p></ul>Tekstgeneratie Inference (TGI) is een toolkit voor het implementeren en bedienen van Large Language Models (LLM's). TGI maakt het mogelijk om tekstgeneratie met hoge prestaties uit te voeren voor de populairste open-source LLM's, waaronder Llama, Falcon, StarCoder, BLOOM, GPT-NeoX en <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">meer</a>. TGI implementeert vele functies, zoals:</p><ul><li>Eenvoudige launcher om de populairste LLM's te bedienen</li>
<li>Productieklaar (gedistribueerde tracing met Open Telemetry, Prometheus-metrics)</li>
<li>Tensor Parallelisme voor snellere inference op meerdere GPU's</li>
<li>Token-streaming met Server-Sent Events (SSE)</li>
<li>Continue batching van binnenkomende verzoeken voor verhoogde totale doorvoer</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">Messages API</a> compatibel met Open AI Chat Completion API</li>
<li>Geoptimaliseerde transformers-code voor inference met <a href="https://github.com/HazyResearch/flash-attention" target="_blank" rel="noopener noreferrer">Flash Attention</a> en <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">Paged Attention</a> op de populairste architecturen</li>
<li>Quantisatie met:</li>
  <li><a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">bitsandbytes</a></li>
  <li><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPT-Q</a></li>
  <li><a href="https://github.com/NetEase-FuXi/EETQ" target="_blank" rel="noopener noreferrer">EETQ</a></li>
  <li><a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener noreferrer">AWQ</a></li>
  <li><a href="https://github.com/IST-DASLab/marlin" target="_blank" rel="noopener noreferrer">Marlin</a></li>
  <li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/" target="_blank" rel="noopener noreferrer">fp8</a></li>
<li><a href="https://github.com/huggingface/safetensors" target="_blank" rel="noopener noreferrer">Safetensors</a> gewichtsladen</li>
<li>Watermerken met <a href="https://arxiv.org/abs/2301.10226" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a></li>
<li>Logits warper (temperatuurschaling, top-p, top-k, herhalingsstraf, meer details zie <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor" target="_blank" rel="noopener noreferrer">transformers.LogitsProcessor</a>)</li>
<li>Stopreeksen</li>
<li>Log waarschijnlijkheden</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation" target="_blank" rel="noopener noreferrer">Speculatie</a> ~2x latentie</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance" target="_blank" rel="noopener noreferrer">Guidance/JSON</a>. Specificeer uitvoerformaat om inference te versnellen en zorg ervoor dat de uitvoer geldig is volgens bepaalde specificaties.</li>
<li>Aangepaste Prompt Generatie: genereer eenvoudig tekst door aangepaste prompts te gebruiken om de output van het model te sturen</li>
<li>Fine-tuning ondersteuning: gebruik fijn-afgestelde modellen voor specifieke taken om hogere nauwkeurigheid en prestaties te bereiken</li></p><p></ul><h3>Hardware ondersteuning</h3></p><ul><li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference" target="_blank" rel="noopener noreferrer">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475" target="_blank" rel="noopener noreferrer">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi" target="_blank" rel="noopener noreferrer">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving" target="_blank" rel="noopener noreferrer">Google TPU</a></li></p><p>
</ul><h2>Aan de slag</h2></p><h3>Docker</h3></p><p>Voor een gedetailleerde startgids, zie de <a href="https://huggingface.co/docs/text-generation-inference/quicktour" target="_blank" rel="noopener noreferrer">Quick Tour</a>. De gemakkelijkste manier om te beginnen is met de officiële Docker-container:</p><pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
<h1>deel een volume met de Docker-container om te voorkomen dat gewichten bij elke run worden gedownload</h1>
volume=$PWD/data</p><p>docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><p>En dan kun je verzoeken maken zoals</p><pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{"inputs":"Wat is Deep Learning?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'</code></pre></p><p>Je kunt ook gebruikmaken van de <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">Messages API van TGI</a> om antwoorden te krijgen die compatibel zijn met de Open AI Chat Completion API.</p><pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "Je bent een behulpzame assistent."
    },
    {
      "role": "user",
      "content": "Wat is deep learning?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}' \
    -H 'Content-Type: application/json'</code></pre></p><p><strong>Opmerking:</strong> Om NVIDIA GPU's te gebruiken, moet je de <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="noopener noreferrer">NVIDIA Container Toolkit</a> installeren. We raden ook aan NVIDIA-drivers met CUDA versie 12.2 of hoger te gebruiken. Voor het draaien van de Docker-container op een machine zonder GPU's of CUDA-ondersteuning is het voldoende om de <code>--gpus all</code> vlag te verwijderen en <code>--disable-custom-kernels</code> toe te voegen. Houd er rekening mee dat CPU niet het beoogde platform is voor dit project, dus de prestaties kunnen ondermaats zijn.</p><p><strong>Opmerking:</strong> TGI ondersteunt AMD Instinct MI210 en MI250 GPU's. Details zijn te vinden in de <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus" target="_blank" rel="noopener noreferrer">Supported Hardware-documentatie</a>. Om AMD GPU's te gebruiken, gebruik in plaats van bovenstaande commando:</p><pre><code class="language-shell">docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code></pre></p><p>Om alle opties te zien om je modellen te bedienen (in de <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/launcher/src/main.rs" target="_blank" rel="noopener noreferrer">code</a> of via de cli):
<pre><code class="language-">text-generation-launcher --help</code></pre></p><h3>API-documentatie</h3></p><p>Je kunt de OpenAPI-documentatie van de <code>text-generation-inference</code> REST API raadplegen via de <code>/docs</code> route.
De Swagger UI is ook beschikbaar op: <a href="https://huggingface.github.io/text-generation-inference" target="_blank" rel="noopener noreferrer">https://huggingface.github.io/text-generation-inference</a>.</p><h3>Een privé- of afgeschermd model gebruiken</h3></p><p>Je hebt de optie om de omgevingsvariabele <code>HF_TOKEN</code> te gebruiken voor het configureren van de token die
door <code>text-generation-inference</code> wordt gebruikt. Dit stelt je in staat om toegang te krijgen tot beveiligde bronnen.</p><p>Als je bijvoorbeeld de afgeschermde Llama V2 modelvarianten wilt bedienen:</p><ul><li>Ga naar https://huggingface.co/settings/tokens</li>
<li>Kopieer je CLI READ-token</li>
<li>Exporteer <code>HF_TOKEN=<jouw CLI READ-token></code></li></p><p></ul>of met Docker:</p><pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # deel een volume met de Docker-container om te voorkomen dat gewichten bij elke run worden gedownload
token=<jouw cli READ-token></p><p>docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><h3>Een opmerking over gedeeld geheugen (shm)</h3></p><p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html" target="_blank" rel="noopener noreferrer"><code>NCCL</code></a> is een communicatieframework dat door
<code>PyTorch</code> wordt gebruikt voor gedistribueerde training/inference. <code>text-generation-inference</code> maakt
gebruik van <code>NCCL</code> om Tensor Parallelisme mogelijk te maken, waardoor inference voor grote taalmodellen aanzienlijk wordt versneld.</p><p>Om data te delen tussen de verschillende apparaten van een <code>NCCL</code> groep, kan <code>NCCL</code> terugvallen op het gebruik van het hostgeheugen als
peer-to-peer via NVLink of PCI niet mogelijk is.</p><p>Om de container 1G gedeeld geheugen te laten gebruiken en SHM-sharing te ondersteunen, voegen we <code>--shm-size 1g</code> toe aan het bovenstaande commando.</p><p>Als je <code>text-generation-inference</code> binnen <code>Kubernetes</code> draait, kun je gedeeld geheugen aan de container toevoegen door
een volume te maken met:</p><pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi</code></pre></p><p>en dit te mounten op <code>/dev/shm</code>.</p><p>Tot slot kun je SHM-sharing ook uitschakelen door de omgevingsvariabele <code>NCCL_SHM_DISABLE=1</code> te gebruiken. Houd er echter rekening mee dat
dit de prestaties zal beïnvloeden.</p><h3>Gedistribueerde tracing</h3></p><p><code>text-generation-inference</code> is geïnstrumenteerd met gedistribueerde tracing via OpenTelemetry. Je kunt deze functie
gebruiken door het adres van een OTLP-collector in te stellen met het argument <code>--otlp-endpoint</code>. De standaard servicenaam kan worden
overschreven met het argument <code>--otlp-service-name</code>.</p><h3>Architectuur</h3></p><p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="TGI architectuur"></p><p>Uitgebreide blogpost door Adyen over de interne werking van TGI: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi" target="_blank" rel="noopener noreferrer">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p><h3>Lokale installatie</h3></p><p>Je kunt er ook voor kiezen om <code>text-generation-inference</code> lokaal te installeren.</p><p>Clone eerst de repository en ga ernaartoe:
<pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference</code></pre></p><p>Installeer vervolgens <a href="https://rustup.rs/" target="_blank" rel="noopener noreferrer">Rust</a> en maak een Python virtuele omgeving aan met ten minste
Python 3.9, bijvoorbeeld met <code>conda</code> of <code>python venv</code>:</p><pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</p><h1>met conda</h1>
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference</p><h1>met python venv</h1>
python3 -m venv .venv
source .venv/bin/activate</code></pre></p><p>Mogelijk moet je ook Protoc installeren.</p><p>Op Linux:</p><pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP</code></pre></p><p>Op MacOS, met Homebrew:</p><pre><code class="language-shell">brew install protobuf</code></pre></p><p>Voer daarna uit:</p><pre><code class="language-shell">BUILD_EXTENSIONS=True make install # Installeer repository en HF/transformers fork met CUDA kernels
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><p><strong>Opmerking:</strong> op sommige systemen heb je mogelijk ook de OpenSSL libraries en gcc nodig. Op Linux systemen, voer uit:</p><pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y</code></pre></p><h3>Lokale installatie (Nix)</h3></p><p>Een andere optie is om <code>text-generation-inference</code> lokaal te installeren met <a href="https://nixos.org" target="_blank" rel="noopener noreferrer">Nix</a>. Momenteel
ondersteunen we alleen Nix op x86_64 Linux met CUDA GPU's. Bij gebruik van Nix kunnen alle dependencies
worden gedownload uit een binaire cache, waardoor lokaal bouwen niet nodig is.</p><p>Volg eerst de instructies om <a href="https://app.cachix.org/cache/huggingface" target="_blank" rel="noopener noreferrer">Cachix te installeren en de Hugging Face cache in te schakelen</a>.
Het instellen van de cache is belangrijk, anders zal Nix veel dependencies lokaal bouwen,
wat uren kan duren.</p><p>Daarna kun je TGI starten met <code>nix run</code>:</p><pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct</code></pre></p><p><strong>Opmerking:</strong> wanneer je Nix gebruikt op een niet-NixOS systeem, moet je <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library" target="_blank" rel="noopener noreferrer">enkele symlinks aanmaken</a>
om de CUDA driver libraries zichtbaar te maken voor Nix pakketten.</p><p>Voor TGI ontwikkeling kun je de <code>impure</code> dev shell gebruiken:</p><pre><code class="language-shell">nix develop .#impure</p><h1>Alleen nodig de eerste keer dat de devshell wordt gestart of na update van protobuf.</h1>
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name "<em>.py" -print0 -exec sed -i -e 's/^\(import.</em>pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)</code></pre></p><p>Alle ontwikkel-dependencies (cargo, Python, Torch), etc. zijn beschikbaar in deze
dev shell.</p><h2>Geoptimaliseerde architecturen</h2></p><p>TGI werkt direct out-of-the-box om geoptimaliseerde modellen te serveren voor alle moderne modellen. Deze zijn te vinden in <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">deze lijst</a>.</p><p>Andere architecturen worden op een best-effort basis ondersteund met:</p><p><code>AutoModelForCausalLM.from_pretrained(<model>, device_map="auto")</code></p><p>of</p><p><code>AutoModelForSeq2SeqLM.from_pretrained(<model>, device_map="auto")</code></p><h2>Lokale uitvoering</h2></p><h3>Run</h3></p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><h3>Kwantisatie</h3></p><p>Je kunt ook vooraf gekwantiseerde gewichten draaien (AWQ, GPTQ, Marlin) of gewichten on-the-fly kwantiseren met bitsandbytes, EETQ, fp8, om het VRAM-verbruik te verlagen:</p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize</code></pre></p><p>4bit kwantisatie is beschikbaar met de <a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank" rel="noopener noreferrer">NF4 en FP4 datatypes van bitsandbytes</a>. Dit kan worden ingeschakeld door <code>--quantize bitsandbytes-nf4</code> of <code>--quantize bitsandbytes-fp4</code> mee te geven als commandoregelargument aan <code>text-generation-launcher</code>.</p><p>Lees meer over kwantisatie in de <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization" target="_blank" rel="noopener noreferrer">Kwantisatie documentatie</a>.</p><h2>Ontwikkeling</h2></p><pre><code class="language-shell">make server-dev
make router-dev</code></pre></p><h2>Testen</h2></p><pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
<h1>of zowel server- als client-tests</h1>
make python-tests
<h1>rust cargo tests</h1>
make rust-tests
<h1>integratietests</h1>
make integration-tests</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>