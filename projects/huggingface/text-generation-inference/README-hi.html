<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - Read text-generation-inference documentation in Hindi. This project has 10206 stars on GitHub.</title>
    <meta name="description" content="Read text-generation-inference documentation in Hindi. This project has 10206 stars on GitHub.">
    <meta name="keywords" content="text-generation-inference, Hindi, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "text-generation-inference",
  "description": "Read text-generation-inference documentation in Hindi. This project has 10206 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "huggingface"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 10206
  },
  "url": "https://OpenAiTx.github.io/projects/huggingface/text-generation-inference/README-hi.html",
  "sameAs": "https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    text-generation-inference
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 10206 stars</span>
                <span class="language">Hindi</span>
                <span>by huggingface</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center"></p><p><a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/assets/thumbnail.png">
</a></p><h1>टेक्स्ट जनरेशन इन्फेरेंस</h1></p><p><a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API documentation" src="https://img.shields.io/badge/API-Swagger-informational">
</a></p><p>टेक्स्ट जनरेशन इन्फेरेंस के लिए एक रस्ट, पायथन और gRPC सर्वर। <a href="https://huggingface.co" target="_blank" rel="noopener noreferrer">Hugging Face</a> में उत्पादन में उपयोग किया जाता है
Hugging Chat, Inference API और Inference Endpoints को पावर देने के लिए।</p><p></div></p><h2>सामग्री तालिका</h2></p><ul><li><a href="#get-started" target="_blank" rel="noopener noreferrer">शुरू करें</a></li>
    <li><a href="#docker" target="_blank" rel="noopener noreferrer">डॉकर</a></li>
    <li><a href="#api-documentation" target="_blank" rel="noopener noreferrer">API दस्तावेज़</a></li>
    <li><a href="#using-a-private-or-gated-model" target="_blank" rel="noopener noreferrer">प्राइवेट या गेटेड मॉडल का उपयोग</a></li>
    <li><a href="#a-note-on-shared-memory-shm" target="_blank" rel="noopener noreferrer">शेयरड मेमोरी (shm) पर एक नोट</a></li>
    <li><a href="#distributed-tracing" target="_blank" rel="noopener noreferrer">डिस्ट्रिब्यूटेड ट्रेसिंग</a></li>
    <li><a href="#architecture" target="_blank" rel="noopener noreferrer">आर्किटेक्चर</a></li>
    <li><a href="#local-install" target="_blank" rel="noopener noreferrer">लोकल इंस्टॉल</a></li>
    <li><a href="#local-install-nix" target="_blank" rel="noopener noreferrer">लोकल इंस्टॉल (Nix)</a></li>
  <li><a href="#optimized-architectures" target="_blank" rel="noopener noreferrer">ऑप्टिमाइज़्ड आर्किटेक्चर</a></li>
  <li><a href="#run-locally" target="_blank" rel="noopener noreferrer">लोकल रूप से चलाएं</a></li>
    <li><a href="#run" target="_blank" rel="noopener noreferrer">चलाएं</a></li>
    <li><a href="#quantization" target="_blank" rel="noopener noreferrer">क्वांटाइजेशन</a></li>
  <li><a href="#develop" target="_blank" rel="noopener noreferrer">डेवलप करें</a></li>
  <li><a href="#testing" target="_blank" rel="noopener noreferrer">टेस्टिंग</a></li></p><p></ul>टेक्स्ट जनरेशन इन्फेरेंस (TGI) बड़े भाषा मॉडल (LLMs) को डिप्लॉय और सर्व करने के लिए एक टूलकिट है। TGI लोकप्रिय ओपन-सोर्स LLMs के लिए उच्च प्रदर्शन वाले टेक्स्ट जनरेशन को सक्षम करता है, जिसमें Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, और <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">और भी</a> शामिल हैं। TGI कई फीचर्स को लागू करता है, जैसे:</p><ul><li>सबसे लोकप्रिय LLMs को सर्व करने के लिए सरल लॉन्चर</li>
<li>प्रोडक्शन रेडी (Open Telemetry के साथ डिस्ट्रिब्यूटेड ट्रेसिंग, Prometheus मेट्रिक्स)</li>
<li>कई GPUs पर तेज़ इन्फेरेंस के लिए टेन्सर पैरेललिज्म</li>
<li>सर्वर-सेंट इवेंट्स (SSE) का उपयोग करते हुए टोकन स्ट्रीमिंग</li>
<li>कुल थ्रूपुट बढ़ाने के लिए आने वाले अनुरोधों का सतत बैचिंग</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">Messages API</a> जो Open AI Chat Completion API के साथ संगत है</li>
<li>सबसे लोकप्रिय आर्किटेक्चर पर <a href="https://github.com/HazyResearch/flash-attention" target="_blank" rel="noopener noreferrer">Flash Attention</a> और <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">Paged Attention</a> का उपयोग करके इन्फेरेंस के लिए ऑप्टिमाइज़्ड ट्रांसफॉर्मर्स कोड</li>
<li>क्वांटाइजेशन :</li>
  <li><a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">bitsandbytes</a></li>
  <li><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPT-Q</a></li>
  <li><a href="https://github.com/NetEase-FuXi/EETQ" target="_blank" rel="noopener noreferrer">EETQ</a></li>
  <li><a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener noreferrer">AWQ</a></li>
  <li><a href="https://github.com/IST-DASLab/marlin" target="_blank" rel="noopener noreferrer">Marlin</a></li>
  <li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/" target="_blank" rel="noopener noreferrer">fp8</a></li>
<li><a href="https://github.com/huggingface/safetensors" target="_blank" rel="noopener noreferrer">Safetensors</a> वेट लोडिंग</li>
<li><a href="https://arxiv.org/abs/2301.10226" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a> के साथ वॉटरमार्किंग</li>
<li>लॉजिट्स वार्पर (टेम्परेचर स्केलिंग, टॉप-p, टॉप-k, रिपीटेशन पेनल्टी, अधिक विवरण देखें <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor" target="_blank" rel="noopener noreferrer">transformers.LogitsProcessor</a>)</li>
<li>स्टॉप सीक्वेंस</li>
<li>लॉग प्रॉबेबिलिटीज़</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation" target="_blank" rel="noopener noreferrer">स्पेकुलेशन</a> ~2x लैटेंसी</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance" target="_blank" rel="noopener noreferrer">Guidance/JSON</a>। आउटपुट फॉर्मेट निर्दिष्ट करें ताकि इन्फेरेंस तेज़ हो और आउटपुट कुछ स्पेक्स के अनुसार वैध हो।</li>
<li>कस्टम प्रॉम्प्ट जनरेशन: मॉडल के आउटपुट को निर्देशित करने के लिए कस्टम प्रॉम्प्ट प्रदान करके आसानी से टेक्स्ट जनरेट करें</li>
<li>फाइन-ट्यूनिंग सपोर्ट: विशिष्ट कार्यों के लिए फाइन-ट्यून किए गए मॉडल का उपयोग करके उच्च सटीकता और प्रदर्शन प्राप्त करें</li></p><p></ul><h3>हार्डवेयर सपोर्ट</h3></p><ul><li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference" target="_blank" rel="noopener noreferrer">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475" target="_blank" rel="noopener noreferrer">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi" target="_blank" rel="noopener noreferrer">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving" target="_blank" rel="noopener noreferrer">Google TPU</a></li></p><p>
</ul><h2>शुरू करें</h2></p><h3>डॉकर</h3></p><p>विस्तृत प्रारंभिक मार्गदर्शिका के लिए कृपया <a href="https://huggingface.co/docs/text-generation-inference/quicktour" target="_blank" rel="noopener noreferrer">Quick Tour</a> देखें। शुरू करने का सबसे आसान तरीका आधिकारिक डॉकर कंटेनर का उपयोग करना है:</p><pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
<h1>प्रत्येक रन में वेट्स डाउनलोड करने से बचने के लिए डॉकर कंटेनर के साथ एक वॉल्यूम साझा करें</h1>
volume=$PWD/data</p><p>docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><p>और फिर आप ऐसे अनुरोध कर सकते हैं</p><pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{"inputs":"डीप लर्निंग क्या है?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'</code></pre></p><p>आप Open AI Chat Completion API संगत प्रतिक्रियाएं प्राप्त करने के लिए <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">TGI के Messages API</a> का भी उपयोग कर सकते हैं।</p><pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "आप एक सहायक सहायक हैं।"
    },
    {
      "role": "user",
      "content": "डीप लर्निंग क्या है?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}' \
    -H 'Content-Type: application/json'</code></pre></p><p><strong>नोट:</strong> NVIDIA GPUs का उपयोग करने के लिए, आपको <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="noopener noreferrer">NVIDIA Container Toolkit</a> स्थापित करना होगा। हम CUDA संस्करण 12.2 या उससे उच्च के साथ NVIDIA ड्राइवरों का उपयोग करने की भी सलाह देते हैं। बिना GPU या CUDA समर्थन वाले मशीन पर Docker कंटेनर चलाने के लिए, <code>--gpus all</code> फ्लैग हटाना और <code>--disable-custom-kernels</code> जोड़ना पर्याप्त है, कृपया ध्यान दें कि CPU इस प्रोजेक्ट के लिए इरादे वाला प्लेटफ़ॉर्म नहीं है, इसलिए प्रदर्शन कम हो सकता है।</p><p><strong>नोट:</strong> TGI AMD Instinct MI210 और MI250 GPUs का समर्थन करता है। विवरण <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus" target="_blank" rel="noopener noreferrer">Supported Hardware documentation</a> में पाया जा सकता है। AMD GPUs का उपयोग करने के लिए, कृपया ऊपर के कमांड के बजाय <code>docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code> उपयोग करें।</p><p>अपने मॉडलों को सर्व करने के सभी विकल्प देखने के लिए (कोड में <a href="https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs" target="_blank" rel="noopener noreferrer">यहाँ</a> या CLI में):
<pre><code class="language-">text-generation-launcher --help</code></pre></p><h3>API दस्तावेज़</h3></p><p>आप <code>/docs</code> मार्ग का उपयोग करके <code>text-generation-inference</code> REST API का OpenAPI दस्तावेज़ देख सकते हैं।
Swagger UI भी उपलब्ध है: <a href="https://huggingface.github.io/text-generation-inference" target="_blank" rel="noopener noreferrer">https://huggingface.github.io/text-generation-inference</a>।</p><h3>प्राइवेट या गेटेड मॉडल का उपयोग</h3></p><p><code>text-generation-inference</code> द्वारा उपयोग किए जाने वाले टोकन को कॉन्फ़िगर करने के लिए आपके पास <code>HF_TOKEN</code> पर्यावरण चर का उपयोग करने का विकल्प है। यह आपको संरक्षित संसाधनों तक पहुंच प्रदान करता है।</p><p>उदाहरण के लिए, यदि आप गेटेड Llama V2 मॉडल वेरिएंट सर्व करना चाहते हैं:</p><ul><li>https://huggingface.co/settings/tokens पर जाएँ</li>
<li>अपनी CLI READ टोकन कॉपी करें</li>
<li><code>HF_TOKEN=<आपकी CLI READ टोकन></code> निर्यात करें</li></p><p></ul>या डॉकर के साथ:</p><pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # प्रत्येक रन में वेट्स डाउनलोड करने से बचने के लिए डॉकर कंटेनर के साथ वॉल्यूम साझा करें
token=<आपकी cli READ टोकन></p><p>docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><h3>शेयरड मेमोरी (shm) पर एक नोट</h3></p><p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html" target="_blank" rel="noopener noreferrer"><code>NCCL</code></a> एक संचार फ्रेमवर्क है जिसका उपयोग
<code>PyTorch</code> वितरित प्रशिक्षण/इन्फेरेंस के लिए करता है। <code>text-generation-inference</code> 
<code>NCCL</code> का उपयोग टेन्सर पैरेललिज्म को सक्षम करने के लिए करता है ताकि बड़े भाषा मॉडलों के लिए इन्फेरेंस को नाटकीय रूप से तेज़ किया जा सके।</p><p><code>NCCL</code> समूह के विभिन्न उपकरणों के बीच डेटा साझा करने के लिए, यदि NVLink या PCI का उपयोग करते हुए पीयर-टू-पीयर संभव नहीं है, तो <code>NCCL</code> होस्ट मेमोरी का उपयोग कर सकता है।</p><p>कंटेनर को 1G शेयरड मेमोरी का उपयोग करने की अनुमति देने और SHM शेयरिंग का समर्थन करने के लिए, ऊपर के कमांड में <code>--shm-size 1g</code> जोड़ा गया है।</p><p>यदि आप <code>text-generation-inference</code> को <code>Kubernetes</code> के अंदर चला रहे हैं, तो आप कंटेनर में शेयरड मेमोरी जोड़ सकते हैं
एक वॉल्यूम बनाकर:</p><pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi</code></pre></p><p>और इसे <code>/dev/shm</code> पर माउंट करें।</p><p>अंत में, आप <code>NCCL_SHM_DISABLE=1</code> पर्यावरण चर का उपयोग करके SHM शेयरिंग को भी अक्षम कर सकते हैं। हालांकि, ध्यान दें कि इससे प्रदर्शन प्रभावित होगा।</p><h3>डिस्ट्रिब्यूटेड ट्रेसिंग</h3></p><p><code>text-generation-inference</code> OpenTelemetry का उपयोग करके डिस्ट्रिब्यूटेड ट्रेसिंग के साथ इंस्ट्रूमेंट किया गया है। आप इस फीचर का उपयोग <code>--otlp-endpoint</code> आर्गुमेंट के साथ OTLP कलेक्टर के पते को सेट करके कर सकते हैं। डिफ़ॉल्ट सेवा नाम को <code>--otlp-service-name</code> आर्गुमेंट के साथ ओवरराइड किया जा सकता है।</p><h3>आर्किटेक्चर</h3></p><p><img src="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/docs/images/TGI.png" alt="TGI architecture"></p><p>TGI के अंदरूनी कामकाज पर Adyen द्वारा विस्तृत ब्लॉगपोस्ट: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi" target="_blank" rel="noopener noreferrer">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p><h3>लोकल इंस्टॉल</h3></p><p>आप <code>text-generation-inference</code> को लोकल रूप से भी इंस्टॉल कर सकते हैं।</p><p>सबसे पहले रिपॉजिटरी क्लोन करें और उसमें डिरेक्टरी बदलें:
<pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference</code></pre></p><p>फिर <a href="https://rustup.rs/" target="_blank" rel="noopener noreferrer">Rust इंस्टॉल करें</a> और कम से कम Python 3.9 के साथ एक Python वर्चुअल वातावरण बनाएं, जैसे <code>conda</code> या <code>python venv</code> का उपयोग करके:</p><pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</p><h1>conda का उपयोग करते हुए</h1>
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference</p><h1>python venv का उपयोग करते हुए</h1>
python3 -m venv .venv
source .venv/bin/activate</code></pre></p><p>आपको Protoc भी इंस्टॉल करना पड़ सकता है।</p><p>Linux पर:</p><pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP</code></pre></p><p>MacOS पर, Homebrew का उपयोग करते हुए:</p><pre><code class="language-shell">brew install protobuf</code></pre></p><p>फिर चलाएं:</p><pre><code class="language-shell">BUILD_EXTENSIONS=True make install # रिपॉजिटरी और HF/transformer फोर्क को CUDA कर्नेल्स के साथ इंस्टॉल करें
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><p><strong>नोट:</strong> कुछ मशीनों पर आपको OpenSSL लाइब्रेरी और gcc की आवश्यकता हो सकती है। Linux मशीनों पर चलाएं:</p><pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y</code></pre></p><h3>स्थानीय इंस्टाल (Nix)</h3></p><p>एक अन्य विकल्प है <code>text-generation-inference</code> को स्थानीय रूप से <a href="https://nixos.org" target="_blank" rel="noopener noreferrer">Nix</a> का उपयोग करके इंस्टॉल करना। वर्तमान में,
हम केवल x86_64 Linux पर CUDA GPUs के साथ Nix का समर्थन करते हैं। Nix का उपयोग करते समय, सभी निर्भरताएँ
बाइनरी कैश से खींची जा सकती हैं, जिससे उन्हें स्थानीय रूप से बनाने की आवश्यकता समाप्त हो जाती है।</p><p>सबसे पहले <a href="https://app.cachix.org/cache/huggingface" target="_blank" rel="noopener noreferrer">Cachix इंस्टॉल करें और Hugging Face कैश सक्षम करें</a>।
कैश सेटअप करना महत्वपूर्ण है, अन्यथा Nix कई निर्भरताओं को स्थानीय रूप से बनाएगा,
जिसमें कई घंटे लग सकते हैं।</p><p>इसके बाद आप TGI को <code>nix run</code> के साथ चला सकते हैं:</p><pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct</code></pre></p><p><strong>नोट:</strong> जब आप गैर-NixOS सिस्टम पर Nix का उपयोग कर रहे हों, तो आपको CUDA ड्राइवर लाइब्रेरीज़ को
Nix पैकेजों के लिए दिखाई देने योग्य बनाने के लिए <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library" target="_blank" rel="noopener noreferrer">कुछ सिमलिंक्स बनानी होती हैं</a>।</p><p>TGI विकास के लिए, आप <code>impure</code> dev शेल का उपयोग कर सकते हैं:</p><pre><code class="language-shell">nix develop .#impure</p><h1>केवल पहली बार devshell शुरू करने पर या protobuf अपडेट करने के बाद आवश्यक है।</h1>
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name "<em>.py" -print0 -exec sed -i -e 's/^\(import.</em>pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)</code></pre></p><p>सभी विकास निर्भरताएँ (cargo, Python, Torch), आदि इस
dev शेल में उपलब्ध हैं।</p><h2>अनुकूलित आर्किटेक्चर</h2></p><p>TGI सभी आधुनिक मॉडलों के लिए अनुकूलित मॉडल सेवा प्रदान करने के लिए तुरंत काम करता है। ये <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">इस सूची</a> में पाए जा सकते हैं।</p><p>अन्य आर्किटेक्चर को सर्वोत्तम प्रयास के आधार पर समर्थन दिया जाता है:</p><p><code>AutoModelForCausalLM.from_pretrained(<model>, device_map="auto")</code></p><p>या</p><p><code>AutoModelForSeq2SeqLM.from_pretrained(<model>, device_map="auto")</code></p><h2>स्थानीय रूप से चलाएँ</h2></p><h3>चलाएँ</h3></p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><h3>क्वांटाइजेशन</h3></p><p>आप पूर्व-क्वांटाइज्ड वेट्स (AWQ, GPTQ, Marlin) भी चला सकते हैं या bitsandbytes, EETQ, fp8 के साथ ऑन-द-फ्लाई क्वांटाइजेशन कर सकते हैं, ताकि VRAM की आवश्यकता कम हो:</p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize</code></pre></p><p>4bit क्वांटाइजेशन <a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank" rel="noopener noreferrer">bitsandbytes के NF4 और FP4 डेटा टाइप</a> का उपयोग करके उपलब्ध है। इसे <code>text-generation-launcher</code> को कमांड लाइन आर्गुमेंट के रूप में <code>--quantize bitsandbytes-nf4</code> या <code>--quantize bitsandbytes-fp4</code> देकर सक्षम किया जा सकता है।</p><p>क्वांटाइजेशन के बारे में अधिक पढ़ें <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization" target="_blank" rel="noopener noreferrer">क्वांटाइजेशन दस्तावेज़</a> में।</p><h2>विकास करें</h2></p><pre><code class="language-shell">make server-dev
make router-dev</code></pre></p><h2>परीक्षण</h2></p><pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
<h1>या सर्वर और क्लाइंट दोनों के टेस्ट</h1>
make python-tests
<h1>rust cargo टेस्ट</h1>
make rust-tests
<h1>इंटीग्रेशन टेस्ट</h1>
make integration-tests</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>