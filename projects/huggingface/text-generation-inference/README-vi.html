<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - Read text-generation-inference documentation in Vietnamese. This project has 10206 stars on GitHub.</title>
    <meta name="description" content="Read text-generation-inference documentation in Vietnamese. This project has 10206 stars on GitHub.">
    <meta name="keywords" content="text-generation-inference, Vietnamese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "text-generation-inference",
  "description": "Read text-generation-inference documentation in Vietnamese. This project has 10206 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "huggingface"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 10206
  },
  "url": "https://OpenAiTx.github.io/projects/huggingface/text-generation-inference/README-vi.html",
  "sameAs": "https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    text-generation-inference
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 10206 stars</span>
                <span class="language">Vietnamese</span>
                <span>by huggingface</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center"></p><p><a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png">
</a></p><h1>Text Generation Inference</h1></p><p><a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API documentation" src="https://img.shields.io/badge/API-Swagger-informational">
</a></p><p>Một server Rust, Python và gRPC cho suy luận sinh văn bản. Được sử dụng trong sản xuất tại <a href="https://huggingface.co" target="_blank" rel="noopener noreferrer">Hugging Face</a>
để cung cấp năng lượng cho Hugging Chat, API Suy luận và Các Điểm cuối Suy luận.</p><p></div></p><h2>Mục lục</h2></p><ul><li><a href="#get-started" target="_blank" rel="noopener noreferrer">Bắt đầu</a></li>
    <li><a href="#docker" target="_blank" rel="noopener noreferrer">Docker</a></li>
    <li><a href="#api-documentation" target="_blank" rel="noopener noreferrer">Tài liệu API</a></li>
    <li><a href="#using-a-private-or-gated-model" target="_blank" rel="noopener noreferrer">Sử dụng mô hình riêng tư hoặc giới hạn</a></li>
    <li><a href="#a-note-on-shared-memory-shm" target="_blank" rel="noopener noreferrer">Ghi chú về Bộ nhớ Chia sẻ (shm)</a></li>
    <li><a href="#distributed-tracing" target="_blank" rel="noopener noreferrer">Theo dõi Phân tán</a></li>
    <li><a href="#architecture" target="_blank" rel="noopener noreferrer">Kiến trúc</a></li>
    <li><a href="#local-install" target="_blank" rel="noopener noreferrer">Cài đặt cục bộ</a></li>
    <li><a href="#local-install-nix" target="_blank" rel="noopener noreferrer">Cài đặt cục bộ (Nix)</a></li>
  <li><a href="#optimized-architectures" target="_blank" rel="noopener noreferrer">Kiến trúc tối ưu hóa</a></li>
  <li><a href="#run-locally" target="_blank" rel="noopener noreferrer">Chạy cục bộ</a></li>
    <li><a href="#run" target="_blank" rel="noopener noreferrer">Chạy</a></li>
    <li><a href="#quantization" target="_blank" rel="noopener noreferrer">Lượng tử hóa</a></li>
  <li><a href="#develop" target="_blank" rel="noopener noreferrer">Phát triển</a></li>
  <li><a href="#testing" target="_blank" rel="noopener noreferrer">Kiểm thử</a></li></p><p></ul>Text Generation Inference (TGI) là một bộ công cụ để triển khai và phục vụ các Mô hình Ngôn ngữ Lớn (LLMs). TGI cho phép sinh văn bản hiệu suất cao cho các LLM mã nguồn mở phổ biến nhất, bao gồm Llama, Falcon, StarCoder, BLOOM, GPT-NeoX và <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">nhiều hơn nữa</a>. TGI triển khai nhiều tính năng, như:</p><ul><li>Trình khởi chạy đơn giản để phục vụ hầu hết các LLM phổ biến</li>
<li>Sẵn sàng cho sản xuất (theo dõi phân tán với Open Telemetry, số liệu Prometheus)</li>
<li>Song song Tensor để suy luận nhanh hơn trên nhiều GPU</li>
<li>Phát trực tiếp token sử dụng Server-Sent Events (SSE)</li>
<li>Gom lô yêu cầu liên tục để tăng tổng thông lượng</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">Messages API</a> tương thích với Open AI Chat Completion API</li>
<li>Mã transformers tối ưu cho suy luận sử dụng <a href="https://github.com/HazyResearch/flash-attention" target="_blank" rel="noopener noreferrer">Flash Attention</a> và <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">Paged Attention</a> trên các kiến trúc phổ biến nhất</li>
<li>Lượng tử hóa với:</li>
  <li><a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">bitsandbytes</a></li>
  <li><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPT-Q</a></li>
  <li><a href="https://github.com/NetEase-FuXi/EETQ" target="_blank" rel="noopener noreferrer">EETQ</a></li>
  <li><a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener noreferrer">AWQ</a></li>
  <li><a href="https://github.com/IST-DASLab/marlin" target="_blank" rel="noopener noreferrer">Marlin</a></li>
  <li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/" target="_blank" rel="noopener noreferrer">fp8</a></li>
<li>Tải trọng số bằng <a href="https://github.com/huggingface/safetensors" target="_blank" rel="noopener noreferrer">Safetensors</a></li>
<li>Đóng dấu bản quyền với <a href="https://arxiv.org/abs/2301.10226" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a></li>
<li>Bộ biến đổi logits (điều chỉnh nhiệt độ, top-p, top-k, phạt lặp lại, xem thêm chi tiết tại <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor" target="_blank" rel="noopener noreferrer">transformers.LogitsProcessor</a>)</li>
<li>Dừng chuỗi</li>
<li>Xác suất log</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation" target="_blank" rel="noopener noreferrer">Dự đoán</a> giảm độ trễ khoảng 2 lần</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance" target="_blank" rel="noopener noreferrer">Hướng dẫn/JSON</a>. Chỉ định định dạng đầu ra để tăng tốc suy luận và đảm bảo đầu ra hợp lệ theo một số tiêu chuẩn.</li>
<li>Tạo Prompt tùy chỉnh: Dễ dàng tạo văn bản bằng cách cung cấp prompt tùy chỉnh để hướng dẫn đầu ra của mô hình</li>
<li>Hỗ trợ tinh chỉnh: Sử dụng các mô hình đã được tinh chỉnh cho các tác vụ cụ thể để đạt độ chính xác và hiệu suất cao hơn</li></p><p></ul><h3>Hỗ trợ phần cứng</h3></p><ul><li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference" target="_blank" rel="noopener noreferrer">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475" target="_blank" rel="noopener noreferrer">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi" target="_blank" rel="noopener noreferrer">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving" target="_blank" rel="noopener noreferrer">Google TPU</a></li></p><p>
</ul><h2>Bắt đầu</h2></p><h3>Docker</h3></p><p>Để có hướng dẫn khởi đầu chi tiết, vui lòng xem <a href="https://huggingface.co/docs/text-generation-inference/quicktour" target="_blank" rel="noopener noreferrer">Quick Tour</a>. Cách dễ nhất để bắt đầu là sử dụng container Docker chính thức:</p><pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
<h1>chia sẻ một volume với container Docker để tránh tải trọng số mỗi lần chạy</h1>
volume=$PWD/data</p><p>docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><p>Và sau đó bạn có thể gửi yêu cầu như</p><pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'</code></pre></p><p>Bạn cũng có thể sử dụng <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">Messages API của TGI</a> để nhận phản hồi tương thích với Open AI Chat Completion API.</p><pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "What is deep learning?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}' \
    -H 'Content-Type: application/json'</code></pre></p><p><strong>Lưu ý:</strong> Để sử dụng GPU NVIDIA, bạn cần cài đặt <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="noopener noreferrer">NVIDIA Container Toolkit</a>. Chúng tôi cũng khuyến nghị sử dụng driver NVIDIA với phiên bản CUDA 12.2 trở lên. Để chạy container Docker trên máy không có GPU hoặc không hỗ trợ CUDA, chỉ cần bỏ flag <code>--gpus all</code> và thêm <code>--disable-custom-kernels</code>, lưu ý CPU không phải nền tảng dự kiến cho dự án này nên hiệu năng có thể không cao.</p><p><strong>Lưu ý:</strong> TGI hỗ trợ GPU AMD Instinct MI210 và MI250. Chi tiết có trong <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus" target="_blank" rel="noopener noreferrer">Tài liệu Phần cứng Hỗ trợ</a>. Để sử dụng GPU AMD, vui lòng dùng lệnh <code>docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code> thay cho lệnh trên.</p><p>Để xem tất cả các tùy chọn phục vụ mô hình của bạn (trong <a href="https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs" target="_blank" rel="noopener noreferrer">mã nguồn</a> hoặc trong cli):
<pre><code class="language-">text-generation-launcher --help</code></pre></p><h3>Tài liệu API</h3></p><p>Bạn có thể tham khảo tài liệu OpenAPI của REST API <code>text-generation-inference</code> thông qua đường dẫn <code>/docs</code>.
Giao diện Swagger UI cũng có sẵn tại: <a href="https://huggingface.github.io/text-generation-inference" target="_blank" rel="noopener noreferrer">https://huggingface.github.io/text-generation-inference</a>.</p><h3>Sử dụng mô hình riêng tư hoặc giới hạn</h3></p><p>Bạn có thể sử dụng biến môi trường <code>HF_TOKEN</code> để cấu hình token được <code>text-generation-inference</code> sử dụng. Điều này cho phép truy cập tài nguyên được bảo vệ.</p><p>Ví dụ, nếu bạn muốn phục vụ các biến thể mô hình Llama V2 giới hạn:</p><ul><li>Truy cập https://huggingface.co/settings/tokens</li>
<li>Sao chép token CLI READ của bạn</li>
<li>Xuất <code>HF_TOKEN=<token CLI READ của bạn></code></li></p><p></ul>hoặc với Docker:</p><pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # chia sẻ volume với container Docker để tránh tải trọng số mỗi lần chạy
token=<token cli READ của bạn></p><p>docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><h3>Ghi chú về Bộ nhớ Chia sẻ (shm)</h3></p><p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html" target="_blank" rel="noopener noreferrer"><code>NCCL</code></a> là một framework giao tiếp được <code>PyTorch</code> sử dụng để huấn luyện/suy luận phân tán. <code>text-generation-inference</code> sử dụng <code>NCCL</code> để kích hoạt Tensor Parallelism giúp tăng tốc suy luận cho các mô hình ngôn ngữ lớn.</p><p>Để chia sẻ dữ liệu giữa các thiết bị trong nhóm <code>NCCL</code>, <code>NCCL</code> có thể dựa vào bộ nhớ host nếu không thể giao tiếp peer-to-peer qua NVLink hoặc PCI.</p><p>Để cho phép container sử dụng 1G Bộ nhớ Chia sẻ và hỗ trợ chia sẻ SHM, chúng tôi thêm <code>--shm-size 1g</code> vào lệnh trên.</p><p>Nếu bạn chạy <code>text-generation-inference</code> trong <code>Kubernetes</code>, bạn cũng có thể thêm Bộ nhớ Chia sẻ vào container bằng cách tạo volume:</p><pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi</code></pre></p><p>và mount nó vào <code>/dev/shm</code>.</p><p>Cuối cùng, bạn cũng có thể tắt chia sẻ SHM bằng biến môi trường <code>NCCL_SHM_DISABLE=1</code>. Tuy nhiên, lưu ý điều này sẽ ảnh hưởng đến hiệu năng.</p><h3>Theo dõi Phân tán</h3></p><p><code>text-generation-inference</code> được tích hợp theo dõi phân tán sử dụng OpenTelemetry. Bạn có thể sử dụng tính năng này bằng cách đặt địa chỉ tới bộ thu OTLP với tham số <code>--otlp-endpoint</code>. Tên dịch vụ mặc định có thể được ghi đè bằng tham số <code>--otlp-service-name</code>.</p><h3>Kiến trúc</h3></p><p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="TGI architecture"></p><p>Bài viết chi tiết của Adyen về hoạt động bên trong TGI: <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi" target="_blank" rel="noopener noreferrer">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p><h3>Cài đặt cục bộ</h3></p><p>Bạn cũng có thể chọn cài đặt <code>text-generation-inference</code> cục bộ.</p><p>Trước tiên, clone repository và chuyển vào thư mục đó:
Dưới đây là bản dịch tài liệu kỹ thuật sang tiếng Việt, giữ nguyên định dạng Markdown và đường dẫn tương đối đã được hoàn chỉnh với https://raw.githubusercontent.com/huggingface/text-generation-inference/main/:</p><pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference</code></pre></p><p>Sau đó <a href="https://rustup.rs/" target="_blank" rel="noopener noreferrer">cài đặt Rust</a> và tạo môi trường ảo Python với ít nhất
Python 3.9, ví dụ sử dụng <code>conda</code> hoặc <code>python venv</code>:</p><pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</p><h1>sử dụng conda</h1>
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference</p><h1>sử dụng python venv</h1>
python3 -m venv .venv
source .venv/bin/activate</code></pre></p><p>Bạn cũng có thể cần cài đặt Protoc.</p><p>Trên Linux:</p><pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP</code></pre></p><p>Trên MacOS, sử dụng Homebrew:</p><pre><code class="language-shell">brew install protobuf</code></pre></p><p>Sau đó chạy:</p><pre><code class="language-shell">BUILD_EXTENSIONS=True make install # Cài đặt kho lưu trữ và bản fork HF/transformer với CUDA kernels
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><p><strong>Lưu ý:</strong> trên một số máy, bạn cũng có thể cần các thư viện OpenSSL và gcc. Trên các máy Linux, chạy:</p><pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y</code></pre></p><h3>Cài đặt cục bộ (Nix)</h3></p><p>Một lựa chọn khác là cài đặt <code>text-generation-inference</code> cục bộ bằng <a href="https://nixos.org" target="_blank" rel="noopener noreferrer">Nix</a>. Hiện tại,
chúng tôi chỉ hỗ trợ Nix trên Linux x86_64 với GPU CUDA. Khi sử dụng Nix, tất cả các phụ thuộc có thể
được lấy từ bộ nhớ đệm nhị phân, loại bỏ nhu cầu xây dựng chúng cục bộ.</p><p>Trước tiên làm theo hướng dẫn để <a href="https://app.cachix.org/cache/huggingface" target="_blank" rel="noopener noreferrer">cài đặt Cachix và kích hoạt bộ nhớ đệm Hugging Face</a>.
Việc thiết lập bộ nhớ đệm rất quan trọng, nếu không Nix sẽ xây dựng nhiều phụ thuộc
tại chỗ, điều này có thể mất nhiều giờ.</p><p>Sau đó bạn có thể chạy TGI với <code>nix run</code>:</p><pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct</code></pre></p><p><strong>Lưu ý:</strong> khi bạn sử dụng Nix trên hệ thống không phải NixOS, bạn phải <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library" target="_blank" rel="noopener noreferrer">tạo một số liên kết tượng trưng</a>
để làm cho các thư viện trình điều khiển CUDA hiển thị với các gói Nix.</p><p>Để phát triển TGI, bạn có thể sử dụng shell phát triển <code>impure</code>:</p><pre><code class="language-shell">nix develop .#impure</p><h1>Chỉ cần thiết lần đầu khởi động devshell hoặc sau khi cập nhật protobuf.</h1>
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name "<em>.py" -print0 -exec sed -i -e 's/^\(import.</em>pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)</code></pre></p><p>Tất cả các phụ thuộc phát triển (cargo, Python, Torch), v.v... đều có trong
shell phát triển này.</p><h2>Kiến trúc tối ưu hóa</h2></p><p>TGI hoạt động ngay lập tức để phục vụ các mô hình tối ưu cho tất cả các mô hình hiện đại. Chúng có thể được tìm thấy trong <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">danh sách này</a>.</p><p>Các kiến trúc khác được hỗ trợ trên cơ sở cố gắng tốt nhất bằng cách sử dụng:</p><p><code>AutoModelForCausalLM.from_pretrained(<model>, device_map="auto")</code></p><p>hoặc</p><p><code>AutoModelForSeq2SeqLM.from_pretrained(<model>, device_map="auto")</code></p><h2>Chạy cục bộ</h2></p><h3>Chạy</h3></p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><h3>Lượng tử hóa</h3></p><p>Bạn cũng có thể chạy trọng số đã được lượng tử hóa trước (AWQ, GPTQ, Marlin) hoặc lượng tử hóa trọng số khi chạy với bitsandbytes, EETQ, fp8, để giảm yêu cầu VRAM:</p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize</code></pre></p><p>Lượng tử 4bit có sẵn sử dụng <a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank" rel="noopener noreferrer">kiểu dữ liệu NF4 và FP4 từ bitsandbytes</a>. Nó có thể được bật bằng cách cung cấp <code>--quantize bitsandbytes-nf4</code> hoặc <code>--quantize bitsandbytes-fp4</code> như một tham số dòng lệnh cho <code>text-generation-launcher</code>.</p><p>Đọc thêm về lượng tử hóa trong <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization" target="_blank" rel="noopener noreferrer">tài liệu Lượng tử hóa</a>.</p><h2>Phát triển</h2></p><pre><code class="language-shell">make server-dev
make router-dev</code></pre></p><h2>Kiểm thử</h2></p><pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
<h1>hoặc kiểm thử cả server và client</h1>
make python-tests
<h1>kiểm thử rust cargo</h1>
make rust-tests
<h1>kiểm thử tích hợp</h1>
make integration-tests</code></pre>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>