<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>text-generation-inference - Read text-generation-inference documentation in French. This project has 10206 stars on GitHub.</title>
    <meta name="description" content="Read text-generation-inference documentation in French. This project has 10206 stars on GitHub.">
    <meta name="keywords" content="text-generation-inference, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "text-generation-inference",
  "description": "Read text-generation-inference documentation in French. This project has 10206 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "huggingface"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 10206
  },
  "url": "https://OpenAiTx.github.io/projects/huggingface/text-generation-inference/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    text-generation-inference
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 10206 stars</span>
                <span class="language">French</span>
                <span>by huggingface</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center"></p><p><a href="https://www.youtube.com/watch?v=jlMAX2Oaht0">
  <img width=560 alt="Making TGI deployment optimal" src="https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png">
</a></p><h1>Inférence de Génération de Texte</h1></p><p><a href="https://github.com/huggingface/text-generation-inference">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social">
</a>
<a href="https://huggingface.github.io/text-generation-inference">
  <img alt="Swagger API documentation" src="https://img.shields.io/badge/API-Swagger-informational">
</a></p><p>Un serveur Rust, Python et gRPC pour l'inférence de génération de texte. Utilisé en production chez <a href="https://huggingface.co" target="_blank" rel="noopener noreferrer">Hugging Face</a>
pour alimenter Hugging Chat, l'API d'Inference et les Endpoints d'Inference.</p><p></div></p><h2>Table des matières</h2></p><ul><li><a href="#get-started" target="_blank" rel="noopener noreferrer">Commencer</a></li>
    <li><a href="#docker" target="_blank" rel="noopener noreferrer">Docker</a></li>
    <li><a href="#api-documentation" target="_blank" rel="noopener noreferrer">Documentation API</a></li>
    <li><a href="#using-a-private-or-gated-model" target="_blank" rel="noopener noreferrer">Utilisation d’un modèle privé ou restreint</a></li>
    <li><a href="#a-note-on-shared-memory-shm" target="_blank" rel="noopener noreferrer">Note sur la mémoire partagée (shm)</a></li>
    <li><a href="#distributed-tracing" target="_blank" rel="noopener noreferrer">Tracing distribué</a></li>
    <li><a href="#architecture" target="_blank" rel="noopener noreferrer">Architecture</a></li>
    <li><a href="#local-install" target="_blank" rel="noopener noreferrer">Installation locale</a></li>
    <li><a href="#local-install-nix" target="_blank" rel="noopener noreferrer">Installation locale (Nix)</a></li>
  <li><a href="#optimized-architectures" target="_blank" rel="noopener noreferrer">Architectures optimisées</a></li>
  <li><a href="#run-locally" target="_blank" rel="noopener noreferrer">Exécution locale</a></li>
    <li><a href="#run" target="_blank" rel="noopener noreferrer">Exécuter</a></li>
    <li><a href="#quantization" target="_blank" rel="noopener noreferrer">Quantification</a></li>
  <li><a href="#develop" target="_blank" rel="noopener noreferrer">Développement</a></li>
  <li><a href="#testing" target="_blank" rel="noopener noreferrer">Tests</a></li></p><p></ul>L'Inférence de Génération de Texte (TGI) est une boîte à outils pour déployer et servir des Modèles de Langage Large (LLMs). TGI permet une génération de texte haute performance pour les LLMs open-source les plus populaires, notamment Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, et <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">plus</a>. TGI implémente de nombreuses fonctionnalités, telles que :</p><ul><li>Lanceur simple pour servir les LLMs les plus populaires</li>
<li>Prêt pour la production (tracing distribué avec OpenTelemetry, métriques Prometheus)</li>
<li>Parallélisme tensoriel pour une inférence plus rapide sur plusieurs GPU</li>
<li>Streaming de tokens via Server-Sent Events (SSE)</li>
<li>Regroupement continu des requêtes entrantes pour augmenter le débit total</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">API Messages</a> compatible avec l'API Open AI Chat Completion</li>
<li>Code transformers optimisé pour l'inférence utilisant <a href="https://github.com/HazyResearch/flash-attention" target="_blank" rel="noopener noreferrer">Flash Attention</a> et <a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">Paged Attention</a> sur les architectures les plus populaires</li>
<li>Quantification avec :</li>
  <li><a href="https://github.com/TimDettmers/bitsandbytes" target="_blank" rel="noopener noreferrer">bitsandbytes</a></li>
  <li><a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="noopener noreferrer">GPT-Q</a></li>
  <li><a href="https://github.com/NetEase-FuXi/EETQ" target="_blank" rel="noopener noreferrer">EETQ</a></li>
  <li><a href="https://github.com/casper-hansen/AutoAWQ" target="_blank" rel="noopener noreferrer">AWQ</a></li>
  <li><a href="https://github.com/IST-DASLab/marlin" target="_blank" rel="noopener noreferrer">Marlin</a></li>
  <li><a href="https://developer.nvidia.com/blog/nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai/" target="_blank" rel="noopener noreferrer">fp8</a></li>
<li>Chargement de poids avec <a href="https://github.com/huggingface/safetensors" target="_blank" rel="noopener noreferrer">Safetensors</a></li>
<li>Filigrane avec <a href="https://arxiv.org/abs/2301.10226" target="_blank" rel="noopener noreferrer">A Watermark for Large Language Models</a></li>
<li>Warper de logits (mise à l’échelle de la température, top-p, top-k, pénalité de répétition, plus de détails dans <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor" target="_blank" rel="noopener noreferrer">transformers.LogitsProcessor</a>)</li>
<li>Séquences d’arrêt</li>
<li>Probabilités logarithmiques</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/speculation" target="_blank" rel="noopener noreferrer">Spéculation</a> ~2x latence</li>
<li><a href="https://huggingface.co/docs/text-generation-inference/conceptual/guidance" target="_blank" rel="noopener noreferrer">Guidance/JSON</a>. Spécifiez le format de sortie pour accélérer l’inférence et garantir que la sortie est valide selon certaines spécifications.</li>
<li>Génération de prompt personnalisée : générez facilement du texte en fournissant des prompts personnalisés pour guider la sortie du modèle</li>
<li>Support du fine-tuning : utilisez des modèles fine-tunés pour des tâches spécifiques afin d’atteindre une meilleure précision et performance</li></p><p></ul><h3>Support matériel</h3></p><ul><li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">Nvidia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference" target="_blank" rel="noopener noreferrer">AMD</a> (-rocm)</li>
<li><a href="https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference" target="_blank" rel="noopener noreferrer">Inferentia</a></li>
<li><a href="https://github.com/huggingface/text-generation-inference/pull/1475" target="_blank" rel="noopener noreferrer">Intel GPU</a></li>
<li><a href="https://github.com/huggingface/tgi-gaudi" target="_blank" rel="noopener noreferrer">Gaudi</a></li>
<li><a href="https://huggingface.co/docs/optimum-tpu/howto/serving" target="_blank" rel="noopener noreferrer">Google TPU</a></li></p><p>
</ul><h2>Commencer</h2></p><h3>Docker</h3></p><p>Pour un guide détaillé de démarrage, veuillez consulter la <a href="https://huggingface.co/docs/text-generation-inference/quicktour" target="_blank" rel="noopener noreferrer">Visite rapide</a>. Le moyen le plus simple de commencer est d’utiliser le conteneur Docker officiel :</p><pre><code class="language-shell">model=HuggingFaceH4/zephyr-7b-beta
<h1>partagez un volume avec le conteneur Docker pour éviter de télécharger les poids à chaque exécution</h1>
volume=$PWD/data</p><p>docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><p>Puis vous pouvez faire des requêtes comme</p><pre><code class="language-bash">curl 127.0.0.1:8080/generate_stream \
    -X POST \
    -d '{"inputs":"Qu’est-ce que l’apprentissage profond ?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'</code></pre></p><p>Vous pouvez également utiliser <a href="https://huggingface.co/docs/text-generation-inference/en/messages_api" target="_blank" rel="noopener noreferrer">l’API Messages de TGI</a> pour obtenir des réponses compatibles avec l'API Open AI Chat Completion.</p><pre><code class="language-bash">curl localhost:8080/v1/chat/completions \
    -X POST \
    -d '{
  "model": "tgi",
  "messages": [
    {
      "role": "system",
      "content": "Vous êtes un assistant utile."
    },
    {
      "role": "user",
      "content": "Qu’est-ce que l’apprentissage profond ?"
    }
  ],
  "stream": true,
  "max_tokens": 20
}' \
    -H 'Content-Type: application/json'</code></pre></p><p><strong>Note :</strong> Pour utiliser les GPU NVIDIA, vous devez installer le <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" target="_blank" rel="noopener noreferrer">NVIDIA Container Toolkit</a>. Nous recommandons également d’utiliser des pilotes NVIDIA avec CUDA version 12.2 ou supérieure. Pour exécuter le conteneur Docker sur une machine sans GPU ni support CUDA, il suffit de retirer le paramètre <code>--gpus all</code> et d’ajouter <code>--disable-custom-kernels</code>, notez que le CPU n’est pas la plateforme prévue pour ce projet, donc la performance peut être inférieure.</p><p><strong>Note :</strong> TGI supporte les GPU AMD Instinct MI210 et MI250. Les détails sont disponibles dans la <a href="https://huggingface.co/docs/text-generation-inference/installation_amd#using-tgi-with-amd-gpus" target="_blank" rel="noopener noreferrer">documentation Matériel Supporté</a>. Pour utiliser les GPU AMD, veuillez utiliser <code>docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:3.3.2-rocm --model-id $model</code> au lieu de la commande ci-dessus.</p><p>Pour voir toutes les options pour servir vos modèles (dans le <a href="https://github.com/huggingface/text-generation-inference/blob/main/launcher/src/main.rs" target="_blank" rel="noopener noreferrer">code</a> ou dans le cli) :
<pre><code class="language-">text-generation-launcher --help</code></pre></p><h3>Documentation API</h3></p><p>Vous pouvez consulter la documentation OpenAPI de l’API REST <code>text-generation-inference</code> en utilisant la route <code>/docs</code>.
L’interface Swagger UI est également disponible à : <a href="https://huggingface.github.io/text-generation-inference" target="_blank" rel="noopener noreferrer">https://huggingface.github.io/text-generation-inference</a>.</p><h3>Utilisation d’un modèle privé ou restreint</h3></p><p>Vous avez la possibilité d’utiliser la variable d’environnement <code>HF_TOKEN</code> pour configurer le jeton utilisé par
<code>text-generation-inference</code>. Cela vous permet d’accéder à des ressources protégées.</p><p>Par exemple, si vous souhaitez servir les variantes du modèle restreint Llama V2 :</p><ul><li>Rendez-vous sur https://huggingface.co/settings/tokens</li>
<li>Copiez votre jeton CLI en lecture seule</li>
<li>Exportez <code>HF_TOKEN=<votre jeton CLI en lecture></code></li></p><p></ul>ou avec Docker :</p><pre><code class="language-shell">model=meta-llama/Meta-Llama-3.1-8B-Instruct
volume=$PWD/data # partagez un volume avec le conteneur Docker pour éviter de télécharger les poids à chaque exécution
token=<votre jeton cli en lecture></p><p>docker run --gpus all --shm-size 1g -e HF_TOKEN=$token -p 8080:80 -v $volume:/data \
    ghcr.io/huggingface/text-generation-inference:3.3.2 --model-id $model</code></pre></p><h3>Note sur la mémoire partagée (shm)</h3></p><p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html" target="_blank" rel="noopener noreferrer"><code>NCCL</code></a> est un cadre de communication utilisé par
<code>PyTorch</code> pour effectuer l’entraînement/inférence distribuée. <code>text-generation-inference</code> utilise
<code>NCCL</code> pour activer le parallélisme tensoriel et accélérer considérablement l’inférence pour les grands modèles de langage.</p><p>Pour partager des données entre les différents dispositifs d’un groupe <code>NCCL</code>, <code>NCCL</code> peut retomber sur l’utilisation de la mémoire hôte si
le peer-to-peer via NVLink ou PCI n’est pas possible.</p><p>Pour permettre au conteneur d’utiliser 1 Go de mémoire partagée et supporter le partage SHM, nous ajoutons <code>--shm-size 1g</code> à la commande ci-dessus.</p><p>Si vous exécutez <code>text-generation-inference</code> dans <code>Kubernetes</code>, vous pouvez aussi ajouter de la mémoire partagée au conteneur en
créant un volume avec :</p><pre><code class="language-yaml">- name: shm
  emptyDir:
   medium: Memory
   sizeLimit: 1Gi</code></pre></p><p>et en le montant sur <code>/dev/shm</code>.</p><p>Enfin, vous pouvez aussi désactiver le partage SHM en utilisant la variable d’environnement <code>NCCL_SHM_DISABLE=1</code>. Cependant, notez que
cela impactera les performances.</p><h3>Tracing distribué</h3></p><p><code>text-generation-inference</code> est instrumenté avec le tracing distribué utilisant OpenTelemetry. Vous pouvez utiliser cette fonctionnalité
en définissant l’adresse d’un collecteur OTLP avec l’argument <code>--otlp-endpoint</code>. Le nom de service par défaut peut être
remplacé avec l’argument <code>--otlp-service-name</code></p><h3>Architecture</h3></p><p><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="Architecture TGI"></p><p>Article détaillé par Adyen sur le fonctionnement interne de TGI : <a href="https://www.adyen.com/knowledge-hub/llm-inference-at-scale-with-tgi" target="_blank" rel="noopener noreferrer">LLM inference at scale with TGI (Martin Iglesias Goyanes - Adyen, 2024)</a></p><h3>Installation locale</h3></p><p>Vous pouvez également choisir d’installer <code>text-generation-inference</code> localement.</p><p>Commencez par cloner le dépôt et changez de répertoire :
(Partie 2 sur 2)</p><pre><code class="language-shell">git clone https://github.com/huggingface/text-generation-inference
cd text-generation-inference</code></pre></p><p>Ensuite, <a href="https://rustup.rs/" target="_blank" rel="noopener noreferrer">installez Rust</a> et créez un environnement virtuel Python avec au moins
Python 3.9, par exemple en utilisant <code>conda</code> ou <code>python venv</code> :</p><pre><code class="language-shell">curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</p><h1>utilisation de conda</h1>
conda create -n text-generation-inference python=3.11
conda activate text-generation-inference</p><h1>utilisation de python venv</h1>
python3 -m venv .venv
source .venv/bin/activate</code></pre></p><p>Vous devrez peut-être aussi installer Protoc.</p><p>Sous Linux :</p><pre><code class="language-shell">PROTOC_ZIP=protoc-21.12-linux-x86_64.zip
curl -OL https://raw.githubusercontent.com/huggingface/text-generation-inference/main/https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP
sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc
sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'
rm -f $PROTOC_ZIP</code></pre></p><p>Sous MacOS, en utilisant Homebrew :</p><pre><code class="language-shell">brew install protobuf</code></pre></p><p>Puis lancez :</p><pre><code class="language-shell">BUILD_EXTENSIONS=True make install # Installe le dépôt et le fork HF/transformer avec les kernels CUDA
text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><p><strong>Remarque :</strong> sur certaines machines, vous aurez aussi besoin des bibliothèques OpenSSL et gcc. Sur les machines Linux, exécutez :</p><pre><code class="language-shell">sudo apt-get install libssl-dev gcc -y</code></pre></p><h3>Installation locale (Nix)</h3></p><p>Une autre option est d’installer <code>text-generation-inference</code> localement en utilisant <a href="https://nixos.org" target="_blank" rel="noopener noreferrer">Nix</a>. Actuellement,
nous ne supportons Nix que sur Linux x86_64 avec GPU CUDA. En utilisant Nix, toutes les dépendances peuvent
être récupérées depuis un cache binaire, évitant ainsi de les compiler localement.</p><p>Commencez par suivre les instructions pour <a href="https://app.cachix.org/cache/huggingface" target="_blank" rel="noopener noreferrer">installer Cachix et activer le cache Hugging Face</a>.
Configurer le cache est important, sinon Nix compilera beaucoup de dépendances
localement, ce qui peut prendre des heures.</p><p>Après cela, vous pouvez lancer TGI avec <code>nix run</code> :</p><pre><code class="language-shell">cd text-generation-inference
nix run --extra-experimental-features nix-command --extra-experimental-features flakes . -- --model-id meta-llama/Llama-3.1-8B-Instruct</code></pre></p><p><strong>Remarque :</strong> lorsque vous utilisez Nix sur un système non-NixOS, vous devez <a href="https://danieldk.eu/Nix-CUDA-on-non-NixOS-systems#make-runopengl-driverlib-and-symlink-the-driver-library" target="_blank" rel="noopener noreferrer">créer certains liens symboliques</a>
pour rendre les bibliothèques du pilote CUDA visibles par les paquets Nix.</p><p>Pour le développement TGI, vous pouvez utiliser le shell dev <code>impure</code> :</p><pre><code class="language-shell">nix develop .#impure</p><h1>Nécessaire uniquement la première fois que le devshell est lancé ou après mise à jour de protobuf.</h1>
(
cd server
mkdir text_generation_server/pb || true
python -m grpc_tools.protoc -I../proto/v3 --python_out=text_generation_server/pb \
       --grpc_python_out=text_generation_server/pb --mypy_out=text_generation_server/pb ../proto/v3/generate.proto
find text_generation_server/pb/ -type f -name "<em>.py" -print0 -exec sed -i -e 's/^\(import.</em>pb2\)/from . \1/g' {} \;
touch text_generation_server/pb/__init__.py
)</code></pre></p><p>Toutes les dépendances de développement (cargo, Python, Torch), etc. sont disponibles dans ce
shell dev.</p><h2>Architectures optimisées</h2></p><p>TGI fonctionne immédiatement pour servir des modèles optimisés pour tous les modèles modernes. Ils peuvent être trouvés dans <a href="https://huggingface.co/docs/text-generation-inference/supported_models" target="_blank" rel="noopener noreferrer">cette liste</a>.</p><p>D’autres architectures sont supportées sur une base de « meilleur effort » en utilisant :</p><p><code>AutoModelForCausalLM.from_pretrained(<model>, device_map="auto")</code></p><p>ou</p><p><code>AutoModelForSeq2SeqLM.from_pretrained(<model>, device_map="auto")</code></p><h2>Exécution locale</h2></p><h3>Lancer</h3></p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2</code></pre></p><h3>Quantification</h3></p><p>Vous pouvez aussi exécuter des poids pré-quantifiés (AWQ, GPTQ, Marlin) ou quantifier les poids à la volée avec bitsandbytes, EETQ, fp8, pour réduire la mémoire VRAM requise :</p><pre><code class="language-shell">text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize</code></pre></p><p>La quantification 4 bits est disponible en utilisant les types de données <a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank" rel="noopener noreferrer">NF4 et FP4 de bitsandbytes</a>. Elle peut être activée en fournissant <code>--quantize bitsandbytes-nf4</code> ou <code>--quantize bitsandbytes-fp4</code> en argument en ligne de commande à <code>text-generation-launcher</code>.</p><p>Lisez plus sur la quantification dans la <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/quantization" target="_blank" rel="noopener noreferrer">documentation Quantification</a>.</p><h2>Développement</h2></p><pre><code class="language-shell">make server-dev
make router-dev</code></pre></p><h2>Tests</h2></p><pre><code class="language-shell"># python
make python-server-tests
make python-client-tests
<h1>ou tests serveur et client</h1>
make python-tests
<h1>tests rust cargo</h1>
make rust-tests
<h1>tests d’intégration</h1>
make integration-tests</code></pre>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/huggingface/text-generation-inference/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>