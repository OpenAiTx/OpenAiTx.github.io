<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>transformers - Read transformers documentation in Polish. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read transformers documentation in Polish. This project has 0 stars on GitHub.">
    <meta name="keywords" content="transformers, Polish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "transformers",
  "description": "Read transformers documentation in Polish. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "huggingface"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/huggingface/transformers/README-pl.html",
  "sameAs": "https://raw.githubusercontent.com/huggingface/transformers/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    transformers
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Polish</span>
                <span>by huggingface</span>
            </div>
        </div>
        
        <div class="content">
            <p><!---
Copyright 2020 Zespół HuggingFace. Wszelkie prawa zastrzeżone.</p><p>Licencjonowane na podstawie licencji Apache, wersja 2.0 („Licencja”);
nie możesz używać tego pliku poza zakresem Licencji.
Kopię Licencji możesz uzyskać pod adresem</p><p>    http://www.apache.org/licenses/LICENSE-2.0</p><p>O ile prawo nie stanowi inaczej lub nie zostało to pisemnie uzgodnione, oprogramowanie
dystrybuowane na podstawie Licencji jest dostarczane na zasadzie „TAK JAK JEST”,
BEZ ŻADNYCH GWARANCJI ANI WARUNKÓW, wyraźnych ani domniemanych.
Zobacz Licencję, aby uzyskać szczegółowe informacje dotyczące uprawnień i ograniczeń wynikających z Licencji.
--></p><p><p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Biblioteka Hugging Face Transformers" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p></p><p><p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpointy na Hubie" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Dokumentacja" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p></p><p><h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">简体中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">繁體中文</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">한국어</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Español</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">日本語</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">हिन्दी</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Русский</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Рortuguês</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">తెలుగు</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Français</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiếng Việt</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">العربية</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">اردو</a> |
    </p>
</h4></p><p><h3 align="center">
    <p>Najnowocześniejsze modele wstępnie wytrenowane do wnioskowania i uczenia</p>
</h3></p><p><h3 align="center">
    <a href="https://hf.co/course"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png"></a>
</h3></p><p>Transformers to biblioteka wstępnie wytrenowanych modeli tekstowych, komputerowego widzenia, audio, wideo i multimodalnych do wnioskowania i uczenia. Użyj Transformers do dostrajania modeli na własnych danych, budowania aplikacji inferencyjnych oraz do zastosowań generatywnej AI w różnych modalnościach.</p><p>Na <a href="https://huggingface.com/models" target="_blank" rel="noopener noreferrer">Hugging Face Hub</a> dostępnych jest ponad 500 tys. <a href="https://huggingface.co/models?library=transformers&sort=trending" target="_blank" rel="noopener noreferrer">checkpointów modeli</a>, których możesz używać.</p><p>Odwiedź <a href="https://huggingface.com/" target="_blank" rel="noopener noreferrer">Hub</a> już dziś, aby znaleźć model i od razu zacząć korzystać z Transformers.</p><h2>Instalacja</h2></p><p>Transformers współpracuje z Pythonem 3.9+, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">PyTorch</a> 2.1+, <a href="https://www.tensorflow.org/install/pip" target="_blank" rel="noopener noreferrer">TensorFlow</a> 2.6+ oraz <a href="https://flax.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer">Flax</a> 0.4.1+.</p><p>Utwórz i aktywuj środowisko wirtualne za pomocą <a href="https://docs.python.org/3/library/venv.html" target="_blank" rel="noopener noreferrer">venv</a> lub <a href="https://docs.astral.sh/uv/" target="_blank" rel="noopener noreferrer">uv</a>, szybkiego menedżera pakietów i projektów Pythona opartego na Rust.</p><pre><code class="language-py"># venv
python -m venv .my-env
source .my-env/bin/activate
<h1>uv</h1>
uv venv .my-env
source .my-env/bin/activate</code></pre></p><p>Zainstaluj Transformers w swoim środowisku wirtualnym.</p><pre><code class="language-py"># pip
pip install "transformers[torch]"</p><h1>uv</h1>
uv pip install "transformers[torch]"</code></pre></p><p>Zainstaluj Transformers ze źródła, jeśli chcesz mieć najnowsze zmiany w bibliotece lub chcesz współtworzyć. Jednak <em>najnowsza</em> wersja może być niestabilna. Jeśli napotkasz błąd, śmiało otwórz <a href="https://github.com/huggingface/transformers/issues" target="_blank" rel="noopener noreferrer">issue</a>.</p><pre><code class="language-shell">git clone https://github.com/huggingface/transformers.git
cd transformers</p><h1>pip</h1>
pip install .[torch]</p><h1>uv</h1>
uv pip install .[torch]</code></pre></p><h2>Szybki start</h2></p><p>Rozpocznij pracę z Transformers od razu dzięki API <a href="https://huggingface.co/docs/transformers/pipeline_tutorial" target="_blank" rel="noopener noreferrer">Pipeline</a>. <code>Pipeline</code> to wysokopoziomowa klasa do wnioskowania wspierająca zadania tekstowe, audio, wizualne i multimodalne. Obsługuje wstępne przetwarzanie danych wejściowych i zwraca odpowiednie wyjście.</p><p>Utwórz pipeline i określ model do generowania tekstu. Model zostanie pobrany i zapisany w cache, więc możesz łatwo użyć go ponownie. Na końcu podaj tekst, aby zainicjować model.</p><pre><code class="language-py">from transformers import pipeline</p><p>pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]</code></pre></p><p>Aby rozmawiać z modelem, wzorzec użycia jest taki sam. Jedyna różnica polega na konieczności utworzenia historii czatu (wejścia do <code>Pipeline</code>) pomiędzy tobą a systemem.</p><blockquote>[!TIP]</blockquote>
<blockquote>Możesz również rozmawiać z modelem bezpośrednio z linii poleceń.</blockquote>
<blockquote><pre><code class="language-shell">> transformers chat Qwen/Qwen2.5-0.5B-Instruct</blockquote>
<blockquote>``<code></blockquote>
</code></pre>py
import torch
from transformers import pipeline</p><p>chat = [
    {"role": "system", "content": "Jesteś zadziornym, dowcipnym robotem wyobrażonym przez Hollywood około 1986 roku."},
    {"role": "user", "content": "Hej, możesz polecić jakieś fajne rzeczy do zrobienia w Nowym Jorku?"}
]</p><p>pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
<pre><code class="language-">
Rozwiń poniższe przykłady, aby zobaczyć jak </code>Pipeline<code> działa dla różnych modalności i zadań.</p><p><details>
<summary>Automatyczne rozpoznawanie mowy</summary>
</code></pre>py
from transformers import pipeline</p><p>pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
<pre><code class="language-">
</details></p><p><details>
<summary>Klasyfikacja obrazów</summary></p><p><h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>
</code></pre>py
from transformers import pipeline</p><p>pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
<pre><code class="language-">
</details></p><p><details>
<summary>Wizualne odpowiadanie na pytania</summary></p><p>
<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>
</code></pre>py
from transformers import pipeline</p><p>pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="Co znajduje się na obrazie?",
)
[{'answer': 'statue of liberty'}]
<pre><code class="language-">
</details></p><h2>Dlaczego warto używać Transformers?</h2></p><ul><li>Łatwe w użyciu najnowocześniejsze modele:</li>
    <li>Wysoka wydajność w zadaniach rozumienia i generowania języka naturalnego, komputerowego widzenia, audio, wideo i multimodalnych.</li>
    <li>Niski próg wejścia dla naukowców, inżynierów i programistów.</li>
    <li>Niewiele abstrakcji skierowanych do użytkownika – wystarczą trzy klasy do nauczenia się.</li>
    <li>Ujednolicone API do korzystania ze wszystkich naszych modeli wstępnie wytrenowanych.</li></p><p><li>Niższe koszty obliczeniowe, mniejszy ślad węglowy:</li>
    <li>Udostępniaj wytrenowane modele zamiast trenować od zera.</li>
    <li>Skróć czas obliczeń i koszty produkcji.</li>
    <li>Dziesiątki architektur modeli z ponad milionem checkpointów wstępnie wytrenowanych we wszystkich modalnościach.</li></p><p><li>Wybierz odpowiedni framework na każdym etapie życia modelu:</li>
    <li>Trenuj najnowocześniejsze modele w 3 linijkach kodu.</li>
    <li>Przenoś pojedynczy model między frameworkami PyTorch/JAX/TF2.0 wedle potrzeb.</li>
    <li>Wybierz odpowiedni framework do trenowania, ewaluacji i wdrożenia.</li></p><p><li>Łatwo dostosuj model lub przykład do swoich potrzeb:</li>
    <li>Dostarczamy przykłady dla każdej architektury, aby odtworzyć wyniki opublikowane przez jej autorów.</li>
    <li>Wnętrze modeli jest eksponowane tak spójnie, jak to możliwe.</li>
    <li>Pliki modeli można używać niezależnie od biblioteki do szybkich eksperymentów.</li></p><p></ul><a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br></p><h2>Dlaczego nie powinieneś używać Transformers?</h2></p><ul><li>Ta biblioteka nie jest modularnym zestawem klocków do budowy sieci neuronowych. Kod w plikach modeli nie jest celowo refaktoryzowany o dodatkowe abstrakcje, aby badacze mogli szybko iterować na każdym modelu bez konieczności zagłębiania się w kolejne warstwy/plików.</li>
<li>API trenowania jest zoptymalizowane pod kątem modeli PyTorch dostarczanych przez Transformers. Do ogólnych pętli uczenia maszynowego powinieneś użyć innej biblioteki, np. <a href="https://huggingface.co/docs/accelerate" target="_blank" rel="noopener noreferrer">Accelerate</a>.</li>
<li><a href="(https://github.com/huggingface/transformers/tree/main/examples" target="_blank" rel="noopener noreferrer">Skrypty przykładów</a>) są tylko <em>przykładami</em>. Mogą nie działać od razu w twoim przypadku i będziesz musiał dostosować kod.</li></p><p></ul><h2>100 projektów wykorzystujących Transformers</h2></p><p>Transformers to więcej niż narzędzie do korzystania z wstępnie wytrenowanych modeli – to społeczność projektów zbudowanych wokół niego i Hugging Face Hub. Chcemy, aby Transformers umożliwiał deweloperom, badaczom, studentom, profesorom, inżynierom i każdemu innemu realizowanie własnych wymarzonych projektów.</p><p>Aby uczcić przekroczenie 100 000 gwiazdek przez Transformers, postanowiliśmy wyróżnić społeczność poprzez stronę <a href="./awesome-transformers.md" target="_blank" rel="noopener noreferrer">awesome-transformers</a>, na której znajdziesz 100 niesamowitych projektów zbudowanych z Transformers.</p><p>Jeśli posiadasz lub używasz projektu, który twoim zdaniem powinien znaleźć się na tej liście, otwórz PR, aby go dodać!</p><h2>Przykładowe modele</h2></p><p>Większość naszych modeli możesz przetestować bezpośrednio na ich <a href="https://huggingface.co/models" target="_blank" rel="noopener noreferrer">stronach na Hubie</a>.</p><p>Rozwiń każdą modalność poniżej, aby zobaczyć kilka przykładowych modeli do różnych zastosowań.</p><p><details>
<summary>Audio</summary></p><ul><li>Klasyfikacja dźwięku z <a href="https://huggingface.co/openai/whisper-large-v3-turbo" target="_blank" rel="noopener noreferrer">Whisper</a></li>
<li>Automatyczne rozpoznawanie mowy z <a href="https://huggingface.co/UsefulSensors/moonshine" target="_blank" rel="noopener noreferrer">Moonshine</a></li>
<li>Wykrywanie słów kluczowych z <a href="https://huggingface.co/superb/wav2vec2-base-superb-ks" target="_blank" rel="noopener noreferrer">Wav2Vec2</a></li>
<li>Generowanie mowy z mowy z <a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16" target="_blank" rel="noopener noreferrer">Moshi</a></li>
<li>Zamiana tekstu na audio z <a href="https://huggingface.co/facebook/musicgen-large" target="_blank" rel="noopener noreferrer">MusicGen</a></li>
<li>Zamiana tekstu na mowę z <a href="https://huggingface.co/suno/bark" target="_blank" rel="noopener noreferrer">Bark</a></li></p><p></ul></details></p><p><details>
<summary>Komputerowe widzenie</summary></p><ul><li>Automatyczne generowanie masek z <a href="https://huggingface.co/facebook/sam-vit-base" target="_blank" rel="noopener noreferrer">SAM</a></li>
<li>Szacowanie głębokości z <a href="https://huggingface.co/apple/DepthPro-hf" target="_blank" rel="noopener noreferrer">DepthPro</a></li>
<li>Klasyfikacja obrazów z <a href="https://huggingface.co/facebook/dinov2-base" target="_blank" rel="noopener noreferrer">DINO v2</a></li>
<li>Wykrywanie punktów kluczowych z <a href="https://huggingface.co/magic-leap-community/superglue_outdoor" target="_blank" rel="noopener noreferrer">SuperGlue</a></li>
<li>Dopasowywanie punktów kluczowych z <a href="https://huggingface.co/magic-leap-community/superglue" target="_blank" rel="noopener noreferrer">SuperGlue</a></li>
<li>Wykrywanie obiektów z <a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd" target="_blank" rel="noopener noreferrer">RT-DETRv2</a></li>
<li>Estymacja pozy z <a href="https://huggingface.co/usyd-community/vitpose-base-simple" target="_blank" rel="noopener noreferrer">VitPose</a></li>
<li>Uniwersalna segmentacja z <a href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large" target="_blank" rel="noopener noreferrer">OneFormer</a></li>
<li>Klasyfikacja wideo z <a href="https://huggingface.co/MCG-NJU/videomae-large" target="_blank" rel="noopener noreferrer">VideoMAE</a></li></p><p></ul></details></p><p><details>
<summary>Multimodalność</summary></p><ul><li>Audio lub tekst na tekst z <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B" target="_blank" rel="noopener noreferrer">Qwen2-Audio</a></li>
<li>Odpowiadanie na pytania o dokumenty z <a href="https://huggingface.co/microsoft/layoutlmv3-base" target="_blank" rel="noopener noreferrer">LayoutLMv3</a></li>
<li>Obraz lub tekst na tekst z <a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct" target="_blank" rel="noopener noreferrer">Qwen-VL</a></li>
<li>Opisywanie obrazów <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" target="_blank" rel="noopener noreferrer">BLIP-2</a></li>
<li>Rozumienie dokumentów OCR z <a href="https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf" target="_blank" rel="noopener noreferrer">GOT-OCR2</a></li>
<li>Odpowiadanie na pytania o tabelach z <a href="https://huggingface.co/google/tapas-base" target="_blank" rel="noopener noreferrer">TAPAS</a></li>
<li>Zunifikowane rozumienie i generowanie multimodalne z <a href="https://huggingface.co/BAAI/Emu3-Gen" target="_blank" rel="noopener noreferrer">Emu3</a></li>
<li>Wizja na tekst z <a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf" target="_blank" rel="noopener noreferrer">Llava-OneVision</a></li>
<li>Wizualne odpowiadanie na pytania z <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf" target="_blank" rel="noopener noreferrer">Llava</a></li>
<li>Wizualna segmentacja wyrażeń referencyjnych z <a href="https://huggingface.co/microsoft/kosmos-2-patch14-224" target="_blank" rel="noopener noreferrer">Kosmos-2</a></li></p><p></ul></details></p><p><details>
<summary>NLP</summary></p><ul><li>Uzupełnianie maskowanych słów z <a href="https://huggingface.co/answerdotai/ModernBERT-base" target="_blank" rel="noopener noreferrer">ModernBERT</a></li>
<li>Rozpoznawanie nazwanych encji z <a href="https://huggingface.co/google/gemma-2-2b" target="_blank" rel="noopener noreferrer">Gemma</a></li>
<li>Odpowiadanie na pytania z <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1" target="_blank" rel="noopener noreferrer">Mixtral</a></li>
<li>Streszczanie z <a href="https://huggingface.co/facebook/bart-large-cnn" target="_blank" rel="noopener noreferrer">BART</a></li>
<li>Tłumaczenie z <a href="https://huggingface.co/google-t5/t5-base" target="_blank" rel="noopener noreferrer">T5</a></li>
<li>Generowanie tekstu z <a href="https://huggingface.co/meta-llama/Llama-3.2-1B" target="_blank" rel="noopener noreferrer">Llama</a></li>
<li>Klasyfikacja tekstu z <a href="https://huggingface.co/Qwen/Qwen2.5-0.5B" target="_blank" rel="noopener noreferrer">Qwen</a></li></p><p></ul></details></p><h2>Cytowanie</h2></p><p>Obecnie mamy <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/" target="_blank" rel="noopener noreferrer">artykuł</a>, który możesz zacytować dla biblioteki 🤗 Transformers:</code></pre>bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
</code>``

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/huggingface/transformers/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>