<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AutoDefense - AutoDefense: Multi-Agent LLM Defense Against Jailbreak Attacks</title>
    <meta name="description" content="AutoDefense: Multi-Agent LLM Defense Against Jailbreak Attacks">
    <meta name="keywords" content="AutoDefense, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "AutoDefense",
  "description": "AutoDefense: Multi-Agent LLM Defense Against Jailbreak Attacks",
  "author": {
    "@type": "Person",
    "name": "XHMY"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 65
  },
  "url": "https://OpenAiTx.github.io/projects/XHMY/AutoDefense/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/XHMY/AutoDefense/main/README.md",
  "datePublished": "2026-02-04",
  "dateModified": "2026-02-04"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/XHMY/AutoDefense" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    AutoDefense
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 65 stars</span>
                <span class="language">English</span>
                <span>by XHMY</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks</h1></p><p><a href="https://microsoft.github.io/autogen/0.2/blog/2024/03/11/AutoDefense/Defending%20LLMs%20Against%20Jailbreak%20Attacks%20with%20AutoDefense/" target="_blank" rel="noopener noreferrer"><strong>Blog</strong></a></p><h2>Installation</h2></p><pre><code class="language-bash">pip install vllm autogen pandas retry openai</code></pre></p><h2>Prepare Inference Service Using <a href="https://docs.vllm.ai/" target="_blank" rel="noopener noreferrer">vLLM</a></h2></p><p>vLLM provides an OpenAI-compatible API server with efficient inference and built-in load balancing across multiple GPUs.</p><h3>Start vLLM Server</h3></p><p>Start the vLLM server with your desired model. For multi-GPU setups, use <code>--data-parallel-size</code> to enable automatic load balancing:</p><p><strong>Single GPU:</strong>
<pre><code class="language-bash">vllm serve Qwen/Qwen3-1.7B --port 8000</code></pre></p><p><strong>Multiple GPUs (e.g., 2 GPUs with data parallelism):</strong>
<pre><code class="language-bash">vllm serve Qwen/Qwen3-1.7B --port 8000 --data-parallel-size 2</code></pre>
<strong>With tensor parallelism for larger models:</strong></p><pre><code class="language-bash">vllm serve <your-large-model> --port 8000 --tensor-parallel-size 4</code></pre></p><p><strong>Combined tensor and data parallelism (8 GPUs, 2-way TP × 4-way DP):</strong>
<pre><code class="language-bash">vllm serve <your-large-model> --port 8000 --tensor-parallel-size 2 --data-parallel-size 4</code></pre></p><p>For more details on data parallel deployment with internal load balancing, see the <a href="https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/" target="_blank" rel="noopener noreferrer">vLLM documentation</a>.</p><h3>Verify the Server</h3></p><p>You can verify the server is running by checking the models endpoint:</p><pre><code class="language-bash">curl http://localhost:8000/v1/models</code></pre></p><h2>Response Generation</h2></p><p>The responses are generated by the target model served by vLLM (default: <code>Qwen/Qwen3-1.7B</code>). Make sure your vLLM server is running before executing the following command.</p><h3>Attack Prompts (Harmful)</h3></p><pre><code class="language-bash">python attack/attack.py --model Qwen/Qwen3-1.7B --host 127.0.0.1 --port 8000</code></pre></p><p>This command will generate responses using an attack prompt template (default: <code>--template v1</code>) loaded from <code>data/prompt/attack_prompt_template.json</code>.
To run multiple repetitions, invoke the script multiple times and vary <code>--output-suffix</code> and/or <code>--cache-seed</code>.</p><h3>Safe Prompts (Benign)</h3></p><p>To generate responses for safe/benign prompts (used for false positive evaluation):</p><pre><code class="language-bash">python attack/attack.py \
    --model Qwen/Qwen3-1.7B \
    --template placeholder \
    --prompts data/prompt/safe_prompts.json \
    --output-prefix safe</code></pre></p><p>The <code>placeholder</code> template passes prompts through without any attack framing, while <code>v1</code> wraps prompts with jailbreak instructions.</p><h2>Run Defense Experiments</h2></p><p>The following command runs the experiments of 1-Agent, 2-Agent, and 3-Agent defense. The <code>--chat-file</code> should point to the harmful outputs generated by <code>attack/attack.py</code> (by default saved under <code>data/harmful_output/<model_dir>/</code>, e.g. <code>data/harmful_output/Qwen-Qwen3-1.7B/attack-dan_0.json</code>).</p><pre><code class="language-bash">export AUTOGEN_USE_DOCKER=0</p><p>python defense/run_defense_exp.py \
  --model Qwen/Qwen3-1.7B \
  --chat-file data/harmful_output/Qwen-Qwen3-1.7B/attack-dan_0.json</code></pre></p><h3>Command Line Arguments</h3></p><p>| Argument | Description | Default |
|----------|-------------|---------|
| <code>--model</code> | Target model served by vLLM | <code>Qwen/Qwen3-1.7B</code> |
| <code>--chat-file</code> | Path to the chat file with harmful outputs | Required |
| <code>--port</code> | Port where vLLM server is running | <code>8000</code> |
| <code>--host</code> | Hostname of the vLLM server | <code>127.0.0.1</code> |
| <code>--output-dir</code> | Output directory | <code>data/defense_output/<model_dir></code> |
| <code>--output-suffix</code> | Suffix for output directory | <code>""</code> |
| <code>--strategies</code> | Defense strategies to run | <code>ex-2 ex-3 ex-cot</code> |
| <code>--workers</code> | Number of parallel workers | <code>128</code> |
| <code>--frequency_penalty</code> | Frequency penalty for generation | <code>0.0</code> |
| <code>--presence_penalty</code> | Presence penalty for generation | <code>0.0</code> |
| <code>--temperature</code> | Temperature for generation | <code>0.7</code> |</p><p>After finishing the defense experiment, the output will appear in <code>data/defense_output/<model_dir>/</code> (e.g. <code>data/defense_output/Qwen-Qwen3-1.7B/</code>).</p><h2>GPT Evaluation (paper uses GPT-4)</h2></p><p>Evaluating harmful output defense:</p><pre><code class="language-bash">python evaluator/gpt4_evaluator.py \
--defense_output_dir data/defense_output/Qwen-Qwen3-1.7B \
--ori_prompt_file_name prompt_dan.json</code></pre></p><p>After finishing the evaluation, the output will appear in the <code>data/defense_output/Qwen-Qwen3-1.7B/asr.csv</code>.
There will be also a <code>score</code> value appearing for each defense output in the output <code>json</code> file.
<code>evaluator/gpt4_evaluator.py</code> uses a GPT model as the evaluator (the original paper uses GPT-4). Set your OpenAI credentials via environment variables (or CLI flags), and you can swap the evaluator to a newer GPT model (e.g., GPT-5) via <code>--model</code>.</p><pre><code class="language-bash">export OPENAI_API_KEY=...
<h1>optional (only if you use an OpenAI-compatible endpoint):</h1>
<h1>export OPENAI_BASE_URL=...</h1></p><p>python evaluator/gpt4_evaluator.py \
  --defense_output_dir data/defense_output/Qwen-Qwen3-1.7B \
  --ori_prompt_file_name prompt_dan.json \
  --model gpt-4-1106-preview</code></pre></p><p>GPT-based evaluation can be costly; we enable caching to avoid repeated evaluation.</p><p>For safe response evaluation, there is an efficient way without using GPT-4. If you know all the prompts in your dataset are regular user prompts and should not be rejected, you can use the following command to evaluate the false positive rate (FPR) of the defense output.</p><pre><code class="language-bash">python evaluator/evaluate_safe.py</code></pre>
This will find all output folders in <code>data/defense_output</code> that contain the keyword <code>-safe</code> and evaluate the false positive rate (FPR).  
The FPR will be saved in the <code>data/defense_output/defense_fp.csv</code> file.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2026-02-04

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/XHMY/AutoDefense/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2026-02-04 
    </div>
    
</body>
</html>