<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>tensorzero - Read tensorzero documentation in French. This project has 5476 stars on GitHub.</title>
    <meta name="description" content="Read tensorzero documentation in French. This project has 5476 stars on GitHub.">
    <meta name="keywords" content="tensorzero, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "tensorzero",
  "description": "Read tensorzero documentation in French. This project has 5476 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "tensorzero"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 5476
  },
  "url": "https://OpenAiTx.github.io/projects/tensorzero/tensorzero/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/tensorzero/tensorzero/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/tensorzero/tensorzero" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    tensorzero
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 5476 stars</span>
                <span class="language">French</span>
                <span>by tensorzero</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/user-attachments/assets/47d67430-386d-4675-82ad-d4734d3262d9" width=128 height=128></p><h1>TensorZero</h1></p><p><strong>TensorZero crée une boucle de rétroaction pour optimiser les applications LLM — transformant les données de production en modèles plus intelligents, plus rapides et moins coûteux.</strong></p><ul><li>Intégrez notre passerelle de modèles</li>
<li>Envoyez des métriques ou des retours d’expérience</li>
<li>Optimisez les prompts, modèles et stratégies d’inférence</li>
<li>Observez l’amélioration de vos LLMs au fil du temps</li></p><p></ul>Il fournit une <strong>boucle de données & d’apprentissage pour les LLMs</strong> en unifiant :</p><ul><li>[x] <strong>Inférence :</strong> une API pour tous les LLMs, avec une surcharge P99 <1ms</li>
<li>[x] <strong>Observabilité :</strong> inférence & retours → votre base de données</li>
<li>[x] <strong>Optimisation :</strong> des prompts au fine-tuning et RL</li>
<li>[x] <strong>Évaluations :</strong> comparer prompts, modèles, stratégies d’inférence</li>
<li>[x] <strong>Expérimentation :</strong> tests A/B intégrés, routage, fallbacks</li></p><p></ul>---</p><p><p align="center">
  <b><a href="https://www.tensorzero.com/" target="_blank">Site Web</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs" target="_blank">Docs</a></b>
  ·
  <b><a href="https://www.x.com/tensorzero" target="_blank">Twitter</a></b>
  ·
  <b><a href="https://www.tensorzero.com/slack" target="_blank">Slack</a></b>
  ·
  <b><a href="https://www.tensorzero.com/discord" target="_blank">Discord</a></b>
  <br>
  <br>
  <b><a href="https://www.tensorzero.com/docs/quickstart" target="_blank">Démarrage rapide (5min)</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank">Tutoriel complet</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Guide de déploiement</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/api-reference" target="_blank">Référence API</a></b>
  ·
  <b><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank">Référence de configuration</a></b>
</p></p><hr></p><p><table>
  <tr>
    <td width="30%" valign="top"><b>Qu’est-ce que TensorZero ?</b></td>
    <td width="70%" valign="top">TensorZero est un framework open-source pour construire des applications LLM de niveau production. Il unifie une passerelle LLM, l’observabilité, l’optimisation, les évaluations et l’expérimentation.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>En quoi TensorZero est-il différent des autres frameworks LLM ?</b></td>
    <td width="70%" valign="top">
      <ul><li>TensorZero vous permet d’optimiser des applications LLM complexes sur la base de métriques de production et de retours humains.<br></li>
      <li>TensorZero répond aux besoins des applications LLM à l’échelle industrielle : faible latence, haut débit, sécurité des types, auto-hébergé, GitOps, personnalisation, etc.<br></li>
      <li>TensorZero unifie toute la stack LLMOps, créant des bénéfices cumulatifs. Par exemple, les évaluations LLM peuvent être utilisées pour affiner les modèles en parallèle des juges IA.</li>
    </ul></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Puis-je utiliser TensorZero avec ___ ?</b></td>
    <td width="70%" valign="top">Oui. Tous les principaux langages de programmation sont pris en charge. Vous pouvez utiliser TensorZero avec notre client Python, n’importe quel SDK OpenAI ou notre API HTTP.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>TensorZero est-il prêt pour la production ?</b></td>
    <td width="70%" valign="top">Oui. Voici une étude de cas : <b><a href="https://www.tensorzero.com/blog/case-study-automating-code-changelogs-at-a-large-bank-with-llms">Automatisation des changelogs de code dans une grande banque avec des LLMs</a></b></td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Combien coûte TensorZero ?</b></td>
    <td width="70%" valign="top">Rien. TensorZero est 100% auto-hébergé et open-source. Il n’y a aucune fonctionnalité payante.</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Qui construit TensorZero ?</b></td>
    <td width="70%" valign="top">Notre équipe technique comprend un ancien mainteneur du compilateur Rust, des chercheurs en machine learning (Stanford, CMU, Oxford, Columbia) avec des milliers de citations, et le chief product officer d’une startup decacorn. Nous sommes soutenus par les mêmes investisseurs que des projets open-source majeurs (par ex. ClickHouse, CockroachDB) et des laboratoires d’IA (par ex. OpenAI, Anthropic).</td>
  </tr>
  <tr>
    <td width="30%" valign="top"><b>Comment démarrer ?</b></td>
    <td width="70%" valign="top">Vous pouvez adopter TensorZero de façon incrémentale. Notre <b><a href="https://www.tensorzero.com/docs/quickstart">Démarrage rapide</a></b> passe d’un simple wrapper OpenAI à une application LLM prête pour la production avec observabilité et fine-tuning en seulement 5 minutes.</td>
  </tr>
</table></p><hr></p><h2>Fonctionnalités</h2></p><h3>🌐 Passerelle LLM</h3></p><blockquote><strong>Intégrez-vous une seule fois à TensorZero et accédez à tous les principaux fournisseurs LLM.</strong></blockquote></p><p><table>
  <tr></tr> <!-- flip highlight order -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Fournisseurs de modèles</b></td>
    <td width="50%" align="center" valign="middle"><b>Fonctionnalités</b></td>
  </tr>
  <tr>
    <td width="50%" align="left" valign="top">
      <p>
        La passerelle TensorZero prend en charge nativement :
      </p>
      <ul>
        <ul><li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/anthropic">Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-bedrock">AWS Bedrock</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/aws-sagemaker">AWS SageMaker</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/azure">Azure OpenAI Service</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/deepseek">DeepSeek</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/fireworks">Fireworks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-anthropic">GCP Vertex AI Anthropic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/gcp-vertex-ai-gemini">GCP Vertex AI Gemini</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/google-ai-studio-gemini">Google AI Studio (Gemini API)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/hyperbolic">Hyperbolic</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/mistral">Mistral</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai">OpenAI</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/together">Together</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/vllm">vLLM</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/xai">xAI</a></b></li>
      </ul></ul>
        <em>
          Besoin d’autre chose ?
          Votre fournisseur est très probablement pris en charge, car TensorZero s’intègre avec <b><a href="https://www.tensorzero.com/docs/gateway/guides/providers/openai-compatible">n’importe quelle API compatible OpenAI (par ex. Ollama)</a></b>.
        </em>
      </p>
    </td>
    <td width="50%" align="left" valign="top">
      <p>
        La passerelle TensorZero prend en charge des fonctionnalités avancées comme :
      </p>
      <ul>
        <ul><li><b><a href="https://www.tensorzero.com/docs/gateway/guides/retries-fallbacks">Reprises & Fallbacks</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations">Optimisations à l’inférence</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/prompt-templates-schemas">Templates & Schémas de prompts</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/tutorial#experimentation">Expérimentation (Tests A/B)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/configuration-reference">Configuration-as-Code (GitOps)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/batch-inference">Inférence par lot</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/multimodal-inference">Inférence multimodale (VLMs)</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-caching">Cache d’inférence</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/metrics-feedback">Métriques & Retours</a></b></li>
        <li><b><a href="https://www.tensorzero.com/docs/gateway/guides/episodes">Workflows LLM multi-étapes (Épisodes)</a></b></li>
        <li><em>& bien d’autres…</em></li>
      </ul></ul>
        La passerelle TensorZero est écrite en Rust 🦀 pour la <b>performance</b> (&lt;1ms de latence P99 @ 10k QPS).
        Voir <b><a href="https://www.tensorzero.com/docs/gateway/benchmarks">Benchmarks</a></b>.<br>
      </p>
      <p>
        Vous pouvez effectuer l’inférence avec le <b>client TensorZero</b> (recommandé), le <b>client OpenAI</b> ou l’<b>API HTTP</b>.
      </p>
    </td>
  </tr>
</table></p><p><br></p><p><details open>
<summary><b>Utilisation : Python &mdash; Client TensorZero (Recommandé)</b></summary></p><p>Vous pouvez accéder à n’importe quel fournisseur avec le client Python TensorZero.</p><ul><li><code>pip install tensorzero</code></li>
<li>Optionnel : configurez TensorZero.</li>
<li>Lancez une inférence :</li></p><p></ul><pre><code class="language-python">from tensorzero import TensorZeroGateway  # ou AsyncTensorZeroGateway</p><p>
with TensorZeroGateway.build_embedded(clickhouse_url="...", config_file="...") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        # Essayez facilement d'autres fournisseurs : "anthropic::claude-3-7-sonnet-20250219"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Écris un haïku sur l’intelligence artificielle.",
                }
            ]
        },
    )</code></pre></p><p>Voir <strong><a href="https://www.tensorzero.com/docs/quickstart" target="_blank" rel="noopener noreferrer">Démarrage rapide</a></strong> pour plus d’informations.</p><p></details></p><p><details>
<summary><b>Utilisation : Python &mdash; Client OpenAI</b></summary></p><p>Vous pouvez accéder à n’importe quel fournisseur avec le client Python OpenAI via TensorZero.</p><ul><li><code>pip install tensorzero</code></li>
<li>Optionnel : configurez TensorZero.</li>
<li>Lancez une inférence :</li></p><p></ul><pre><code class="language-python">from openai import OpenAI  # ou AsyncOpenAI
from tensorzero import patch_openai_client</p><p>client = OpenAI()
patch_openai_client(
    client,
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
    config_file="config/tensorzero.toml",
    async_setup=False,
)</p><p>response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    # Essayez d'autres fournisseurs facilement : "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
    messages=[
        {
            "role": "user",
            "content": "Écris un haïku sur l'intelligence artificielle.",
        }
    ],
)</code></pre></p><p>Voir <strong><a href="https://www.tensorzero.com/docs/quickstart" target="_blank" rel="noopener noreferrer">Démarrage rapide</a></strong> pour plus d'informations.</p><p></details></p><p><details>
<summary><b>Utilisation : JavaScript / TypeScript (Node) &mdash; Client OpenAI</b></summary></p><p>Vous pouvez accéder à n'importe quel fournisseur en utilisant le client OpenAI Node avec TensorZero.</p><ul><li>Déployez <code>tensorzero/gateway</code> à l'aide de Docker.</li>
   </ul><strong><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank" rel="noopener noreferrer">Instructions détaillées →</a></strong>
<ul><li>Configurez la configuration TensorZero.</li>
<li>Lancez l'inférence :</li></p><p></ul><pre><code class="language-ts">import OpenAI from "openai";</p><p>const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});</p><p>const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  // Essayez d'autres fournisseurs facilement : "tensorzero::model_name::anthropic::claude-3-7-sonnet-20250219"
  messages: [
    {
      role: "user",
      content: "Écris un haïku sur l'intelligence artificielle.",
    },
  ],
});</code></pre></p><p>Voir <strong><a href="https://www.tensorzero.com/docs/quickstart" target="_blank" rel="noopener noreferrer">Démarrage rapide</a></strong> pour plus d'informations.</p><p></details></p><p><details>
<summary><b>Utilisation : Autres Langages & Plateformes &mdash; API HTTP</b></summary></p><p>TensorZero prend en charge pratiquement tous les langages de programmation ou plateformes via son API HTTP.</p><ul><li>Déployez <code>tensorzero/gateway</code> à l'aide de Docker.</li>
   </ul><strong><a href="https://www.tensorzero.com/docs/gateway/deployment" target="_blank" rel="noopener noreferrer">Instructions détaillées →</a></strong>
<ul><li>Optionnel : Configurez TensorZero.</li>
<li>Lancez l'inférence :</li></p><p></ul><pre><code class="language-bash">curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "Écris un haïku sur l'intelligence artificielle."
        }
      ]
    }
  }'</code></pre></p><p>Voir <strong><a href="https://www.tensorzero.com/docs/quickstart" target="_blank" rel="noopener noreferrer">Démarrage rapide</a></strong> pour plus d'informations.</p><p></details></p><p><br></p><h3>📈 Optimisation LLM</h3></p><blockquote><strong>Envoyez des métriques de production et des retours humains pour optimiser facilement vos prompts, modèles et stratégies d'inférence &mdash; via l'interface utilisateur ou par programmation.</strong></blockquote></p><p>#### Optimisation de modèle</p><p>Optimisez les modèles propriétaires et open-source en utilisant l'ajustement supervisé (SFT) et l'ajustement par préférence (DPO).</p><p><table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Ajustement supervisé &mdash; UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Ajustement par préférence (DPO) &mdash; Jupyter Notebook</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/cf7acf66-732b-43b3-af2a-5eba1ce40f6f"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/a67a0634-04a7-42b0-b934-9130cb7cdf51"></td>
  </tr>
</table></p><p>#### Optimisation à l'inférence</p><p>Améliorez les performances en mettant à jour dynamiquement vos prompts avec des exemples pertinents, en combinant les réponses de plusieurs inférences, et plus encore.</p><p><table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">Best-of-N Sampling</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#mixture-of-n-sampling">Mixture-of-N Sampling</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/c0edfa4c-713c-4996-9964-50c0d26e6970"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/75b5bf05-4c1f-43c4-b158-d69d1b8d05be"></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl">Dynamic In-Context Learning (DICL)</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#chain-of-thought-cot">Chain-of-Thought (CoT)</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d8489e92-ce93-46ac-9aab-289ce19bb67d"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/ea13d73c-76a4-4e0c-a35b-0c648f898311" height="320"></td>
  </tr>
</table></p><p>_D'autres arrivent bientôt..._</p><p><br></p><p>#### Optimisation de prompt</p><p>Optimisez vos prompts par programmation à l'aide de techniques d'optimisation issues de la recherche.</p><p><table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#best-of-n-sampling">MIPROv2</a></b></td>
    <td width="50%" align="center" valign="middle"><b><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy">Intégration DSPy</a></b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/d81a7c37-382f-4c46-840f-e6c2593301db" alt="MIPROv2 diagram"></td>
    <td width="50%" align="center" valign="middle">
      TensorZero est fourni avec plusieurs recettes d'optimisation, mais vous pouvez également créer facilement les vôtres.
      Cet exemple montre comment optimiser une fonction TensorZero en utilisant un outil arbitraire — ici, DSPy, une bibliothèque populaire pour l'ingénierie de prompts automatisée.
    </td>
  </tr>
</table></p><p>_D'autres arrivent bientôt..._</p><p><br></p><h3>🔍 Observabilité LLM</h3></p><blockquote><strong>Zoomez pour déboguer les appels API individuels, ou dézoomez pour surveiller les métriques à travers les modèles et prompts dans le temps &mdash; tout cela grâce à l'interface open-source de TensorZero.</strong></blockquote></p><p><table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Observabilité » Inférence</b></td>
    <td width="50%" align="center" valign="middle"><b>Observabilité » Fonction</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/2cc3cc9a-f33f-4e94-b8de-07522326f80a"></td>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/00ae6605-8fa0-4efd-8238-ae8ea589860f"></td>
  </tr>
</table></p><p><br></p><h3>📊 Évaluations LLM</h3></p><blockquote><strong>Comparez les prompts, modèles et stratégies d'inférence à l'aide des évaluations TensorZero &mdash; avec prise en charge des heuristiques et juges LLM.</strong></blockquote></p><p><table>
  <tr></tr> <!-- inverser l'ordre de surbrillance -->
  <tr>
    <td width="50%" align="center" valign="middle"><b>Évaluation » UI</b></td>
    <td width="50%" align="center" valign="middle"><b>Évaluation » CLI</b></td>
  </tr>
  <tr>
    <td width="50%" align="center" valign="middle"><img src="https://github.com/user-attachments/assets/f4bf54e3-1b63-46c8-be12-2eaabf615699"></td>
    <td width="50%" align="left" valign="middle">
<pre><code class="language-bash">docker compose run --rm evaluations \
  --evaluation-name extract_data \
  --dataset-name hard_test_cases \
  --variant-name gpt_4o \
  --concurrency 5</code></pre>
<pre><code class="language-bash">Run ID: 01961de9-c8a4-7c60-ab8d-15491a9708e4
Number of datapoints: 100
██████████████████████████████████████ 100/100
exact_match: 0.83 ± 0.03
semantic_match : 0,98 ± 0,01  
item_count : 7,15 ± 0,39</code></pre>
    </td>
  </tr>
</table></p><h2>Démo</h2></p><blockquote><strong>Regardez les LLMs s'améliorer en extraction de données en temps réel avec TensorZero !</strong></blockquote>
>
<blockquote><strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations#dynamic-in-context-learning-dicl" target="_blank" rel="noopener noreferrer">Dynamic in-context learning (DICL)</a></strong> est une puissante optimisation au moment de l'inférence, disponible immédiatement avec TensorZero.</blockquote>
<blockquote>Cela améliore la performance des LLM en intégrant automatiquement des exemples historiques pertinents dans le prompt, sans avoir besoin d'un ajustement du modèle.</blockquote></p><p>https://github.com/user-attachments/assets/4df1022e-886e-48c2-8f79-6af3cdad79cb</p><h2>Ingénierie LLM avec TensorZero</h2></p><p><br>
<p align="center" >
  <a href="https://www.tensorzero.com/docs">
    <picture>
      <source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6">
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/e8bc699b-6378-4c2a-9cc1-6d189025e270">
      <img alt="TensorZero Flywheel" src="https://github.com/user-attachments/assets/34a92c18-242e-4d76-a99c-861283de68a6" width=720>
    </picture>
  </a>
</p>
<br></p><ul><li>La <strong><a href="https://www.tensorzero.com/docs/gateway/" target="_blank" rel="noopener noreferrer">TensorZero Gateway</a></strong> est une passerelle de modèles haute performance écrite en Rust 🦀 qui offre une interface API unifiée pour tous les principaux fournisseurs de LLM, permettant une intégration et des solutions de repli multiplateformes transparentes.</li>
<li>Elle gère l'inférence structurée basée sur des schémas avec une latence P99 &lt;1ms (voir <strong><a href="https://www.tensorzero.com/docs/gateway/benchmarks" target="_blank" rel="noopener noreferrer">Benchmarks</a></strong>), l'observabilité intégrée, l'expérimentation et les <strong><a href="https://www.tensorzero.com/docs/gateway/guides/inference-time-optimizations" target="_blank" rel="noopener noreferrer">optimisations au moment de l'inférence</a></strong>.</li>
<li>Elle collecte également des métriques et des retours associés à ces inférences, avec une prise en charge native des systèmes LLM multi-étapes.</li>
<li>Toutes les données sont stockées dans un entrepôt de données ClickHouse que vous contrôlez, pour une analytique en temps réel, évolutive et conviviale pour les développeurs.</li>
<li>Au fil du temps, les <strong><a href="https://www.tensorzero.com/docs/recipes" target="_blank" rel="noopener noreferrer">Recettes TensorZero</a></strong> exploitent cet ensemble de données structuré pour optimiser vos prompts et modèles : exécutez des recettes pré-construites pour des workflows courants comme le fine-tuning, ou créez les vôtres avec une flexibilité totale, en utilisant n'importe quel langage et plateforme.</li>
<li>Enfin, les fonctionnalités d'expérimentation et l'orchestration GitOps de la passerelle vous permettent d'itérer et de déployer en toute confiance, qu'il s'agisse d'un seul LLM ou de milliers de LLMs.</li></p><p></ul>Notre objectif est d'aider les ingénieurs à construire, gérer et optimiser la prochaine génération d'applications LLM : des systèmes qui apprennent de l'expérience réelle.
En savoir plus sur notre <strong><a href="https://www.tensorzero.com/docs/vision-roadmap/" target="_blank" rel="noopener noreferrer">Vision & Feuille de route</a></strong>.</p><h2>Commencer</h2></p><p><strong>Commencez à construire dès aujourd'hui.</strong>
Le <strong><a href="https://www.tensorzero.com/docs/quickstart" target="_blank" rel="noopener noreferrer">Démarrage rapide</a></strong> montre qu'il est facile de mettre en place une application LLM avec TensorZero.
Si vous souhaitez aller plus loin, le <strong><a href="https://www.tensorzero.com/docs/gateway/tutorial" target="_blank" rel="noopener noreferrer">Tutoriel</a></strong> vous apprend à construire un chatbot simple, un copilote email, un système météo RAG, et un pipeline d'extraction de données structurées.</p><p><strong>Des questions ?</strong>
Rejoignez-nous sur <strong><a href="https://www.tensorzero.com/slack" target="_blank" rel="noopener noreferrer">Slack</a></strong> ou <strong><a href="https://www.tensorzero.com/discord" target="_blank" rel="noopener noreferrer">Discord</a></strong>.</p><p><strong>Vous utilisez TensorZero au travail ?</strong>
Écrivez-nous à <strong><a href="mailto:hello@tensorzero.com" target="_blank" rel="noopener noreferrer">hello@tensorzero.com</a></strong> pour créer un canal Slack ou Teams avec votre équipe (gratuitement).</p><p><strong>Rejoignez-nous.</strong>
Nous <strong><a href="https://www.tensorzero.com/jobs" target="_blank" rel="noopener noreferrer">recrutons à New York</a></strong>.
Nous accueillons également les <strong><a href="https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">contributions open-source</a></strong> !</p><h2>Exemples</h2></p><p>Nous préparons une série de <strong>cas d'usage complets et exécutables</strong> illustrant le flywheel de données & d'apprentissage de TensorZero.</p><blockquote><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner" target="_blank" rel="noopener noreferrer">Optimisation de l'extraction de données (NER) avec TensorZero</a></strong></blockquote>
>
<blockquote>Cet exemple montre comment utiliser TensorZero pour optimiser un pipeline d'extraction de données.</blockquote>
<blockquote>Nous démontrons des techniques comme le fine-tuning et l'apprentissage dynamique en contexte (DICL).</blockquote>
<blockquote>Au final, un modèle GPT-4o Mini optimisé surpasse GPT-4o sur cette tâche &mdash; pour une fraction du coût et de la latence &mdash; en utilisant une petite quantité de données d'entraînement.</blockquote></p><blockquote><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/" target="_blank" rel="noopener noreferrer">Agentic RAG — Questionnement multi-sauts avec LLMs</a></strong></blockquote>
>
<blockquote>Cet exemple montre comment construire un agent de recherche multi-sauts avec TensorZero.</blockquote>
<blockquote>L'agent recherche de manière itérative sur Wikipédia pour rassembler des informations, et décide quand il dispose de suffisamment de contexte pour répondre à une question complexe.</blockquote></p><blockquote><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences" target="_blank" rel="noopener noreferrer">Générer des haïkus pour satisfaire un juge aux préférences cachées</a></strong></blockquote>
>
<blockquote>Cet exemple ajuste GPT-4o Mini pour générer des haïkus adaptés à un goût particulier.</blockquote>
<blockquote>Vous verrez le "data flywheel in a box" de TensorZero en action : de meilleures variantes produisent de meilleures données, et de meilleures données mènent à de meilleures variantes.</blockquote>
<blockquote>Vous verrez la progression en ré-entraînant le LLM plusieurs fois.</blockquote></p><blockquote><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/" target="_blank" rel="noopener noreferrer">Améliorer les capacités aux échecs d'un LLM avec le Best-of-N Sampling</a></strong></blockquote>
>
<blockquote>Cet exemple montre comment le best-of-N sampling peut considérablement améliorer les performances d'un LLM aux échecs en sélectionnant les coups les plus prometteurs parmi plusieurs options générées.</blockquote></p><blockquote><strong><a href="https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy" target="_blank" rel="noopener noreferrer">Améliorer le raisonnement mathématique avec une recette personnalisée pour l’ingénierie automatisée des prompts (DSPy)</a></strong></blockquote>
>
<blockquote>TensorZero propose un certain nombre de recettes d'optimisation prêtes à l'emploi pour les workflows classiques d'ingénierie LLM.</blockquote>
<blockquote>Mais vous pouvez aussi facilement créer vos propres recettes et workflows !</blockquote>
<blockquote>Cet exemple montre comment optimiser une fonction TensorZero à l'aide d'un outil arbitraire — ici, DSPy.</blockquote></p><p>_& bien d'autres à venir !_

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-09

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/tensorzero/tensorzero/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>