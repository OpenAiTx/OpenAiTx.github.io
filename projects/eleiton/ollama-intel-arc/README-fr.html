<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ollama-intel-arc - Utilisez le GPU Intel Arc Series pour ex&#233;cuter Ollama, StableDiffusion, Whisper et Open WebUI, pour la g&#233;n&#233;ration d&#39;images, la reconnaissance vocale et l&#39;interaction avec les grands mod&#232;les de langage (LLM).</title>
    <meta name="description" content="Utilisez le GPU Intel Arc Series pour ex&#233;cuter Ollama, StableDiffusion, Whisper et Open WebUI, pour la g&#233;n&#233;ration d&#39;images, la reconnaissance vocale et l&#39;interaction avec les grands mod&#232;les de langage (LLM).">
    <meta name="keywords" content="ollama-intel-arc, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ollama-intel-arc",
  "description": "Utilisez le GPU Intel Arc Series pour exécuter Ollama, StableDiffusion, Whisper et Open WebUI, pour la génération d'images, la reconnaissance vocale et l'interaction avec les grands modèles de langage (LLM).",
  "author": {
    "@type": "Person",
    "name": "eleiton"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 241
  },
  "url": "https://OpenAiTx.github.io/projects/eleiton/ollama-intel-arc/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/README.md",
  "datePublished": "2026-02-16",
  "dateModified": "2026-02-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/eleiton/ollama-intel-arc" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ollama-intel-arc
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 241 stars</span>
                <span class="language">French</span>
                <span>by eleiton</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Exécutez Ollama, Stable Diffusion et la Reconnaissance Automatique de la Parole avec votre GPU Intel Arc</h1></p><p><a href="https://blog.eleiton.dev/posts/llm-and-genai-in-docker/" target="_blank" rel="noopener noreferrer">[Blog</a>]</p><p>Déployez facilement une solution basée sur Docker qui utilise <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noopener noreferrer">Open WebUI</a> comme interface utilisateur conviviale  
IA et <a href="https://github.com/ollama/ollama" target="_blank" rel="noopener noreferrer">Ollama</a> pour l’intégration des grands modèles de langage (LLM).</p><p>De plus, vous pouvez exécuter des conteneurs Docker <a href="https://github.com/comfyanonymous/ComfyUI" target="_blank" rel="noopener noreferrer">ComfyUI</a> ou <a href="https://github.com/vladmandic/sdnext" target="_blank" rel="noopener noreferrer">SD.Next</a>  
pour simplifier les capacités de Stable Diffusion.</p><p>Vous pouvez également exécuter un conteneur Docker optionnel avec <a href="https://github.com/openai/whisper" target="_blank" rel="noopener noreferrer">OpenAI Whisper</a> pour effectuer des tâches de Reconnaissance Automatique de la Parole (ASR).</p><p>Tous ces conteneurs ont été optimisés pour les GPU Intel Arc Series sous Linux en utilisant <a href="https://github.com/intel/intel-extension-for-pytorch" target="_blank" rel="noopener noreferrer">Intel® Extension for PyTorch</a>.</p><p><img src="https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/resources/open-webui.png" alt="screenshot"></p><h2>Services</h2>
<ul><li>Ollama  </li>
   <li>Exécute llama.cpp et Ollama avec IPEX-LLM sur votre ordinateur Linux avec GPU Intel Arc.  </li>
   <li>Construit selon les directives de <a href="https://github.com/intel/ipex-llm/blob/main/docs/mddocs/DockerGuides/README.md" target="_blank" rel="noopener noreferrer">Intel</a>.  </li>
   <li>Utilise l’image docker officielle <a href="https://hub.docker.com/r/intelanalytics/ipex-llm-inference-cpp-xpu" target="_blank" rel="noopener noreferrer">Intel ipex-llm</a> comme conteneur de base.  </li>
   <li>Utilise les dernières versions des paquets requis, privilégiant les fonctionnalités de pointe plutôt que la stabilité.  </li>
   <li>Expose le port <code>11434</code> pour connecter d’autres outils à votre service Ollama.</li></p><p><li>Open WebUI  </li>
   <li>Utilise la distribution officielle d’Open WebUI.  </li>
   <li><code>WEBUI_AUTH</code> est désactivé pour une utilisation sans authentification.  </li>
   <li>Les drapeaux <code>ENABLE_OPENAI_API</code> et <code>ENABLE_OLLAMA_API</code> sont respectivement désactivés et activés, permettant les interactions uniquement via Ollama.  </li>
   <li><code>ENABLE_IMAGE_GENERATION</code> est activé, vous permettant de générer des images depuis l’interface.  </li>
   <li><code>IMAGE_GENERATION_ENGINE</code> est réglé sur automatic1111 (SD.Next est compatible).</li></p><p><li>ComfyUI  </li>
   <li>L’interface graphique la plus puissante et modulaire pour les modèles de diffusion, API et backend avec interface graphique par nœuds.  </li>
   <li>Utilise comme conteneur de base l’<a href="https://pytorch-extension.intel.com/installation?platform=gpu" target="_blank" rel="noopener noreferrer">Intel® Extension for PyTorch</a> officiel.</li></p><p><li>SD.Next  </li>
   <li>Solution tout-en-un pour l’image générative IA basée sur Automatic1111.  </li>
   <li>Utilise comme conteneur de base l’<a href="https://pytorch-extension.intel.com/installation?platform=gpu" target="_blank" rel="noopener noreferrer">Intel® Extension for PyTorch</a> officiel.  </li>
   <li>Utilise une version personnalisée du <a href="https://github.com/vladmandic/sdnext/blob/dev/configs/Dockerfile.ipex" target="_blank" rel="noopener noreferrer">docker file</a> de SD.Next, le rendant compatible avec l’image Intel Extension for Pytorch.</li></p><p><li>OpenAI Whisper</li>
   <li>Reconnaissance vocale robuste via une supervision faible à grande échelle</li>
   <li>Utilise comme conteneur de base l'<a href="https://pytorch-extension.intel.com/installation?platform=gpu" target="_blank" rel="noopener noreferrer">extension officielle Intel® pour PyTorch</a></li></p><p></ul><h2>Configuration</h2>
Exécutez les commandes suivantes pour démarrer votre instance Ollama avec Open WebUI
<pre><code class="language-bash">$ git clone https://github.com/eleiton/ollama-intel-arc.git
$ cd ollama-intel-arc
$ podman compose up</code></pre></p><p>De plus, si vous souhaitez exécuter un ou plusieurs des outils de génération d'images, lancez ces commandes dans un terminal différent :</p><p>Pour ComfyUI
<pre><code class="language-bash">$ podman compose -f docker-compose.comfyui.yml up</code></pre>
Pour SD.Next</p><pre><code class="language-bash">$ podman compose -f docker-compose.sdnext.yml up</code></pre></p><p>Si vous souhaitez utiliser Whisper pour la reconnaissance automatique de la parole, exécutez cette commande dans un autre terminal :
<pre><code class="language-bash">$ podman compose -f docker-compose.whisper.yml up</code></pre></p><h2>Valider</h2>
Exécutez la commande suivante pour vérifier que votre instance Ollama est opérationnelle
<pre><code class="language-bash">$ curl http://localhost:11434/
Ollama is running</code></pre>
Lors de l'utilisation de Open WebUI, vous devriez voir cette sortie partielle dans votre console, indiquant que votre GPU arc a été détecté
<pre><code class="language-bash">[ollama-intel-arc] | Found 1 SYCL devices:
[ollama-intel-arc] | |  |                   |                                       |       |Max    |        |Max  |Global |                     |
[ollama-intel-arc] | |  |                   |                                       |       |compute|Max work|sub  |mem    |                     |
[ollama-intel-arc] | |ID|        Device Type|                                   Name|Version|units  |group   |group|size   |       Driver version|
[ollama-intel-arc] | |--|-------------------|---------------------------------------|-------|-------|--------|-----|-------|---------------------|
[ollama-intel-arc] | | 0| [level_zero:gpu:0]|                     Intel Arc Graphics|  12.71|    128|    1024|   32| 62400M|         1.6.32224+14|</code></pre></p><h2>Utilisation de la génération d'images</h2>
<ul><li>Ouvrez votre navigateur web à l'adresse http://localhost:7860 pour accéder à la page web SD.Next.</li>
<li>Pour cette démonstration, nous utiliserons le modèle <a href="https://civitai.com/models/4384/dreamshaper" target="_blank" rel="noopener noreferrer">DreamShaper</a>.</li>
<li>Suivez ces étapes :</li>
<li>Téléchargez le modèle <code>dreamshaper_8</code> en cliquant sur son image (1).</li>
<li>Attendez qu'il soit téléchargé (~2Go) puis sélectionnez-le dans le menu déroulant (2).</li>
<li>(Optionnel) Si vous souhaitez rester dans l'interface SD.Next, n'hésitez pas à explorer (3).</li>
</ul><img src="https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/resources/sd.next.png" alt="screenshot">
<ul><li>Pour plus d'informations sur l'utilisation de SD.Next, consultez la <a href="https://vladmandic.github.io/sdnext-docs/" target="_blank" rel="noopener noreferrer">documentation officielle</a>.</li>
<li>Ouvrez votre navigateur web à l'adresse http://localhost:4040 pour accéder à la page web Open WebUI.</li>
<li>Rendez-vous sur la page des <a href="http://localhost:4040/admin/settings" target="_blank" rel="noopener noreferrer">paramètres</a> administrateur.</li>
<li>Allez dans la section Image (1)</li>
<li>Assurez-vous que tous les paramètres sont corrects, et validez-les en appuyant sur le bouton de rafraîchissement (2)</li>
<li>(Optionnel) Enregistrez les modifications si vous en avez fait. (3)</li>
</ul><img src="https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/resources/open-webui-settings.png" alt="screenshot">
<ul><li>Pour plus d'informations sur l'utilisation d'Open WebUI, consultez la <a href="https://docs.openwebui.com/" target="_blank" rel="noopener noreferrer">documentation officielle</a></li>
<li>Voilà, retournez à la page principale d'Open WebUI et commencez à discuter. Assurez-vous de sélectionner le bouton <code>Image</code> pour indiquer que vous souhaitez générer des images.</li>
</ul><img src="https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/resources/open-webui-chat.png" alt="screenshot"></p><h2>Utilisation de la reconnaissance automatique de la parole</h2>
<ul><li>Voici un exemple de commande pour transcrire des fichiers audio :</li>
</ul><pre><code class="language-bash">  podman exec -it  whisper-ipex whisper https://www.lightbulblanguages.co.uk/resources/ge-audio/hobbies-ge.mp3 --device xpu --model small --language German --task transcribe</code></pre>
<ul><li>Réponse :</li>
</ul><pre><code class="language-bash">  [00:00.000 --> 00:08.000]  Ich habe viele Hobbys. In meiner Freizeit mache ich sehr gerne Sport, wie zum Beispiel Wasserball oder Radfahren.
  [00:08.000 --> 00:13.000]  Außerdem lese ich gerne und lerne auch gerne Fremdsprachen.
  [00:13.000 --> 00:19.000]  Ich gehe gerne ins Kino, höre gerne Musik und treffe mich mit meinen Freunden.
  [00:19.000 --> 00:22.000]  Früher habe ich auch viel Basketball gespielt.
  [00:22.000 --> 00:26.000]  Im Frühling und im Sommer werde ich viele Radtouren machen.
  [00:26.000 --> 00:29.000]  Außerdem werde ich viel schwimmen gehen.
  [00:29.000 --> 00:33.000]  Am liebsten würde ich das natürlich im Meer machen.</code></pre>
<ul><li>Ceci est un exemple de commande pour traduire des fichiers audio :</li>
</ul><pre><code class="language-bash">  podman exec -it  whisper-ipex whisper https://www.lightbulblanguages.co.uk/resources/ge-audio/hobbies-ge.mp3 --device xpu --model small --language German --task translate</code></pre>
<ul><li>Réponse :</li>
</ul><pre><code class="language-bash">  [00:00.000 --> 00:02.000]  I have a lot of hobbies.
  [00:02.000 --> 00:05.000]  In my free time I like to do sports,
  [00:05.000 --> 00:08.000]  such as water ball or cycling.
  [00:08.000 --> 00:10.000]  Besides, I like to read
  [00:10.000 --> 00:13.000]  and also like to learn foreign languages.
  [00:13.000 --> 00:15.000]  I like to go to the cinema,
  [00:15.000 --> 00:16.000]  like to listen to music
  [00:16.000 --> 00:19.000]  and meet my friends.
  [00:19.000 --> 00:22.000]  I used to play a lot of basketball.
  [00:22.000 --> 00:26.000]  In spring and summer I will do a lot of cycling tours.
  [00:26.000 --> 00:29.000]  Besides, I will go swimming a lot.
  [00:29.000 --> 00:33.000]  Of course, I would prefer to do this in the sea.</code></pre>
<ul><li>Pour utiliser vos propres fichiers audio au lieu des fichiers web, placez-les dans le dossier <code>~/whisper-files</code> et accédez-y ainsi :</li>
</ul><pre><code class="language-bash">  podman exec -it  whisper-ipex whisper YOUR_FILE_NAME.mp3 --device xpu --model small --task translate</code></pre></p><h2>Mise à jour des conteneurs</h2>
S'il y a de nouvelles mises à jour dans l'image Docker <a href="https://hub.docker.com/r/intelanalytics/ipex-llm-inference-cpp-xpu" target="_blank" rel="noopener noreferrer">ipex-llm-inference-cpp-xpu</a> ou dans l'image Docker Open WebUI, vous souhaiterez peut-être mettre à jour vos conteneurs pour rester à jour.</p><p>Avant toute mise à jour, assurez-vous d'arrêter vos conteneurs
<pre><code class="language-bash">$ podman compose down </code></pre></p><p>Ensuite, exécutez simplement une commande pull pour récupérer les images <code>latest</code>.
<pre><code class="language-bash">$ podman compose pull</code></pre>
Après cela, vous pouvez exécuter compose up pour redémarrer vos services.</p><pre><code class="language-bash">$ podman compose up</code></pre></p><h2>Connexion manuelle à votre conteneur Ollama</h2>
Vous pouvez vous connecter directement à votre conteneur Ollama en exécutant ces commandes :</p><pre><code class="language-bash">$ podman exec -it ollama-intel-arc /bin/bash
$ /llm/ollama/ollama -v</code></pre></p><h2>Mon environnement de développement :</h2>
<ul><li>Core Ultra 7 155H</li>
<li>Intel® Arc™ Graphics (Meteor Lake-P)</li>
<li>Fedora 41</li></p><p></ul><h2>Références </h2>
<ul><li><a href="https://docs.openwebui.com/" target="_blank" rel="noopener noreferrer">Documentation Open WebUI</a></li>
<li><a href="https://hub.docker.com/r/intelanalytics/ipex-llm-serving-xpu/tags" target="_blank" rel="noopener noreferrer">Docker - tags Intel ipex-llm</a></li>
<li><a href="https://hub.docker.com/r/intel/intel-extension-for-pytorch/tags" target="_blank" rel="noopener noreferrer">Docker - extension Intel pour pytorch</a></li>
<li><a href="https://github.com/intel/ipex-llm/tags" target="_blank" rel="noopener noreferrer">GitHub - tags Intel ipex-llm</a></li>
<li><a href="https://github.com/intel/intel-extension-for-pytorch/tags" target="_blank" rel="noopener noreferrer">GitHub - extension Intel pour pytorch</a></li>

</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2026-02-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/eleiton/ollama-intel-arc/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2026-02-16 
    </div>
    
</body>
</html>