<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Dutch. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Dutch. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Dutch, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Dutch. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-nl.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Dutch</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch is een Python-pakket dat twee belangrijke kenmerken biedt:
<ul><li>Tensor-berekeningen (zoals NumPy) met krachtige GPU-versnelling</li>
<li>Diepe neurale netwerken gebouwd op een tape-gebaseerd autograd-systeem</li></p><p></ul>Je kunt je favoriete Python-pakketten zoals NumPy, SciPy en Cython hergebruiken om PyTorch uit te breiden wanneer nodig.</p><p>Onze trunk health (Continuous Integration-signalen) zijn te vinden op <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#meer-over-pytorch" target="_blank" rel="noopener noreferrer">Meer Over PyTorch</a></li>
  <li><a href="#een-gpu-klare-tensorbibliotheek" target="_blank" rel="noopener noreferrer">Een GPU-Klare Tensorbibliotheek</a></li>
  <li><a href="#dynamische-neurale-netwerken-tape-based-autograd" target="_blank" rel="noopener noreferrer">Dynamische Neurale Netwerken: Tape-Based Autograd</a></li>
  <li><a href="#python-eerst" target="_blank" rel="noopener noreferrer">Python Eerst</a></li>
  <li><a href="#imperatieve-ervaringen" target="_blank" rel="noopener noreferrer">Imperatieve Ervaringen</a></li>
  <li><a href="#snel-en-lichtgewicht" target="_blank" rel="noopener noreferrer">Snel en Lichtgewicht</a></li>
  <li><a href="#uitbreidingen-zonder-pijn" target="_blank" rel="noopener noreferrer">Uitbreidingen Zonder Pijn</a></li>
<li><a href="#installatie" target="_blank" rel="noopener noreferrer">Installatie</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">Binaries</a></li>
    <li><a href="#nvidia-jetson-platformen" target="_blank" rel="noopener noreferrer">NVIDIA Jetson Platformen</a></li>
  <li><a href="#vanuit-broncode" target="_blank" rel="noopener noreferrer">Vanuit Broncode</a></li>
    <li><a href="#vereisten" target="_blank" rel="noopener noreferrer">Vereisten</a></li>
      <li><a href="#nvidia-cuda-ondersteuning" target="_blank" rel="noopener noreferrer">NVIDIA CUDA Ondersteuning</a></li>
      <li><a href="#amd-rocm-ondersteuning" target="_blank" rel="noopener noreferrer">AMD ROCm Ondersteuning</a></li>
      <li><a href="#intel-gpu-ondersteuning" target="_blank" rel="noopener noreferrer">Intel GPU Ondersteuning</a></li>
    <li><a href="#download-de-pytorch-broncode" target="_blank" rel="noopener noreferrer">Download de PyTorch Broncode</a></li>
    <li><a href="#installeer-afhankelijkheden" target="_blank" rel="noopener noreferrer">Installeer Afhankelijkheden</a></li>
    <li><a href="#installeer-pytorch" target="_blank" rel="noopener noreferrer">Installeer PyTorch</a></li>
      <li><a href="#aanpassen-van-build-opties-optioneel" target="_blank" rel="noopener noreferrer">Aanpassen van Build Opties (Optioneel)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Docker Image</a></li>
    <li><a href="#gebruik-van-vooraf-gebouwde-images" target="_blank" rel="noopener noreferrer">Gebruik van vooraf gebouwde images</a></li>
    <li><a href="#image-zelf-bouwen" target="_blank" rel="noopener noreferrer">Image zelf bouwen</a></li>
  <li><a href="#documentatie-bouwen" target="_blank" rel="noopener noreferrer">Documentatie Bouwen</a></li>
    <li><a href="#pdf-bouwen" target="_blank" rel="noopener noreferrer">PDF Bouwen</a></li>
  <li><a href="#vorige-versies" target="_blank" rel="noopener noreferrer">Vorige Versies</a></li>
<li><a href="#aan-de-slag" target="_blank" rel="noopener noreferrer">Aan de slag</a></li>
<li><a href="#bronnen" target="_blank" rel="noopener noreferrer">Bronnen</a></li>
<li><a href="#communicatie" target="_blank" rel="noopener noreferrer">Communicatie</a></li>
<li><a href="#releases-en-bijdragen" target="_blank" rel="noopener noreferrer">Releases en Bijdragen</a></li>
<li><a href="#het-team" target="_blank" rel="noopener noreferrer">Het Team</a></li>
<li><a href="#licentie" target="_blank" rel="noopener noreferrer">Licentie</a></li></p><p></ul><!-- tocstop --></p><h2>Meer Over PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Leer de basis van PyTorch</a></p><p>Op gedetailleerd niveau is PyTorch een bibliotheek die bestaat uit de volgende componenten:</p><p>| Component | Beschrijving |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Een Tensor-bibliotheek zoals NumPy, met sterke GPU-ondersteuning |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Een tape-gebaseerde automatische differentiatiebibliotheek die alle differentieerbare Tensor-operaties in torch ondersteunt |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Een compilatiestack (TorchScript) om seraliseerbare en optimaliseerbare modellen te creëren vanuit PyTorch-code  |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Een neurale netwerken-bibliotheek diep geïntegreerd met autograd, ontworpen voor maximale flexibiliteit |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Python multiprocessing, maar met magisch geheugen delen van torch Tensors tussen processen. Handig voor dataloading en Hogwild training |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader en andere handige hulpfuncties |</p><p>Meestal wordt PyTorch gebruikt als:</p><ul><li>Een vervanging voor NumPy om de kracht van GPU's te benutten.</li>
<li>Een deep learning onderzoeksplatform dat maximale flexibiliteit en snelheid biedt.</li></p><p></ul>Uitgebreidere uitleg:</p><h3>Een GPU-Klare Tensorbibliotheek</h3></p><p>Als je NumPy gebruikt, heb je Tensors (ook wel ndarray genoemd) gebruikt.</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustratie"></p><p>PyTorch biedt Tensors die zowel op de CPU als op de GPU kunnen draaien en versnelt
de berekening aanzienlijk.</p><p>We bieden een breed scala aan tensor-routines om je wetenschappelijke berekeningsbehoeften te versnellen en te ondersteunen, zoals slicing, indexering, wiskundige operaties, lineaire algebra, reducties.
En ze zijn snel!</p><h3>Dynamische Neurale Netwerken: Tape-Based Autograd</h3></p><p>PyTorch heeft een unieke manier om neurale netwerken te bouwen: door het gebruik van en het afspelen van een bandrecorder.</p><p>De meeste frameworks zoals TensorFlow, Theano, Caffe en CNTK hebben een statische kijk op de wereld.
Je moet een neuraal netwerk bouwen en steeds dezelfde structuur opnieuw gebruiken.
Als je het gedrag van het netwerk wilt veranderen, moet je helemaal opnieuw beginnen.</p><p>Met PyTorch gebruiken we een techniek die reverse-mode auto-differentiation heet, waarmee je het gedrag van je netwerk willekeurig kunt aanpassen zonder vertraging of overhead. Onze inspiratie komt
van verschillende onderzoeksartikelen over dit onderwerp, evenals huidig en vorig werk zoals
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a>, enz.</p><p>Hoewel deze techniek niet uniek is voor PyTorch, is het een van de snelste implementaties tot nu toe.
Je krijgt het beste van snelheid en flexibiliteit voor je wilde onderzoeksdoelen.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamisch graaf"></p><h3>Python Eerst</h3></p><p>PyTorch is geen Python-binding voor een monolithisch C++-framework.
Het is gebouwd om diep geïntegreerd te zijn in Python.
Je kunt het op een natuurlijke manier gebruiken, zoals je <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> zou gebruiken.
Je kunt je nieuwe neurale netwerk-lagen direct in Python schrijven, met je favoriete bibliotheken
en pakketten zoals <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> en <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Ons doel is om het wiel niet opnieuw uit te vinden waar dat niet nodig is.</p><h3>Imperatieve Ervaringen</h3></p><p>PyTorch is ontworpen om intuïtief, lineair in denken en eenvoudig in gebruik te zijn.
Wanneer je een regel code uitvoert, wordt deze uitgevoerd. Er is geen asynchroon wereldbeeld.
Wanneer je een debugger gebruikt of foutmeldingen en stacktraces ontvangt, zijn deze gemakkelijk te begrijpen.
De stacktrace wijst precies naar waar je code is gedefinieerd.
We hopen dat je nooit uren hoeft te besteden aan het debuggen van je code door slechte stacktraces of asynchrone en ondoorzichtige uitvoering-engines.</p><h3>Snel en Lichtgewicht</h3></p><p>PyTorch heeft minimale framework overhead. We integreren versnellingsbibliotheken
zoals <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> en NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) om snelheid te maximaliseren.
In de kern zijn de CPU- en GPU-Tensor- en neurale netwerkbackends
volwassen en al jarenlang getest.</p><p>PyTorch is dus erg snel — of je nu kleine of grote neurale netwerken draait.</p><p>Het geheugengebruik in PyTorch is extreem efficiënt in vergelijking met Torch of sommige alternatieven.
We hebben aangepaste geheugenallocators voor de GPU geschreven om ervoor te zorgen dat
je deep learning modellen maximaal geheugenefficiënt zijn.
Hierdoor kun je grotere deep learning modellen trainen dan voorheen.</p><h3>Uitbreidingen Zonder Pijn</h3></p><p>Het schrijven van nieuwe neurale netwerkmodules, of het koppelen aan de Tensor-API van PyTorch, is ontworpen om eenvoudig en met minimale abstracties te zijn.</p><p>Je kunt nieuwe neurale netwerk-lagen schrijven in Python met behulp van de torch API
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">of je favoriete op NumPy gebaseerde bibliotheken zoals SciPy</a>.</p><p>Als je je lagen in C/C++ wilt schrijven, bieden we een handige extension API die efficiënt is en minimale boilerplate vereist.
Er hoeft geen wrapper-code te worden geschreven. Zie <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">een tutorial hier</a> en <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">een voorbeeld hier</a>.</p><h2>Installatie</h2></p><h3>Binaries</h3>
Commando’s om binaries te installeren via Conda of pip wheels staan op onze website: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>
#### NVIDIA Jetson Platformen</p><p>Python wheels voor NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, en Jetson AGX Orin zijn <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">hier</a> beschikbaar en de L4T-container wordt gepubliceerd <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">hier</a></p><p>Ze vereisen JetPack 4.2 en hoger, en <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> en <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> onderhouden deze.</p><h3>Vanuit Broncode</h3></p><p>#### Vereisten
Als je vanuit broncode installeert, heb je nodig:
<ul><li>Python 3.9 of hoger</li>
<li>Een compiler die volledig C++17 ondersteunt, zoals clang of gcc (gcc 9.4.0 of nieuwer is vereist op Linux)</li>
<li>Visual Studio of Visual Studio Build Tool (alleen Windows)</li></p><p></ul>\* PyTorch CI gebruikt Visual C++ BuildTools, die bij Visual Studio Enterprise,
Professional, of Community Editions worden geleverd. Je kunt de build tools ook installeren via
https://visualstudio.microsoft.com/visual-cpp-build-tools/. De build tools worden <em>niet</em>
standaard meegeleverd met Visual Studio Code.</p><p>Een voorbeeld van een omgevingssetup wordt hieronder getoond:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### NVIDIA CUDA Ondersteuning
Als je wilt compileren met CUDA-ondersteuning, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">selecteer een ondersteunde versie van CUDA uit onze supportmatrix</a>, en installeer dan het volgende:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 of hoger</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compiler</a> compatibel met CUDA</li></p><p></ul>Opmerking: Je kunt verwijzen naar de <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">cuDNN Support Matrix</a> voor cuDNN-versies met de verschillende ondersteunde CUDA, CUDA-driver en NVIDIA-hardware</p><p>Als je CUDA-ondersteuning wilt uitschakelen, exporteer dan de omgevingsvariabele <code>USE_CUDA=0</code>.
Andere mogelijk nuttige omgevingsvariabelen zijn te vinden in <code>setup.py</code>.</p><p>Als je bouwt voor NVIDIA's Jetson-platformen (Jetson Nano, TX1, TX2, AGX Xavier), zijn de instructies om PyTorch te installeren voor Jetson Nano <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">hier beschikbaar</a></p><p>##### AMD ROCm Ondersteuning
Als je wilt compileren met ROCm-ondersteuning, installeer dan
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 of hoger</li>
<li>ROCm wordt momenteel alleen ondersteund voor Linux-systemen.</li></p><p></ul>Standaard verwacht het buildsysteem dat ROCm is geïnstalleerd in <code>/opt/rocm</code>. Als ROCm in een andere map is geïnstalleerd, moet de omgevingsvariabele <code>ROCM_PATH</code> worden ingesteld op de ROCm-installatiemap. Het buildsysteem detecteert de AMD GPU-architectuur automatisch. Optioneel kan de AMD GPU-architectuur expliciet worden ingesteld met de omgevingsvariabele <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">AMD GPU architectuur</a></p><p>Als je ROCm-ondersteuning wilt uitschakelen, exporteer dan de omgevingsvariabele <code>USE_ROCM=0</code>.
Andere mogelijk nuttige omgevingsvariabelen zijn te vinden in <code>setup.py</code>.</p><p>##### Intel GPU Ondersteuning
Als je wilt compileren met Intel GPU-ondersteuning, volg dan deze
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">PyTorch Vereisten voor Intel GPU's</a> instructies.</li>
<li>Intel GPU wordt ondersteund voor Linux en Windows.</li></p><p></ul>Als je Intel GPU-ondersteuning wilt uitschakelen, exporteer dan de omgevingsvariabele <code>USE_XPU=0</code>.
Andere mogelijk nuttige omgevingsvariabelen zijn te vinden in <code>setup.py</code>.</p><p>#### Download de PyTorch Broncode
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>als je een bestaande checkout bijwerkt</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Installeer Afhankelijkheden</p><p><strong>Algemeen</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Voer dit commando uit in de PyTorch-map nadat je de broncode hebt gekloond met het gedeelte “Download de PyTorch Broncode” hierboven</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Op Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Alleen CUDA: Voeg LAPACK-ondersteuning voor de GPU toe indien nodig</h1>
<h1>magma installatie: voer uit met actieve conda-omgeving. Specificeer CUDA-versie om te installeren</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(optioneel) Als je torch.compile gebruikt met inductor/triton, installeer de bijpassende versie van triton</h1>
<h1>Voer uit vanuit de pytorch-map na het klonen</h1>
<h1>Voor Intel GPU-ondersteuning, graag expliciet <code>export USE_XPU=1</code> uitvoeren voor dit commando.</h1>
make triton</code></pre></p><p><strong>Op MacOS</strong></p><pre><code class="language-bash"># Voeg dit pakket toe op Intel x86-processormachines
pip install mkl-static mkl-include
<h1>Voeg deze pakketten toe als torch.distributed nodig is</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Op Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Voeg deze pakketten toe als torch.distributed nodig is.</h1>
<h1>Distributed package support op Windows is een prototypefunctie en kan veranderen.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Installeer PyTorch
<strong>Op Linux</strong></p><p>Als je compileert voor AMD ROCm voer dan eerst dit commando uit:
<pre><code class="language-bash"># Alleen uitvoeren als je compileert voor ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Installeer PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>Op macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Op Windows</strong></p><p>Als je legacy python-code wilt bouwen, raadpleeg dan <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Building on legacy code and CUDA</a></p><p><strong>Alleen CPU-builds</strong></p><p>In deze modus draaien PyTorch-berekeningen op je CPU, niet op je GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Opmerking over OpenMP: De gewenste OpenMP-implementatie is Intel OpenMP (iomp). Om te linken tegen iomp, moet je de bibliotheek handmatig downloaden en de buildomgeving instellen door <code>CMAKE_INCLUDE_PATH</code> en <code>LIB</code> aan te passen. De instructie <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">hier</a> is een voorbeeld voor het instellen van zowel MKL als Intel OpenMP. Zonder deze CMake-configuraties wordt Microsoft Visual C OpenMP-runtime (vcomp) gebruikt.</p><p><strong>CUDA-gebaseerde build</strong></p><p>In deze modus maken PyTorch-berekeningen gebruik van je GPU via CUDA voor snellere nummerverwerking</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> is nodig om Pytorch met CUDA te bouwen.
NVTX is onderdeel van de CUDA-distributie, waar het "Nsight Compute" heet. Om het te installeren op een reeds geïnstalleerde CUDA, voer de CUDA-installatie opnieuw uit en vink het juiste selectievakje aan.
Zorg dat CUDA met Nsight Compute is geïnstalleerd na Visual Studio.</p><p>Momenteel worden VS 2017 / 2019, en Ninja ondersteund als generator voor CMake. Als <code>ninja.exe</code> wordt gedetecteerd in <code>PATH</code>, wordt Ninja gebruikt als standaardgenerator, anders wordt VS 2017 / 2019 gebruikt.
<br/> Als Ninja is geselecteerd als generator, wordt de nieuwste MSVC geselecteerd als de onderliggende toolchain.</p><p>Aanvullende bibliotheken zoals
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, ook bekend als MKLDNN of DNNL</a>, en <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> zijn vaak nodig. Raadpleeg de <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> om ze te installeren.</p><p>Je kunt verwijzen naar het <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> script voor andere omgevingsvariabelenconfiguraties.</p><pre><code class="language-cmd">cmd</p><p>:: Stel de omgevingsvariabelen in nadat je het mkl-pakket hebt gedownload en uitgepakt,
:: anders geeft CMake een fout als <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Je map}\mkl\include
set LIB={Je map}\mkl\lib;%LIB%</p><p>:: Lees de inhoud in het vorige gedeelte zorgvuldig voordat je doorgaat.
:: [Optioneel] Als je de onderliggende toolset voor Ninja en Visual Studio met CUDA wilt overschrijven, voer dan het volgende scriptblok uit.
:: "Visual Studio 2019 Developer Command Prompt" wordt automatisch uitgevoerd.
:: Zorg dat je CMake >= 3.12 hebt voordat je dit doet als je de Visual Studio-generator gebruikt.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Optioneel] Als je de CUDA host compiler wilt overschrijven
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Intel GPU builds</strong></p><p>In deze modus wordt PyTorch met Intel GPU-ondersteuning gebouwd.</p><p>Zorg dat <a href="#vereisten" target="_blank" rel="noopener noreferrer">de algemene vereisten</a> evenals <a href="#intel-gpu-ondersteuning" target="_blank" rel="noopener noreferrer">de vereisten voor Intel GPU</a> correct zijn geïnstalleerd en de omgevingsvariabelen zijn geconfigureerd voordat je begint met bouwen. Voor ondersteuning van build tools is <code>Visual Studio 2022</code> vereist.</p><p>Daarna kan PyTorch worden gebouwd met het commando:</p><pre><code class="language-cmd">:: CMD Commando's:
:: Stel de CMAKE_PREFIX_PATH in om de juiste pakketten te vinden
:: %CONDA_PREFIX% werkt alleen na <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Aanpassen van Build Opties (Optioneel)</p><p>Je kunt de configuratie van CMake-variabelen optioneel aanpassen (zonder eerst te bouwen) door het volgende te doen. Bijvoorbeeld het aanpassen van de vooraf gedetecteerde mappen voor CuDNN of BLAS kan zo worden gedaan.</p><p>Op Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # of cmake-gui build</code></pre></p><p>Op macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # of cmake-gui build</code></pre></p><h3>Docker Image</h3></p><p>#### Gebruik van vooraf gebouwde images</p><p>Je kunt ook een vooraf gebouwde docker image van Docker Hub halen en draaien met docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Let op: PyTorch gebruikt gedeeld geheugen om data tussen processen te delen, dus als torch multiprocessing wordt gebruikt (bijv.
voor multithreaded data loaders) is het standaardgedeelde geheugensegment dat de container gebruikt niet groot genoeg, en moet je de grootte van het gedeelde geheugen verhogen met de <code>--ipc=host</code> of <code>--shm-size</code> commandoregelopties voor <code>nvidia-docker run</code>.</p><p>#### Image zelf bouwen</p><p><strong>OPMERKING:</strong> Moet worden gebouwd met een docker-versie > 18.06</p><p>Het <code>Dockerfile</code> wordt meegeleverd om images te bouwen met CUDA 11.1-ondersteuning en cuDNN v8.
Je kunt de <code>PYTHON_VERSION=x.y</code> make-variabele meegeven om de gewenste Python-versie te specificeren voor Miniconda, of deze leeg laten om de standaard te gebruiken.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>images worden getagd als docker.io/${your_docker_username}/pytorch</code></pre></h1></p><p>Je kunt ook de omgevingsvariabele <code>CMAKE_VARS="..."</code> doorgeven om extra CMake-variabelen te specificeren die aan CMake worden doorgegeven tijdens het bouwen.
Zie <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> voor de lijst met beschikbare variabelen.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Documentatie Bouwen</h3></p><p>Om documentatie in verschillende formaten te bouwen, heb je <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
en het pytorch_sphinx_theme2 nodig.</p><p>Voordat je de documentatie lokaal bouwt, zorg dat <code>torch</code> is
geïnstalleerd in je omgeving. Voor kleine aanpassingen kun je de
nightly versie installeren zoals beschreven in <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Aan de slag</a>.</p><p>Voor meer complexe aanpassingen, zoals het toevoegen van een nieuw module en docstrings voor
de nieuwe module, moet je mogelijk torch <a href="#vanuit-broncode" target="_blank" rel="noopener noreferrer">vanuit broncode installeren</a>.
Zie <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Docstring Guidelines</a>
voor docstring conventies.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Voer <code>make</code> uit om een lijst te krijgen van alle beschikbare uitvoerformaten.</p><p>Als je een katex-fout krijgt, voer dan <code>npm install katex</code> uit.  Als het blijft aanhouden, probeer dan
<code>npm install -g katex</code></p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Als je <code>nodejs</code> met een andere pakketmanager hebt geïnstalleerd (bijv.</blockquote>
<blockquote><code>conda</code>) dan zal <code>npm</code> waarschijnlijk een versie van <code>katex</code> installeren die niet</blockquote>
<blockquote>compatibel is met je versie van <code>nodejs</code> en zullen doc builds mislukken.</blockquote>
<blockquote>Een combinatie van versies die werkt is <code>node@6.13.1</code> en</blockquote>
<blockquote><code>katex@0.13.18</code>. Om laatstgenoemde met <code>npm</code> te installeren kun je uitvoeren</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!OPMERKING]</blockquote>
<blockquote>Als je een numpy-incompatibiliteitsfout ziet, voer uit:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Als je wijzigingen aanbrengt in de afhankelijkheden die door CI worden uitgevoerd, bewerk dan het bestand
</code>.ci/docker/requirements-docs.txt<code>.</p><p>#### PDF Bouwen</p><p>Om een PDF van alle PyTorch-documentatie te compileren, zorg dat je
</code>texlive<code> en LaTeX hebt geïnstalleerd. Op macOS kun je deze installeren met:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Om de PDF te maken:</p><ul><li>Voer uit:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Dit genereert de benodigde bestanden in de map </code>build/latex<code>.</p><ul><li>Navigeer naar deze map en voer uit:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Dit produceert een </code>pytorch.pdf` met de gewenste inhoud. Voer dit
   commando nogmaals uit zodat de juiste inhoudsopgave
   en index worden gegenereerd.</p><blockquote>[!OPMERKING]</blockquote>
<blockquote>Om de inhoudsopgave te bekijken, schakel over naar de <strong>Inhoudsopgave</strong></blockquote>
<blockquote>weergave in je PDF-lezer.</blockquote></p><h3>Vorige Versies</h3></p><p>Installatie-instructies en binaries voor eerdere PyTorch-versies zijn te vinden
op <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">onze website</a>.</p><h2>Aan de slag</h2></p><p>Drie startpunten om je op weg te helpen:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutorials: helpen je op weg met het begrijpen en gebruiken van PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Voorbeelden: gemakkelijk te begrijpen PyTorch-code in alle domeinen</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">De API Referentie</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Woordenlijst</a></li></p><p></ul><h2>Bronnen</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">PyTorch Tutorials</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">PyTorch Voorbeelden</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">PyTorch Modellen</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Intro tot Deep Learning met PyTorch van Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Intro tot Machine Learning met PyTorch van Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Diepe Neurale Netwerken met PyTorch van Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Communicatie</h2>
<ul><li>Forums: Bespreek implementaties, onderzoek, enz. https://discuss.pytorch.org</li>
<li>GitHub Issues: Bugrapporten, feature requests, installatieproblemen, RFC's, ideeën, enz.</li>
<li>Slack: De <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">PyTorch Slack</a> is vooral voor gevorderde PyTorch-gebruikers en ontwikkelaars voor algemene chat, online discussies, samenwerking, enz. Als je een beginner bent die hulp zoekt, is het primaire medium <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch Forums</a>. Als je een slack-uitnodiging nodig hebt, vul dan dit formulier in: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Nieuwsbrief: Geen spam, een eenzijdige e-mailnieuwsbrief met belangrijke aankondigingen over PyTorch. Je kunt je hier aanmelden: https://eepurl.com/cbG0rv</li>
<li>Facebook-pagina: Belangrijke aankondigingen over PyTorch. https://www.facebook.com/pytorch</li>
<li>Voor merkrichtlijnen, bezoek onze website op <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Releases en Bijdragen</h2></p><p>Typisch heeft PyTorch drie kleine releases per jaar. Laat het ons weten als je een bug tegenkomt door <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">een issue in te dienen</a>.</p><p>We waarderen alle bijdragen. Als je van plan bent om bugfixes bij te dragen, doe dit dan zonder verdere discussie.</p><p>Als je van plan bent om nieuwe functies, hulpfuncties of extensies aan de kern toe te voegen, open dan eerst een issue en bespreek de functie met ons.
Een PR sturen zonder discussie kan resulteren in een afgewezen PR omdat we de kern mogelijk in een andere richting sturen dan je verwacht.</p><p>Meer leren over bijdragen aan Pytorch? Zie onze <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Bijdragepagina</a>. Meer informatie over PyTorch-releases vind je op de <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">Releasepagina</a>.</p><h2>Het Team</h2></p><p>PyTorch is een community-gedreven project met verschillende bekwame ingenieurs en onderzoekers die bijdragen.</p><p>PyTorch wordt momenteel onderhouden door <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, en <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a> met grote bijdragen van honderden getalenteerde individuen in diverse vormen en middelen.
Een niet-uitputtende maar groeiende lijst noemt: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Opmerking: Dit project is niet gerelateerd aan <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> met dezelfde naam. Hugh is een waardevolle bijdrager aan de Torch-community en heeft veel bijgedragen aan Torch en PyTorch.</p><h2>Licentie</h2></p><p>PyTorch heeft een BSD-achtige licentie, zoals te vinden in het <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a> bestand.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>