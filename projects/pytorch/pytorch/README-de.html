<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in German. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in German. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in German. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">German</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch ist ein Python-Paket, das zwei Hauptfunktionen bereitstellt:
<ul><li>Tensorberechnungen (ähnlich wie NumPy) mit starker GPU-Beschleunigung</li>
<li>Tiefe neuronale Netze, die auf einem bandbasierten Autograd-System basieren</li></p><p></ul>Sie können Ihre bevorzugten Python-Pakete wie NumPy, SciPy und Cython wiederverwenden, um PyTorch nach Bedarf zu erweitern.</p><p>Unsere Trunk-Gesundheit (Continuous Integration-Signale) finden Sie unter <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#mehr-über-pytorch" target="_blank" rel="noopener noreferrer">Mehr über PyTorch</a></li>
  <li><a href="#eine-gpu-bereite-tensor-bibliothek" target="_blank" rel="noopener noreferrer">Eine GPU-bereite Tensor-Bibliothek</a></li>
  <li><a href="#dynamische-neuronale-netze-bandbasierter-autograd" target="_blank" rel="noopener noreferrer">Dynamische neuronale Netze: Bandbasierter Autograd</a></li>
  <li><a href="#python-zuerst" target="_blank" rel="noopener noreferrer">Python zuerst</a></li>
  <li><a href="#imperative-erfahrungen" target="_blank" rel="noopener noreferrer">Imperative Erfahrungen</a></li>
  <li><a href="#schnell-und-schlank" target="_blank" rel="noopener noreferrer">Schnell und schlank</a></li>
  <li><a href="#erweiterungen-ohne-schmerzen" target="_blank" rel="noopener noreferrer">Erweiterungen ohne Schmerzen</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">Installation</a></li>
  <li><a href="#binärdateien" target="_blank" rel="noopener noreferrer">Binärdateien</a></li>
    <li><a href="#nvidia-jetson-plattformen" target="_blank" rel="noopener noreferrer">NVIDIA Jetson Plattformen</a></li>
  <li><a href="#aus-dem-quellcode" target="_blank" rel="noopener noreferrer">Aus dem Quellcode</a></li>
    <li><a href="#voraussetzungen" target="_blank" rel="noopener noreferrer">Voraussetzungen</a></li>
      <li><a href="#nvidia-cuda-unterstützung" target="_blank" rel="noopener noreferrer">NVIDIA CUDA Unterstützung</a></li>
      <li><a href="#amd-rocm-unterstützung" target="_blank" rel="noopener noreferrer">AMD ROCm Unterstützung</a></li>
      <li><a href="#intel-gpu-unterstützung" target="_blank" rel="noopener noreferrer">Intel GPU Unterstützung</a></li>
    <li><a href="#pytorch-quellcode-holen" target="_blank" rel="noopener noreferrer">PyTorch-Quellcode holen</a></li>
    <li><a href="#abhängigkeiten-installieren" target="_blank" rel="noopener noreferrer">Abhängigkeiten installieren</a></li>
    <li><a href="#pytorch-installieren" target="_blank" rel="noopener noreferrer">PyTorch installieren</a></li>
      <li><a href="#build-optionen-anpassen-optional" target="_blank" rel="noopener noreferrer">Build-Optionen anpassen (optional)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Docker-Image</a></li>
    <li><a href="#verwendung-vorgefertigter-images" target="_blank" rel="noopener noreferrer">Verwendung vorgefertigter Images</a></li>
    <li><a href="#image-selbst-bauen" target="_blank" rel="noopener noreferrer">Image selbst bauen</a></li>
  <li><a href="#dokumentation-bauen" target="_blank" rel="noopener noreferrer">Dokumentation bauen</a></li>
    <li><a href="#pdf-erstellung" target="_blank" rel="noopener noreferrer">PDF-Erstellung</a></li>
  <li><a href="#frühere-versionen" target="_blank" rel="noopener noreferrer">Frühere Versionen</a></li>
<li><a href="#erste-schritte" target="_blank" rel="noopener noreferrer">Erste Schritte</a></li>
<li><a href="#ressourcen" target="_blank" rel="noopener noreferrer">Ressourcen</a></li>
<li><a href="#kommunikation" target="_blank" rel="noopener noreferrer">Kommunikation</a></li>
<li><a href="#releases-und-mitwirkung" target="_blank" rel="noopener noreferrer">Releases und Mitwirkung</a></li>
<li><a href="#das-team" target="_blank" rel="noopener noreferrer">Das Team</a></li>
<li><a href="#lizenz" target="_blank" rel="noopener noreferrer">Lizenz</a></li></p><p></ul><!-- tocstop --></p><h2>Mehr über PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Erlernen Sie die Grundlagen von PyTorch</a></p><p>Im Detail ist PyTorch eine Bibliothek, die aus den folgenden Komponenten besteht:</p><p>| Komponente | Beschreibung |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Eine Tensor-Bibliothek wie NumPy mit starker GPU-Unterstützung |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Eine bandbasierte automatische Differenzierungsbibliothek, die alle differenzierbaren Tensoroperationen in torch unterstützt |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Ein Kompilierungs-Stack (TorchScript), um aus PyTorch-Code serialisierbare und optimierbare Modelle zu erstellen |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Eine mit Autograd tief integrierte Bibliothek für neuronale Netze, die maximale Flexibilität bietet |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Python-Multiprocessing, aber mit magischem Speicheraustausch von torch-Tensoren zwischen Prozessen. Nützlich für das Laden von Daten und Hogwild-Training |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader und andere Hilfsfunktionen zur Vereinfachung |</p><p>Normalerweise wird PyTorch wie folgt verwendet:</p><ul><li>Als Ersatz für NumPy, um die Leistung von GPUs zu nutzen.</li>
<li>Als Deep-Learning-Forschungsplattform, die maximale Flexibilität und Geschwindigkeit bietet.</li></p><p></ul>Weitere Erläuterungen:</p><h3>Eine GPU-bereite Tensor-Bibliothek</h3></p><p>Wenn Sie NumPy verwenden, haben Sie bereits Tensoren (auch bekannt als ndarray) verwendet.</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustration"></p><p>PyTorch bietet Tensoren, die entweder auf der CPU oder der GPU leben können und beschleunigt die
Berechnung enorm.</p><p>Wir bieten eine große Auswahl an Tensor-Routinen, um Ihre wissenschaftlichen Rechenanforderungen zu beschleunigen und anzupassen, wie z.B. Slicing, Indexierung, mathematische Operationen, lineare Algebra, Reduktionen.
Und sie sind schnell!</p><h3>Dynamische neuronale Netze: Bandbasierter Autograd</h3></p><p>PyTorch hat eine einzigartige Methode zum Aufbau neuronaler Netze: die Verwendung und Wiedergabe eines Bandrekorders.</p><p>Die meisten Frameworks wie TensorFlow, Theano, Caffe und CNTK haben eine statische Sicht auf die Welt.
Man muss ein neuronales Netz aufbauen und immer wieder die gleiche Struktur verwenden.
Wenn sich das Verhalten des Netzes ändern soll, muss man von vorne anfangen.</p><p>Mit PyTorch verwenden wir eine Technik namens Reverse-Mode Auto-Differenzierung, die es Ihnen ermöglicht,
das Verhalten Ihres Netzes beliebig zu ändern – ohne Verzögerung oder Overhead. Unsere Inspiration stammt
aus mehreren Forschungsarbeiten zu diesem Thema sowie aktueller und früherer Arbeiten wie
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a> usw.</p><p>Obwohl diese Technik nicht einzigartig für PyTorch ist, ist es eine der schnellsten Implementierungen davon bis heute.
Sie bekommen das Beste aus Geschwindigkeit und Flexibilität für Ihre anspruchsvolle Forschung.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamic graph"></p><h3>Python zuerst</h3></p><p>PyTorch ist keine Python-Bindung in ein monolithisches C++-Framework.
Es wurde entwickelt, um tief in Python integriert zu sein.
Sie können es ganz natürlich verwenden, wie Sie <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> usw. verwenden würden.
Sie können Ihre neuen neuronalen Netzwerkschichten direkt in Python mit Ihren bevorzugten Bibliotheken schreiben
und Pakete wie <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> und <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a> verwenden.
Unser Ziel ist es, das Rad nicht neu zu erfinden, wo es nicht nötig ist.</p><h3>Imperative Erfahrungen</h3></p><p>PyTorch wurde entwickelt, um intuitiv, linear im Denken und einfach zu bedienen zu sein.
Wenn Sie eine Codezeile ausführen, wird sie auch ausgeführt. Es gibt keine asynchrone Sicht auf die Welt.
Wenn Sie in einen Debugger eintauchen oder Fehlermeldungen und Stacktraces erhalten, ist das Verständnis einfach.
Der Stacktrace zeigt genau an, wo Ihr Code definiert wurde.
Wir hoffen, dass Sie nie Stunden mit dem Debuggen Ihres Codes verbringen, weil Stacktraces schlecht oder Ausführungs-Engines asynchron und undurchsichtig sind.</p><h3>Schnell und schlank</h3></p><p>PyTorch hat minimalen Framework-Overhead. Wir integrieren Beschleunigungsbibliotheken
wie <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> und NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>), um die Geschwindigkeit zu maximieren.
Im Kern sind die CPU- und GPU-Tensor- und neuronalen Netz-Backends
ausgereift und wurden über Jahre getestet.</p><p>Daher ist PyTorch sehr schnell – egal ob Sie kleine oder große neuronale Netze ausführen.</p><p>Der Speicherverbrauch von PyTorch ist im Vergleich zu Torch oder einigen Alternativen äußerst effizient.
Wir haben benutzerdefinierte Speicherallokatoren für die GPU geschrieben, um sicherzustellen, dass
Ihre Deep-Learning-Modelle maximal speichereffizient sind.
Dadurch können Sie größere Deep-Learning-Modelle als zuvor trainieren.</p><h3>Erweiterungen ohne Schmerzen</h3></p><p>Das Schreiben neuer neuronaler Netzwerk-Module oder die Interaktion mit der Tensor-API von PyTorch wurde so konzipiert, dass es einfach ist
und minimale Abstraktionen erfordert.</p><p>Sie können neue neuronale Netzwerkschichten in Python mit der torch-API schreiben
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">oder Ihre bevorzugten NumPy-basierten Bibliotheken wie SciPy verwenden</a>.</p><p>Wenn Sie Ihre Schichten in C/C++ schreiben möchten, bieten wir eine praktische Erweiterungs-API, die effizient ist und mit minimalem Overhead auskommt.
Es muss kein Wrapper-Code geschrieben werden. Siehe <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">ein Tutorial hier</a> und <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">ein Beispiel hier</a>.</p><h2>Installation</h2></p><h3>Binärdateien</h3>
Befehle zur Installation von Binärdateien via Conda oder pip wheels finden Sie auf unserer Webseite: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>
#### NVIDIA Jetson Plattformen</p><p>Python Wheels für NVIDIA Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX und Jetson AGX Orin finden Sie <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">hier</a> und der L4T-Container wird <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">hier</a> veröffentlicht.</p><p>Sie benötigen JetPack 4.2 oder höher, und <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> sowie <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> pflegen diese.</p><h3>Aus dem Quellcode</h3></p><p>#### Voraussetzungen
Wenn Sie aus dem Quellcode installieren, benötigen Sie:
<ul><li>Python 3.9 oder neuer</li>
<li>Einen Compiler, der C++17 vollständig unterstützt, wie clang oder gcc (gcc 9.4.0 oder neuer ist unter Linux erforderlich)</li>
<li>Visual Studio oder Visual Studio Build Tool (nur Windows)</li></p><p></ul>\* PyTorch CI verwendet Visual C++ BuildTools, die mit Visual Studio Enterprise,
Professional oder Community Editions mitgeliefert werden. Sie können die Build-Tools auch unter
https://visualstudio.microsoft.com/visual-cpp-build-tools/ installieren. Die Build-Tools <em>sind nicht</em>
standardmäßig bei Visual Studio Code enthalten.</p><p>Ein Beispiel für das Einrichten der Umgebung ist unten dargestellt:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### NVIDIA CUDA Unterstützung
Wenn Sie mit CUDA-Unterstützung kompilieren möchten, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">wählen Sie eine unterstützte CUDA-Version aus unserer Support-Matrix</a> und installieren Sie dann Folgendes:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 oder höher</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compiler</a>, der mit CUDA kompatibel ist</li></p><p></ul>Hinweis: Sie können die <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">cuDNN Support Matrix</a> für cuDNN-Versionen mit den verschiedenen unterstützten CUDA-, CUDA-Treiber- und NVIDIA-Hardware-Kombinationen zu Rate ziehen.</p><p>Wenn Sie die CUDA-Unterstützung deaktivieren möchten, exportieren Sie die Umgebungsvariable <code>USE_CUDA=0</code>.
Weitere nützliche Umgebungsvariablen finden Sie in <code>setup.py</code>.</p><p>Wenn Sie für NVIDIA Jetson Plattformen (Jetson Nano, TX1, TX2, AGX Xavier) bauen, finden Sie die Anweisungen zur Installation von PyTorch für Jetson Nano <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">hier</a></p><p>##### AMD ROCm Unterstützung
Wenn Sie mit ROCm-Unterstützung kompilieren möchten, installieren Sie
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 oder höher</li>
<li>ROCm wird aktuell nur für Linux-Systeme unterstützt.</li></p><p></ul>Standardmäßig erwartet das Build-System, dass ROCm in <code>/opt/rocm</code> installiert ist. Falls ROCm in einem anderen Verzeichnis installiert ist, muss die Umgebungsvariable <code>ROCM_PATH</code> auf das Installationsverzeichnis gesetzt werden. Die AMD-GPU-Architektur wird automatisch erkannt. Optional kann die Architektur explizit mit der Umgebungsvariable <code>PYTORCH_ROCM_ARCH</code> gesetzt werden <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">AMD GPU Architektur</a></p><p>Wenn Sie ROCm-Unterstützung deaktivieren möchten, exportieren Sie die Umgebungsvariable <code>USE_ROCM=0</code>.
Weitere nützliche Umgebungsvariablen finden Sie in <code>setup.py</code>.</p><p>##### Intel GPU Unterstützung
Wenn Sie mit Intel GPU-Unterstützung kompilieren möchten, folgen Sie diesen Anweisungen:
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">PyTorch-Voraussetzungen für Intel GPUs</a>.</li>
<li>Intel GPU wird für Linux und Windows unterstützt.</li></p><p></ul>Wenn Sie die Intel GPU-Unterstützung deaktivieren möchten, exportieren Sie die Umgebungsvariable <code>USE_XPU=0</code>.
Weitere nützliche Umgebungsvariablen finden Sie in <code>setup.py</code>.</p><p>#### PyTorch-Quellcode holen
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>wenn Sie ein bestehendes Checkout aktualisieren</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Abhängigkeiten installieren</p><p><strong>Allgemein</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Führen Sie diesen Befehl aus dem PyTorch-Verzeichnis aus, nachdem Sie den Quellcode mit dem Abschnitt „PyTorch-Quellcode holen“ geklont haben</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Unter Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Nur CUDA: Fügen Sie bei Bedarf LAPACK-Unterstützung für die GPU hinzu</h1>
<h1>magma-Installation: Führen Sie dies mit aktiviertem Conda-Umfeld aus. Geben Sie die zu installierende CUDA-Version an</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(optional) Wenn Sie torch.compile mit inductor/triton verwenden, installieren Sie die passende Version von triton</h1>
<h1>Führen Sie dies aus dem pytorch-Verzeichnis nach dem Klonen aus</h1>
<h1>Für Intel GPU-Unterstützung bitte explizit <code>export USE_XPU=1</code> vor dem Ausführen des Befehls setzen.</h1>
make triton</code></pre></p><p><strong>Unter MacOS</strong></p><pre><code class="language-bash"># Fügen Sie dieses Paket nur auf Intel x86-Prozessoren hinzu
pip install mkl-static mkl-include
<h1>Fügen Sie diese Pakete hinzu, wenn torch.distributed benötigt wird</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Unter Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Fügen Sie diese Pakete hinzu, wenn torch.distributed benötigt wird.</h1>
<h1>Unterstützung für das Distributed-Paket unter Windows ist ein Prototyp und kann sich ändern.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### PyTorch installieren
<strong>Unter Linux</strong></p><p>Wenn Sie für AMD ROCm kompilieren, führen Sie zuerst diesen Befehl aus:
<pre><code class="language-bash"># Nur ausführen, wenn Sie für ROCm kompilieren
python tools/amd_build/build_amd.py</code></pre></p><p>PyTorch installieren
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>Unter macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Unter Windows</strong></p><p>Wenn Sie Legacy-Python-Code bauen möchten, siehe <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Building on legacy code and CUDA</a></p><p><strong>CPU-only Builds</strong></p><p>In diesem Modus laufen PyTorch-Berechnungen auf Ihrer CPU, nicht auf Ihrer GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Hinweis zu OpenMP: Die gewünschte OpenMP-Implementierung ist Intel OpenMP (iomp). Um gegen iomp zu linken, müssen Sie die Bibliothek manuell herunterladen und die Build-Umgebung durch Anpassen von <code>CMAKE_INCLUDE_PATH</code> und <code>LIB</code> einrichten. Die Anleitung <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">hier</a> ist ein Beispiel für die Einrichtung von MKL und Intel OpenMP. Ohne diese Konfigurationen für CMake wird Microsoft Visual C OpenMP Runtime (vcomp) verwendet.</p><p><strong>CUDA-basierter Build</strong></p><p>In diesem Modus nutzt PyTorch Ihre GPU über CUDA für schnellere Berechnungen.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> ist erforderlich, um PyTorch mit CUDA zu bauen.
NVTX ist Bestandteil der CUDA-Distribution, wo es „Nsight Compute“ genannt wird. Um es zu einer bereits installierten CUDA-Installation hinzuzufügen, führen Sie die CUDA-Installation erneut aus und setzen Sie das entsprechende Häkchen.
Stellen Sie sicher, dass CUDA mit Nsight Compute nach Visual Studio installiert ist.</p><p>Derzeit werden VS 2017 / 2019 und Ninja als Generator für CMake unterstützt. Wenn <code>ninja.exe</code> im <code>PATH</code> gefunden wird, wird Ninja als Standardgenerator verwendet, andernfalls VS 2017 / 2019.
<br/> Wenn Ninja als Generator ausgewählt wird, wird das neueste MSVC als zugrunde liegende Toolchain gewählt.</p><p>Zusätzliche Bibliotheken wie
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, auch bekannt als MKLDNN oder DNNL</a>, und <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> werden häufig benötigt. Weitere Informationen zur Installation finden Sie im <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a>.</p><p>Sie können sich das Skript <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> für weitere Konfigurationen von Umgebungsvariablen ansehen.</p><pre><code class="language-cmd">cmd</p><p>:: Setzen Sie die Umgebungsvariablen, nachdem Sie das mkl-Paket heruntergeladen und entpackt haben,
:: andernfalls gibt CMake den Fehler <code>Could NOT find OpenMP</code> aus.
set CMAKE_INCLUDE_PATH={Ihr Verzeichnis}\mkl\include
set LIB={Ihr Verzeichnis}\mkl\lib;%LIB%</p><p>:: Lesen Sie den Inhalt des vorherigen Abschnitts sorgfältig, bevor Sie fortfahren.
:: [Optional] Wenn Sie das zugrunde liegende Toolset von Ninja und Visual Studio mit CUDA überschreiben möchten, führen Sie den folgenden Skriptblock aus.
:: Die „Visual Studio 2019 Developer Command Prompt“ wird automatisch ausgeführt.
:: Stellen Sie sicher, dass Sie CMake >= 3.12 verwenden, wenn Sie den Visual Studio-Generator nutzen.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Optional] Wenn Sie den CUDA-Host-Compiler überschreiben möchten
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Intel GPU Builds</strong></p><p>In diesem Modus wird PyTorch mit Intel GPU-Unterstützung gebaut.</p><p>Bitte stellen Sie sicher, dass <a href="#voraussetzungen" target="_blank" rel="noopener noreferrer">die allgemeinen Voraussetzungen</a> und <a href="#intel-gpu-unterstützung" target="_blank" rel="noopener noreferrer">die Voraussetzungen für Intel GPU</a> ordnungsgemäß installiert und die Umgebungsvariablen konfiguriert sind, bevor Sie mit dem Build beginnen. Für Build-Tool-Unterstützung ist <code>Visual Studio 2022</code> erforderlich.</p><p>Dann kann PyTorch mit folgendem Befehl gebaut werden:</p><pre><code class="language-cmd">:: CMD-Befehle:
:: Setzen Sie CMAKE_PREFIX_PATH, um die entsprechenden Pakete zu finden
:: %CONDA_PREFIX% funktioniert nur nach <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Build-Optionen anpassen (optional)</p><p>Sie können die Konfiguration der CMake-Variablen optional (ohne vorheriges Bauen) anpassen,
indem Sie wie folgt vorgehen. Zum Beispiel kann das Anpassen der erkannten Verzeichnisse für CuDNN oder BLAS
auf diese Weise erfolgen.</p><p>Unter Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # oder cmake-gui build</code></pre></p><p>Unter macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # oder cmake-gui build</code></pre></p><h3>Docker-Image</h3></p><p>#### Verwendung vorgefertigter Images</p><p>Sie können auch ein vorgefertigtes Docker-Image von Docker Hub ziehen und mit Docker v19.03+ ausführen</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Bitte beachten Sie, dass PyTorch Shared Memory verwendet, um Daten zwischen Prozessen zu teilen. Wenn torch multiprocessing verwendet wird (z. B.
bei multithreaded Data Loaders), ist die Standardgröße des Shared Memory Segments für Container nicht ausreichend, und Sie
sollten die Größe des Shared Memory entweder mit den Befehlszeilenoptionen <code>--ipc=host</code> oder <code>--shm-size</code> für <code>nvidia-docker run</code> erhöhen.</p><p>#### Image selbst bauen</p><p><strong>HINWEIS:</strong> Muss mit einer Docker-Version > 18.06 gebaut werden</p><p>Die <code>Dockerfile</code> wird bereitgestellt, um Images mit CUDA 11.1-Unterstützung und cuDNN v8 zu bauen.
Sie können die Make-Variable <code>PYTHON_VERSION=x.y</code> angeben, um die von Miniconda verwendete Python-Version festzulegen, oder sie
nicht setzen, um die Standardversion zu verwenden.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>Images werden als docker.io/${your_docker_username}/pytorch getaggt</code></pre></h1></p><p>Sie können auch die Umgebungsvariable <code>CMAKE_VARS="..."</code> angeben, um zusätzliche CMake-Variablen während des Builds an CMake zu übergeben.
Siehe <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> für eine Liste der verfügbaren Variablen.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Dokumentation bauen</h3></p><p>Um Dokumentation in verschiedenen Formaten zu erstellen, benötigen Sie <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
und das pytorch_sphinx_theme2.</p><p>Bevor Sie die Dokumentation lokal bauen, stellen Sie sicher, dass <code>torch</code> in Ihrer Umgebung installiert ist. Für kleinere Korrekturen können Sie die
Nightly-Version wie unter <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Erste Schritte</a> beschrieben installieren.</p><p>Für komplexere Korrekturen, wie das Hinzufügen eines neuen Moduls und Docstrings für
das neue Modul, müssen Sie möglicherweise torch <a href="#aus-dem-quellcode" target="_blank" rel="noopener noreferrer">aus dem Quellcode</a> installieren.
Siehe <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Docstring Guidelines</a>
für Konventionen zu Docstrings.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Führen Sie <code>make</code> aus, um eine Liste aller verfügbaren Ausgabeformate zu erhalten.</p><p>Wenn Sie einen katex-Fehler erhalten, führen Sie <code>npm install katex</code> aus. Falls das Problem weiterhin besteht,
<code>npm install -g katex</code></p><blockquote>[!HINWEIS]</blockquote>
<blockquote>Wenn Sie <code>nodejs</code> mit einem anderen Paketmanager (z. B.</blockquote>
<blockquote><code>conda</code>) installiert haben, wird <code>npm</code> wahrscheinlich eine Version von <code>katex</code> installieren, die nicht</blockquote>
<blockquote>mit Ihrer Version von <code>nodejs</code> kompatibel ist und der Dokumentationsbuild fehlschlägt.</blockquote>
<blockquote>Eine Kombination, die funktioniert, ist <code>node@6.13.1</code> und</blockquote>
<blockquote><code>katex@0.13.18</code>. Um letzteres mit <code>npm</code> zu installieren, führen Sie</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!HINWEIS]</blockquote>
<blockquote>Wenn Sie einen NumPy-Inkompatibilitätsfehler sehen, führen Sie aus:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Wenn Sie Änderungen an den von CI verwendeten Abhängigkeiten vornehmen, bearbeiten Sie die
Datei </code>.ci/docker/requirements-docs.txt<code>.</p><p>#### PDF-Erstellung</p><p>Um ein PDF der gesamten PyTorch-Dokumentation zu erstellen, stellen Sie sicher, dass Sie
</code>texlive<code> und LaTeX installiert haben. Unter macOS können Sie dies mit folgendem Befehl installieren:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>So erstellen Sie das PDF:</p><ul><li>Führen Sie aus:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Dadurch werden die notwendigen Dateien im Verzeichnis </code>build/latex<code> erstellt.</p><ul><li>Wechseln Sie in dieses Verzeichnis und führen Sie aus:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Dadurch wird eine </code>pytorch.pdf` mit dem gewünschten Inhalt erzeugt. Führen Sie diesen
   Befehl noch einmal aus, damit das korrekte Inhaltsverzeichnis und der Index erstellt werden.</p><blockquote>[!HINWEIS]</blockquote>
<blockquote>Um das Inhaltsverzeichnis anzuzeigen, wechseln Sie in die <strong>Inhaltsverzeichnis</strong>-Ansicht Ihres PDF-Viewers.</blockquote></p><h3>Frühere Versionen</h3></p><p>Installationsanleitungen und Binärdateien für frühere PyTorch-Versionen finden Sie
auf <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">unserer Webseite</a>.</p><h2>Erste Schritte</h2></p><p>Drei Hinweise, um Ihnen den Einstieg zu erleichtern:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutorials: Einstieg in das Verständnis und die Anwendung von PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Beispiele: leicht verständlicher PyTorch-Code aus allen Bereichen</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">Die API-Referenz</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Glossar</a></li></p><p></ul><h2>Ressourcen</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">PyTorch Tutorials</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">PyTorch Beispiele</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">PyTorch Modelle</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Intro zu Deep Learning mit PyTorch von Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Intro zu Machine Learning mit PyTorch von Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Deep Neural Networks mit PyTorch von Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Kommunikation</h2>
<ul><li>Foren: Diskussionen zu Implementierungen, Forschung usw. https://discuss.pytorch.org</li>
<li>GitHub-Issues: Fehlerberichte, Funktionswünsche, Installationsprobleme, RFCs, Gedanken usw.</li>
<li>Slack: Der <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">PyTorch Slack</a> richtet sich primär an fortgeschrittene PyTorch-Nutzer und -Entwickler für allgemeinen Chat, Diskussionen, Zusammenarbeit usw. Wenn Sie Anfänger sind und Hilfe benötigen, ist das Hauptmedium das <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch Forum</a>. Für eine Slack-Einladung füllen Sie bitte dieses Formular aus: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Ein einseitiger E-Mail-Newsletter mit wichtigen PyTorch-Ankündigungen. Anmeldung hier: https://eepurl.com/cbG0rv</li>
<li>Facebook-Seite: Wichtige Ankündigungen zu PyTorch. https://www.facebook.com/pytorch</li>
<li>Für Markenrichtlinien besuchen Sie bitte unsere Webseite unter <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Releases und Mitwirkung</h2></p><p>Typischerweise gibt es drei kleinere Releases von PyTorch pro Jahr. Bitte melden Sie uns Fehler, indem Sie ein <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">Issue eröffnen</a>.</p><p>Wir schätzen alle Beiträge. Wenn Sie Bugfixes beitragen möchten, tun Sie dies bitte ohne weitere Rücksprache.</p><p>Wenn Sie neue Funktionen, Hilfsfunktionen oder Erweiterungen für den Core beitragen möchten, eröffnen Sie bitte zuerst ein Issue und diskutieren die Funktion mit uns.
Das Senden eines PRs ohne Diskussion kann dazu führen, dass Ihr PR abgelehnt wird, weil wir den Core vielleicht in eine andere Richtung entwickeln, als Ihnen bekannt ist.</p><p>Mehr dazu, wie Sie zu PyTorch beitragen können, finden Sie auf unserer <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Beitragsseite</a>. Weitere Informationen zu PyTorch-Releases finden Sie auf der <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">Release-Seite</a>.</p><h2>Das Team</h2></p><p>PyTorch ist ein Community-getriebenes Projekt, zu dem viele erfahrene Ingenieure und Forscher beitragen.</p><p>PyTorch wird aktuell von <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a> und <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a> betreut, mit wichtigen Beiträgen von Hunderten talentierter Personen in verschiedenster Form.
Eine nicht abschließende, aber wachsende Liste: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Hinweis: Dieses Projekt steht in keinem Zusammenhang mit <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> mit demselben Namen. Hugh ist ein wertvoller Contributor der Torch-Community und hat bei vielen Dingen zu Torch und PyTorch beigetragen.</p><h2>Lizenz</h2></p><p>PyTorch steht unter einer BSD-ähnlichen Lizenz, wie in der Datei <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a> beschrieben.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>