<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Italian. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Italian. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Italian. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Italian</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch è un pacchetto Python che offre due funzionalità di alto livello:
<ul><li>Calcolo con tensori (simile a NumPy) con potente accelerazione GPU</li>
<li>Reti neurali profonde basate su un sistema autograd a nastro</li></p><p></ul>È possibile riutilizzare i tuoi pacchetti Python preferiti come NumPy, SciPy e Cython per estendere PyTorch quando necessario.</p><p>La salute della nostra trunk (segnali di Continuous Integration) è disponibile su <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#ulteriori-informazioni-su-pytorch" target="_blank" rel="noopener noreferrer">Ulteriori informazioni su PyTorch</a></li>
  <li><a href="#una-libreria-di-tensori-pronta-per-gpu" target="_blank" rel="noopener noreferrer">Una libreria di tensori pronta per GPU</a></li>
  <li><a href="#reti-neurali-dinamiche-autograd-a-nastro" target="_blank" rel="noopener noreferrer">Reti neurali dinamiche: Autograd a nastro</a></li>
  <li><a href="#prima-di-tutto-python" target="_blank" rel="noopener noreferrer">Prima di tutto Python</a></li>
  <li><a href="#esperienze-imperative" target="_blank" rel="noopener noreferrer">Esperienze imperative</a></li>
  <li><a href="#veloce-e-snello" target="_blank" rel="noopener noreferrer">Veloce e snello</a></li>
  <li><a href="#estensioni-senza-complicazioni" target="_blank" rel="noopener noreferrer">Estensioni senza complicazioni</a></li>
<li><a href="#installazione" target="_blank" rel="noopener noreferrer">Installazione</a></li>
  <li><a href="#binari" target="_blank" rel="noopener noreferrer">Binari</a></li>
    <li><a href="#piattaforme-nvidia-jetson" target="_blank" rel="noopener noreferrer">Piattaforme NVIDIA Jetson</a></li>
  <li><a href="#dal-sorgente" target="_blank" rel="noopener noreferrer">Dal sorgente</a></li>
    <li><a href="#prerequisiti" target="_blank" rel="noopener noreferrer">Prerequisiti</a></li>
      <li><a href="#supporto-nvidia-cuda" target="_blank" rel="noopener noreferrer">Supporto NVIDIA CUDA</a></li>
      <li><a href="#supporto-amd-rocm" target="_blank" rel="noopener noreferrer">Supporto AMD ROCm</a></li>
      <li><a href="#supporto-gpu-intel" target="_blank" rel="noopener noreferrer">Supporto GPU Intel</a></li>
    <li><a href="#ottenere-il-sorgente-pytorch" target="_blank" rel="noopener noreferrer">Ottenere il sorgente PyTorch</a></li>
    <li><a href="#installare-le-dipendenze" target="_blank" rel="noopener noreferrer">Installare le dipendenze</a></li>
    <li><a href="#installare-pytorch" target="_blank" rel="noopener noreferrer">Installare PyTorch</a></li>
      <li><a href="#regolare-le-opzioni-di-compilazione-opzionale" target="_blank" rel="noopener noreferrer">Regolare le opzioni di compilazione (Opzionale)</a></li>
  <li><a href="#immagine-docker" target="_blank" rel="noopener noreferrer">Immagine Docker</a></li>
    <li><a href="#utilizzo-di-immagini-precompilate" target="_blank" rel="noopener noreferrer">Utilizzo di immagini precompilate</a></li>
    <li><a href="#costruire-limmagine-manualmente" target="_blank" rel="noopener noreferrer">Costruire l'immagine manualmente</a></li>
  <li><a href="#compilare-la-documentazione" target="_blank" rel="noopener noreferrer">Compilare la documentazione</a></li>
    <li><a href="#compilazione-di-un-pdf" target="_blank" rel="noopener noreferrer">Compilazione di un PDF</a></li>
  <li><a href="#versioni-precedenti" target="_blank" rel="noopener noreferrer">Versioni precedenti</a></li>
<li><a href="#primi-passi" target="_blank" rel="noopener noreferrer">Primi passi</a></li>
<li><a href="#risorse" target="_blank" rel="noopener noreferrer">Risorse</a></li>
<li><a href="#comunicazione" target="_blank" rel="noopener noreferrer">Comunicazione</a></li>
<li><a href="#rilasci-e-contributi" target="_blank" rel="noopener noreferrer">Rilasci e contributi</a></li>
<li><a href="#il-team" target="_blank" rel="noopener noreferrer">Il team</a></li>
<li><a href="#licenza" target="_blank" rel="noopener noreferrer">Licenza</a></li></p><p></ul><!-- tocstop --></p><h2>Ulteriori informazioni su PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Impara le basi di PyTorch</a></p><p>A livello granulare, PyTorch è una libreria che consiste nei seguenti componenti:</p><p>| Componente | Descrizione |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Una libreria di tensori simile a NumPy, con forte supporto GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Una libreria di differenziazione automatica basata su nastro che supporta tutte le operazioni sui tensori differenziabili in torch |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Uno stack di compilazione (TorchScript) per creare modelli serializzabili e ottimizzabili dal codice PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Una libreria di reti neurali profondamente integrata con autograd, progettata per la massima flessibilità |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Multiprocessing Python, ma con condivisione magica della memoria dei tensori torch tra i processi. Utile per il caricamento dati e l’addestramento Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader e altre funzioni di utilità per comodità |</p><p>Di solito, PyTorch è utilizzato come:</p><ul><li>Sostituto di NumPy per sfruttare la potenza delle GPU.</li>
<li>Piattaforma di ricerca per deep learning che offre la massima flessibilità e velocità.</li></p><p></ul>Approfondimento:</p><h3>Una libreria di tensori pronta per GPU</h3></p><p>Se usi NumPy, hai già utilizzato tensori (anche noti come ndarray).</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustration"></p><p>PyTorch fornisce tensori che possono risiedere sia su CPU che su GPU e accelera il calcolo in modo significativo.</p><p>Offriamo una vasta gamma di routine sui tensori per accelerare e adattarsi alle tue esigenze di calcolo scientifico, come slicing, indicizzazione, operazioni matematiche, algebra lineare, riduzioni.
E sono veloci!</p><h3>Reti neurali dinamiche: Autograd a nastro</h3></p><p>PyTorch ha un modo unico di costruire reti neurali: usando e riproducendo un registratore a nastro.</p><p>La maggior parte dei framework come TensorFlow, Theano, Caffe e CNTK ha una visione statica del mondo.
Bisogna costruire una rete neurale e riutilizzare la stessa struttura più e più volte.
Cambiare il comportamento della rete significa dover ripartire da zero.</p><p>Con PyTorch, utilizziamo una tecnica chiamata auto-differenziazione in modalità inversa, che ti consente di
cambiare il comportamento della rete in modo arbitrario senza latenza o overhead. La nostra ispirazione deriva
da diversi articoli di ricerca su questo argomento, oltre che da lavori attuali e passati come
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a>, ecc.</p><p>Sebbene questa tecnica non sia unica di PyTorch, è una delle implementazioni più veloci disponibili.
Ottieni il meglio in termini di velocità e flessibilità per le tue ricerche più creative.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamic graph"></p><h3>Prima di tutto Python</h3></p><p>PyTorch non è un binding Python in un framework C++ monolitico.
È costruito per essere profondamente integrato in Python.
Puoi usarlo in modo naturale come useresti <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> ecc.
Puoi scrivere i tuoi nuovi layer di reti neurali direttamente in Python, usando le tue librerie preferite
e utilizzare pacchetti come <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> e <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Il nostro obiettivo è non reinventare la ruota dove non necessario.</p><h3>Esperienze imperative</h3></p><p>PyTorch è progettato per essere intuitivo, lineare nel pensiero e facile da usare.
Quando esegui una riga di codice, essa viene eseguita. Non esiste una visione asincrona del mondo.
Quando entri in un debugger o ricevi messaggi di errore e stack trace, capirli è semplice.
Lo stack trace punta esattamente al punto in cui il tuo codice è stato definito.
Speriamo che tu non debba mai passare ore a fare debugging a causa di stack trace errati o motori di esecuzione asincroni e opachi.</p><h3>Veloce e snello</h3></p><p>PyTorch ha un overhead minimo. Integria librerie di accelerazione
come <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> e NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) per massimizzare la velocità.
Al cuore, i backend per tensori e reti neurali su CPU e GPU
sono maturi e testati da anni.</p><p>Pertanto, PyTorch è molto veloce — sia che tu esegua reti neurali piccole che grandi.</p><p>L’utilizzo di memoria in PyTorch è estremamente efficiente rispetto a Torch o ad alcune alternative.
Abbiamo scritto allocatori di memoria personalizzati per la GPU per assicurarci che
i tuoi modelli di deep learning siano il più possibile efficienti in memoria.
Ciò ti consente di addestrare modelli di deep learning più grandi rispetto a prima.</p><h3>Estensioni senza complicazioni</h3></p><p>Scrivere nuovi moduli di reti neurali, o interfacciarsi con l’API dei tensori di PyTorch è stato progettato per essere semplice
e con astrazioni minime.</p><p>Puoi scrivere nuovi layer di reti neurali in Python usando l’API torch
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">o le tue librerie preferite basate su NumPy come SciPy</a>.</p><p>Se desideri scrivere i tuoi layer in C/C++, offriamo una comoda API per le estensioni efficiente e con boilerplate minimo.
Non è necessario scrivere codice wrapper. Puoi vedere <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">un tutorial qui</a> e <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">un esempio qui</a>.</p><h2>Installazione</h2></p><h3>Binari</h3>
I comandi per installare i binari tramite Conda o pip wheels sono disponibili sul nostro sito: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>
#### Piattaforme NVIDIA Jetson</p><p>I pacchetti wheel Python per NVIDIA Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX e Jetson AGX Orin sono disponibili <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">qui</a> e il container L4T è pubblicato <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">qui</a></p><p>Richiedono JetPack 4.2 o superiore, e <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> e <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> li mantengono.</p><h3>Dal sorgente</h3></p><p>#### Prerequisiti
Se installi dal sorgente, avrai bisogno di:
<ul><li>Python 3.9 o successivo</li>
<li>Un compilatore che supporti completamente C++17, come clang o gcc (gcc 9.4.0 o successivo è richiesto su Linux)</li>
<li>Visual Studio o Visual Studio Build Tool (solo Windows)</li></p><p></ul>\* Il CI di PyTorch utilizza Visual C++ BuildTools, che sono inclusi in Visual Studio Enterprise,
Professional o Community Edition. Puoi anche installare i build tools da
https://visualstudio.microsoft.com/visual-cpp-build-tools/. I build tools <em>non sono</em> inclusi in Visual Studio Code di default.</p><p>Un esempio di configurazione dell’ambiente è mostrato di seguito:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### Supporto NVIDIA CUDA
Se vuoi compilare con supporto CUDA, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">seleziona una versione supportata di CUDA dalla nostra matrice di supporto</a>, quindi installa:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 o superiore</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compilatore</a> compatibile con CUDA</li></p><p></ul>Nota: Puoi consultare la <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">matrice di supporto cuDNN</a> per le versioni cuDNN con le varie versioni di CUDA, driver CUDA e hardware NVIDIA supportati.</p><p>Se vuoi disabilitare il supporto CUDA, esporta la variabile d’ambiente <code>USE_CUDA=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p><p>Se stai compilando per piattaforme NVIDIA Jetson (Jetson Nano, TX1, TX2, AGX Xavier), le istruzioni per installare PyTorch per Jetson Nano sono <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">disponibili qui</a></p><p>##### Supporto AMD ROCm
Se vuoi compilare con supporto ROCm, installa
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 o superiore</li>
<li>ROCm è attualmente supportato solo su sistemi Linux.</li></p><p></ul>Di default il sistema di build si aspetta ROCm installato in <code>/opt/rocm</code>. Se ROCm è installato in una directory diversa, la variabile d’ambiente <code>ROCM_PATH</code> deve essere impostata sulla directory di installazione ROCm. Il sistema di build rileva automaticamente l’architettura GPU AMD. Facoltativamente, l’architettura GPU AMD può essere impostata esplicitamente con la variabile d’ambiente <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">Architettura GPU AMD</a></p><p>Se vuoi disabilitare il supporto ROCm, esporta la variabile d’ambiente <code>USE_ROCM=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p><p>##### Supporto GPU Intel
Se vuoi compilare con supporto GPU Intel, segui queste
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Istruzioni PyTorch Prerequisites for Intel GPUs</a>.</li>
<li>La GPU Intel è supportata per Linux e Windows.</li></p><p></ul>Se vuoi disabilitare il supporto GPU Intel, esporta la variabile d’ambiente <code>USE_XPU=0</code>.
Altre variabili d’ambiente utili possono essere trovate in <code>setup.py</code>.</p><p>#### Ottenere il sorgente PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>se stai aggiornando una copia locale esistente</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Installare le dipendenze</p><p><strong>Comuni</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Esegui questo comando dalla directory PyTorch dopo aver clonato il codice sorgente usando la sezione “Ottenere il sorgente PyTorch” qui sopra</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Su Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Solo CUDA: Aggiungi il supporto LAPACK per la GPU se necessario</h1>
<h1>installazione magma: esegui con ambiente conda attivo. specifica la versione CUDA da installare</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(opzionale) Se utilizzi torch.compile con inductor/triton, installa la versione corrispondente di triton</h1>
<h1>Esegui dalla directory pytorch dopo il clone</h1>
<h1>Per il supporto GPU Intel, esporta esplicitamente <code>USE_XPU=1</code> prima di eseguire il comando.</h1>
make triton</code></pre></p><p><strong>Su MacOS</strong></p><pre><code class="language-bash"># Aggiungi questo pacchetto solo su macchine con processore intel x86
pip install mkl-static mkl-include
<h1>Aggiungi questi pacchetti se torch.distributed è necessario</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Su Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Aggiungi questi pacchetti se torch.distributed è necessario.</h1>
<h1>Il supporto del pacchetto distributed su Windows è una funzionalità prototipo e soggetta a cambiamenti.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Installare PyTorch
<strong>Su Linux</strong></p><p>Se stai compilando per AMD ROCm esegui prima questo comando:
<pre><code class="language-bash"># Esegui solo se stai compilando per ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Installa PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>Su macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Su Windows</strong></p><p>Se desideri compilare codice python legacy, consulta <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Building on legacy code and CUDA</a></p><p><strong>Compilazioni solo CPU</strong></p><p>In questa modalità i calcoli PyTorch verranno eseguiti sulla CPU, non sulla GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Nota su OpenMP: L’implementazione OpenMP preferita è Intel OpenMP (iomp). Per collegarti a iomp, devi scaricare manualmente la libreria e configurare l’ambiente di compilazione modificando <code>CMAKE_INCLUDE_PATH</code> e <code>LIB</code>. Le istruzioni <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">qui</a> sono un esempio per configurare sia MKL che Intel OpenMP. Senza queste configurazioni per CMake, verrà utilizzato il runtime OpenMP di Microsoft Visual C (vcomp).</p><p><strong>Compilazione basata su CUDA</strong></p><p>In questa modalità i calcoli PyTorch sfrutteranno la GPU tramite CUDA per prestazioni più rapide</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> è necessario per compilare Pytorch con CUDA.
NVTX è incluso nella distribuzione CUDA, dove è chiamato "Nsight Compute". Per installarlo su una CUDA già installata, esegui nuovamente l’installazione CUDA e seleziona la relativa casella.
Assicurati che CUDA con Nsight Compute sia installato dopo Visual Studio.</p><p>Attualmente, VS 2017 / 2019 e Ninja sono supportati come generatori di CMake. Se <code>ninja.exe</code> è rilevato nel <code>PATH</code>, Ninja verrà usato come generatore predefinito, altrimenti si userà VS 2017 / 2019.
<br/> Se Ninja è selezionato come generatore, verrà selezionato automaticamente l’ultima versione di MSVC come toolchain.</p><p>Ulteriori librerie come
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, noto anche come MKLDNN o DNNL</a>, e <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> sono spesso necessarie. Consulta <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> per installarle.</p><p>Puoi consultare lo script <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> per altre configurazioni di variabili d’ambiente</p><pre><code class="language-cmd">cmd</p><p>:: Imposta le variabili di ambiente dopo aver scaricato e decompresso il pacchetto mkl,
:: altrimenti CMake restituirà un errore come <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%</p><p>:: Leggi attentamente la sezione precedente prima di procedere.
:: [Opzionale] Se vuoi sovrascrivere il toolset usato da Ninja e Visual Studio con CUDA, esegui il seguente blocco di script.
:: "Visual Studio 2019 Developer Command Prompt" verrà avviato automaticamente.
:: Assicurati di avere CMake >= 3.12 prima di farlo quando usi il generatore Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Opzionale] Se vuoi sovrascrivere il compilatore host CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Compilazioni GPU Intel</strong></p><p>In questa modalità verrà compilato PyTorch con supporto GPU Intel.</p><p>Assicurati che <a href="#prerequisiti" target="_blank" rel="noopener noreferrer">i prerequisiti comuni</a> e <a href="#supporto-gpu-intel" target="_blank" rel="noopener noreferrer">i prerequisiti per GPU Intel</a> siano installati correttamente e che le variabili d’ambiente siano configurate prima di iniziare la build. Per il supporto agli strumenti di build è richiesto <code>Visual Studio 2022</code>.</p><p>Quindi PyTorch può essere compilato con il comando:</p><pre><code class="language-cmd">:: Comandi CMD:
:: Imposta CMAKE_PREFIX_PATH per trovare i pacchetti corrispondenti
:: %CONDA_PREFIX% funziona solo dopo <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Regolare le opzioni di compilazione (Opzionale)</p><p>Puoi regolare la configurazione delle variabili cmake opzionalmente (senza compilare prima), facendo
quanto segue. Ad esempio, regolare le directory pre-rilevate per CuDNN o BLAS può essere fatto
con questo passaggio.</p><p>Su Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # o cmake-gui build</code></pre></p><p>Su macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # o cmake-gui build</code></pre></p><h3>Immagine Docker</h3></p><p>#### Utilizzo di immagini precompilate</p><p>Puoi anche scaricare un’immagine docker precompilata da Docker Hub ed eseguire con docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Nota che PyTorch utilizza la memoria condivisa per condividere dati tra processi, quindi se viene utilizzato torch multiprocessing (ad esempio
per data loader multithread), la dimensione predefinita del segmento di memoria condivisa con cui il container viene eseguito non è sufficiente, e devi
aumentare la dimensione della memoria condivisa usando le opzioni da linea di comando <code>--ipc=host</code> o <code>--shm-size</code> in <code>nvidia-docker run</code>.</p><p>#### Costruire l'immagine manualmente</p><p><strong>NOTA:</strong> Deve essere costruita con una versione di docker > 18.06</p><p>Il <code>Dockerfile</code> è fornito per costruire immagini con supporto CUDA 11.1 e cuDNN v8.
Puoi passare la variabile make <code>PYTHON_VERSION=x.y</code> per specificare la versione di Python che verrà usata da Miniconda, oppure lasciarla
non impostata per usare quella predefinita.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>le immagini sono taggate come docker.io/${your_docker_username}/pytorch</code></pre></h1></p><p>Puoi anche passare la variabile d’ambiente <code>CMAKE_VARS="..."</code> per specificare ulteriori variabili CMake da passare durante la build.
Consulta <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> per la lista delle variabili disponibili.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Compilare la documentazione</h3></p><p>Per compilare la documentazione in vari formati, avrai bisogno di <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
e del tema pytorch_sphinx_theme2.</p><p>Prima di compilare la documentazione in locale, assicurati che <code>torch</code> sia
installato nel tuo ambiente. Per piccole modifiche, puoi installare la
versione nightly come descritto in <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Primi passi</a>.</p><p>Per modifiche più complesse, come aggiungere un nuovo modulo e docstring per
il nuovo modulo, potresti dover installare torch <a href="#dal-sorgente" target="_blank" rel="noopener noreferrer">dal sorgente</a>.
Consulta le <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Linee guida Docstring</a>
per le convenzioni sui docstring.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Esegui <code>make</code> per ottenere una lista di tutti i formati di output disponibili.</p><p>Se ottieni un errore katex esegui <code>npm install katex</code>. Se persiste, prova
<code>npm install -g katex</code></p><blockquote>[!NOTA]</blockquote>
<blockquote>Se hai installato <code>nodejs</code> con un gestore di pacchetti diverso (ad es.</blockquote>
<blockquote><code>conda</code>) allora <code>npm</code> probabilmente installerà una versione di <code>katex</code> non</blockquote>
<blockquote>compatibile con la tua versione di <code>nodejs</code> e la compilazione della documentazione fallirà.</blockquote>
<blockquote>Una combinazione di versioni nota per funzionare è <code>node@6.13.1</code> e</blockquote>
<blockquote><code>katex@0.13.18</code>. Per installare quest’ultimo con <code>npm</code> puoi eseguire</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!NOTA]</blockquote>
<blockquote>Se vedi un errore di incompatibilità numpy, esegui:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Quando apporti modifiche alle dipendenze eseguite dal CI, modifica il file
</code>.ci/docker/requirements-docs.txt<code>.</p><p>#### Compilazione di un PDF</p><p>Per compilare un PDF di tutta la documentazione PyTorch, assicurati di avere
</code>texlive<code> e LaTeX installati. Su macOS, puoi installarli usando:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Per creare il PDF:</p><ul><li>Esegui:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Questo genererà i file necessari nella directory </code>build/latex<code>.</p><ul><li>Naviga in questa directory ed esegui:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Questo produrrà un file </code>pytorch.pdf` con il contenuto desiderato. Esegui questo
   comando ancora una volta per generare l’indice e la tabella dei contenuti corretti.</p><blockquote>[!NOTA]</blockquote>
<blockquote>Per visualizzare la Tabella dei Contenuti, passa alla vista <strong>Table of Contents</strong></blockquote>
<blockquote>nel tuo visualizzatore PDF.</blockquote></p><h3>Versioni precedenti</h3></p><p>Le istruzioni di installazione e i binari delle versioni precedenti di PyTorch sono disponibili
sul <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">nostro sito</a>.</p><h2>Primi passi</h2></p><p>Tre suggerimenti per iniziare:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutorial: per iniziare a comprendere e utilizzare PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Esempi: codice PyTorch di facile comprensione in tutti i domini</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">La documentazione API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Glossario</a></li></p><p></ul><h2>Risorse</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutorial PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Esempi PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">Modelli PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Intro to Deep Learning with PyTorch da Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Intro to Machine Learning with PyTorch da Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Deep Neural Networks with PyTorch da Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Comunicazione</h2>
<ul><li>Forum: Discuti di implementazioni, ricerca, ecc. https://discuss.pytorch.org</li>
<li>GitHub Issues: Segnalazioni di bug, richieste di funzionalità, problemi di installazione, RFC, idee, ecc.</li>
<li>Slack: Lo <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">Slack PyTorch</a> ospita un pubblico principale di utenti e sviluppatori PyTorch di livello medio-avanzato per chat generali, discussioni online, collaborazione, ecc. Se sei un principiante in cerca di aiuto, il canale principale è <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch Forums</a>. Se hai bisogno di un invito Slack, compila questo modulo: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Nessun rumore, una newsletter unidirezionale con annunci importanti su PyTorch. Puoi iscriverti qui: https://eepurl.com/cbG0rv</li>
<li>Pagina Facebook: Annunci importanti su PyTorch. https://www.facebook.com/pytorch</li>
<li>Per le linee guida sul brand, consulta il nostro sito <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Rilasci e contributi</h2></p><p>Tipicamente, PyTorch ha tre rilasci minori all’anno. Segnalaci eventuali bug <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">aprendo un issue</a>.</p><p>Apprezziamo tutti i contributi. Se hai intenzione di contribuire con bug-fix, fallo pure senza ulteriori discussioni.</p><p>Se vuoi contribuire con nuove funzionalità, funzioni di utilità o estensioni al core, apri prima un issue e discuti la funzionalità con noi.
Inviare una PR senza discussione potrebbe portare a un rifiuto della PR perché potremmo avere una direzione diversa da quella che immagini.</p><p>Per saperne di più su come contribuire a Pytorch, consulta la nostra <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">pagina dei contributi</a>. Per ulteriori informazioni sui rilasci PyTorch, vedi la <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">pagina dei rilasci</a>.</p><h2>Il team</h2></p><p>PyTorch è un progetto guidato dalla comunità con diversi ingegneri e ricercatori di talento che vi contribuiscono.</p><p>PyTorch è attualmente mantenuto da <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, e <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a> con contributi importanti da centinaia di persone di talento in varie forme e modalità.
Un elenco non esaustivo ma in crescita include: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Nota: Questo progetto non è collegato a <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> con lo stesso nome. Hugh è un valido contributore della comunità Torch e ha aiutato in molti aspetti Torch e PyTorch.</p><h2>Licenza</h2></p><p>PyTorch ha una licenza in stile BSD, come riportato nel file <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a>.</p><p>
---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>