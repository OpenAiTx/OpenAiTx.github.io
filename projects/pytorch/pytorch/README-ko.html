<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Korean. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Korean. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Korean, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Korean. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-ko.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Korean</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch 로고"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch는 두 가지 고수준 기능을 제공하는 Python 패키지입니다.
<ul><li>강력한 GPU 가속을 지원하는 텐서 연산(NumPy와 유사)</li>
<li>테이프 기반 autograd 시스템 위에 구축된 딥 뉴럴 네트워크</li></p><p></ul>필요시 NumPy, SciPy, Cython 등 선호하는 Python 패키지를 재사용하여 PyTorch를 확장할 수 있습니다.</p><p>트렁크 상태(지속적 통합 신호)는 <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>에서 확인할 수 있습니다.</p><p><!-- toc --></p><ul><li><a href="#more-about-pytorch" target="_blank" rel="noopener noreferrer">PyTorch에 대해 더 알아보기</a></li>
  <li><a href="#a-gpu-ready-tensor-library" target="_blank" rel="noopener noreferrer">GPU에 최적화된 텐서 라이브러리</a></li>
  <li><a href="#dynamic-neural-networks-tape-based-autograd" target="_blank" rel="noopener noreferrer">동적 신경망: 테이프 기반 Autograd</a></li>
  <li><a href="#python-first" target="_blank" rel="noopener noreferrer">Python 우선</a></li>
  <li><a href="#imperative-experiences" target="_blank" rel="noopener noreferrer">명령형 경험</a></li>
  <li><a href="#fast-and-lean" target="_blank" rel="noopener noreferrer">빠르고 효율적</a></li>
  <li><a href="#extensions-without-pain" target="_blank" rel="noopener noreferrer">확장성</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">설치</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">바이너리</a></li>
    <li><a href="#nvidia-jetson-platforms" target="_blank" rel="noopener noreferrer">NVIDIA Jetson 플랫폼</a></li>
  <li><a href="#from-source" target="_blank" rel="noopener noreferrer">소스에서 설치</a></li>
    <li><a href="#prerequisites" target="_blank" rel="noopener noreferrer">사전 준비</a></li>
      <li><a href="#nvidia-cuda-support" target="_blank" rel="noopener noreferrer">NVIDIA CUDA 지원</a></li>
      <li><a href="#amd-rocm-support" target="_blank" rel="noopener noreferrer">AMD ROCm 지원</a></li>
      <li><a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Intel GPU 지원</a></li>
    <li><a href="#get-the-pytorch-source" target="_blank" rel="noopener noreferrer">PyTorch 소스 받기</a></li>
    <li><a href="#install-dependencies" target="_blank" rel="noopener noreferrer">의존성 설치</a></li>
    <li><a href="#install-pytorch" target="_blank" rel="noopener noreferrer">PyTorch 설치</a></li>
      <li><a href="#adjust-build-options-optional" target="_blank" rel="noopener noreferrer">빌드 옵션 조정(선택 사항)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">도커 이미지</a></li>
    <li><a href="#using-pre-built-images" target="_blank" rel="noopener noreferrer">미리 빌드된 이미지 사용</a></li>
    <li><a href="#building-the-image-yourself" target="_blank" rel="noopener noreferrer">직접 이미지 빌드</a></li>
  <li><a href="#building-the-documentation" target="_blank" rel="noopener noreferrer">문서 빌드</a></li>
    <li><a href="#building-a-pdf" target="_blank" rel="noopener noreferrer">PDF 빌드</a></li>
  <li><a href="#previous-versions" target="_blank" rel="noopener noreferrer">이전 버전</a></li>
<li><a href="#getting-started" target="_blank" rel="noopener noreferrer">시작하기</a></li>
<li><a href="#resources" target="_blank" rel="noopener noreferrer">자료</a></li>
<li><a href="#communication" target="_blank" rel="noopener noreferrer">커뮤니케이션</a></li>
<li><a href="#releases-and-contributing" target="_blank" rel="noopener noreferrer">릴리즈 및 기여</a></li>
<li><a href="#the-team" target="_blank" rel="noopener noreferrer">팀</a></li>
<li><a href="#license" target="_blank" rel="noopener noreferrer">라이선스</a></li></p><p></ul><!-- tocstop --></p><h2>PyTorch에 대해 더 알아보기</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">PyTorch 기초 배우기</a></p><p>세부적으로, PyTorch는 다음과 같은 구성 요소로 이루어진 라이브러리입니다.</p><p>| 구성 요소 | 설명 |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | NumPy와 유사한 텐서 라이브러리, 강력한 GPU 지원 |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | torch의 모든 미분 가능한 텐서 연산을 지원하는 테이프 기반 자동 미분 라이브러리 |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | PyTorch 코드를 직렬화 및 최적화 가능한 모델로 변환하는 TorchScript 컴파일 스택 |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | autograd와 깊이 통합된 신경망 라이브러리로 최대 유연성을 제공 |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Python 멀티프로세싱이지만, 프로세스 간 torch 텐서의 마법같은 메모리 공유. 데이터 로딩과 Hogwild 학습에 유용 |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | 편의를 위한 DataLoader 및 기타 유틸리티 함수 |</p><p>일반적으로 PyTorch는 다음과 같이 사용됩니다.</p><ul><li>GPU의 성능을 활용하기 위한 NumPy 대체재로 사용</li>
<li>최대한의 유연성과 속도를 제공하는 딥러닝 연구 플랫폼</li></p><p></ul>자세히 설명하면:</p><h3>GPU에 최적화된 텐서 라이브러리</h3></p><p>NumPy를 사용해본 적이 있다면, 텐서(=ndarray)를 사용한 경험이 있습니다.</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Tensor illustration"></p><p>PyTorch는 CPU 또는 GPU에서 작동할 수 있는 텐서를 제공하며,
연산을 대폭 가속화합니다.</p><p>슬라이싱, 인덱싱, 수학 연산, 선형 대수, 감소 연산 등
과학적 계산에 필요한 다양한 텐서 루틴을 제공하며,
매우 빠릅니다!</p><h3>동적 신경망: 테이프 기반 Autograd</h3></p><p>PyTorch는 신경망을 구축하는 독특한 방식, 즉 테이프 레코더처럼 사용하고 재생하는 방식을 채택합니다.</p><p>TensorFlow, Theano, Caffe, CNTK 등 대부분의 프레임워크는
세상의 구조를 정적으로 봅니다.
즉, 신경망을 한 번 만들고 같은 구조를 반복해서 재사용해야 합니다.
네트워크의 동작 방식을 변경하려면 처음부터 다시 시작해야 합니다.</p><p>PyTorch는 역방향 자동 미분(reverse-mode auto-differentiation) 기법을 사용하여
네트워크의 동작 방식을 자유롭게, 지연이나 오버헤드 없이 변경할 수 있습니다.
이 방식은 <a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a> 등 여러 연구 논문과 프로젝트에서 영감을 받았습니다.</p><p>이 기술이 PyTorch에만 있는 것은 아니지만, 현재까지 가장 빠른 구현 중 하나입니다.
속도와 유연성을 모두 얻을 수 있습니다.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamic graph"></p><h3>Python 우선</h3></p><p>PyTorch는 거대한 C++ 프레임워크에 대한 Python 바인딩이 아닙니다.
Python과 깊이 통합되도록 설계되었습니다.
<a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> 등을 사용하듯 자연스럽게 사용할 수 있습니다.
선호하는 라이브러리와 <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a>이나 <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>와 함께 직접 Python에서 신경망 레이어를 작성할 수 있습니다.
적절하다면 바퀴를 다시 만들지 않는 것이 목표입니다.</p><h3>명령형 경험</h3></p><p>PyTorch는 직관적이고, 생각의 흐름이 직선적이며, 사용하기 쉽도록 설계되었습니다.
코드 한 줄을 실행하면 바로 실행됩니다. 비동기적인 시각이 아닙니다.
디버거에 진입하거나 오류 메시지, 스택 트레이스를 받을 때, 이해하기 쉽습니다.
스택 트레이스는 정확히 코드가 정의된 위치를 가리킵니다.
불명확한 스택 트레이스나 비동기적, 불투명한 실행 엔진 때문에
오랜 시간 디버깅하지 않기를 바랍니다.</p><h3>빠르고 효율적</h3></p><p>PyTorch는 프레임워크 오버헤드가 최소화되어 있습니다.
<a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a>, NVIDIA(<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) 등의 가속 라이브러리를 통합하여 최대 속도를 냅니다.
CPU와 GPU 텐서 및 신경망 백엔드는 수년간 검증된 성숙한 코드입니다.</p><p>따라서 PyTorch는 신경망이 크든 작든 매우 빠릅니다.</p><p>PyTorch의 메모리 사용은 Torch나 일부 대안에 비해 매우 효율적입니다.
GPU를 위한 커스텀 메모리 할당기를 직접 작성하여
딥러닝 모델이 최대한 메모리 효율적으로 학습될 수 있도록 했습니다.
이 덕분에 이전보다 더 큰 딥러닝 모델을 훈련할 수 있습니다.</p><h3>확장성</h3></p><p>새 신경망 모듈을 작성하거나 PyTorch의 텐서 API와 연동하는 것은
최소한의 추상화로 간단하게 설계되었습니다.</p><p>torch API 또는
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">선호하는 NumPy 기반 라이브러리(SciPy 등)</a>를 사용하여
Python에서 신경망 레이어를 새로 작성할 수 있습니다.</p><p>C/C++로 레이어를 작성하려면, 효율적이고 최소한의 보일러플레이트만 필요한 편리한 확장 API를 제공합니다.
랩퍼 코드를 별도로 작성할 필요가 없습니다.
<a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">튜토리얼</a>과 <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">예시</a>에서 자세히 확인할 수 있습니다.</p><h2>설치</h2></p><h3>바이너리</h3>
Conda 또는 pip wheel을 통한 바이너리 설치 명령은 웹사이트에서 확인하세요: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>#### NVIDIA Jetson 플랫폼</p><p>NVIDIA의 Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, Jetson AGX Orin용 Python wheel은 <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">여기</a>에서 제공되며, L4T 컨테이너는 <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">여기</a>에 게시되어 있습니다.</p><p>JetPack 4.2 이상이 필요하며, <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a>와 <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a>이 이를 관리합니다.</p><h3>소스에서 설치</h3></p><p>#### 사전 준비
소스에서 설치하려면 다음이 필요합니다.
<ul><li>Python 3.9 이상</li>
<li>C++17을 완전히 지원하는 컴파일러(예: clang 또는 gcc, Linux의 경우 gcc 9.4.0 이상 필요)</li>
<li>Visual Studio 또는 Visual Studio Build Tool(Windows 전용)</li></p><p></ul>\* PyTorch CI는 Visual C++ BuildTools를 사용합니다. 이 도구는 Visual Studio Enterprise, Professional, Community Edition에 포함되어 있습니다. 또는 https://visualstudio.microsoft.com/visual-cpp-build-tools/에서 빌드 도구만 설치할 수 있습니다. Visual Studio Code에는 기본적으로 포함되어 있지 않습니다.</p><p>환경 설정 예시:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### NVIDIA CUDA 지원
CUDA 지원 컴파일을 원한다면, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">지원되는 CUDA 버전을 지원 매트릭스에서 선택</a>한 후 다음을 설치하세요.
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 이상</li>
<li>CUDA와 호환되는 <a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">컴파일러</a></li></p><p></ul>참고: 다양한 CUDA, CUDA 드라이버, NVIDIA 하드웨어와 호환되는 cuDNN 버전은 <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">cuDNN 지원 매트릭스</a>를 참고하세요.</p><p>CUDA 지원을 비활성화하려면 환경 변수 <code>USE_CUDA=0</code>을 내보내세요.
기타 유용한 환경 변수는 <code>setup.py</code>에서 확인할 수 있습니다.</p><p>NVIDIA Jetson 플랫폼(Jetson Nano, TX1, TX2, AGX Xavier) 빌드 시, Jetson Nano용 PyTorch 설치 방법은 <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">여기</a>를 참고하세요.</p><p>##### AMD ROCm 지원
ROCm 지원 컴파일을 원한다면 아래를 설치하세요.
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 이상</li>
<li>ROCm은 현재 Linux에서만 지원됩니다.</li></p><p></ul>기본적으로 빌드 시스템은 ROCm이 <code>/opt/rocm</code>에 설치되어 있다고 가정합니다. 다른 디렉터리에 설치했다면 <code>ROCM_PATH</code> 환경 변수를 지정해야 합니다. 빌드 시스템은 AMD GPU 아키텍처를 자동 감지합니다. 필요시 <code>PYTORCH_ROCM_ARCH</code> 환경 변수로 명시적으로 지정할 수 있습니다. <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">AMD GPU 아키텍처 참고</a></p><p>ROCm 지원을 비활성화하려면 환경 변수 <code>USE_ROCM=0</code>을 내보내세요.
기타 유용한 환경 변수는 <code>setup.py</code>에서 확인할 수 있습니다.</p><p>##### Intel GPU 지원
Intel GPU 지원 컴파일을 원한다면 아래를 참고하세요.
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Intel GPU용 PyTorch 사전 준비</a> 안내</li>
<li>Intel GPU는 Linux와 Windows에서 지원됩니다.</li></p><p></ul>Intel GPU 지원을 비활성화하려면 환경 변수 <code>USE_XPU=0</code>을 내보내세요.
기타 유용한 환경 변수는 <code>setup.py</code>에서 확인할 수 있습니다.</p><p>#### PyTorch 소스 받기
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>기존 체크아웃을 업데이트하는 경우</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### 의존성 설치</p><p><strong>공통</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>아래 "PyTorch 소스 받기" 섹션의 소스 코드 클론 후 PyTorch 디렉터리에서 실행</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>CUDA만 해당: 필요시 GPU의 LAPACK 지원 추가</h1>
<h1>magma 설치: 활성화된 conda 환경에서 실행, 설치할 CUDA 버전 명시</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(선택) torch.compile에서 inductor/triton 사용 시, 트라이튼(triton) 버전 설치</h1>
<h1>소스 클론 후 pytorch 디렉터리에서 실행</h1>
<h1>Intel GPU 지원 시 <code>export USE_XPU=1</code> 후 실행</h1>
make triton</code></pre></p><p><strong>MacOS</strong></p><pre><code class="language-bash"># 인텔 x86 프로세서 머신에서만 추가
pip install mkl-static mkl-include
<h1>torch.distributed 필요시 패키지 추가</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>torch.distributed 필요시 패키지 추가</h1>
<h1>Windows의 분산 패키지 지원은 프로토타입 기능이며 변경될 수 있음</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### PyTorch 설치
<strong>Linux</strong></p><p>AMD ROCm 컴파일 시 먼저 아래 명령을 실행하세요.
<pre><code class="language-bash"># ROCm 컴파일 시에만 실행
python tools/amd_build/build_amd.py</code></pre></p><p>PyTorch 설치
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Windows</strong></p><p>레거시 Python 코드 빌드를 원한다면 <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Building on legacy code and CUDA</a>를 참고하세요.</p><p><strong>CPU 전용 빌드</strong></p><p>이 모드에서는 PyTorch 연산이 GPU가 아니라 CPU에서 실행됩니다.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>OpenMP 참고: 권장 OpenMP 구현체는 Intel OpenMP(iomp)입니다. iomp와 링크하려면 라이브러리를 수동으로 다운로드 후, <code>CMAKE_INCLUDE_PATH</code>와 <code>LIB</code> 환경 변수를 설정해야 합니다. <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">설정 예시</a>를 참고하세요. CMake에 대해 이 구성을 하지 않으면 MS Visual C OpenMP 런타임(vcomp)이 사용됩니다.</p><p><strong>CUDA 기반 빌드</strong></p><p>이 모드에서는 PyTorch 연산이 CUDA를 통해 GPU를 활용하여 더 빠른 계산이 가능합니다.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a>는 CUDA 빌드 시 필요합니다.
NVTX는 CUDA 배포판의 일부이며 "Nsight Compute"로 불립니다. 이미 CUDA를 설치했다면, 설치를 한 번 더 실행하여 해당 체크박스를 선택하면 됩니다.
Visual Studio 설치 후 CUDA와 Nsight Compute가 설치되었는지 확인하세요.</p><p>현재 VS 2017 / 2019와 Ninja가 CMake의 제너레이터로 지원됩니다. <code>ninja.exe</code>가 <code>PATH</code>에 있으면 Ninja가 기본 제너레이터로 사용되며, 그렇지 않으면 VS 2017 / 2019가 사용됩니다.
<br/> Ninja 선택 시 최신 MSVC가 도구체인으로 선택됩니다.</p><p><a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN(MKLDNN 또는 DNNL)</a>, <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> 등 추가 라이브러리가 필요할 수 있습니다. <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">설치 도우미</a>를 참고하세요.</p><p>기타 환경 변수 설정은 <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> 스크립트를 참고하세요.</p><pre><code class="language-cmd">cmd</p><p>:: mkl 패키지 다운로드 및 압축 해제 후 환경 변수 설정
:: CMake가 <code>Could NOT find OpenMP</code> 에러를 내지 않도록 반드시 설정
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%</p><p>:: 이전 섹션 내용을 꼭 읽고 진행하세요
:: [선택] Ninja와 Visual Studio에서 CUDA의 툴셋을 오버라이드하려면 아래 스크립트 블록 실행
:: "Visual Studio 2019 Developer Command Prompt"가 자동 실행됨
:: Visual Studio 제너레이터 사용시 CMake >= 3.12 필요
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [선택] CUDA 호스트 컴파일러 지정
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Intel GPU 빌드</strong></p><p>이 모드에서는 Intel GPU 지원이 포함된 PyTorch를 빌드합니다.</p><p><a href="#prerequisites" target="_blank" rel="noopener noreferrer">공통 사전 준비</a> 및 <a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Intel GPU용 사전 준비</a>가 설치되어 있고 환경 변수가 올바르게 설정되어 있는지 확인하세요. 빌드 도구로는 <code>Visual Studio 2022</code>가 필요합니다.</p><p>PyTorch 빌드는 다음 명령으로 가능합니다.</p><pre><code class="language-cmd">:: CMD 명령:
:: 패키지 검색을 돕기 위해 CMAKE_PREFIX_PATH 설정
:: %CONDA_PREFIX%는 <code>conda activate custom_env</code> 이후에만 동작</p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### 빌드 옵션 조정(선택 사항)</p><p>CMake 변수의 구성을(빌드 전) 선택적으로 조정할 수 있습니다. 예를 들어, 미리 감지된 CuDNN 또는 BLAS 디렉토리 조정 등이 가능합니다.</p><p>Linux에서
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # 또는 cmake-gui build</code></pre></p><p>macOS에서
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # 또는 cmake-gui build</code></pre></p><h3>도커 이미지</h3></p><p>#### 미리 빌드된 이미지 사용</p><p>도커 허브에서 미리 빌드된 이미지를 받아 docker v19.03+에서 실행할 수 있습니다.</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>PyTorch는 프로세스 간 데이터 공유를 위해 공유 메모리를 사용합니다. torch 멀티프로세싱(예: 멀티스레드 데이터 로더) 사용 시 기본 공유 메모리 세그먼트 크기는 충분하지 않으므로, <code>--ipc=host</code> 또는 <code>--shm-size</code> 옵션으로 공유 메모리 크기를 늘려야 합니다.</p><p>#### 직접 이미지 빌드</p><p><strong>참고:</strong> docker 버전 > 18.06에서만 빌드해야 합니다.</p><p><code>Dockerfile</code>은 CUDA 11.1 지원 및 cuDNN v8과 함께 이미지를 빌드하도록 제공됩니다.
<code>PYTHON_VERSION=x.y</code> make 변수를 넘겨 Miniconda에서 사용할 Python 버전을 지정할 수 있으며, 지정하지 않으면 기본값이 사용됩니다.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>이미지는 docker.io/${your_docker_username}/pytorch로 태깅됨</code></pre></h1></p><p>빌드 중 CMake에 추가 변수 전달이 필요하면 <code>CMAKE_VARS="..."</code> 환경변수를 사용할 수 있습니다.
사용 가능한 변수 목록은 <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a>를 참고하세요.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>문서 빌드</h3></p><p>여러 형식으로 문서를 빌드하려면 <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>와 pytorch_sphinx_theme2가 필요합니다.</p><p>로컬에서 문서를 빌드하기 전, <code>torch</code>가 환경에 설치되어 있어야 합니다. 간단한 수정은 <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Getting Started</a>에 설명된 대로 nightly 버전을 설치해도 됩니다.</p><p>새 모듈 추가나 모듈용 docstring 작성처럼 복잡한 수정을 위해서는 <a href="#from-source" target="_blank" rel="noopener noreferrer">소스에서 torch 설치</a>가 필요할 수 있습니다. Docstring 규칙은 <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Docstring Guidelines</a>를 참고하세요.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p><code>make</code> 명령으로 모든 출력 형식 목록을 확인할 수 있습니다.</p><p>katex 오류가 발생하면 <code>npm install katex</code>를 실행하세요. 계속 오류가 나면
<code>npm install -g katex</code>를 시도해보세요.</p><blockquote>[!NOTE]</blockquote>
<blockquote><code>nodejs</code>를 다른 패키지 매니저(예: <code>conda</code>)로 설치했다면,</blockquote>
<blockquote><code>npm</code>이 설치하는 <code>katex</code> 버전이 nodejs와 호환되지 않아 문서 빌드가 실패할 수 있습니다.</blockquote>
<blockquote>호환이 검증된 조합은 <code>node@6.13.1</code>과 <code>katex@0.13.18</code>입니다.</blockquote>
<blockquote>latter를 npm으로 설치하려면 다음을 실행하세요.</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!NOTE]</blockquote>
<blockquote>numpy 호환성 오류가 발생하면 다음을 실행하세요:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>CI에서 실행되는 의존성에 변경이 있으면 </code>.ci/docker/requirements-docs.txt<code> 파일을 수정하세요.</p><p>#### PDF 빌드</p><p>PyTorch 전체 문서의 PDF를 컴파일하려면
</code>texlive<code>와 LaTeX가 필요합니다. macOS에서는 아래로 설치할 수 있습니다.
</code></pre>
brew install --cask mactex
</code>`<code></p><p>PDF를 생성하려면:</p><ul><li>아래를 실행하세요.</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   </code>build/latex<code> 디렉터리에 필요한 파일이 생성됩니다.</p><ul><li>해당 디렉터리로 이동해 다음을 실행하세요.</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   원하는 내용을 담은 </code>pytorch.pdf`가 생성됩니다. 목차와 색인이 제대로 생성되도록 한 번 더 실행하세요.</p><blockquote>[!NOTE]</blockquote>
<blockquote>PDF 뷰어에서 목차를 보려면 <strong>Table of Contents</strong> 보기로 전환하세요.</blockquote></p><h3>이전 버전</h3></p><p>이전 PyTorch 버전 설치 안내 및 바이너리는
<a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">웹사이트</a>에서 확인할 수 있습니다.</p><h2>시작하기</h2></p><p>시작을 위한 세 가지 포인터:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">튜토리얼: PyTorch 이해 및 사용법 익히기</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">예제: 모든 분야의 이해하기 쉬운 PyTorch 코드</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">API 레퍼런스</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">용어집</a></li></p><p></ul><h2>자료</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">PyTorch 튜토리얼</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">PyTorch 예제</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">PyTorch 모델</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Udacity: PyTorch로 딥러닝 입문</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Udacity: PyTorch로 머신러닝 입문</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Coursera: PyTorch로 딥 뉴럴 네트워크</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch 트위터</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch 블로그</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch 유튜브</a></li></p><p></ul><h2>커뮤니케이션</h2>
<ul><li>포럼: 구현, 연구 등 논의 https://discuss.pytorch.org</li>
<li>GitHub 이슈: 버그 리포트, 기능 요청, 설치 문제, RFC 등</li>
<li>Slack: <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">PyTorch Slack</a>은 중급 이상 PyTorch 사용자 및 개발자가 일반 대화, 온라인 토론, 협업 등 다양한 목적으로 사용합니다. 초보자라면 <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch 포럼</a>이 주요 매체입니다. Slack 초대가 필요하면 https://goo.gl/forms/PP1AGvNHpSaJP8to1 양식을 작성하세요.</li>
<li>뉴스레터: PyTorch의 주요 공지 사항만 전달하는 단방향 이메일 뉴스레터입니다. 가입: https://eepurl.com/cbG0rv</li>
<li>Facebook 페이지: PyTorch의 주요 공지 사항. https://www.facebook.com/pytorch</li>
<li>브랜드 가이드라인은 <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">공식 웹사이트</a>를 참고하세요.</li></p><p></ul><h2>릴리즈 및 기여</h2></p><p>PyTorch는 일반적으로 연간 세 차례 마이너 릴리스를 진행합니다. 버그를 발견하면 <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">이슈 등록</a>을 부탁드립니다.</p><p>모든 기여를 환영합니다. 버그 수정은 별도 논의 없이 바로 기여해 주세요.</p><p>새 기능, 유틸리티 함수, 핵심 확장 기여를 원한다면 먼저 이슈를 열어 제안하고 논의 후에 PR을 보내주세요.
논의 없이 PR을 보내면, 프로젝트의 방향과 맞지 않을 경우 거절될 수 있습니다.</p><p>PyTorch 기여 방법은 <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Contribution page</a>, 릴리즈 정보는 <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">Release page</a>를 참고하세요.</p><h2>팀</h2></p><p>PyTorch는 여러 숙련된 엔지니어와 연구자의 커뮤니티 주도 프로젝트입니다.</p><p>현재 <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a>가 PyTorch를 관리하고 있으며, 수백 명의 재능 있는 기여자들이 다양한 방식으로 참여하고 있습니다.
비록 완전하진 않지만 반드시 언급해야 할 사람들: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>참고: 이 프로젝트는 이름이 같은 <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a>와 관련이 없습니다. Hugh는 Torch 커뮤니티의 소중한 기여자이며 Torch와 PyTorch에 많은 도움을 주셨습니다.</p><h2>라이선스</h2></p><p>PyTorch는 <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a> 파일에 명시된 BSD 스타일 라이선스를 따릅니다.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>