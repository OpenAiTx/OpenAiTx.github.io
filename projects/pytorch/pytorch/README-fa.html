<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Persian. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Persian. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Persian. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Persian</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="لوگوی PyTorch"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch یک بسته پایتون است که دو ویژگی سطح بالای زیر را فراهم می‌کند:
<ul><li>محاسبه تنسور (مانند NumPy) با شتاب‌دهی قدرتمند GPU</li>
<li>شبکه‌های عصبی عمیق ساخته شده بر پایه سیستم autograd مبتنی بر نوار</li></p><p></ul>شما می‌توانید بسته‌های محبوب پایتون مانند NumPy، SciPy و Cython را برای گسترش PyTorch در مواقع مورد نیاز مجدداً استفاده کنید.</p><p>سلامت شاخه اصلی ما (سیگنال‌های ادغام مداوم) در <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a> قابل مشاهده است.</p><p><!-- toc --></p><ul><li><a href="#اطلاعات-بیشتر-درباره-pytorch" target="_blank" rel="noopener noreferrer">اطلاعات بیشتر درباره PyTorch</a></li>
  <li><a href="#کتابخانه-تنسور-آماده-gpu" target="_blank" rel="noopener noreferrer">کتابخانه تنسور آماده GPU</a></li>
  <li><a href="#شبکه‌های-عصبی-پویا-autograd-مبتنی-بر-نوار" target="_blank" rel="noopener noreferrer">شبکه‌های عصبی پویا: Autograd مبتنی بر نوار</a></li>
  <li><a href="#اول-پایتون" target="_blank" rel="noopener noreferrer">اول پایتون</a></li>
  <li><a href="#تجربیات-دستوری" target="_blank" rel="noopener noreferrer">تجربیات دستوری</a></li>
  <li><a href="#سریع-و-کم‌حجم" target="_blank" rel="noopener noreferrer">سریع و کم‌حجم</a></li>
  <li><a href="#گسترش-بدون-دردسر" target="_blank" rel="noopener noreferrer">گسترش بدون دردسر</a></li>
<li><a href="#نصب" target="_blank" rel="noopener noreferrer">نصب</a></li>
  <li><a href="#باینری‌ها" target="_blank" rel="noopener noreferrer">باینری‌ها</a></li>
    <li><a href="#پلتفرم‌های-nvidia-jetson" target="_blank" rel="noopener noreferrer">پلتفرم‌های NVIDIA Jetson</a></li>
  <li><a href="#از-سورس" target="_blank" rel="noopener noreferrer">از سورس</a></li>
    <li><a href="#پیش‌نیازها" target="_blank" rel="noopener noreferrer">پیش‌نیازها</a></li>
      <li><a href="#پشتیبانی-nvidia-cuda" target="_blank" rel="noopener noreferrer">پشتیبانی NVIDIA CUDA</a></li>
      <li><a href="#پشتیبانی-amd-rocm" target="_blank" rel="noopener noreferrer">پشتیبانی AMD ROCm</a></li>
      <li><a href="#پشتیبانی-gpu-اینتل" target="_blank" rel="noopener noreferrer">پشتیبانی GPU اینتل</a></li>
    <li><a href="#دریافت-سورس-pytorch" target="_blank" rel="noopener noreferrer">دریافت سورس PyTorch</a></li>
    <li><a href="#نصب-وابستگی‌ها" target="_blank" rel="noopener noreferrer">نصب وابستگی‌ها</a></li>
    <li><a href="#نصب-pytorch" target="_blank" rel="noopener noreferrer">نصب PyTorch</a></li>
      <li><a href="#تنظیم-گزینه‌های-ساخت-اختیاری" target="_blank" rel="noopener noreferrer">تنظیم گزینه‌های ساخت (اختیاری)</a></li>
  <li><a href="#ایمیج-داکر" target="_blank" rel="noopener noreferrer">ایمیج داکر</a></li>
    <li><a href="#استفاده-از-ایمیج‌های-آماده" target="_blank" rel="noopener noreferrer">استفاده از ایمیج‌های آماده</a></li>
    <li><a href="#ساخت-ایمیج-به-صورت-دستی" target="_blank" rel="noopener noreferrer">ساخت ایمیج به صورت دستی</a></li>
  <li><a href="#ساخت-مستندات" target="_blank" rel="noopener noreferrer">ساخت مستندات</a></li>
    <li><a href="#ساخت-pdf" target="_blank" rel="noopener noreferrer">ساخت PDF</a></li>
  <li><a href="#نسخه‌های-قبلی" target="_blank" rel="noopener noreferrer">نسخه‌های قبلی</a></li>
<li><a href="#شروع-به-کار" target="_blank" rel="noopener noreferrer">شروع به کار</a></li>
<li><a href="#منابع" target="_blank" rel="noopener noreferrer">منابع</a></li>
<li><a href="#ارتباطات" target="_blank" rel="noopener noreferrer">ارتباطات</a></li>
<li><a href="#انتشارها-و-مشارکت" target="_blank" rel="noopener noreferrer">انتشارها و مشارکت</a></li>
<li><a href="#تیم" target="_blank" rel="noopener noreferrer">تیم</a></li>
<li><a href="#مجوز" target="_blank" rel="noopener noreferrer">مجوز</a></li></p><p></ul><!-- tocstop --></p><h2>اطلاعات بیشتر درباره PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">مبانی PyTorch را بیاموزید</a></p><p>در سطح جزئی‌تر، PyTorch کتابخانه‌ای است که از اجزای زیر تشکیل شده است:</p><p>| مؤلفه | توضیحات |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | کتابخانه تنسور مانند NumPy با پشتیبانی قدرتمند GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | کتابخانه مشتق‌گیری خودکار مبتنی بر نوار که همه عملیات‌های تنسور قابل مشتق در torch را پشتیبانی می‌کند |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | پشته کامپایل (TorchScript) برای ایجاد مدل‌های قابل سریال‌سازی و بهینه‌سازی از کد PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | کتابخانه شبکه‌های عصبی که با autograd یکپارچه شده و برای بیشینه انعطاف‌پذیری طراحی شده است |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | چندفرایندی پایتون، اما با اشتراک‌گذاری جادویی حافظه تنسورها بین فرایندها. مفید برای بارگذاری داده و آموزش Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader و سایر توابع کمکی برای راحتی بیشتر |</p><p>معمولاً PyTorch به دو صورت استفاده می‌شود:</p><ul><li>جایگزینی برای NumPy برای بهره‌گیری از قدرت GPUها.</li>
<li>پلتفرم تحقیقاتی یادگیری عمیق که بیشینه انعطاف‌پذیری و سرعت را فراهم می‌کند.</li></p><p></ul>توضیحات بیشتر:</p><h3>کتابخانه تنسور آماده GPU</h3></p><p>اگر از NumPy استفاده کرده‌اید، پس با تنسورها (همان ndarray) آشنا هستید.</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="تصویرسازی تنسور"></p><p>PyTorch تنسورهایی ارائه می‌دهد که می‌توانند روی CPU یا GPU قرار گیرند و محاسبات را با شتاب بسیار بالا انجام می‌دهند.</p><p>ما مجموعه‌ای گسترده از توابع تنسور برای پاسخ‌گویی به نیازهای محاسبات علمی شما ارائه می‌دهیم، مانند برش، اندیس‌گذاری، عملیات ریاضی، جبر خطی، کاهش‌ها و غیره.
و بسیار سریع هستند!</p><h3>شبکه‌های عصبی پویا: Autograd مبتنی بر نوار</h3></p><p>PyTorch روشی منحصربه‌فرد برای ساخت شبکه‌های عصبی دارد: استفاده و بازپخش ضبط‌کننده نوار.</p><p>بیشتر فریم‌ورک‌ها مانند TensorFlow، Theano، Caffe و CNTK دیدگاه ایستا دارند.
باید یک شبکه عصبی بسازید و بارها از همان ساختار استفاده کنید.
تغییر رفتار شبکه به معنای شروع از ابتدا است.</p><p>در PyTorch از تکنیکی به نام مشتق‌گیری خودکار معکوس (reverse-mode auto-differentiation) استفاده می‌کنیم که به شما اجازه می‌دهد رفتار شبکه خود را به طور دلخواه و بدون تأخیر یا سربار تغییر دهید. الهام ما از چندین مقاله تحقیقاتی و پروژه‌هایی مانند <a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>، <a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>، <a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a> و غیره گرفته شده است.</p><p>در حالی که این تکنیک منحصراً متعلق به PyTorch نیست، اما یکی از سریع‌ترین پیاده‌سازی‌های آن تا به امروز است.
شما بهترین سرعت و انعطاف‌پذیری را برای تحقیقات پیشرفته خود خواهید داشت.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="گراف پویا"></p><h3>اول پایتون</h3></p><p>PyTorch یک اتصال پایتون به یک فریم‌ورک C++ یکپارچه نیست.
بلکه به گونه‌ای ساخته شده که عمیقاً با پایتون یکپارچه باشد.
می‌توانید به طور طبیعی مانند <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> و غیره از آن استفاده کنید.
می‌توانید لایه‌های جدید شبکه عصبی را مستقیماً در پایتون با استفاده از کتابخانه‌های مورد علاقه خود بنویسید و از بسته‌هایی مانند <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> و <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a> بهره ببرید.
هدف ما این است که در صورت امکان، چرخ را از نو اختراع نکنیم.</p><h3>تجربیات دستوری</h3></p><p>PyTorch طوری طراحی شده که شهودی، خطی در تفکر و آسان برای استفاده باشد.
هر خط کد را که اجرا می‌کنید، همان لحظه اجرا می‌شود. هیچ دیدگاه غیرهمزمانی وجود ندارد.
وقتی وارد اشکال‌زدا می‌شوید یا پیام خطا و trace می‌گیرید، فهم آن‌ها مستقیم است.
trace دقیقاً به جایی اشاره می‌کند که کد شما تعریف شده است.
امیدواریم هیچ‌گاه ساعت‌ها وقت خود را صرف اشکال‌زدایی به خاطر traceهای بد یا موتورهای اجرای غیرهمزمان و مبهم نکنید.</p><h3>سریع و کم‌حجم</h3></p><p>PyTorch سربار چارچوبی حداقلی دارد. ما کتابخانه‌های شتاب‌دهنده‌ای مانند <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> و NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>، <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) را برای بیشینه سرعت یکپارچه کرده‌ایم.
در هسته خود، backendهای تنسور و شبکه عصبی PyTorch برای CPU و GPU بالغ هستند و سال‌ها تست شده‌اند.</p><p>در نتیجه، PyTorch بسیار سریع است، چه شبکه‌های کوچک و چه بزرگ اجرا کنید.</p><p>مصرف حافظه در PyTorch نسبت به Torch یا برخی جایگزین‌ها بسیار کارآمد است.
ما برای GPU تخصیص‌دهنده حافظه اختصاصی نوشته‌ایم تا مدل‌های یادگیری عمیق شما بیشینه بهره‌وری حافظه را داشته باشند.
این امکان را فراهم می‌کند تا مدل‌های یادگیری عمیق بزرگ‌تری نسبت به گذشته آموزش دهید.</p><h3>گسترش بدون دردسر</h3></p><p>نوشتن ماژول‌های جدید شبکه عصبی یا ارتباط با API تنسور PyTorch طوری طراحی شده که ساده و با کمترین انتزاع باشد.</p><p>می‌توانید لایه‌های جدید شبکه عصبی را در پایتون با استفاده از API torch <a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">یا کتابخانه‌های مبتنی بر NumPy مانند SciPy</a> بنویسید.</p><p>اگر می‌خواهید لایه‌های خود را در C/C++ بنویسید، ما یک API گسترش آسان و کارآمد با حداقل کد اضافی ارائه می‌دهیم.
نیازی به نوشتن کد wrapper نیست. <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">آموزش اینجا</a> و <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">مثال اینجا</a> را ببینید.</p><h2>نصب</h2></p><h3>باینری‌ها</h3>
دستورات نصب باینری‌ها از طریق Conda یا pip wheel در سایت ما موجود است: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>#### پلتفرم‌های NVIDIA Jetson</p><p>Wheelهای پایتون برای Jetson Nano، Jetson TX1/TX2، Jetson Xavier NX/AGX و Jetson AGX Orin <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">اینجا</a> ارائه شده‌اند و کانتینر L4T <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">اینجا</a> منتشر شده است.</p><p>آن‌ها نیازمند JetPack 4.2 به بالا هستند و <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> و <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> نگهداری آن‌ها را بر عهده دارند.</p><h3>از سورس</h3></p><p>#### پیش‌نیازها
اگر قصد نصب از سورس را دارید، به موارد زیر نیاز دارید:
<ul><li>پایتون ۳.۹ یا بالاتر</li>
<li>کامپایلری که کاملاً از C++17 پشتیبانی کند، مانند clang یا gcc (در لینوکس gcc 9.4.0 یا جدیدتر مورد نیاز است)</li>
<li>Visual Studio یا Visual Studio Build Tool (فقط ویندوز)</li></p><p></ul>\* CI PyTorch از Visual C++ BuildTools استفاده می‌کند، که همراه با نسخه‌های Enterprise، Professional یا Community ویژوال استودیو ارائه می‌شود. همچنین می‌توانید ابزار ساخت را از آدرس https://visualstudio.microsoft.com/visual-cpp-build-tools/ نصب کنید. ابزار ساخت به طور پیش‌فرض همراه Visual Studio Code ارائه نمی‌شود.</p><p>یک نمونه تنظیم محیط به شرح زیر است:</p><ul><li>لینوکس:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>ویندوز:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### پشتیبانی NVIDIA CUDA
اگر می‌خواهید با پشتیبانی CUDA کامپایل کنید، <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">یک نسخه پشتیبانی‌شده CUDA از جدول پشتیبانی ما انتخاب کنید</a>، سپس موارد زیر را نصب کنید:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> نسخه ۸.۵ یا بالاتر</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">کامپایلر</a> سازگار با CUDA</li></p><p></ul>توجه: می‌توانید به <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">جدول پشتیبانی cuDNN</a> برای نسخه‌های cuDNN با CUDAهای مختلف، درایور CUDA و سخت‌افزار NVIDIA مراجعه کنید.</p><p>اگر می‌خواهید پشتیبانی CUDA را غیرفعال کنید، متغیر محیطی <code>USE_CUDA=0</code> را صادر کنید.
سایر متغیرهای محیطی مفید در <code>setup.py</code> موجود است.</p><p>اگر برای پلتفرم‌های Jetson شرکت NVIDIA (Jetson Nano، TX1، TX2، AGX Xavier) کامپایل می‌کنید، <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">دستورالعمل نصب PyTorch برای Jetson Nano اینجاست</a></p><p>##### پشتیبانی AMD ROCm
اگر می‌خواهید با پشتیبانی ROCm کامپایل کنید، نصب کنید:
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> نسخه ۴.۰ به بالا</li>
<li>ROCm در حال حاضر فقط برای سیستم‌های لینوکس پشتیبانی می‌شود.</li></p><p></ul>به طور پیش‌فرض، سیستم ساخت انتظار دارد ROCm در مسیر <code>/opt/rocm</code> نصب شده باشد. اگر ROCm در مسیر دیگری نصب شده، متغیر محیطی <code>ROCM_PATH</code> را به مسیر نصب ROCm تنظیم کنید. سیستم ساخت به طور خودکار معماری GPUهای AMD را تشخیص می‌دهد. در صورت نیاز، معماری GPUهای AMD را می‌توان با متغیر محیطی <code>PYTORCH_ROCM_ARCH</code> به صورت صریح تنظیم کرد. <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">لیست GPUهای پشتیبانی شده</a></p><p>اگر می‌خواهید پشتیبانی ROCm را غیرفعال کنید، متغیر محیطی <code>USE_ROCM=0</code> را صادر کنید.
سایر متغیرهای محیطی مفید در <code>setup.py</code> موجود است.</p><p>##### پشتیبانی GPU اینتل
اگر می‌خواهید با پشتیبانی GPU اینتل کامپایل کنید، مراحل زیر را دنبال کنید:
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">دستورالعمل‌های پیش‌نیاز PyTorch برای GPU اینتل</a></li>
<li>GPU اینتل برای لینوکس و ویندوز پشتیبانی می‌شود.</li></p><p></ul>اگر می‌خواهید پشتیبانی GPU اینتل را غیرفعال کنید، متغیر محیطی <code>USE_XPU=0</code> را صادر کنید.
سایر متغیرهای محیطی مفید در <code>setup.py</code> موجود است.</p><p>#### دریافت سورس PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>اگر checkout قبلی دارید</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### نصب وابستگی‌ها</p><p><strong>مشترک</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>پس از کلون سورس کد طبق بخش “دریافت سورس PyTorch” این دستور را اجرا کنید</h1>
pip install -r requirements.txt</code></pre></p><p><strong>در لینوکس</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>فقط CUDA: اگر نیاز است پشتیبانی LAPACK برای GPU اضافه کنید</h1>
<h1>نصب magma: این دستور را با محیط فعال conda و با تعیین نسخه CUDA اجرا کنید</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(اختیاری) اگر از torch.compile با inductor/triton استفاده می‌کنید، نسخه متناسب triton را نصب کنید</h1>
<h1>از مسیر پوشه pytorch پس از کلون اجرا کنید</h1>
<h1>برای پشتیبانی GPU اینتل، لطفا قبل از اجرا صراحتاً <code>export USE_XPU=1</code> را تنظیم کنید.</h1>
make triton</code></pre></p><p><strong>در macOS</strong></p><pre><code class="language-bash"># این بسته را فقط روی ماشین‌های پردازنده x86 اینتل اضافه کنید
pip install mkl-static mkl-include
<h1>اگر torch.distributed نیاز است این بسته‌ها را اضافه کنید</h1>
conda install pkg-config libuv</code></pre></p><p><strong>در ویندوز</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>اگر torch.distributed نیاز است این بسته‌ها را اضافه کنید.</h1>
<h1>پشتیبانی بسته distributed در ویندوز ویژگی نمونه‌ای است و ممکن است تغییر کند.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### نصب PyTorch
<strong>در لینوکس</strong></p><p>اگر برای AMD ROCm کامپایل می‌کنید ابتدا این دستور را اجرا کنید:
<pre><code class="language-bash"># فقط اگر برای ROCm کامپایل می‌کنید اجرا کنید
python tools/amd_build/build_amd.py</code></pre></p><p>نصب PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>در macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>در ویندوز</strong></p><p>اگر می‌خواهید کد پایتون قدیمی را کامپایل کنید، به <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">ساخت روی کد قدیمی و CUDA</a> مراجعه کنید.</p><p><strong>ساخت فقط برای CPU</strong></p><p>در این حالت محاسبات PyTorch روی CPU اجرا می‌شود، نه روی GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>نکته درباره OpenMP: پیاده‌سازی مورد نظر OpenMP، Intel OpenMP (iomp) است. برای لینک با iomp باید کتابخانه را به صورت دستی دانلود و محیط ساخت را با تغییر <code>CMAKE_INCLUDE_PATH</code> و <code>LIB</code> تنظیم کنید. دستورالعمل <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">اینجا</a> مثالی برای تنظیم هر دو MKL و Intel OpenMP است. بدون این تنظیمات، runtime پیش‌فرض Microsoft Visual C OpenMP (vcomp) استفاده خواهد شد.</p><p><strong>ساخت مبتنی بر CUDA</strong></p><p>در این حالت محاسبات PyTorch از GPU شما از طریق CUDA برای سرعت بیشتر استفاده می‌کند.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> برای ساخت PyTorch با CUDA مورد نیاز است.
NVTX بخشی از بسته CUDA است و به نام "Nsight Compute" شناخته می‌شود. برای نصب آن روی CUDA نصب‌شده، نصب CUDA را مجدداً اجرا و چک‌باکس مربوطه را فعال کنید.
اطمینان حاصل کنید CUDA با Nsight Compute پس از Visual Studio نصب شده باشد.</p><p>در حال حاضر، VS 2017 / 2019 و Ninja به عنوان generator CMake پشتیبانی می‌شوند. اگر <code>ninja.exe</code> در <code>PATH</code> باشد، Ninja به عنوان generator پیش‌فرض استفاده می‌شود، در غیر این صورت VS 2017 / 2019.
<br/> اگر Ninja انتخاب شود، جدیدترین MSVC به عنوان toolchain زیرین انتخاب می‌شود.</p><p>کتابخانه‌های اضافی مانند
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>، <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN، با نام MKLDNN یا DNNL</a> و <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> اغلب نیاز هستند. لطفاً برای نصب آن‌ها به <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> مراجعه کنید.</p><p>می‌توانید برای پیکربندی متغیرهای محیطی دیگر به اسکریپت <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> مراجعه کنید.</p><pre><code class="language-cmd">cmd</p><p>:: متغیرهای محیطی را پس از دانلود و استخراج بسته mkl تنظیم کنید،
:: در غیر این صورت CMake خطای <code>Could NOT find OpenMP</code> می‌دهد.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%</p><p>:: محتوای بخش قبلی را با دقت بخوانید قبل از ادامه.
:: [اختیاری] اگر می‌خواهید toolset زیرین Ninja و Visual Studio با CUDA را override کنید، اسکریپت زیر را اجرا کنید.
:: "Visual Studio 2019 Developer Command Prompt" به طور خودکار اجرا می‌شود.
:: اطمینان حاصل کنید CMake >= 3.12 را قبل از این کار دارید (در صورت استفاده از Visual Studio generator).
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [اختیاری] اگر می‌خواهید CUDA host compiler را override کنید
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>ساخت GPU اینتل</strong></p><p>در این حالت PyTorch با پشتیبانی GPU اینتل ساخته می‌شود.</p><p>لطفاً اطمینان حاصل کنید <a href="#پیش‌نیازها" target="_blank" rel="noopener noreferrer">پیش‌نیازهای مشترک</a> و همچنین <a href="#پشتیبانی-gpu-اینتل" target="_blank" rel="noopener noreferrer">پیش‌نیازهای GPU اینتل</a> به درستی نصب و متغیرهای محیطی قبل از شروع ساخت تنظیم شده باشند. برای پشتیبانی ابزار ساخت، <code>Visual Studio 2022</code> مورد نیاز است.</p><p>سپس می‌توانید PyTorch را با دستور زیر بسازید:</p><pre><code class="language-cmd">:: دستورات CMD:
:: CMAKE_PREFIX_PATH را تنظیم کنید تا بسته‌های مربوطه پیدا شوند
:: %CONDA_PREFIX% فقط پس از <code>conda activate custom_env</code> کار می‌کند</p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### تنظیم گزینه‌های ساخت (اختیاری)</p><p>می‌توانید پیکربندی متغیرهای cmake را به طور اختیاری (بدون ساخت اولیه) با انجام موارد زیر تنظیم کنید. برای مثال، تنظیم مسیرهای پیش‌فرض تشخیص داده شده برای CuDNN یا BLAS با این روش انجام می‌شود.</p><p>در لینوکس
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # یا cmake-gui build</code></pre></p><p>در macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # یا cmake-gui build</code></pre></p><h3>ایمیج داکر</h3></p><p>#### استفاده از ایمیج‌های آماده</p><p>همچنین می‌توانید یک ایمیج آماده داکر را از Docker Hub بکشید و با docker v19.03+ اجرا کنید</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>توجه داشته باشید که PyTorch برای اشتراک داده بین فرایندها از حافظه اشتراکی استفاده می‌کند، بنابراین اگر از multiprocessing در torch استفاده می‌شود (مانند بارگذاری داده چندریسمانی)، اندازه پیش‌فرض بخش حافظه اشتراکی کانتینر کافی نیست و باید اندازه حافظه اشتراکی را با گزینه‌های خط فرمان <code>--ipc=host</code> یا <code>--shm-size</code> برای <code>nvidia-docker run</code> افزایش دهید.</p><p>#### ساخت ایمیج به صورت دستی</p><p><strong>نکته:</strong> باید با نسخه داکر بالاتر از ۱۸.۰۶ ساخته شود</p><p><code>Dockerfile</code> برای ساخت ایمیج با پشتیبانی CUDA 11.1 و cuDNN v8 ارائه شده است.
می‌توانید متغیر PYTHON_VERSION=x.y را برای تعیین نسخه پایتون مورد استفاده Miniconda تعیین کنید، یا آن را خالی بگذارید تا مقدار پیش‌فرض استفاده شود.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>ایمیج‌ها با docker.io/${your_docker_username}/pytorch تگ می‌شوند</code></pre></h1></p><p>همچنین می‌توانید متغیر محیطی <code>CMAKE_VARS="..."</code> را برای تعیین متغیرهای اضافی CMake به عنوان آرگومان‌های build به کار ببرید.
برای لیست متغیرهای موجود به <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> مراجعه کنید.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>ساخت مستندات</h3></p><p>برای ساخت مستندات در قالب‌های مختلف به <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
و پوسته pytorch_sphinx_theme2 نیاز دارید.</p><p>قبل از ساخت مستندات به صورت محلی، اطمینان حاصل کنید که <code>torch</code>
در محیط شما نصب است. برای اصلاحات جزئی می‌توانید نسخه nightly را طبق <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">شروع به کار</a> نصب کنید.</p><p>برای اصلاحات پیچیده‌تر، مانند افزودن ماژول جدید و docstringهای آن، شاید نیاز باشد torch را <a href="#از-سورس" target="_blank" rel="noopener noreferrer">از سورس</a> نصب کنید.
برای راهنمایی نگارش docstring به <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Docstring Guidelines</a> مراجعه کنید.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>برای دریافت لیست فرمت‌های خروجی موجود، دستور <code>make</code> را اجرا کنید.</p><p>اگر خطای katex دریافت کردید، <code>npm install katex</code> را اجرا کنید. اگر ادامه داشت،
<code>npm install -g katex</code> را امتحان کنید.</p><blockquote>[!نکته]</blockquote>
<blockquote>اگر <code>nodejs</code> را با یک package manager دیگر (مثلاً</blockquote>
<blockquote><code>conda</code>) نصب کرده‌اید، احتمالاً <code>npm</code> نسخه‌ای از <code>katex</code> نصب می‌کند که با نسخه <code>nodejs</code> شما ناسازگار است و ساخت مستندات شکست می‌خورد.</blockquote>
<blockquote>ترکیب نسخه‌هایی که کار می‌کند: <code>node@6.13.1</code> و</blockquote>
<blockquote><code>katex@0.13.18</code>. برای نصب دومی با <code>npm</code> اجرا کنید</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!نکته]</blockquote>
<blockquote>اگر خطای ناسازگاری numpy مشاهده کردید، اجرا کنید:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>زمانی که وابستگی‌های اجراشده توسط CI را تغییر می‌دهید، فایل
</code>.ci/docker/requirements-docs.txt<code> را ویرایش کنید.</p><p>#### ساخت PDF</p><p>برای کامپایل PDF از کل مستندات PyTorch، اطمینان حاصل کنید
</code>texlive<code> و LaTeX نصب است. در macOS، می‌توانید با دستور زیر نصب کنید:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>برای ساخت PDF:</p><ul><li>اجرا کنید:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   این کار فایل‌های لازم را در مسیر </code>build/latex<code> ایجاد می‌کند.</p><ul><li>به این پوشه بروید و اجرا کنید:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   این کار یک فایل </code>pytorch.pdf` با محتوای مورد نظر ایجاد می‌کند. این دستور را یک بار دیگر اجرا کنید تا فهرست مطالب و نمایه به درستی تولید شود.</p><blockquote>[!نکته]</blockquote>
<blockquote>برای مشاهده فهرست مطالب، در نمایشگر PDF به نمای <strong>Table of Contents</strong></blockquote>
<blockquote>بروید.</blockquote></p><h3>نسخه‌های قبلی</h3></p><p>دستورالعمل نصب و باینری‌های نسخه‌های قبلی PyTorch را می‌توانید
در <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">سایت ما</a> بیابید.</p><h2>شروع به کار</h2></p><p>سه راهنما برای شروع:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">آموزش‌ها: برای شروع یادگیری و استفاده از PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">مثال‌ها: کدهای ساده و قابل فهم PyTorch در همه حوزه‌ها</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">مرجع API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">واژه‌نامه</a></li></p><p></ul><h2>منابع</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">آموزش‌های PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">مثال‌های PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">مدل‌های PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">مقدمه‌ای بر یادگیری عمیق با PyTorch از Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">مقدمه‌ای بر یادگیری ماشین با PyTorch از Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">شبکه‌های عصبی عمیق با PyTorch از Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch توییتر</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch بلاگ</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch یوتیوب</a></li></p><p></ul><h2>ارتباطات</h2>
<ul><li>انجمن‌ها: بحث درباره پیاده‌سازی‌ها، پژوهش و غیره https://discuss.pytorch.org</li>
<li>مشکلات گیت‌هاب: گزارش باگ، درخواست ویژگی، مشکلات نصب، RFC، ایده‌ها و غیره.</li>
<li>اسلک: <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">اسلک PyTorch</a> میزبان کاربران و توسعه‌دهندگان متوسط تا حرفه‌ای برای گپ عمومی، بحث آنلاین، همکاری و غیره است. اگر مبتدی هستید و کمک می‌خواهید، انجمن اصلی <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch Forums</a> است. برای دعوت به اسلک، این فرم را پر کنید: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>خبرنامه: خبرنامه‌ای یک‌طرفه و بدون اسپم برای اطلاع‌رسانی‌های مهم PyTorch. ثبت‌نام: https://eepurl.com/cbG0rv</li>
<li>صفحه فیسبوک: اطلاعیه‌های مهم درباره PyTorch. https://www.facebook.com/pytorch</li>
<li>برای راهنمای برندینگ به سایت <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a> مراجعه کنید.</li></p><p></ul><h2>انتشارها و مشارکت</h2></p><p>معمولاً PyTorch سه انتشار جزئی در سال دارد. اگر باگی مشاهده کردید لطفاً با <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">ثبت issue</a> ما را مطلع کنید.</p><p>از همه مشارکت‌ها استقبال می‌کنیم. اگر قصد دارید باگ‌فیکس ارسال کنید، لطفاً بدون نیاز به بحث بیشتر این کار را انجام دهید.</p><p>اگر قصد دارید ویژگی جدید، توابع کمکی یا گسترش برای هسته ارسال کنید، ابتدا issue باز کنید و درباره آن ویژگی با ما بحث کنید.
ارسال PR بدون بحث ممکن است به رد آن منجر شود چرا که شاید ما هسته را در جهتی متفاوت پیش می‌بریم.</p><p>برای اطلاعات بیشتر درباره مشارکت به <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">صفحه مشارکت</a> و برای اطلاعات درباره انتشارها به <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">صفحه انتشار</a> مراجعه کنید.</p><h2>تیم</h2></p><p>PyTorch پروژه‌ای مبتنی بر جامعه با مشارکت چندین مهندس و پژوهشگر ماهر است.</p><p>PyTorch هم‌اکنون توسط <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>، <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>، <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>، <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a> و <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a> نگهداری می‌شود و کمک‌های عمده از صدها نفر با استعداد در قالب‌ها و روش‌های مختلف صورت گرفته است.
لیست ناقص اما رو به رشدی از مشارکت‌کنندگان: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>، <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>، <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>، <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>، <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>، <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>، <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>، <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>، <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>، <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>، <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>، <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>، <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>، <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>، <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>، <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>، <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>، <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>، <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>نکته: این پروژه هیچ ارتباطی با <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> با همین نام ندارد. Hugh از مشارکت‌کنندگان ارزشمند جامعه Torch است و در بسیاری از امور Torch و PyTorch کمک کرده است.</p><h2>مجوز</h2></p><p>PyTorch دارای مجوز سبک BSD است که در فایل <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a> آمده است.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>