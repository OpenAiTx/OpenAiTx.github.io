<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Spanish. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Spanish. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Spanish. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Spanish</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="PyTorch Logo"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch es un paquete de Python que proporciona dos características de alto nivel:
<ul><li>Cálculo con tensores (como NumPy) con fuerte aceleración en GPU</li>
<li>Redes neuronales profundas construidas sobre un sistema de autograd basado en cinta</li></p><p></ul>Puedes reutilizar tus paquetes favoritos de Python como NumPy, SciPy y Cython para extender PyTorch cuando sea necesario.</p><p>El estado actual de nuestro trunk (señales de Integración Continua) puede encontrarse en <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#more-about-pytorch" target="_blank" rel="noopener noreferrer">Más sobre PyTorch</a></li>
  <li><a href="#a-gpu-ready-tensor-library" target="_blank" rel="noopener noreferrer">Una biblioteca de tensores lista para GPU</a></li>
  <li><a href="#dynamic-neural-networks-tape-based-autograd" target="_blank" rel="noopener noreferrer">Redes neuronales dinámicas: Autograd basado en cinta</a></li>
  <li><a href="#python-first" target="_blank" rel="noopener noreferrer">Python primero</a></li>
  <li><a href="#imperative-experiences" target="_blank" rel="noopener noreferrer">Experiencia imperativa</a></li>
  <li><a href="#fast-and-lean" target="_blank" rel="noopener noreferrer">Rápido y liviano</a></li>
  <li><a href="#extensions-without-pain" target="_blank" rel="noopener noreferrer">Extensiones sin dolor</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">Instalación</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">Binarios</a></li>
    <li><a href="#nvidia-jetson-platforms" target="_blank" rel="noopener noreferrer">Plataformas NVIDIA Jetson</a></li>
  <li><a href="#from-source" target="_blank" rel="noopener noreferrer">Desde el código fuente</a></li>
    <li><a href="#prerequisites" target="_blank" rel="noopener noreferrer">Prerrequisitos</a></li>
      <li><a href="#nvidia-cuda-support" target="_blank" rel="noopener noreferrer">Soporte NVIDIA CUDA</a></li>
      <li><a href="#amd-rocm-support" target="_blank" rel="noopener noreferrer">Soporte AMD ROCm</a></li>
      <li><a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Soporte Intel GPU</a></li>
    <li><a href="#get-the-pytorch-source" target="_blank" rel="noopener noreferrer">Obtener el código fuente de PyTorch</a></li>
    <li><a href="#install-dependencies" target="_blank" rel="noopener noreferrer">Instalar dependencias</a></li>
    <li><a href="#install-pytorch" target="_blank" rel="noopener noreferrer">Instalar PyTorch</a></li>
      <li><a href="#adjust-build-options-optional" target="_blank" rel="noopener noreferrer">Ajustar opciones de compilación (Opcional)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Imagen Docker</a></li>
    <li><a href="#using-pre-built-images" target="_blank" rel="noopener noreferrer">Uso de imágenes preconstruidas</a></li>
    <li><a href="#building-the-image-yourself" target="_blank" rel="noopener noreferrer">Construyendo la imagen por ti mismo</a></li>
  <li><a href="#building-the-documentation" target="_blank" rel="noopener noreferrer">Construcción de la documentación</a></li>
    <li><a href="#building-a-pdf" target="_blank" rel="noopener noreferrer">Creando un PDF</a></li>
  <li><a href="#previous-versions" target="_blank" rel="noopener noreferrer">Versiones anteriores</a></li>
<li><a href="#getting-started" target="_blank" rel="noopener noreferrer">Primeros pasos</a></li>
<li><a href="#resources" target="_blank" rel="noopener noreferrer">Recursos</a></li>
<li><a href="#communication" target="_blank" rel="noopener noreferrer">Comunicación</a></li>
<li><a href="#releases-and-contributing" target="_blank" rel="noopener noreferrer">Lanzamientos y contribuciones</a></li>
<li><a href="#the-team" target="_blank" rel="noopener noreferrer">El equipo</a></li>
<li><a href="#license" target="_blank" rel="noopener noreferrer">Licencia</a></li></p><p></ul><!-- tocstop --></p><h2>Más sobre PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Aprende lo básico de PyTorch</a></p><p>A nivel granular, PyTorch es una biblioteca que consta de los siguientes componentes:</p><p>| Componente | Descripción |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Una biblioteca de tensores como NumPy, con fuerte soporte para GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Una biblioteca de diferenciación automática basada en cinta que soporta todas las operaciones diferenciables de Tensor en torch |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Una pila de compilación (TorchScript) para crear modelos serializables y optimizables a partir de código PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Una biblioteca de redes neuronales profundamente integrada con autograd y diseñada para máxima flexibilidad |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Multiprocesamiento en Python, pero con compartición mágica de memoria de tensores torch entre procesos. Útil para carga de datos y entrenamiento Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader y otras funciones utilitarias para conveniencia |</p><p>Usualmente, PyTorch se utiliza de dos formas:</p><ul><li>Como reemplazo de NumPy para aprovechar el poder de las GPUs.</li>
<li>Como una plataforma de investigación en aprendizaje profundo que proporciona máxima flexibilidad y velocidad.</li></p><p></ul>Expandiendo más:</p><h3>Una biblioteca de tensores lista para GPU</h3></p><p>Si usas NumPy, entonces ya has utilizado tensores (también conocidos como ndarray).</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Ilustración de tensor"></p><p>PyTorch proporciona tensores que pueden residir tanto en la CPU como en la GPU y acelera el
cálculo significativamente.</p><p>Ofrecemos una amplia variedad de rutinas de tensores para acelerar y cubrir tus necesidades de cálculo científico,
como slicing, indexado, operaciones matemáticas, álgebra lineal, reducciones.
¡Y son rápidas!</p><h3>Redes neuronales dinámicas: Autograd basado en cinta</h3></p><p>PyTorch tiene una forma única de construir redes neuronales: utilizando y reproduciendo una grabadora de cinta.</p><p>La mayoría de los frameworks como TensorFlow, Theano, Caffe y CNTK tienen una visión estática del mundo.
Se debe construir una red neuronal y reutilizar la misma estructura una y otra vez.
Cambiar el comportamiento de la red implica empezar desde cero.</p><p>Con PyTorch, usamos una técnica llamada auto-diferenciación en modo reverso, que te permite
cambiar el comportamiento de tu red de forma arbitraria sin retraso ni sobrecarga. Nuestra inspiración proviene
de varios artículos de investigación sobre este tema, así como de trabajos actuales y pasados como
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a>, etc.</p><p>Aunque esta técnica no es exclusiva de PyTorch, es una de las implementaciones más rápidas hasta la fecha.
Obtienes lo mejor en velocidad y flexibilidad para tu investigación.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Grafo dinámico"></p><h3>Python primero</h3></p><p>PyTorch no es un binding de Python a un framework monolítico en C++.
Está construido para integrarse profundamente en Python.
Puedes usarlo de forma natural como usarías <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> etc.
Puedes escribir tus nuevas capas de redes neuronales en Python mismo, usando tus bibliotecas favoritas
y paquetes como <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> y <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Nuestro objetivo es no reinventar la rueda cuando sea apropiado.</p><h3>Experiencia imperativa</h3></p><p>PyTorch está diseñado para ser intuitivo, lineal en el pensamiento y fácil de usar.
Cuando ejecutas una línea de código, se ejecuta. No hay una visión asincrónica del mundo.
Cuando entras en un depurador o recibes mensajes de error y trazas de pila, entenderlos es sencillo.
La traza de pila apunta exactamente a dónde fue definido tu código.
Esperamos que nunca pases horas depurando tu código debido a malas trazas de pila o motores de ejecución asíncronos y opacos.</p><h3>Rápido y liviano</h3></p><p>PyTorch tiene una sobrecarga de framework mínima. Integramos bibliotecas de aceleración
como <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> y NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) para maximizar la velocidad.
En su núcleo, sus backends de Tensor y redes neuronales para CPU y GPU
son maduros y han sido probados durante años.</p><p>Por lo tanto, PyTorch es bastante rápido, ya sea que ejecutes redes neuronales pequeñas o grandes.</p><p>El uso de memoria en PyTorch es extremadamente eficiente en comparación con Torch o algunas alternativas.
Hemos escrito asignadores de memoria personalizados para la GPU para asegurarnos de que
tus modelos de deep learning sean lo más eficientes posible en el uso de memoria.
Esto te permite entrenar modelos de deep learning más grandes que antes.</p><h3>Extensiones sin dolor</h3></p><p>La escritura de nuevos módulos de redes neuronales, o la integración con la API de tensores de PyTorch está diseñada para ser sencilla
y con mínimas abstracciones.</p><p>Puedes escribir nuevas capas de redes neuronales en Python usando la API de torch
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">o tus bibliotecas favoritas basadas en NumPy como SciPy</a>.</p><p>Si quieres escribir tus capas en C/C++, proporcionamos una API de extensión conveniente, eficiente y con poco código repetitivo.
No necesitas escribir código wrapper. Puedes ver <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">un tutorial aquí</a> y <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">un ejemplo aquí</a>.</p><h2>Instalación</h2></p><h3>Binarios</h3>
Los comandos para instalar binarios vía Conda o pip wheels están en nuestro sitio web: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>
#### Plataformas NVIDIA Jetson</p><p>Los wheels de Python para Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX y Jetson AGX Orin de NVIDIA están disponibles <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">aquí</a> y el contenedor L4T está publicado <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">aquí</a></p><p>Requieren JetPack 4.2 o superior, y <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> y <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> los mantienen.</p><h3>Desde el código fuente</h3></p><p>#### Prerrequisitos
Si vas a instalar desde el código fuente, necesitarás:
<ul><li>Python 3.9 o superior</li>
<li>Un compilador que soporte completamente C++17, como clang o gcc (gcc 9.4.0 o superior es requerido en Linux)</li>
<li>Visual Studio o Visual Studio Build Tool (solo en Windows)</li></p><p></ul>\* PyTorch CI usa Visual C++ BuildTools, que vienen con Visual Studio Enterprise,
Professional o Community Editions. También puedes instalar las build tools desde
https://visualstudio.microsoft.com/visual-cpp-build-tools/. Las build tools <em>no</em>
vienen con Visual Studio Code por defecto.</p><p>Un ejemplo de configuración de entorno se muestra a continuación:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### Soporte NVIDIA CUDA
Si deseas compilar con soporte para CUDA, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">selecciona una versión soportada de CUDA de nuestra matriz de soporte</a>, luego instala lo siguiente:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 o superior</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compilador</a> compatible con CUDA</li></p><p></ul>Nota: Puedes consultar la <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">matriz de soporte de cuDNN</a> para versiones de cuDNN con las diferentes versiones de CUDA, controladores de CUDA y hardware NVIDIA soportados.</p><p>Si deseas deshabilitar el soporte para CUDA, exporta la variable de entorno <code>USE_CUDA=0</code>.
Otras variables de entorno potencialmente útiles pueden encontrarse en <code>setup.py</code>.</p><p>Si estás construyendo para plataformas Jetson de NVIDIA (Jetson Nano, TX1, TX2, AGX Xavier), las instrucciones para instalar PyTorch para Jetson Nano están <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">disponibles aquí</a></p><p>##### Soporte AMD ROCm
Si deseas compilar con soporte para ROCm, instala
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> versión 4.0 o superior</li>
<li>ROCm actualmente solo se soporta en sistemas Linux.</li></p><p></ul>Por defecto el sistema de compilación espera que ROCm esté instalado en <code>/opt/rocm</code>. Si ROCm está instalado en un directorio diferente, la variable de entorno <code>ROCM_PATH</code> debe establecerse en el directorio de instalación de ROCm. El sistema de compilación detecta automáticamente la arquitectura de la GPU AMD. Opcionalmente, la arquitectura de la GPU AMD puede establecerse explícitamente con la variable de entorno <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">Arquitectura de GPU AMD</a></p><p>Si deseas deshabilitar el soporte para ROCm, exporta la variable de entorno <code>USE_ROCM=0</code>.
Otras variables de entorno potencialmente útiles pueden encontrarse en <code>setup.py</code>.</p><p>##### Soporte Intel GPU
Si deseas compilar con soporte para Intel GPU, sigue estas
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Instrucciones de prerrequisitos de PyTorch para Intel GPUs</a>.</li>
<li>Intel GPU es soportado en Linux y Windows.</li></p><p></ul>Si deseas deshabilitar el soporte para Intel GPU, exporta la variable de entorno <code>USE_XPU=0</code>.
Otras variables de entorno potencialmente útiles pueden encontrarse en <code>setup.py</code>.</p><p>#### Obtener el código fuente de PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>si estás actualizando un checkout existente</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Instalar dependencias</p><p><strong>Común</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Ejecuta este comando desde el directorio de PyTorch después de clonar el código fuente usando la sección “Obtener el código fuente de PyTorch“</h1>
pip install -r requirements.txt</code></pre></p><p><strong>En Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Solo CUDA: agrega soporte LAPACK para GPU si es necesario</h1>
<h1>instalación de magma: ejecutar con el entorno conda activo. especifica la versión de CUDA a instalar</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(opcional) Si usas torch.compile con inductor/triton, instala la versión correspondiente de triton</h1>
<h1>Ejecuta desde el directorio pytorch después de clonar</h1>
<h1>Para soporte Intel GPU, por favor exporta explícitamente <code>export USE_XPU=1</code> antes de ejecutar el comando.</h1>
make triton</code></pre></p><p><strong>En MacOS</strong></p><pre><code class="language-bash"># Agrega este paquete solo en máquinas con procesadores intel x86
pip install mkl-static mkl-include
<h1>Agrega estos paquetes si necesitas torch.distributed</h1>
conda install pkg-config libuv</code></pre></p><p><strong>En Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Agrega estos paquetes si necesitas torch.distributed.</h1>
<h1>El soporte de paquetes distribuidos en Windows es una característica prototipo y está sujeta a cambios.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Instalar PyTorch
<strong>En Linux</strong></p><p>Si estás compilando para AMD ROCm, primero ejecuta este comando:
<pre><code class="language-bash"># Solo ejecuta esto si estás compilando para ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Instala PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>En macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>En Windows</strong></p><p>Si deseas compilar código Python legado, por favor consulta <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Construcción sobre código legado y CUDA</a></p><p><strong>Compilaciones solo CPU</strong></p><p>En este modo, los cálculos de PyTorch se ejecutarán en tu CPU, no en tu GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Nota sobre OpenMP: La implementación deseada de OpenMP es Intel OpenMP (iomp). Para enlazar con iomp, tendrás que descargar manualmente la biblioteca y configurar el entorno de compilación ajustando <code>CMAKE_INCLUDE_PATH</code> y <code>LIB</code>. La instrucción <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">aquí</a> es un ejemplo para configurar tanto MKL como Intel OpenMP. Sin estas configuraciones para CMake, se usará el runtime de OpenMP de Microsoft Visual C (vcomp).</p><p><strong>Compilación basada en CUDA</strong></p><p>En este modo, los cálculos de PyTorch aprovecharán tu GPU mediante CUDA para un procesamiento más rápido.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> es necesario para construir PyTorch con CUDA.
NVTX es parte de la distribución de CUDA, donde se llama "Nsight Compute". Para instalarlo sobre una instalación de CUDA ya existente, ejecuta nuevamente el instalador de CUDA y marca la casilla correspondiente.
Asegúrate de que CUDA con Nsight Compute esté instalado después de Visual Studio.</p><p>Actualmente, VS 2017 / 2019 y Ninja son compatibles como generadores de CMake. Si se detecta <code>ninja.exe</code> en el <code>PATH</code>, entonces Ninja se usará como generador predeterminado, de lo contrario, se usará VS 2017 / 2019.
<br/> Si se selecciona Ninja como generador, el MSVC más reciente se seleccionará como la herramienta subyacente.</p><p>Se necesitan librerías adicionales como
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, también conocido como MKLDNN o DNNL</a>, y <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a>. Por favor, consulta el <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> para instalarlas.</p><p>Puedes consultar el script <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> para otras configuraciones de variables de entorno.</p><pre><code class="language-cmd">cmd</p><p>:: Establece las variables de entorno después de haber descargado y descomprimido el paquete mkl,
:: de lo contrario, CMake lanzará un error como <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Tu directorio}\mkl\include
set LIB={Tu directorio}\mkl\lib;%LIB%</p><p>:: Lee cuidadosamente el contenido de la sección anterior antes de continuar.
:: [Opcional] Si deseas sobrescribir el toolset subyacente usado por Ninja y Visual Studio con CUDA, por favor ejecuta el siguiente bloque de script.
:: El "Visual Studio 2019 Developer Command Prompt" se ejecutará automáticamente.
:: Asegúrate de tener CMake >= 3.12 antes de hacer esto si usas el generador de Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Opcional] Si deseas sobrescribir el compilador host de CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Compilaciones Intel GPU</strong></p><p>En este modo, se construirá PyTorch con soporte para Intel GPU.</p><p>Por favor, asegúrate de que <a href="#prerequisites" target="_blank" rel="noopener noreferrer">los prerrequisitos comunes</a> así como <a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">los prerrequisitos para Intel GPU</a> estén correctamente instalados y que las variables de entorno estén configuradas antes de comenzar la construcción. Para el soporte de herramientas de compilación, se requiere <code>Visual Studio 2022</code>.</p><p>Luego PyTorch puede ser construido con el comando:</p><pre><code class="language-cmd">:: Comandos CMD:
:: Establece CMAKE_PREFIX_PATH para ayudar a encontrar los paquetes correspondientes
:: %CONDA_PREFIX% solo funciona después de <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Ajustar opciones de compilación (Opcional)</p><p>Puedes ajustar la configuración de variables cmake opcionalmente (sin compilar primero), haciendo lo siguiente. Por ejemplo, ajustar los directorios pre-detectados para CuDNN o BLAS puede hacerse con este paso.</p><p>En Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # o cmake-gui build</code></pre></p><p>En macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # o cmake-gui build</code></pre></p><h3>Imagen Docker</h3></p><p>#### Uso de imágenes preconstruidas</p><p>También puedes descargar una imagen docker preconstruida desde Docker Hub y ejecutar con docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Por favor, ten en cuenta que PyTorch utiliza memoria compartida para compartir datos entre procesos, por lo que si se usa multiprocesamiento torch (por ejemplo,
para data loaders multihilo), el segmento de memoria compartida predeterminado con el que se ejecuta el contenedor no es suficiente, y deberías aumentar el tamaño de la memoria compartida ya sea con las opciones de línea de comandos <code>--ipc=host</code> o <code>--shm-size</code> para <code>nvidia-docker run</code>.</p><p>#### Construyendo la imagen por ti mismo</p><p><strong>NOTA:</strong> Debe construirse con una versión de docker > 18.06</p><p>El <code>Dockerfile</code> se suministra para construir imágenes con soporte CUDA 11.1 y cuDNN v8.
Puedes pasar la variable make <code>PYTHON_VERSION=x.y</code> para especificar qué versión de Python será utilizada por Miniconda, o dejarla
sin establecer para usar la predeterminada.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>las imágenes se etiquetan como docker.io/${tu_usuario_docker}/pytorch</code></pre></h1></p><p>También puedes pasar la variable de entorno <code>CMAKE_VARS="..."</code> para especificar variables adicionales de CMake a ser pasadas durante la compilación.
Consulta <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> para la lista de variables disponibles.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Construcción de la documentación</h3></p><p>Para construir la documentación en varios formatos, necesitarás <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
y el tema pytorch_sphinx_theme2.</p><p>Antes de construir la documentación localmente, asegúrate de que <code>torch</code> esté
instalado en tu entorno. Para pequeños arreglos, puedes instalar la
versión nightly como se describe en <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Primeros Pasos</a>.</p><p>Para arreglos más complejos, como agregar un nuevo módulo y docstrings para
el nuevo módulo, puede que necesites instalar torch <a href="#from-source" target="_blank" rel="noopener noreferrer">desde el código fuente</a>.
Consulta <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Guía de Docstrings</a>
para las convenciones de docstring.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Ejecuta <code>make</code> para obtener una lista de todos los formatos de salida disponibles.</p><p>Si obtienes un error de katex ejecuta <code>npm install katex</code>. Si persiste, intenta
<code>npm install -g katex</code></p><blockquote>[!NOTA]</blockquote>
<blockquote>Si instalaste <code>nodejs</code> con un gestor de paquetes diferente (por ejemplo,</blockquote>
<blockquote><code>conda</code>), probablemente <code>npm</code> instalará una versión de <code>katex</code> que no es</blockquote>
<blockquote>compatible con tu versión de <code>nodejs</code> y la compilación de la documentación fallará.</blockquote>
<blockquote>Una combinación de versiones que se sabe funciona es <code>node@6.13.1</code> y</blockquote>
<blockquote><code>katex@0.13.18</code>. Para instalar esta última con <code>npm</code> puedes ejecutar</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!NOTA]</blockquote>
<blockquote>Si ves un error de incompatibilidad de numpy, ejecuta:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Cuando realices cambios en las dependencias ejecutadas por CI, edita el
archivo </code>.ci/docker/requirements-docs.txt<code>.</p><p>#### Creando un PDF</p><p>Para compilar un PDF de toda la documentación de PyTorch, asegúrate de tener
</code>texlive<code> y LaTeX instalados. En macOS, puedes instalarlos usando:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Para crear el PDF:</p><ul><li>Ejecuta:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Esto generará los archivos necesarios en el directorio </code>build/latex<code>.</p><ul><li>Navega a este directorio y ejecuta:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Esto producirá un </code>pytorch.pdf` con el contenido deseado. Ejecuta este
   comando una vez más para que genere la tabla de contenidos
   y el índice correctamente.</p><blockquote>[!NOTA]</blockquote>
<blockquote>Para ver la Tabla de Contenidos, cambia a la vista <strong>Tabla de Contenidos</strong></blockquote>
<blockquote>en tu visor de PDF.</blockquote></p><h3>Versiones anteriores</h3></p><p>Las instrucciones de instalación y binarios para versiones anteriores de PyTorch pueden encontrarse
en <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">nuestro sitio web</a>.</p><h2>Primeros pasos</h2></p><p>Tres recursos para comenzar:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriales: te ayudan a entender y usar PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Ejemplos: código PyTorch fácil de entender en todos los dominios</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">La Referencia de la API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Glosario</a></li></p><p></ul><h2>Recursos</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriales de PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Ejemplos de PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">Modelos de PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Introducción al Deep Learning con PyTorch de Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Introducción al Machine Learning con PyTorch de Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Redes neuronales profundas con PyTorch de Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">Blog de PyTorch</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Comunicación</h2>
<ul><li>Foros: Discute implementaciones, investigación, etc. https://discuss.pytorch.org</li>
<li>GitHub Issues: Reportes de errores, solicitudes de características, problemas de instalación, RFCs, ideas, etc.</li>
<li>Slack: El <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">Slack de PyTorch</a> alberga principalmente usuarios y desarrolladores de PyTorch con experiencia para charlas generales, discusiones en línea, colaboración, etc. Si eres principiante y buscas ayuda, el medio principal es <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">PyTorch Forums</a>. Si necesitas una invitación a Slack, por favor completa este formulario: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Sin ruido, un boletín unidireccional con anuncios importantes sobre PyTorch. Puedes suscribirte aquí: https://eepurl.com/cbG0rv</li>
<li>Página de Facebook: Anuncios importantes sobre PyTorch. https://www.facebook.com/pytorch</li>
<li>Para directrices de marca, por favor visita nuestro sitio web en <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Lanzamientos y contribuciones</h2></p><p>Normalmente, PyTorch tiene tres lanzamientos menores al año. Por favor, háznos saber si encuentras un error <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">abriendo un issue</a>.</p><p>Apreciamos todas las contribuciones. Si planeas contribuir con correcciones de errores, hazlo sin más discusión.</p><p>Si planeas contribuir con nuevas características, funciones utilitarias o extensiones al núcleo, primero abre un issue y discútelo con nosotros.
Enviar un PR sin discusión puede resultar en un rechazo porque podríamos estar llevando el núcleo en una dirección diferente de la que conoces.</p><p>Para aprender más sobre cómo contribuir a PyTorch, por favor consulta nuestra <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">página de contribuciones</a>. Para más información sobre lanzamientos de PyTorch, consulta la <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">página de lanzamientos</a>.</p><h2>El equipo</h2></p><p>PyTorch es un proyecto impulsado por la comunidad con varios ingenieros e investigadores talentosos que contribuyen.</p><p>PyTorch es mantenido actualmente por <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, y <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a> con importantes contribuciones de cientos de individuos talentosos en diversas formas y medios.
Una lista no exhaustiva pero en crecimiento incluye: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Nota: Este proyecto no está relacionado con <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> que tiene el mismo nombre. Hugh es un valioso colaborador de la comunidad Torch y ha ayudado con muchos aspectos de Torch y PyTorch.</p><h2>Licencia</h2></p><p>PyTorch tiene una licencia de estilo BSD, como se encuentra en el archivo <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>