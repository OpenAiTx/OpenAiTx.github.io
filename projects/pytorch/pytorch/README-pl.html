<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Polish. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Polish. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Polish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Polish. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-pl.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Polish</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="Logo PyTorch"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch to pakiet Pythona, który oferuje dwie główne funkcje:
<ul><li>Obliczenia tensorowe (podobnie jak NumPy) z silnym wsparciem GPU</li>
<li>Głębokie sieci neuronowe zbudowane na systemie autograd opartym na taśmie</li></p><p></ul>Możesz ponownie używać swoich ulubionych pakietów Pythona, takich jak NumPy, SciPy i Cython, aby rozszerzać PyTorch w razie potrzeby.</p><p>Stan głównej gałęzi (sygnały Continuous Integration) można znaleźć na <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#more-about-pytorch" target="_blank" rel="noopener noreferrer">Więcej o PyTorch</a></li>
  <li><a href="#a-gpu-ready-tensor-library" target="_blank" rel="noopener noreferrer">Biblioteka tensorów gotowa na GPU</a></li>
  <li><a href="#dynamic-neural-networks-tape-based-autograd" target="_blank" rel="noopener noreferrer">Dynamiczne sieci neuronowe: Tape-Based Autograd</a></li>
  <li><a href="#python-first" target="_blank" rel="noopener noreferrer">Python przede wszystkim</a></li>
  <li><a href="#imperative-experiences" target="_blank" rel="noopener noreferrer">Doświadczenia imperatywne</a></li>
  <li><a href="#fast-and-lean" target="_blank" rel="noopener noreferrer">Szybki i lekki</a></li>
  <li><a href="#extensions-without-pain" target="_blank" rel="noopener noreferrer">Rozszerzenia bez bólu</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">Instalacja</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">Binaria</a></li>
    <li><a href="#nvidia-jetson-platforms" target="_blank" rel="noopener noreferrer">Platformy NVIDIA Jetson</a></li>
  <li><a href="#from-source" target="_blank" rel="noopener noreferrer">Ze źródeł</a></li>
    <li><a href="#prerequisites" target="_blank" rel="noopener noreferrer">Wymagania wstępne</a></li>
      <li><a href="#nvidia-cuda-support" target="_blank" rel="noopener noreferrer">Wsparcie NVIDIA CUDA</a></li>
      <li><a href="#amd-rocm-support" target="_blank" rel="noopener noreferrer">Wsparcie AMD ROCm</a></li>
      <li><a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Wsparcie GPU Intel</a></li>
    <li><a href="#get-the-pytorch-source" target="_blank" rel="noopener noreferrer">Pobierz źródła PyTorch</a></li>
    <li><a href="#install-dependencies" target="_blank" rel="noopener noreferrer">Instalacja zależności</a></li>
    <li><a href="#install-pytorch" target="_blank" rel="noopener noreferrer">Instalacja PyTorch</a></li>
      <li><a href="#adjust-build-options-optional" target="_blank" rel="noopener noreferrer">Dostosowywanie opcji budowania (opcjonalne)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Obraz Dockera</a></li>
    <li><a href="#using-pre-built-images" target="_blank" rel="noopener noreferrer">Korzystanie z gotowych obrazów</a></li>
    <li><a href="#building-the-image-yourself" target="_blank" rel="noopener noreferrer">Budowanie obrazu samodzielnie</a></li>
  <li><a href="#building-the-documentation" target="_blank" rel="noopener noreferrer">Budowanie dokumentacji</a></li>
    <li><a href="#building-a-pdf" target="_blank" rel="noopener noreferrer">Budowanie PDF</a></li>
  <li><a href="#previous-versions" target="_blank" rel="noopener noreferrer">Poprzednie wersje</a></li>
<li><a href="#getting-started" target="_blank" rel="noopener noreferrer">Pierwsze kroki</a></li>
<li><a href="#resources" target="_blank" rel="noopener noreferrer">Zasoby</a></li>
<li><a href="#communication" target="_blank" rel="noopener noreferrer">Komunikacja</a></li>
<li><a href="#releases-and-contributing" target="_blank" rel="noopener noreferrer">Wydania i kontrybucje</a></li>
<li><a href="#the-team" target="_blank" rel="noopener noreferrer">Zespół</a></li>
<li><a href="#license" target="_blank" rel="noopener noreferrer">Licencja</a></li></p><p></ul><!-- tocstop --></p><h2>Więcej o PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Poznaj podstawy PyTorch</a></p><p>Na szczegółowym poziomie, PyTorch to biblioteka składająca się z następujących komponentów:</p><p>| Komponent | Opis |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Biblioteka Tensorów jak NumPy, z silnym wsparciem GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Biblioteka automatycznego różniczkowania oparta na taśmie, obsługująca wszystkie różniczkowalne operacje Tensorów w torch |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Stos kompilacyjny (TorchScript) do tworzenia serializowalnych i optymalizowalnych modeli z kodu PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Biblioteka sieci neuronowych głęboko zintegrowana z autograd, zaprojektowana dla maksymalnej elastyczności |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Multiprocessing w Pythonie, ale z magicznym współdzieleniem pamięci Tensorów torch między procesami. Przydatne do ładowania danych i treningu Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader oraz inne funkcje pomocnicze dla wygody |</p><p>PyTorch jest zwykle używany jako:</p><ul><li>Zamiennik NumPy do korzystania z mocy GPU.</li>
<li>Platforma badawcza deep learning zapewniająca maksymalną elastyczność i szybkość.</li></p><p></ul>Szczegóły:</p><h3>Biblioteka tensorów gotowa na GPU</h3></p><p>Jeśli używasz NumPy, to już korzystałeś z Tensorów (czyli ndarray).</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Ilustracja tensora"></p><p>PyTorch dostarcza Tensory, które mogą działać zarówno na CPU, jak i GPU, znacznie przyspieszając obliczenia.</p><p>Oferujemy szeroki wachlarz operacji na tensorach, które przyspieszają i dostosowują się do Twoich potrzeb obliczeń naukowych, takich jak cięcie (slicing), indeksowanie, operacje matematyczne, algebra liniowa, redukcje.
I są szybkie!</p><h3>Dynamiczne sieci neuronowe: Tape-Based Autograd</h3></p><p>PyTorch ma unikalny sposób budowy sieci neuronowych: wykorzystując i odtwarzając rejestrator taśmy.</p><p>Większość frameworków, takich jak TensorFlow, Theano, Caffe i CNTK, ma statyczny obraz świata.
Trzeba zbudować sieć neuronową i używać tej samej struktury wielokrotnie.
Zmiana zachowania sieci oznacza konieczność rozpoczęcia od nowa.</p><p>W PyTorch używamy techniki zwanej automatycznym różniczkowaniem w trybie odwrotnym (reverse-mode auto-differentiation), która pozwala dowolnie zmieniać zachowanie sieci bez opóźnień i narzutu. Inspirację czerpaliśmy z kilku prac naukowych na ten temat, jak również z obecnych i przeszłych rozwiązań, takich jak <a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>, <a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>, <a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a> itd.</p><p>Choć ta technika nie jest unikalna dla PyTorch, jest to jedna z najszybszych jej implementacji.
Otrzymujesz najlepsze połączenie szybkości i elastyczności dla swoich najbardziej wymagających badań.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Dynamiczny graf"></p><h3>Python przede wszystkim</h3></p><p>PyTorch nie jest tylko powiązaniem Pythona z monolitycznym frameworkiem C++.
Jest zbudowany tak, by być głęboko zintegrowany z Pythonem.
Możesz używać go naturalnie, jak <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> itd.
Możesz pisać nowe warstwy sieci neuronowych w samym Pythonie, używając swoich ulubionych bibliotek
i korzystać z pakietów takich jak <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> i <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Naszym celem jest nie wymyślać koła na nowo tam, gdzie to niepotrzebne.</p><h3>Doświadczenia imperatywne</h3></p><p>PyTorch został zaprojektowany, by być intuicyjnym, logicznym i łatwym w użyciu.
Gdy wykonujesz linię kodu, ona się wykonuje. Nie ma asynchronicznego obrazu świata.
Gdy korzystasz z debuggera lub otrzymujesz komunikaty błędów i stack trace, ich zrozumienie jest proste.
Stack trace wskazuje dokładnie, gdzie Twój kod został zdefiniowany.
Mamy nadzieję, że nigdy nie spędzisz godzin na debugowaniu kodu przez złe stack trace lub asynchroniczne i nieprzejrzyste silniki wykonawcze.</p><h3>Szybki i lekki</h3></p><p>PyTorch ma minimalny narzut frameworka. Integrujemy biblioteki przyspieszające
takie jak <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> oraz NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>), aby zmaksymalizować szybkość.
Rdzeniem są dojrzałe i przetestowane przez lata backendy tensorów CPU i GPU oraz sieci neuronowych.</p><p>Dlatego PyTorch jest bardzo szybki — niezależnie czy uruchamiasz małe, czy duże sieci neuronowe.</p><p>Zużycie pamięci w PyTorch jest niezwykle wydajne w porównaniu do Torch lub niektórych alternatyw.
Napisaliśmy własne alokatory pamięci dla GPU, aby zapewnić maksymalną efektywność pamięciową Twoich modeli deep learning.
Pozwala to trenować większe modele niż dotychczas.</p><h3>Rozszerzenia bez bólu</h3></p><p>Pisanie nowych modułów sieci neuronowych lub interfejsowanie z API Tensorów PyTorch zostało zaprojektowane tak, by było proste i z minimalnym poziomem abstrakcji.</p><p>Możesz pisać nowe warstwy sieci neuronowych w Pythonie, korzystając z API torch <a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">lub swoich ulubionych bibliotek opartych na NumPy, takich jak SciPy</a>.</p><p>Jeśli chcesz napisać swoje warstwy w C/C++, zapewniamy wygodne i wydajne API rozszerzeń z minimalną ilością kodu szablonowego.
Nie trzeba pisać kodu opakowującego. Przykład znajdziesz <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">w tym tutorialu</a> i <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">tutaj</a>.</p><h2>Instalacja</h2></p><h3>Binaria</h3>
Polecenia do instalacji binariów przez Conda lub pip wheels znajdują się na naszej stronie: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>#### Platformy NVIDIA Jetson</p><p>Koła Pythona dla NVIDIA Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX oraz Jetson AGX Orin są dostępne <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">tutaj</a>, a kontener L4T jest publikowany <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">tutaj</a></p><p>Wymagają JetPack 4.2 lub nowszego, a <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> oraz <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> je utrzymują.</p><h3>Ze źródeł</h3></p><p>#### Wymagania wstępne
Jeśli instalujesz ze źródeł, będziesz potrzebować:
<ul><li>Python 3.9 lub nowszy</li>
<li>Kompilator z pełnym wsparciem C++17, taki jak clang lub gcc (wymagany gcc 9.4.0 lub nowszy na Linuksie)</li>
<li>Visual Studio lub Visual Studio Build Tool (tylko Windows)</li></p><p></ul>\<em> PyTorch CI używa Visual C++ BuildTools, które są dostępne z Visual Studio Enterprise, Professional lub Community. Możesz także zainstalować build tools ze strony https://visualstudio.microsoft.com/visual-cpp-build-tools/. Build tools </em>nie są* domyślnie dostępne z Visual Studio Code.</p><p>Przykład konfiguracji środowiska poniżej:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### Wsparcie NVIDIA CUDA
Jeśli chcesz kompilować ze wsparciem CUDA, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">wybierz wspieraną wersję CUDA z naszej tabeli wsparcia</a>, a następnie zainstaluj:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 lub nowszy</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Kompilator</a> kompatybilny z CUDA</li></p><p></ul>Uwaga: Możesz sprawdzić <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">tabelę wsparcia cuDNN</a> dla wersji cuDNN kompatybilnych z różnymi wersjami CUDA, sterowników CUDA i sprzętu NVIDIA.</p><p>Jeśli chcesz wyłączyć wsparcie CUDA, ustaw zmienną środowiskową <code>USE_CUDA=0</code>.
Inne przydatne zmienne środowiskowe znajdują się w <code>setup.py</code>.</p><p>Jeśli budujesz dla platform NVIDIA Jetson (Jetson Nano, TX1, TX2, AGX Xavier), instrukcje instalacji PyTorch dla Jetson Nano są <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">tutaj</a></p><p>##### Wsparcie AMD ROCm
Jeśli chcesz kompilować ze wsparciem ROCm, zainstaluj
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 lub nowszy</li>
<li>ROCm jest obecnie wspierany tylko na systemach Linux.</li></p><p></ul>Domyślnie system budowania oczekuje instalacji ROCm w <code>/opt/rocm</code>. Jeśli ROCm jest zainstalowany w innym katalogu, ustaw zmienną środowiskową <code>ROCM_PATH</code> na katalog instalacji ROCm. System budowania automatycznie wykrywa architekturę GPU AMD. Opcjonalnie architekturę GPU AMD można ustawić jawnie przez zmienną <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">architektura GPU AMD</a></p><p>Aby wyłączyć wsparcie ROCm, ustaw zmienną środowiskową <code>USE_ROCM=0</code>.
Inne przydatne zmienne środowiskowe znajdują się w <code>setup.py</code>.</p><p>##### Wsparcie GPU Intel
Aby kompilować ze wsparciem GPU Intel, postępuj według:
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Instrukcji wstępnych dla GPU Intel</a>.</li>
<li>GPU Intel jest wspierany na systemach Linux i Windows.</li></p><p></ul>Aby wyłączyć wsparcie GPU Intel, ustaw zmienną środowiskową <code>USE_XPU=0</code>.
Inne przydatne zmienne środowiskowe znajdują się w <code>setup.py</code>.</p><p>#### Pobierz źródła PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>jeśli aktualizujesz już istniejące repozytorium</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Instalacja zależności</p><p><strong>Wspólne</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Wykonaj to polecenie w katalogu PyTorch po sklonowaniu źródeł sekcją „Pobierz źródła PyTorch“</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Na Linuksie</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Tylko CUDA: dodaj wsparcie LAPACK dla GPU jeśli potrzebne</h1>
<h1>instalacja magma: uruchom z aktywnym środowiskiem conda, określ wersję CUDA do instalacji</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(opcjonalnie) Jeśli używasz torch.compile z inductor/triton, zainstaluj pasującą wersję triton</h1>
<h1>Uruchom z katalogu pytorch po sklonowaniu</h1>
<h1>Dla wsparcia GPU Intel, jawnie <code>export USE_XPU=1</code> przed uruchomieniem polecenia.</h1>
make triton</code></pre></p><p><strong>Na MacOS</strong></p><pre><code class="language-bash"># Dodaj ten pakiet tylko na maszynach z procesorem intel x86
pip install mkl-static mkl-include
<h1>Dodaj te pakiety jeśli potrzebny jest torch.distributed</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Na Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Dodaj te pakiety jeśli potrzebny jest torch.distributed.</h1>
<h1>Wsparcie dla distributed na Windows to funkcja prototypowa i może ulec zmianie.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Instalacja PyTorch
<strong>Na Linuksie</strong></p><p>Jeśli kompilujesz dla AMD ROCm, najpierw uruchom to polecenie:
<pre><code class="language-bash"># Uruchom tylko jeśli kompilujesz dla ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Instalacja PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>Na macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Na Windows</strong></p><p>Jeśli chcesz budować legacy python code, zobacz <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Building on legacy code and CUDA</a></p><p><strong>Budowa tylko CPU</strong></p><p>W tym trybie obliczenia PyTorch będą wykonywane na CPU, a nie na GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Uwaga dotycząca OpenMP: Preferowana implementacja OpenMP to Intel OpenMP (iomp). Aby się do niej podłączyć, musisz ręcznie pobrać bibliotekę i dostosować środowisko budowania przez ustawienie <code>CMAKE_INCLUDE_PATH</code> i <code>LIB</code>. Instrukcja <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">tutaj</a> jest przykładem ustawienia zarówno MKL, jak i Intel OpenMP. Bez tej konfiguracji CMake użyje domyślnego środowiska uruchomieniowego OpenMP Microsoft Visual C (vcomp).</p><p><strong>Budowa z CUDA</strong></p><p>W tym trybie obliczenia PyTorch będą wykorzystywać GPU przez CUDA dla szybszego przetwarzania liczb.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> jest wymagane do budowy PyTorch z CUDA.
NVTX jest częścią dystrybucji CUDA, gdzie nazywa się "Nsight Compute". Aby zainstalować go do już zainstalowanej CUDA, uruchom instalator CUDA ponownie i zaznacz odpowiednie pole.
Upewnij się, że CUDA z Nsight Compute została zainstalowana po Visual Studio.</p><p>Aktualnie VS 2017 / 2019 oraz Ninja są wspierane jako generator CMake. Jeśli <code>ninja.exe</code> jest wykryty w <code>PATH</code>, Ninja zostanie użyty jako domyślny generator, w przeciwnym razie będzie to VS 2017 / 2019.
<br/> Jeśli Ninja zostanie wybrany jako generator, najnowszy MSVC zostanie wybrany jako narzędzie kompilujące.</p><p>Często wymagane są dodatkowe biblioteki, takie jak
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, czyli MKLDNN lub DNNL</a>, oraz <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a>. Zobacz <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> jak je zainstalować.</p><p>Możesz zobaczyć skrypt <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> dla innych konfiguracji zmiennych środowiskowych</p><pre><code class="language-cmd">cmd</p><p>:: Ustaw zmienne środowiskowe po pobraniu i rozpakowaniu pakietu mkl,
:: w przeciwnym razie CMake zgłosi błąd <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Twój katalog}\mkl\include
set LIB={Twój katalog}\mkl\lib;%LIB%</p><p>:: Przeczytaj uważnie poprzednią sekcję przed kontynuacją.
:: [Opcjonalnie] Jeśli chcesz nadpisać narzędzia Ninja i Visual Studio dla CUDA, uruchom poniższy blok skryptu.
:: "Visual Studio 2019 Developer Command Prompt" zostanie uruchomiony automatycznie.
:: Upewnij się, że masz CMake >= 3.12 przy użyciu generatora Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Opcjonalnie] Jeśli chcesz nadpisać kompilator hosta CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Budowa GPU Intel</strong></p><p>W tym trybie zostanie zbudowany PyTorch ze wsparciem GPU Intel.</p><p>Upewnij się, że <a href="#prerequisites" target="_blank" rel="noopener noreferrer">wspólne wymagania wstępne</a> oraz <a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">wymagania dla GPU Intel</a> są poprawnie zainstalowane, a zmienne środowiskowe skonfigurowane przed rozpoczęciem budowy. Do wsparcia narzędzi budowania wymagany jest <code>Visual Studio 2022</code>.</p><p>Następnie PyTorch można zbudować poleceniem:</p><pre><code class="language-cmd">:: Polecenia CMD:
:: Ustaw CMAKE_PREFIX_PATH aby ułatwić znajdowanie odpowiednich pakietów
:: %CONDA_PREFIX% działa tylko po <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Dostosowywanie opcji budowania (opcjonalnie)</p><p>Możesz opcjonalnie dostosować konfigurację zmiennych cmake (bez wcześniejszego budowania), wykonując poniższe kroki. Na przykład, dostosowanie wykrytych katalogów CuDNN lub BLAS można przeprowadzić w ten sposób.</p><p>Na Linuksie
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # lub cmake-gui build</code></pre></p><p>Na macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # lub cmake-gui build</code></pre></p><h3>Obraz Dockera</h3></p><p>#### Korzystanie z gotowych obrazów</p><p>Możesz również pobrać gotowy obraz dockera z Docker Hub i uruchomić go za pomocą docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Zwróć uwagę, że PyTorch używa współdzielonej pamięci do wymiany danych między procesami, więc jeśli korzystasz z torch multiprocessing (np. wielowątkowe ładowanie danych), domyślny rozmiar segmentu współdzielonej pamięci w kontenerze jest niewystarczający i należy go zwiększyć opcjami <code>--ipc=host</code> lub <code>--shm-size</code> w poleceniu <code>nvidia-docker run</code>.</p><p>#### Budowanie obrazu samodzielnie</p><p><strong>UWAGA:</strong> Musisz zbudować obraz na dockerze w wersji > 18.06</p><p>Dołączony <code>Dockerfile</code> pozwala zbudować obrazy ze wsparciem CUDA 11.1 oraz cuDNN v8.
Możesz przekazać zmienną <code>PYTHON_VERSION=x.y</code>, by wskazać wersję Pythona używaną przez Minicondę, lub pozostawić ją domyślną.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>obrazy są tagowane jako docker.io/${twoja_nazwa_użytkownika_dockera}/pytorch</code></pre></h1></p><p>Możesz też przekazać zmienną środowiskową <code>CMAKE_VARS="..."</code>, by określić dodatkowe zmienne CMake przekazywane do CMake podczas budowania.
Zobacz <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> po listę dostępnych zmiennych.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Budowanie dokumentacji</h3></p><p>Aby budować dokumentację w różnych formatach, potrzebujesz <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
oraz pytorch_sphinx_theme2.</p><p>Przed lokalnym budowaniem dokumentacji upewnij się, że <code>torch</code> jest
zainstalowany w Twoim środowisku. Dla drobnych poprawek możesz zainstalować
wersję nightly zgodnie z opisem w <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Pierwsze kroki</a>.</p><p>Dla bardziej złożonych poprawek, takich jak dodanie nowego modułu i docstringów,
może być konieczna instalacja torch <a href="#from-source" target="_blank" rel="noopener noreferrer">ze źródeł</a>.
Zobacz <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Wytyczne dotyczące docstringów</a>
dla konwencji docstringów.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Uruchom <code>make</code>, aby zobaczyć listę wszystkich dostępnych formatów wyjściowych.</p><p>Jeśli pojawi się błąd katex, uruchom <code>npm install katex</code>. Jeśli nadal występuje, spróbuj
<code>npm install -g katex</code></p><blockquote>[!UWAGA]</blockquote>
<blockquote>Jeśli zainstalowałeś <code>nodejs</code> innym menedżerem pakietów (np.</blockquote>
<blockquote><code>conda</code>), to <code>npm</code> prawdopodobnie zainstaluje wersję <code>katex</code> niekompatybilną z Twoją wersją <code>nodejs</code> i budowanie dokumentacji się nie powiedzie.</blockquote>
<blockquote>Znana, działająca kombinacja to <code>node@6.13.1</code> i</blockquote>
<blockquote><code>katex@0.13.18</code>. Aby zainstalować tę wersję przez <code>npm</code> uruchom</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!UWAGA]</blockquote>
<blockquote>Jeśli pojawia się błąd niekompatybilności numpy, uruchom:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Kiedy wprowadzasz zmiany w zależnościach uruchamianych przez CI, edytuj plik
</code>.ci/docker/requirements-docs.txt<code>.</p><p>#### Budowanie PDF</p><p>Aby wygenerować PDF całej dokumentacji PyTorch, upewnij się, że masz
</code>texlive<code> oraz LaTeX. Na macOS można je zainstalować przez:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Aby utworzyć PDF:</p><ul><li>Uruchom:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   To wygeneruje niezbędne pliki w katalogu </code>build/latex<code>.</p><ul><li>Przejdź do tego katalogu i uruchom:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   To utworzy plik </code>pytorch.pdf` z pożądanymi treściami. Uruchom to
   polecenie jeszcze raz, aby wygenerować prawidłowy spis treści i indeks.</p><blockquote>[!UWAGA]</blockquote>
<blockquote>Aby zobaczyć spis treści, przełącz się na widok <strong>Table of Contents</strong></blockquote>
<blockquote>w przeglądarce PDF.</blockquote></p><h3>Poprzednie wersje</h3></p><p>Instrukcje instalacji i binaria dla poprzednich wersji PyTorch można znaleźć
<a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">na naszej stronie</a>.</p><h2>Pierwsze kroki</h2></p><p>Trzy wskazówki, jak zacząć:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriale: nauczą Cię korzystania i rozumienia PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Przykłady: łatwy do zrozumienia kod PyTorch z różnych dziedzin</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">Dokumentacja API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Słowniczek</a></li></p><p></ul><h2>Zasoby</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriale PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Przykłady PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">Modele PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Intro do Deep Learning z PyTorch na Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Intro do Machine Learning z PyTorch na Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Deep Neural Networks z PyTorch na Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Komunikacja</h2>
<ul><li>Forum: Dyskusje o implementacjach, badaniach itd. https://discuss.pytorch.org</li>
<li>GitHub Issues: Zgłaszanie błędów, propozycje funkcjonalności, problemy z instalacją, RFC, pomysły itd.</li>
<li>Slack: <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">PyTorch Slack</a> skupia przede wszystkim średniozaawansowanych i zaawansowanych użytkowników PyTorch oraz deweloperów do ogólnego czatu, dyskusji online, współpracy itd. Jeśli jesteś początkującym użytkownikiem, głównym kanałem są <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">Fora PyTorch</a>. Jeśli potrzebujesz zaproszenia na slacka, wypełnij ten formularz: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Jednokierunkowy newsletter e-mail z najważniejszymi ogłoszeniami o PyTorch. Możesz się zapisać tutaj: https://eepurl.com/cbG0rv</li>
<li>Facebook Page: Ważne ogłoszenia o PyTorch. https://www.facebook.com/pytorch</li>
<li>Wytyczne dotyczące marki znajdziesz na naszej stronie <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Wydania i kontrybucje</h2></p><p>Zazwyczaj PyTorch ma trzy mniejsze wydania rocznie. Prosimy o zgłaszanie błędów przez <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">utworzenie zgłoszenia</a>.</p><p>Doceniamy wszystkie kontrybucje. Jeśli planujesz przesłać poprawki błędów, zrób to bez dodatkowej dyskusji.</p><p>Jeśli chcesz dodać nowe funkcje, funkcje pomocnicze lub rozszerzenia do core, najpierw otwórz zgłoszenie i skonsultuj funkcję z nami.
Wysyłanie PR bez dyskusji może skończyć się jego odrzuceniem, ponieważ możemy prowadzić core w innym kierunku, niż się spodziewasz.</p><p>Aby dowiedzieć się więcej o kontrybucji do PyTorch, zobacz <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">stronę kontrybucji</a>. Więcej informacji o wydaniach PyTorch znajdziesz na <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">stronie wydań</a>.</p><h2>Zespół</h2></p><p>PyTorch to projekt tworzony przez społeczność, do którego przyczynia się wielu utalentowanych inżynierów i badaczy.</p><p>PyTorch jest obecnie utrzymywany przez <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a> i <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a>, z dużymi kontrybucjami setek uzdolnionych osób w różnych formach.
Niewyczerpująca, ale rosnąca lista obejmuje: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Uwaga: Ten projekt nie jest powiązany z <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> o tej samej nazwie. Hugh jest cennym kontrybutorem społeczności Torch i pomógł w wielu aspektach Torch i PyTorch.</p><h2>Licencja</h2></p><p>PyTorch ma licencję typu BSD, dostępną w pliku <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>