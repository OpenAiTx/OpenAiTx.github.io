<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in French. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in French. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in French. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">French</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="Logo PyTorch"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch est un paquet Python qui fournit deux fonctionnalités de haut niveau :
<ul><li>Calcul de tenseurs (comme NumPy) avec une forte accélération GPU</li>
<li>Réseaux de neurones profonds construits sur un système d'autograd basé sur une bande (tape-based)</li></p><p></ul>Vous pouvez réutiliser vos paquets Python préférés tels que NumPy, SciPy et Cython pour étendre PyTorch si nécessaire.</p><p>L’état de santé de notre branche principale (signaux d’intégration continue) peut être consulté sur <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#more-about-pytorch" target="_blank" rel="noopener noreferrer">En savoir plus sur PyTorch</a></li>
  <li><a href="#a-gpu-ready-tensor-library" target="_blank" rel="noopener noreferrer">Une bibliothèque de tenseurs prête pour le GPU</a></li>
  <li><a href="#dynamic-neural-networks-tape-based-autograd" target="_blank" rel="noopener noreferrer">Réseaux de neurones dynamiques : autograd basé sur une bande</a></li>
  <li><a href="#python-first" target="_blank" rel="noopener noreferrer">Python d’abord</a></li>
  <li><a href="#imperative-experiences" target="_blank" rel="noopener noreferrer">Expériences impératives</a></li>
  <li><a href="#fast-and-lean" target="_blank" rel="noopener noreferrer">Rapide et léger</a></li>
  <li><a href="#extensions-without-pain" target="_blank" rel="noopener noreferrer">Extensions sans douleur</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">Installation</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">Binaires</a></li>
    <li><a href="#nvidia-jetson-platforms" target="_blank" rel="noopener noreferrer">Plateformes NVIDIA Jetson</a></li>
  <li><a href="#from-source" target="_blank" rel="noopener noreferrer">Depuis les sources</a></li>
    <li><a href="#prerequisites" target="_blank" rel="noopener noreferrer">Prérequis</a></li>
      <li><a href="#nvidia-cuda-support" target="_blank" rel="noopener noreferrer">Support NVIDIA CUDA</a></li>
      <li><a href="#amd-rocm-support" target="_blank" rel="noopener noreferrer">Support AMD ROCm</a></li>
      <li><a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Support GPU Intel</a></li>
    <li><a href="#get-the-pytorch-source" target="_blank" rel="noopener noreferrer">Obtenir le code source de PyTorch</a></li>
    <li><a href="#install-dependencies" target="_blank" rel="noopener noreferrer">Installer les dépendances</a></li>
    <li><a href="#install-pytorch" target="_blank" rel="noopener noreferrer">Installer PyTorch</a></li>
      <li><a href="#adjust-build-options-optional" target="_blank" rel="noopener noreferrer">Ajuster les options de compilation (optionnel)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Image Docker</a></li>
    <li><a href="#using-pre-built-images" target="_blank" rel="noopener noreferrer">Utilisation d’images pré-construites</a></li>
    <li><a href="#building-the-image-yourself" target="_blank" rel="noopener noreferrer">Construire l’image soi-même</a></li>
  <li><a href="#building-the-documentation" target="_blank" rel="noopener noreferrer">Générer la documentation</a></li>
    <li><a href="#building-a-pdf" target="_blank" rel="noopener noreferrer">Générer un PDF</a></li>
  <li><a href="#previous-versions" target="_blank" rel="noopener noreferrer">Versions précédentes</a></li>
<li><a href="#getting-started" target="_blank" rel="noopener noreferrer">Premiers pas</a></li>
<li><a href="#resources" target="_blank" rel="noopener noreferrer">Ressources</a></li>
<li><a href="#communication" target="_blank" rel="noopener noreferrer">Communication</a></li>
<li><a href="#releases-and-contributing" target="_blank" rel="noopener noreferrer">Versions et contribution</a></li>
<li><a href="#the-team" target="_blank" rel="noopener noreferrer">L’équipe</a></li>
<li><a href="#license" target="_blank" rel="noopener noreferrer">Licence</a></li></p><p></ul><!-- tocstop --></p><h2>En savoir plus sur PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Apprenez les bases de PyTorch</a></p><p>À un niveau granulaire, PyTorch est une bibliothèque composée des éléments suivants :</p><p>| Composant | Description |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Une bibliothèque de tenseurs comme NumPy, avec un fort support GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Une bibliothèque de différentiation automatique basée sur une bande qui prend en charge toutes les opérations différentiables sur les tenseurs dans torch |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Une pile de compilation (TorchScript) pour créer des modèles sérialisables et optimisables à partir de code PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Une bibliothèque de réseaux de neurones profondément intégrée à autograd, conçue pour une flexibilité maximale |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Multiprocessing Python, mais avec partage magique de la mémoire des tenseurs torch entre processus. Utile pour le chargement des données et l’entraînement Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader et autres fonctions utilitaires pour plus de commodité |</p><p>En général, PyTorch est utilisé soit comme :</p><ul><li>Un remplacement de NumPy pour utiliser la puissance des GPU.</li>
<li>Une plateforme de recherche en deep learning offrant flexibilité maximale et rapidité.</li></p><p></ul>Plus de détails :</p><h3>Une bibliothèque de tenseurs prête pour le GPU</h3></p><p>Si vous utilisez NumPy, alors vous avez utilisé des tenseurs (a.k.a. ndarray).</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Illustration de tenseur"></p><p>PyTorch fournit des tenseurs qui peuvent résider soit sur le CPU soit sur le GPU et accélère
les calculs de manière considérable.</p><p>Nous proposons une large variété de routines sur les tenseurs pour accélérer et répondre à vos besoins scientifiques :
découpage, indexation, opérations mathématiques, algèbre linéaire, réductions.
Et elles sont rapides !</p><h3>Réseaux de neurones dynamiques : autograd basé sur une bande</h3></p><p>PyTorch a une façon unique de construire les réseaux de neurones : en utilisant et rejouant un magnétophone.</p><p>La plupart des frameworks comme TensorFlow, Theano, Caffe et CNTK ont une vision statique du monde.
Il faut construire un réseau de neurones et réutiliser la même structure encore et encore.
Changer le comportement du réseau signifie repartir de zéro.</p><p>Avec PyTorch, nous utilisons une technique appelée auto-différentiation en mode inverse, qui vous permet
de modifier arbitrairement le comportement de votre réseau sans aucun délai ni surcoût. Notre inspiration vient
de plusieurs articles de recherche sur ce sujet, ainsi que de travaux passés et actuels tels que
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a>, etc.</p><p>Bien que cette technique ne soit pas propre à PyTorch, c’en est l’une des implémentations les plus rapides à ce jour.
Vous obtenez le meilleur de la vitesse et de la flexibilité pour vos recherches les plus folles.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Graphe dynamique"></p><h3>Python d’abord</h3></p><p>PyTorch n’est pas une simple liaison Python vers un framework C++ monolithique.
Il est conçu pour être profondément intégré à Python.
Vous pouvez l’utiliser naturellement comme vous le feriez avec <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a>, etc.
Vous pouvez écrire vos nouvelles couches de réseau de neurones directement en Python, en utilisant vos bibliothèques préférées,
et utiliser des paquets comme <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> et <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Notre objectif est de ne pas réinventer la roue là où ce n’est pas nécessaire.</p><h3>Expériences impératives</h3></p><p>PyTorch est conçu pour être intuitif, linéaire dans la réflexion et facile à utiliser.
Lorsque vous exécutez une ligne de code, elle s’exécute immédiatement. Il n’y a pas de vision asynchrone du monde.
Lorsque vous entrez dans un débogueur ou recevez des messages d’erreur et des traces de pile, leur compréhension est directe.
La trace de pile indique exactement où votre code a été défini.
Nous espérons que vous ne passerez jamais des heures à déboguer votre code à cause de mauvaises traces ou d’engins d’exécution asynchrones et opaques.</p><h3>Rapide et léger</h3></p><p>PyTorch a un surcoût de framework minimal. Nous intégrons des bibliothèques d’accélération
telles que <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> et NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) pour maximiser la vitesse.
Au cœur, ses backends de tenseur et de réseau de neurones CPU et GPU
sont matures et éprouvés depuis des années.</p><p>Ainsi, PyTorch est très rapide — que vous exécutiez de petits ou de grands réseaux de neurones.</p><p>L’utilisation de la mémoire sous PyTorch est extrêmement efficace comparée à Torch ou à d’autres alternatives.
Nous avons écrit des allocateurs de mémoire personnalisés pour le GPU afin de garantir que
vos modèles de deep learning soient aussi économes en mémoire que possible.
Cela vous permet d’entraîner des modèles plus grands qu’auparavant.</p><h3>Extensions sans douleur</h3></p><p>Écrire de nouveaux modules de réseau de neurones ou interfacer avec l’API Tensor de PyTorch a été conçu pour être simple
et avec un minimum d’abstractions.</p><p>Vous pouvez écrire de nouvelles couches de réseau en Python en utilisant l’API torch
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">ou vos bibliothèques préférées basées sur NumPy comme SciPy</a>.</p><p>Si vous souhaitez écrire vos couches en C/C++, nous fournissons une API d’extension pratique, efficace et sans boilerplate.
Aucun code wrapper n’est nécessaire. Voir <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">un tutoriel ici</a> et <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">un exemple ici</a>.</p><h2>Installation</h2></p><h3>Binaires</h3>
Les commandes pour installer les binaires via Conda ou pip wheels sont disponibles sur notre site web : <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>#### Plateformes NVIDIA Jetson</p><p>Des roues Python pour les Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX et Jetson AGX Orin de NVIDIA sont disponibles <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">ici</a> et le conteneur L4T est publié <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">ici</a></p><p>Elles nécessitent JetPack 4.2 et supérieur, et <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> ainsi que <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> les maintiennent.</p><h3>Depuis les sources</h3></p><p>#### Prérequis
Si vous installez depuis les sources, vous aurez besoin de :
<ul><li>Python 3.9 ou supérieur</li>
<li>Un compilateur prenant en charge entièrement C++17, tel que clang ou gcc (gcc 9.4.0 ou plus récent requis sous Linux)</li>
<li>Visual Studio ou Visual Studio Build Tool (Windows uniquement)</li></p><p></ul>\* L’intégration continue de PyTorch utilise Visual C++ BuildTools, qui sont inclus dans Visual Studio Enterprise,
Professional ou Community Editions. Vous pouvez aussi installer les outils de build depuis
https://visualstudio.microsoft.com/visual-cpp-build-tools/. Les outils de build <em>ne sont pas</em>
inclus par défaut dans Visual Studio Code.</p><p>Un exemple de configuration d’environnement est présenté ci-dessous :</p><ul><li>Linux :</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows :</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### Support NVIDIA CUDA
Si vous souhaitez compiler avec le support CUDA, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">sélectionnez une version prise en charge de CUDA dans notre matrice de support</a>, puis installez :
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 ou supérieur</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compilateur</a> compatible avec CUDA</li></p><p></ul>Note : Vous pouvez consulter la <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">matrice de support cuDNN</a> pour les versions cuDNN supportées avec les différentes versions CUDA, pilotes CUDA et matériels NVIDIA.</p><p>Si vous souhaitez désactiver le support CUDA, exportez la variable d’environnement <code>USE_CUDA=0</code>.
D’autres variables d’environnement utiles peuvent être trouvées dans <code>setup.py</code>.</p><p>Si vous compilez pour les plateformes Jetson de NVIDIA (Jetson Nano, TX1, TX2, AGX Xavier), les instructions d’installation de PyTorch pour Jetson Nano sont <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">disponibles ici</a></p><p>##### Support AMD ROCm
Si vous souhaitez compiler avec le support ROCm, installez
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> version 4.0 ou supérieure</li>
<li>ROCm est actuellement uniquement pris en charge pour les systèmes Linux.</li></p><p></ul>Par défaut, le système de build s’attend à ce que ROCm soit installé dans <code>/opt/rocm</code>. Si ROCm est installé dans un autre répertoire, la variable d’environnement <code>ROCM_PATH</code> doit être définie sur ce répertoire. Le système de build détecte automatiquement l’architecture GPU AMD. Optionnellement, l’architecture GPU AMD peut être explicitement définie avec la variable d’environnement <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">Architecture GPU AMD</a></p><p>Pour désactiver le support ROCm, exportez la variable d’environnement <code>USE_ROCM=0</code>.
D’autres variables d’environnement utiles peuvent être trouvées dans <code>setup.py</code>.</p><p>##### Support GPU Intel
Si vous souhaitez compiler avec le support GPU Intel, suivez ces
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Instructions des prérequis PyTorch pour GPU Intel</a>.</li>
<li>Le GPU Intel est pris en charge sous Linux et Windows.</li></p><p></ul>Pour désactiver le support GPU Intel, exportez la variable d’environnement <code>USE_XPU=0</code>.
D’autres variables d’environnement utiles peuvent être trouvées dans <code>setup.py</code>.</p><p>#### Obtenir le code source de PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>si vous mettez à jour un dépôt existant</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Installer les dépendances</p><p><strong>Commun</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Exécutez cette commande depuis le dossier PyTorch après avoir cloné le code source selon la section “Obtenir le code source de PyTorch” ci-dessus</h1>
pip install -r requirements.txt</code></pre></p><p><strong>Sous Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>CUDA uniquement : ajouter le support LAPACK pour le GPU si nécessaire</h1>
<h1>installation de magma : exécutez avec un environnement conda actif. spécifiez la version CUDA à installer</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(optionnel) Si vous utilisez torch.compile avec inductor/triton, installez la version correspondante de triton</h1>
<h1>Depuis le dossier pytorch après clonage</h1>
<h1>Pour le support GPU Intel, veuillez explicitement <code>export USE_XPU=1</code> avant d’exécuter la commande.</h1>
make triton</code></pre></p><p><strong>Sous MacOS</strong></p><pre><code class="language-bash"># Ajouter ce paquet uniquement sur les machines processeur intel x86
pip install mkl-static mkl-include
<h1>Ajouter ces paquets si torch.distributed est nécessaire</h1>
conda install pkg-config libuv</code></pre></p><p><strong>Sous Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Ajouter ces paquets si torch.distributed est nécessaire.</h1>
<h1>Le support du package distributed sous Windows est une fonctionnalité prototype sujette à modifications.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Installer PyTorch
<strong>Sous Linux</strong></p><p>Si vous compilez pour AMD ROCm, exécutez d’abord cette commande :
<pre><code class="language-bash"># Exécutez ceci uniquement si vous compilez pour ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Installer PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>Sous macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>Sous Windows</strong></p><p>Si vous souhaitez compiler du code python hérité, veuillez consulter <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Compilation de code hérité et CUDA</a></p><p><strong>Compilations CPU uniquement</strong></p><p>Dans ce mode, les calculs PyTorch s’exécutent sur votre CPU, pas votre GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Note sur OpenMP : L’implémentation OpenMP souhaitée est Intel OpenMP (iomp). Pour lier iomp, vous devez télécharger manuellement la bibliothèque et configurer l’environnement de build en ajustant <code>CMAKE_INCLUDE_PATH</code> et <code>LIB</code>. Les instructions <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">ici</a> montrent comment configurer MKL et Intel OpenMP. Sans ces configurations pour CMake, le runtime OpenMP de Microsoft Visual C (vcomp) sera utilisé.</p><p><strong>Compilation basée sur CUDA</strong></p><p>Dans ce mode, les calculs PyTorch utiliseront votre GPU via CUDA pour des calculs plus rapides.</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> est nécessaire pour compiler PyTorch avec CUDA.
NVTX fait partie de la distribution CUDA, où il est appelé "Nsight Compute". Pour l’installer sur une CUDA déjà installée, relancez l’installation CUDA et cochez la case correspondante.
Assurez-vous que CUDA avec Nsight Compute est installé après Visual Studio.</p><p>Actuellement, VS 2017/2019 et Ninja sont supportés comme générateurs CMake. Si <code>ninja.exe</code> est détecté dans <code>PATH</code>, Ninja sera utilisé par défaut, sinon VS 2017/2019.
<br/> Si Ninja est sélectionné comme générateur, la dernière version de MSVC sera sélectionnée comme toolchain sous-jacente.</p><p>Des bibliothèques additionnelles telles que
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, aussi appelé MKLDNN ou DNNL</a>, et <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> sont souvent nécessaires. Veuillez consulter le <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">helper d’installation</a> pour les installer.</p><p>Vous pouvez vous référer au script <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> pour d’autres configurations de variables d’environnement.</p><pre><code class="language-cmd">cmd</p><p>:: Définissez les variables d’environnement après avoir téléchargé et décompressé le paquet mkl,
:: sinon CMake renverra une erreur <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Votre dossier}\mkl\include
set LIB={Votre dossier}\mkl\lib;%LIB%</p><p>:: Lisez attentivement la section précédente avant de continuer.
:: [Optionnel] Pour forcer le toolset sous-jacent utilisé par Ninja et Visual Studio avec CUDA, exécutez le bloc suivant.
:: L’invite "Visual Studio 2019 Developer Command Prompt" sera lancée automatiquement.
:: Assurez-vous d’avoir CMake >= 3.12 avant cela si vous utilisez le générateur Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Optionnel] Pour surcharger le compilateur hôte CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Compilations GPU Intel</strong></p><p>Dans ce mode, PyTorch avec le support GPU Intel sera compilé.</p><p>Veuillez vous assurer que <a href="#prerequisites" target="_blank" rel="noopener noreferrer">les prérequis communs</a> ainsi que <a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">les prérequis pour GPU Intel</a> sont correctement installés et que les variables d’environnement sont configurées avant de commencer la compilation. Pour le support des outils de build, <code>Visual Studio 2022</code> est requis.</p><p>PyTorch peut alors être compilé avec la commande suivante :</p><pre><code class="language-cmd">:: Commandes CMD :
:: Définir CMAKE_PREFIX_PATH pour aider à trouver les paquets correspondants
:: %CONDA_PREFIX% ne fonctionne qu’après <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Ajuster les options de compilation (optionnel)</p><p>Vous pouvez ajuster la configuration des variables cmake optionnellement (sans compiler d’abord), en procédant ainsi. Par exemple, ajuster les répertoires prédétectés pour CuDNN ou BLAS peut se faire
avec cette étape.</p><p>Sous Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # ou cmake-gui build</code></pre></p><p>Sous macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # ou cmake-gui build</code></pre></p><h3>Image Docker</h3></p><p>#### Utilisation d’images pré-construites</p><p>Vous pouvez également récupérer une image docker pré-construite depuis Docker Hub et l’exécuter avec docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Veuillez noter que PyTorch utilise la mémoire partagée pour partager les données entre processus, donc si le multiprocessing torch est utilisé (par exemple
pour les data loaders multithreadés) la taille de segment mémoire partagée par défaut du conteneur n’est pas suffisante, et vous
devez augmenter la taille de la mémoire partagée avec les options de ligne de commande <code>--ipc=host</code> ou <code>--shm-size</code> à <code>nvidia-docker run</code>.</p><p>#### Construire l’image soi-même</p><p><strong>REMARQUE :</strong> Doit être construit avec une version de docker > 18.06</p><p>Le <code>Dockerfile</code> est fourni pour construire des images avec le support CUDA 11.1 et cuDNN v8.
Vous pouvez passer la variable make <code>PYTHON_VERSION=x.y</code> pour spécifier quelle version de Python doit être utilisée par Miniconda, ou la laisser non définie pour utiliser la valeur par défaut.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>les images sont taguées comme docker.io/${votre_nom_utilisateur_docker}/pytorch</code></pre></h1></p><p>Vous pouvez aussi passer la variable d’environnement <code>CMAKE_VARS="..."</code> pour spécifier des variables CMake supplémentaires à transmettre à CMake lors de la compilation.
Voir <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> pour la liste des variables disponibles.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Générer la documentation</h3></p><p>Pour générer la documentation sous différents formats, vous aurez besoin de <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
et du thème pytorch_sphinx_theme2.</p><p>Avant de générer la documentation localement, assurez-vous que <code>torch</code> est
installé dans votre environnement. Pour de petites corrections, vous pouvez installer la
version nightly comme décrit dans <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Premiers pas</a>.</p><p>Pour des corrections plus complexes, comme l’ajout d’un nouveau module et des docstrings pour
ce module, il peut être nécessaire d’installer torch <a href="#from-source" target="_blank" rel="noopener noreferrer">depuis les sources</a>.
Voir <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Directives pour les docstrings</a>
pour les conventions de docstring.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Exécutez <code>make</code> pour obtenir la liste de tous les formats de sortie disponibles.</p><p>Si vous obtenez une erreur katex exécutez <code>npm install katex</code>.  Si cela persiste, essayez
<code>npm install -g katex</code></p><blockquote>[!NOTE]</blockquote>
<blockquote>Si vous avez installé <code>nodejs</code> avec un gestionnaire de paquets différent (par exemple,</blockquote>
<blockquote><code>conda</code>) alors <code>npm</code> installera probablement une version de <code>katex</code> qui n’est pas</blockquote>
<blockquote>compatible avec votre version de <code>nodejs</code> et la génération de la doc échouera.</blockquote>
<blockquote>Une combinaison de versions connue pour fonctionner est <code>node@6.13.1</code> et</blockquote>
<blockquote><code>katex@0.13.18</code>. Pour installer cette dernière avec <code>npm</code>, exécutez</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!NOTE]</blockquote>
<blockquote>Si vous voyez une erreur d’incompatibilité numpy, exécutez :</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Si vous modifiez les dépendances utilisées par l’IC, éditez le fichier
</code>.ci/docker/requirements-docs.txt<code>.</p><p>#### Générer un PDF</p><p>Pour compiler un PDF de toute la documentation PyTorch, assurez-vous d’avoir
</code>texlive<code> et LaTeX installés. Sous macOS, vous pouvez les installer via :
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Pour créer le PDF :</p><ul><li>Exécutez :</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Cela générera les fichiers nécessaires dans le répertoire </code>build/latex<code>.</p><ul><li>Naviguez dans ce répertoire et exécutez :</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Cela produira un </code>pytorch.pdf` avec le contenu souhaité. Exécutez cette
   commande une fois de plus afin de générer la table des matières
   et l’index corrects.</p><blockquote>[!NOTE]</blockquote>
<blockquote>Pour visualiser la table des matières, passez en mode <strong>Table des matières</strong></blockquote>
<blockquote>dans votre lecteur PDF.</blockquote></p><h3>Versions précédentes</h3></p><p>Les instructions d’installation et binaires pour les versions précédentes de PyTorch sont disponibles
sur <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">notre site web</a>.</p><h2>Premiers pas</h2></p><p>Trois liens pour commencer :
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriels : pour comprendre et utiliser PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Exemples : du code PyTorch facile à comprendre dans tous les domaines</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">La référence de l’API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Glossaire</a></li></p><p></ul><h2>Ressources</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriels PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Exemples PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">Modèles PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Intro à l’apprentissage profond avec PyTorch par Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Intro au machine learning avec PyTorch par Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Réseaux de neurones profonds avec PyTorch sur Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">PyTorch Twitter</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">PyTorch Blog</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">PyTorch YouTube</a></li></p><p></ul><h2>Communication</h2>
<ul><li>Forums : Discuter d’implémentations, de la recherche, etc. https://discuss.pytorch.org</li>
<li>GitHub Issues : Rapports de bugs, suggestions de fonctionnalités, problèmes d’installation, RFC, idées, etc.</li>
<li>Slack : Le <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">Slack PyTorch</a> accueille principalement des utilisateurs et développeurs PyTorch de niveau intermédiaire à avancé pour chat général, discussions en ligne, collaborations, etc. Si vous êtes débutant et cherchez de l’aide, le principal canal est <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">les forums PyTorch</a>. Pour une invitation Slack, remplissez ce formulaire : https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter : Sans bruit, une newsletter email unidirectionnelle avec les annonces importantes concernant PyTorch. Inscrivez-vous ici : https://eepurl.com/cbG0rv</li>
<li>Page Facebook : Annonces importantes à propos de PyTorch. https://www.facebook.com/pytorch</li>
<li>Pour les guides de marque, veuillez consulter notre site : <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Versions et contribution</h2></p><p>En général, PyTorch a trois versions mineures par an. Merci de nous signaler tout bug en <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">créant une issue</a>.</p><p>Nous apprécions toute contribution. Si vous prévoyez de proposer des corrections de bugs, faites-le sans discussion préalable.</p><p>Si vous envisagez de contribuer de nouvelles fonctionnalités, fonctions utilitaires ou extensions au cœur du projet, ouvrez d’abord une issue pour discuter de la fonctionnalité avec nous.
Envoyer une PR sans discussion pourrait conduire à un refus, car nous pourrions orienter le cœur du projet dans une direction différente de ce que vous imaginez.</p><p>Pour en savoir plus sur la contribution à PyTorch, consultez notre <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">page de contribution</a>. Pour plus d’informations sur les versions de PyTorch, voir <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">page des versions</a>.</p><h2>L’équipe</h2></p><p>PyTorch est un projet communautaire avec de nombreux ingénieurs et chercheurs talentueux qui y contribuent.</p><p>PyTorch est actuellement maintenu par <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, et <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a>, avec des contributions majeures venant de centaines d’individus talentueux sous diverses formes.
Une liste non exhaustive, mais croissante, inclut : <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Remarque : Ce projet n’est pas lié à <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> portant le même nom. Hugh est un contributeur précieux à la communauté Torch et a beaucoup aidé sur Torch et PyTorch.</p><h2>Licence</h2></p><p>PyTorch possède une licence de type BSD, comme indiqué dans le fichier <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>