<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pytorch - Read pytorch documentation in Portuguese. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read pytorch documentation in Portuguese. This project has 0 stars on GitHub.">
    <meta name="keywords" content="pytorch, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "pytorch",
  "description": "Read pytorch documentation in Portuguese. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "pytorch"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/pytorch/pytorch/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/pytorch/pytorch/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    pytorch
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Portuguese</span>
                <span>by pytorch</span>
            </div>
        </div>
        
        <div class="content">
            <p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png" alt="Logotipo PyTorch"></p><p>--------------------------------------------------------------------------------</p><p>PyTorch é um pacote Python que oferece dois recursos de alto nível:
<ul><li>Computação de tensores (como o NumPy) com forte aceleração por GPU</li>
<li>Redes neurais profundas construídas sobre um sistema de autograd baseado em fitas (tape-based)</li></p><p></ul>Você pode reutilizar seus pacotes Python favoritos, como NumPy, SciPy e Cython, para estender o PyTorch quando necessário.</p><p>A saúde do nosso trunk (sinais de Integração Contínua) pode ser encontrada em <a href="https://hud.pytorch.org/ci/pytorch/pytorch/main" target="_blank" rel="noopener noreferrer">hud.pytorch.org</a>.</p><p><!-- toc --></p><ul><li><a href="#more-about-pytorch" target="_blank" rel="noopener noreferrer">Mais Sobre o PyTorch</a></li>
  <li><a href="#a-gpu-ready-tensor-library" target="_blank" rel="noopener noreferrer">Uma Biblioteca de Tensores Pronta para GPU</a></li>
  <li><a href="#dynamic-neural-networks-tape-based-autograd" target="_blank" rel="noopener noreferrer">Redes Neurais Dinâmicas: Autograd Baseado em Fita</a></li>
  <li><a href="#python-first" target="_blank" rel="noopener noreferrer">Python em Primeiro Lugar</a></li>
  <li><a href="#imperative-experiences" target="_blank" rel="noopener noreferrer">Experiências Imperativas</a></li>
  <li><a href="#fast-and-lean" target="_blank" rel="noopener noreferrer">Rápido e Enxuto</a></li>
  <li><a href="#extensions-without-pain" target="_blank" rel="noopener noreferrer">Extensões sem Dor</a></li>
<li><a href="#installation" target="_blank" rel="noopener noreferrer">Instalação</a></li>
  <li><a href="#binaries" target="_blank" rel="noopener noreferrer">Binários</a></li>
    <li><a href="#nvidia-jetson-platforms" target="_blank" rel="noopener noreferrer">Plataformas NVIDIA Jetson</a></li>
  <li><a href="#from-source" target="_blank" rel="noopener noreferrer">A Partir do Código-Fonte</a></li>
    <li><a href="#prerequisites" target="_blank" rel="noopener noreferrer">Pré-requisitos</a></li>
      <li><a href="#nvidia-cuda-support" target="_blank" rel="noopener noreferrer">Suporte NVIDIA CUDA</a></li>
      <li><a href="#amd-rocm-support" target="_blank" rel="noopener noreferrer">Suporte AMD ROCm</a></li>
      <li><a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">Suporte a GPU Intel</a></li>
    <li><a href="#get-the-pytorch-source" target="_blank" rel="noopener noreferrer">Obtenha o Código-Fonte do PyTorch</a></li>
    <li><a href="#install-dependencies" target="_blank" rel="noopener noreferrer">Instale as Dependências</a></li>
    <li><a href="#install-pytorch" target="_blank" rel="noopener noreferrer">Instale o PyTorch</a></li>
      <li><a href="#adjust-build-options-optional" target="_blank" rel="noopener noreferrer">Ajustar Opções de Build (Opcional)</a></li>
  <li><a href="#docker-image" target="_blank" rel="noopener noreferrer">Imagem Docker</a></li>
    <li><a href="#using-pre-built-images" target="_blank" rel="noopener noreferrer">Usando imagens pré-construídas</a></li>
    <li><a href="#building-the-image-yourself" target="_blank" rel="noopener noreferrer">Construindo a imagem você mesmo</a></li>
  <li><a href="#building-the-documentation" target="_blank" rel="noopener noreferrer">Construindo a Documentação</a></li>
    <li><a href="#building-a-pdf" target="_blank" rel="noopener noreferrer">Gerando um PDF</a></li>
  <li><a href="#previous-versions" target="_blank" rel="noopener noreferrer">Versões Anteriores</a></li>
<li><a href="#getting-started" target="_blank" rel="noopener noreferrer">Primeiros Passos</a></li>
<li><a href="#resources" target="_blank" rel="noopener noreferrer">Recursos</a></li>
<li><a href="#communication" target="_blank" rel="noopener noreferrer">Comunicação</a></li>
<li><a href="#releases-and-contributing" target="_blank" rel="noopener noreferrer">Lançamentos e Contribuição</a></li>
<li><a href="#the-team" target="_blank" rel="noopener noreferrer">O Time</a></li>
<li><a href="#license" target="_blank" rel="noopener noreferrer">Licença</a></li></p><p></ul><!-- tocstop --></p><h2>Mais Sobre o PyTorch</h2></p><p><a href="https://pytorch.org/tutorials/beginner/basics/intro.html" target="_blank" rel="noopener noreferrer">Aprenda o básico do PyTorch</a></p><p>No nível granular, o PyTorch é uma biblioteca que consiste nos seguintes componentes:</p><p>| Componente | Descrição |
| ---- | --- |
| <a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener noreferrer"><strong>torch</strong></a> | Uma biblioteca de tensores como o NumPy, com forte suporte a GPU |
| <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener noreferrer"><strong>torch.autograd</strong></a> | Uma biblioteca de diferenciação automática baseada em fita que suporta todas as operações de tensor diferenciáveis em torch |
| <a href="https://pytorch.org/docs/stable/jit.html" target="_blank" rel="noopener noreferrer"><strong>torch.jit</strong></a> | Uma pilha de compilação (TorchScript) para criar modelos serializáveis e otimizáveis a partir de código PyTorch |
| <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener noreferrer"><strong>torch.nn</strong></a> | Uma biblioteca de redes neurais profundamente integrada ao autograd projetada para máxima flexibilidade |
| <a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener noreferrer"><strong>torch.multiprocessing</strong></a> | Multiprocessamento Python, mas com compartilhamento mágico de memória de tensores torch entre processos. Útil para carregamento de dados e treinamento Hogwild |
| <a href="https://pytorch.org/docs/stable/data.html" target="_blank" rel="noopener noreferrer"><strong>torch.utils</strong></a> | DataLoader e outras funções utilitárias para conveniência |</p><p>Geralmente, o PyTorch é usado como:</p><ul><li>Um substituto do NumPy para usar o poder das GPUs.</li>
<li>Uma plataforma de pesquisa em deep learning que oferece máxima flexibilidade e velocidade.</li></p><p></ul>Elaborando mais:</p><h3>Uma Biblioteca de Tensores Pronta para GPU</h3></p><p>Se você usa NumPy, então já utilizou Tensores (também conhecidos como ndarray).</p><p><img src="./docs/source/_static/img/tensor_illustration.png" alt="Ilustração de Tensor"></p><p>O PyTorch fornece Tensores que podem estar tanto na CPU quanto na GPU e acelera
o cálculo consideravelmente.</p><p>Oferecemos uma grande variedade de rotinas de tensores para acelerar e atender às suas necessidades de computação científica,
como fatiamento, indexação, operações matemáticas, álgebra linear, reduções.
E elas são rápidas!</p><h3>Redes Neurais Dinâmicas: Autograd Baseado em Fita</h3></p><p>O PyTorch tem uma forma única de construir redes neurais: usando e reproduzindo um gravador de fita.</p><p>A maioria dos frameworks como TensorFlow, Theano, Caffe e CNTK possui uma visão estática do mundo.
É necessário construir uma rede neural e reutilizar a mesma estrutura repetidamente.
Mudar o comportamento da rede significa que é preciso começar do zero.</p><p>Com o PyTorch, usamos uma técnica chamada auto-diferenciação de modo reverso, que permite
alterar o comportamento da sua rede arbitrariamente, sem lag ou sobrecarga. Nossa inspiração vem
de vários artigos de pesquisa sobre este tópico, bem como trabalhos atuais e anteriores como
<a href="https://github.com/twitter/torch-autograd" target="_blank" rel="noopener noreferrer">torch-autograd</a>,
<a href="https://github.com/HIPS/autograd" target="_blank" rel="noopener noreferrer">autograd</a>,
<a href="https://chainer.org" target="_blank" rel="noopener noreferrer">Chainer</a>, etc.</p><p>Embora essa técnica não seja exclusiva do PyTorch, é uma das implementações mais rápidas até hoje.
Você obtém o melhor de velocidade e flexibilidade para suas pesquisas inovadoras.</p><p><img src="https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif" alt="Grafo dinâmico"></p><h3>Python em Primeiro Lugar</h3></p><p>PyTorch não é apenas um binding Python para um framework C++ monolítico.
Ele foi construído para ser profundamente integrado ao Python.
Você pode usá-lo naturalmente, como faria com <a href="https://www.numpy.org/" target="_blank" rel="noopener noreferrer">NumPy</a> / <a href="https://www.scipy.org/" target="_blank" rel="noopener noreferrer">SciPy</a> / <a href="https://scikit-learn.org" target="_blank" rel="noopener noreferrer">scikit-learn</a> etc.
Você pode escrever suas novas camadas de rede neural em Python, usando suas bibliotecas favoritas
e utilizar pacotes como <a href="https://cython.org/" target="_blank" rel="noopener noreferrer">Cython</a> e <a href="http://numba.pydata.org/" target="_blank" rel="noopener noreferrer">Numba</a>.
Nosso objetivo é não reinventar a roda onde apropriado.</p><h3>Experiências Imperativas</h3></p><p>O PyTorch foi projetado para ser intuitivo, linear no pensamento e fácil de usar.
Quando você executa uma linha de código, ela é executada imediatamente. Não existe uma visão assíncrona do mundo.
Quando você entra em um depurador ou recebe mensagens de erro e rastreamentos de pilha, entendê-los é direto.
O rastreamento de pilha aponta exatamente onde seu código foi definido.
Esperamos que você nunca passe horas depurando seu código devido a rastreamentos de pilha ruins ou motores de execução assíncronos e opacos.</p><h3>Rápido e Enxuto</h3></p><p>O PyTorch tem sobrecarga mínima de framework. Integramos bibliotecas de aceleração
como <a href="https://software.intel.com/mkl" target="_blank" rel="noopener noreferrer">Intel MKL</a> e NVIDIA (<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">cuDNN</a>, <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener noreferrer">NCCL</a>) para maximizar a velocidade.
No núcleo, seus backends de tensor e rede neural para CPU e GPU
são maduros e foram testados por anos.</p><p>Portanto, o PyTorch é bastante rápido — seja você executando redes neurais pequenas ou grandes.</p><p>O uso de memória no PyTorch é extremamente eficiente em comparação ao Torch ou algumas alternativas.
Escrevemos alocadores de memória personalizados para a GPU para garantir que
seus modelos de deep learning sejam o mais eficiente possível em memória.
Isso permite que você treine modelos de deep learning maiores do que antes.</p><h3>Extensões sem Dor</h3></p><p>Escrever novos módulos de rede neural, ou integrar com a API de Tensor do PyTorch foi projetado para ser direto
e com abstrações mínimas.</p><p>Você pode escrever novas camadas de rede neural em Python usando a API torch
<a href="https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html" target="_blank" rel="noopener noreferrer">ou suas bibliotecas favoritas baseadas em NumPy, como SciPy</a>.</p><p>Se quiser escrever suas camadas em C/C++, fornecemos uma API de extensão conveniente, eficiente e com mínimo boilerplate.
Não é necessário escrever código wrapper. Você pode ver <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank" rel="noopener noreferrer">um tutorial aqui</a> e <a href="https://github.com/pytorch/extension-cpp" target="_blank" rel="noopener noreferrer">um exemplo aqui</a>.</p><h2>Instalação</h2></p><h3>Binários</h3>
Comandos para instalar binários via Conda ou pip wheels estão em nosso site: <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">https://pytorch.org/get-started/locally/</a></p><p>
#### Plataformas NVIDIA Jetson</p><p>Wheels Python para Jetson Nano da NVIDIA, Jetson TX1/TX2, Jetson Xavier NX/AGX e Jetson AGX Orin estão disponíveis <a href="https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048" target="_blank" rel="noopener noreferrer">aqui</a> e o container L4T é publicado <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch" target="_blank" rel="noopener noreferrer">aqui</a></p><p>Eles requerem JetPack 4.2 ou superior, e <a href="https://github.com/dusty-nv" target="_blank" rel="noopener noreferrer">@dusty-nv</a> e <a href="https://github.com/ptrblck" target="_blank" rel="noopener noreferrer">@ptrblck</a> são os responsáveis pela manutenção.</p><h3>A Partir do Código-Fonte</h3></p><p>#### Pré-requisitos
Se você for instalar a partir do código-fonte, precisará de:
<ul><li>Python 3.9 ou superior</li>
<li>Um compilador que suporte totalmente C++17, como clang ou gcc (gcc 9.4.0 ou mais recente é necessário, no Linux)</li>
<li>Visual Studio ou Visual Studio Build Tool (apenas no Windows)</li></p><p></ul>\* O CI do PyTorch usa Visual C++ BuildTools, que vem com o Visual Studio Enterprise,
Professional ou Community. Você também pode instalar as ferramentas de build em
https://visualstudio.microsoft.com/visual-cpp-build-tools/. As ferramentas de build <em>não</em>
vêm com o Visual Studio Code por padrão.</p><p>Um exemplo de configuração de ambiente está abaixo:</p><ul><li>Linux:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>/bin/activate
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME></code></pre></p><ul><li>Windows:</li></p><p></ul><pre><code class="language-bash">$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat
$ conda create -y -n <CONDA_NAME>
$ conda activate <CONDA_NAME>
$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64</code></pre></p><p>##### Suporte NVIDIA CUDA
Se quiser compilar com suporte a CUDA, <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">selecione uma versão suportada de CUDA em nossa matriz de suporte</a>, depois instale o seguinte:
<ul><li><a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener noreferrer">NVIDIA CUDA</a></li>
<li><a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener noreferrer">NVIDIA cuDNN</a> v8.5 ou superior</li>
<li><a href="https://gist.github.com/ax3l/9489132" target="_blank" rel="noopener noreferrer">Compilador</a> compatível com CUDA</li></p><p></ul>Nota: Você pode consultar a <a href="https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html" target="_blank" rel="noopener noreferrer">Matriz de Suporte cuDNN</a> para versões do cuDNN com as várias versões suportadas de CUDA, driver CUDA e hardware NVIDIA</p><p>Se quiser desabilitar o suporte a CUDA, exporte a variável de ambiente <code>USE_CUDA=0</code>.
Outras variáveis de ambiente potencialmente úteis podem ser encontradas em <code>setup.py</code>.</p><p>Se estiver construindo para as plataformas Jetson da NVIDIA (Jetson Nano, TX1, TX2, AGX Xavier), as instruções para instalar o PyTorch no Jetson Nano estão <a href="https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/" target="_blank" rel="noopener noreferrer">disponíveis aqui</a></p><p>##### Suporte AMD ROCm
Se quiser compilar com suporte a ROCm, instale
<ul><li><a href="https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html" target="_blank" rel="noopener noreferrer">AMD ROCm</a> 4.0 ou superior</li>
<li>O ROCm atualmente é suportado apenas em sistemas Linux.</li></p><p></ul>Por padrão, o sistema de build espera que o ROCm esteja instalado em <code>/opt/rocm</code>. Se o ROCm estiver em outro diretório, a variável de ambiente <code>ROCM_PATH</code> deve ser definida para o diretório de instalação do ROCm. O sistema de build detecta automaticamente a arquitetura da GPU AMD. Opcionalmente, a arquitetura AMD GPU pode ser definida explicitamente com a variável de ambiente <code>PYTORCH_ROCM_ARCH</code> <a href="https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus" target="_blank" rel="noopener noreferrer">Arquitetura GPU AMD</a></p><p>Se quiser desabilitar o suporte a ROCm, exporte a variável de ambiente <code>USE_ROCM=0</code>.
Outras variáveis de ambiente potencialmente úteis podem ser encontradas em <code>setup.py</code>.</p><p>##### Suporte a GPU Intel
Se quiser compilar com suporte a GPU Intel, siga estas
<ul><li><a href="https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html" target="_blank" rel="noopener noreferrer">Pré-requisitos do PyTorch para GPUs Intel</a> instruções.</li>
<li>GPU Intel é suportada para Linux e Windows.</li></p><p></ul>Se quiser desabilitar o suporte a GPU Intel, exporte a variável de ambiente <code>USE_XPU=0</code>.
Outras variáveis de ambiente potencialmente úteis podem ser encontradas em <code>setup.py</code>.</p><p>#### Obtenha o Código-Fonte do PyTorch
<pre><code class="language-bash">git clone https://github.com/pytorch/pytorch
cd pytorch
<h1>se você estiver atualizando um checkout existente</h1>
git submodule sync
git submodule update --init --recursive</code></pre></p><p>#### Instale as Dependências</p><p><strong>Comum</strong></p><pre><code class="language-bash">conda install cmake ninja
<h1>Execute este comando do diretório do PyTorch após clonar o código-fonte usando a seção “Obtenha o Código-Fonte do PyTorch” acima</h1>
pip install -r requirements.txt</code></pre></p><p><strong>No Linux</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Somente CUDA: Adicione suporte a LAPACK para a GPU, se necessário</h1>
<h1>instalação do magma: execute com o ambiente conda ativo. especifique a versão CUDA para instalar</h1>
.ci/docker/common/install_magma_conda.sh 12.4</p><h1>(opcional) Se estiver usando torch.compile com inductor/triton, instale a versão correspondente do triton</h1>
<h1>Execute do diretório pytorch após clonar</h1>
<h1>Para suporte a GPU Intel, por favor, <code>export USE_XPU=1</code> explicitamente antes de rodar o comando.</h1>
make triton</code></pre></p><p><strong>No MacOS</strong></p><pre><code class="language-bash"># Adicione este pacote apenas em máquinas com processador intel x86
pip install mkl-static mkl-include
<h1>Adicione estes pacotes se torch.distributed for necessário</h1>
conda install pkg-config libuv</code></pre></p><p><strong>No Windows</strong></p><pre><code class="language-bash">pip install mkl-static mkl-include
<h1>Adicione estes pacotes se torch.distributed for necessário.</h1>
<h1>O suporte ao pacote Distributed no Windows é um recurso protótipo e está sujeito a alterações.</h1>
conda install -c conda-forge libuv=1.39</code></pre></p><p>#### Instale o PyTorch
<strong>No Linux</strong></p><p>Se estiver compilando para AMD ROCm, execute primeiro este comando:
<pre><code class="language-bash"># Execute isso apenas se estiver compilando para ROCm
python tools/amd_build/build_amd.py</code></pre></p><p>Instale o PyTorch
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py develop</code></pre></p><p><strong>No macOS</strong></p><pre><code class="language-bash">python3 setup.py develop</code></pre></p><p><strong>No Windows</strong></p><p>Se quiser construir código python legado, consulte <a href="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda" target="_blank" rel="noopener noreferrer">Construindo em código legado e CUDA</a></p><p><strong>Builds apenas para CPU</strong></p><p>Neste modo, os cálculos do PyTorch rodarão em sua CPU, não na GPU.</p><pre><code class="language-cmd">python setup.py develop</code></pre></p><p>Nota sobre OpenMP: A implementação desejada do OpenMP é a Intel OpenMP (iomp). Para linkar contra iomp, você precisará baixar manualmente a biblioteca e configurar o ambiente de build ajustando <code>CMAKE_INCLUDE_PATH</code> e <code>LIB</code>. A instrução <a href="https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source" target="_blank" rel="noopener noreferrer">aqui</a> é um exemplo para configurar tanto o MKL quanto o Intel OpenMP. Sem essas configurações para o CMake, o runtime OpenMP do Microsoft Visual C (vcomp) será usado.</p><p><strong>Build baseado em CUDA</strong></p><p>Neste modo, os cálculos do PyTorch utilizarão sua GPU via CUDA para processamento mais rápido</p><p><a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm" target="_blank" rel="noopener noreferrer">NVTX</a> é necessário para compilar o Pytorch com CUDA.
NVTX faz parte da distribuição CUDA, onde é chamado de "Nsight Compute". Para instalá-lo em uma instalação CUDA já existente, execute a instalação CUDA novamente e marque a caixa correspondente.
Certifique-se de que o CUDA com Nsight Compute está instalado após o Visual Studio.</p><p>Atualmente, VS 2017 / 2019 e Ninja são suportados como geradores do CMake. Se <code>ninja.exe</code> for detectado no <code>PATH</code>, Ninja será usado como gerador padrão, caso contrário, será usado VS 2017 / 2019.
<br/> Se Ninja for selecionado como gerador, o MSVC mais recente será selecionado como toolchain subjacente.</p><p>Bibliotecas adicionais como
<a href="https://developer.nvidia.com/magma" target="_blank" rel="noopener noreferrer">Magma</a>, <a href="https://github.com/oneapi-src/oneDNN" target="_blank" rel="noopener noreferrer">oneDNN, também conhecido como MKLDNN ou DNNL</a>, e <a href="https://github.com/mozilla/sccache" target="_blank" rel="noopener noreferrer">Sccache</a> são frequentemente necessárias. Consulte o <a href="https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers" target="_blank" rel="noopener noreferrer">installation-helper</a> para instalá-las.</p><p>Você pode consultar o script <a href="https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat" target="_blank" rel="noopener noreferrer">build_pytorch.bat</a> para outras configurações de variáveis de ambiente</p><pre><code class="language-cmd">cmd</p><p>:: Defina as variáveis de ambiente depois de baixar e descompactar o pacote mkl,
:: senão o CMake retornará erro como <code>Could NOT find OpenMP</code>.
set CMAKE_INCLUDE_PATH={Seu diretório}\mkl\include
set LIB={Seu diretório}\mkl\lib;%LIB%</p><p>:: Leia atentamente o conteúdo na seção anterior antes de prosseguir.
:: [Opcional] Se quiser sobrescrever o toolset usado pelo Ninja e Visual Studio com CUDA, execute o seguinte bloco de script.
:: O "Prompt de Comando do Desenvolvedor para Visual Studio 2019" será executado automaticamente.
:: Certifique-se de ter CMake >= 3.12 antes de fazer isso ao usar o gerador Visual Studio.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f "usebackq tokens=<em>" %i in (<code>"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products </em> -latest -property installationPath</code>) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%</p><p>:: [Opcional] Se quiser sobrescrever o compilador host CUDA
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe</p><p>python setup.py develop
</code></pre></p><p><strong>Builds para GPU Intel</strong></p><p>Neste modo o PyTorch com suporte a GPU Intel será construído.</p><p>Certifique-se de que <a href="#prerequisites" target="_blank" rel="noopener noreferrer">os pré-requisitos comuns</a> bem como <a href="#intel-gpu-support" target="_blank" rel="noopener noreferrer">os pré-requisitos para GPU Intel</a> estejam devidamente instalados e as variáveis de ambiente configuradas antes de iniciar a build. Para suporte à ferramenta de build, <code>Visual Studio 2022</code> é obrigatório.</p><p>Então o PyTorch pode ser construído com o comando:</p><pre><code class="language-cmd">:: Comandos CMD:
:: Defina o CMAKE_PREFIX_PATH para ajudar a encontrar os pacotes correspondentes
:: %CONDA_PREFIX% só funciona após <code>conda activate custom_env</code></p><p>if defined CMAKE_PREFIX_PATH (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"
) else (
    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"
)</p><p>python setup.py develop</code></pre></p><p>##### Ajustar Opções de Build (Opcional)</p><p>Você pode ajustar a configuração das variáveis do cmake opcionalmente (sem buildar primeiro), fazendo
o seguinte. Por exemplo, ajustar os diretórios pré-detectados para CuDNN ou BLAS pode ser feito
com tal passo.</p><p>No Linux
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
python setup.py build --cmake-only
ccmake build  # ou cmake-gui build</code></pre></p><p>No macOS
<pre><code class="language-bash">export CMAKE_PREFIX_PATH="${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}"
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # ou cmake-gui build</code></pre></p><h3>Imagem Docker</h3></p><p>#### Usando imagens pré-construídas</p><p>Você também pode puxar uma imagem docker pré-construída do Docker Hub e rodar com docker v19.03+</p><pre><code class="language-bash">docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest</code></pre></p><p>Observe que o PyTorch usa memória compartilhada para compartilhar dados entre processos, então se o torch multiprocessing for utilizado (por exemplo,
para data loaders multithread), o tamanho padrão do segmento de memória compartilhada que o container executa não é suficiente, e você
deve aumentar o tamanho da memória compartilhada usando as opções de linha de comando <code>--ipc=host</code> ou <code>--shm-size</code> ao rodar o <code>nvidia-docker run</code>.</p><p>#### Construindo a imagem você mesmo</p><p><strong>NOTA:</strong> Deve ser construída com uma versão do docker > 18.06</p><p>O <code>Dockerfile</code> é fornecido para construir imagens com suporte a CUDA 11.1 e cuDNN v8.
Você pode passar a variável make <code>PYTHON_VERSION=x.y</code> para especificar qual versão do Python será usada pelo Miniconda, ou deixar
em branco para usar o padrão.</p><pre><code class="language-bash">make -f docker.Makefile
<h1>imagens são marcadas como docker.io/${seu_usuario_docker}/pytorch</code></pre></h1></p><p>Você também pode passar a variável de ambiente <code>CMAKE_VARS="..."</code> para especificar variáveis CMake adicionais a serem passadas ao CMake durante a build.
Veja <a href="./setup.py" target="_blank" rel="noopener noreferrer">setup.py</a> para a lista de variáveis disponíveis.</p><pre><code class="language-bash">make -f docker.Makefile</code></pre></p><h3>Construindo a Documentação</h3></p><p>Para construir a documentação em vários formatos, você precisará do <a href="http://www.sphinx-doc.org" target="_blank" rel="noopener noreferrer">Sphinx</a>
e do pytorch_sphinx_theme2.</p><p>Antes de construir a documentação localmente, certifique-se de que o <code>torch</code> esteja
instalado em seu ambiente. Para pequenas correções, você pode instalar a
versão nightly conforme descrito em <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">Primeiros Passos</a>.</p><p>Para correções mais complexas, como adicionar um novo módulo e docstrings para
o novo módulo, talvez você precise instalar o torch <a href="#from-source" target="_blank" rel="noopener noreferrer">a partir do código-fonte</a>.
Veja as <a href="https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines" target="_blank" rel="noopener noreferrer">Diretrizes para Docstrings</a>
para convenções de docstring.</p><pre><code class="language-bash">cd docs/
pip install -r requirements.txt
make html
make serve</code></pre></p><p>Execute <code>make</code> para obter uma lista de todos os formatos de saída disponíveis.</p><p>Se receber um erro do katex execute <code>npm install katex</code>. Se persistir, tente
<code>npm install -g katex</code></p><blockquote>[!NOTA]</blockquote>
<blockquote>Se você instalou <code>nodejs</code> com um gerenciador de pacotes diferente (por exemplo,</blockquote>
<blockquote><code>conda</code>) então <code>npm</code> provavelmente instalará uma versão do <code>katex</code> que não é</blockquote>
<blockquote>compatível com sua versão do <code>nodejs</code> e a construção da documentação falhará.</blockquote>
<blockquote>Uma combinação de versões que se sabe funcionar é <code>node@6.13.1</code> e</blockquote>
<blockquote><code>katex@0.13.18</code>. Para instalar a última com <code>npm</code> você pode executar</blockquote>
<blockquote>``<code>npm install -g katex@0.13.18<pre><code class="language-"></blockquote>
<blockquote>[!NOTA]</blockquote>
<blockquote>Se você vir um erro de incompatibilidade do numpy, execute:</blockquote>
<blockquote></code>`<code></blockquote>
<blockquote>pip install 'numpy<2'</blockquote>
<blockquote></code>`<code></blockquote></p><p>Quando fizer alterações nas dependências executadas pelo CI, edite o
arquivo </code>.ci/docker/requirements-docs.txt<code>.</p><p>#### Gerando um PDF</p><p>Para compilar um PDF de toda a documentação do PyTorch, certifique-se de ter
</code>texlive<code> e LaTeX instalados. No macOS, você pode instalá-los usando:
</code></pre>
brew install --cask mactex
</code>`<code></p><p>Para criar o PDF:</p><ul><li>Execute:</li></p><p>   </ul></code>`<code>
   make latexpdf
   </code>`<code></p><p>   Isso irá gerar os arquivos necessários no diretório </code>build/latex<code>.</p><ul><li>Navegue até esse diretório e execute:</li></p><p>   </ul></code>`<code>
   make LATEXOPTS="-interaction=nonstopmode"
   </code>`<code></p><p>   Isso irá produzir um </code>pytorch.pdf` com o conteúdo desejado. Execute esse
   comando mais uma vez para gerar o índice e o sumário corretos.</p><blockquote>[!NOTA]</blockquote>
<blockquote>Para ver o Sumário, mude para a visualização <strong>Sumário</strong></blockquote>
<blockquote>em seu visualizador de PDF.</blockquote></p><h3>Versões Anteriores</h3></p><p>Instruções de instalação e binários para versões anteriores do PyTorch podem ser encontrados
em <a href="https://pytorch.org/get-started/previous-versions" target="_blank" rel="noopener noreferrer">nosso site</a>.</p><h2>Primeiros Passos</h2></p><p>Três indicações para você começar:
<ul><li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriais: para você começar a entender e usar o PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Exemplos: código PyTorch fácil de entender em todos os domínios</a></li>
<li><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer">Referência da API</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md" target="_blank" rel="noopener noreferrer">Glossário</a></li></p><p></ul><h2>Recursos</h2></p><ul><li><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch.org</a></li>
<li><a href="https://pytorch.org/tutorials/" target="_blank" rel="noopener noreferrer">Tutoriais PyTorch</a></li>
<li><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener noreferrer">Exemplos PyTorch</a></li>
<li><a href="https://pytorch.org/hub/" target="_blank" rel="noopener noreferrer">Modelos PyTorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener noreferrer">Introdução ao Deep Learning com PyTorch da Udacity</a></li>
<li><a href="https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229" target="_blank" rel="noopener noreferrer">Introdução ao Machine Learning com PyTorch da Udacity</a></li>
<li><a href="https://www.coursera.org/learn/deep-neural-networks-with-pytorch" target="_blank" rel="noopener noreferrer">Deep Neural Networks with PyTorch da Coursera</a></li>
<li><a href="https://twitter.com/PyTorch" target="_blank" rel="noopener noreferrer">Twitter PyTorch</a></li>
<li><a href="https://pytorch.org/blog/" target="_blank" rel="noopener noreferrer">Blog PyTorch</a></li>
<li><a href="https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw" target="_blank" rel="noopener noreferrer">YouTube PyTorch</a></li></p><p></ul><h2>Comunicação</h2>
<ul><li>Fóruns: Discuta implementações, pesquisas, etc. https://discuss.pytorch.org</li>
<li>GitHub Issues: Relatórios de bugs, solicitações de recursos, problemas de instalação, RFCs, ideias, etc.</li>
<li>Slack: O <a href="https://pytorch.slack.com/" target="_blank" rel="noopener noreferrer">Slack do PyTorch</a> reúne principalmente usuários e desenvolvedores moderados a experientes do PyTorch para bate-papo, discussões online, colaboração, etc. Se você é iniciante procurando ajuda, o principal canal é o <a href="https://discuss.pytorch.org" target="_blank" rel="noopener noreferrer">Fórum PyTorch</a>. Se precisar de convite para o slack, preencha este formulário: https://goo.gl/forms/PP1AGvNHpSaJP8to1</li>
<li>Newsletter: Sem spam, um boletim informativo de mão única com anúncios importantes sobre PyTorch. Inscreva-se aqui: https://eepurl.com/cbG0rv</li>
<li>Página no Facebook: Anúncios importantes sobre PyTorch. https://www.facebook.com/pytorch</li>
<li>Para diretrizes de marca, visite nosso site em <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">pytorch.org</a></li></p><p></ul><h2>Lançamentos e Contribuição</h2></p><p>Normalmente, o PyTorch tem três lançamentos menores por ano. Por favor, nos avise se encontrar um bug <a href="https://github.com/pytorch/pytorch/issues" target="_blank" rel="noopener noreferrer">abrindo uma issue</a>.</p><p>Agradecemos todas as contribuições. Se você planeja contribuir com correções de bugs, faça isso sem necessidade de discussão prévia.</p><p>Se você planeja contribuir com novos recursos, funções utilitárias ou extensões ao core, por favor, abra primeiro uma issue e discuta o recurso conosco.
Enviar um PR sem discussão pode resultar em rejeição, pois podemos estar levando o core em uma direção diferente da que você imagina.</p><p>Para saber mais sobre como contribuir com o Pytorch, veja nossa <a href="CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">página de contribuição</a>. Para mais informações sobre os lançamentos do PyTorch, veja a <a href="RELEASE.md" target="_blank" rel="noopener noreferrer">página de lançamentos</a>.</p><h2>O Time</h2></p><p>PyTorch é um projeto conduzido pela comunidade com vários engenheiros e pesquisadores talentosos contribuindo para ele.</p><p>O PyTorch é atualmente mantido por <a href="http://soumith.ch" target="_blank" rel="noopener noreferrer">Soumith Chintala</a>, <a href="https://github.com/gchanan" target="_blank" rel="noopener noreferrer">Gregory Chanan</a>, <a href="https://github.com/dzhulgakov" target="_blank" rel="noopener noreferrer">Dmytro Dzhulgakov</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a> e <a href="https://github.com/malfet" target="_blank" rel="noopener noreferrer">Nikita Shulga</a>, com grandes contribuições de centenas de pessoas talentosas de várias formas e meios.
Uma lista não exaustiva mas crescente inclui: <a href="https://github.com/killeent" target="_blank" rel="noopener noreferrer">Trevor Killeen</a>, <a href="https://github.com/chsasank" target="_blank" rel="noopener noreferrer">Sasank Chilamkurthy</a>, <a href="https://github.com/szagoruyko" target="_blank" rel="noopener noreferrer">Sergey Zagoruyko</a>, <a href="https://github.com/adamlerer" target="_blank" rel="noopener noreferrer">Adam Lerer</a>, <a href="https://github.com/fmassa" target="_blank" rel="noopener noreferrer">Francisco Massa</a>, <a href="https://github.com/alykhantejani" target="_blank" rel="noopener noreferrer">Alykhan Tejani</a>, <a href="https://github.com/lantiga" target="_blank" rel="noopener noreferrer">Luca Antiga</a>, <a href="https://github.com/albanD" target="_blank" rel="noopener noreferrer">Alban Desmaison</a>, <a href="https://github.com/andreaskoepf" target="_blank" rel="noopener noreferrer">Andreas Koepf</a>, <a href="https://github.com/jekbradbury" target="_blank" rel="noopener noreferrer">James Bradbury</a>, <a href="https://github.com/ebetica" target="_blank" rel="noopener noreferrer">Zeming Lin</a>, <a href="https://github.com/yuandong-tian" target="_blank" rel="noopener noreferrer">Yuandong Tian</a>, <a href="https://github.com/glample" target="_blank" rel="noopener noreferrer">Guillaume Lample</a>, <a href="https://github.com/Maratyszcza" target="_blank" rel="noopener noreferrer">Marat Dukhan</a>, <a href="https://github.com/ngimel" target="_blank" rel="noopener noreferrer">Natalia Gimelshein</a>, <a href="https://github.com/csarofeen" target="_blank" rel="noopener noreferrer">Christian Sarofeen</a>, <a href="https://github.com/martinraison" target="_blank" rel="noopener noreferrer">Martin Raison</a>, <a href="https://github.com/ezyang" target="_blank" rel="noopener noreferrer">Edward Yang</a>, <a href="https://github.com/zdevito" target="_blank" rel="noopener noreferrer">Zachary Devito</a>.</p><p>Nota: Este projeto não está relacionado ao <a href="https://github.com/hughperkins/pytorch" target="_blank" rel="noopener noreferrer">hughperkins/pytorch</a> de mesmo nome. Hugh é um valioso contribuidor da comunidade Torch e ajudou com muitas coisas no Torch e PyTorch.</p><h2>Licença</h2></p><p>O PyTorch tem uma licença no estilo BSD, conforme encontrado no arquivo <a href="LICENSE" target="_blank" rel="noopener noreferrer">LICENSE</a>.

---

<a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Powered By OpenAiTx</a>

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/pytorch/pytorch/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>