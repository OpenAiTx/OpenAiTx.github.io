<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - Read ax documentation in German. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read ax documentation in German. This project has 0 stars on GitHub.">
    <meta name="keywords" content="ax, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ax",
  "description": "Read ax documentation in German. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "ax-llm"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/ax-llm/ax/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/ax-llm/ax/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/ax-llm/ax" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ax
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">German</span>
                <span>by ax-llm</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Ax, DSPy für Typescript</h1></p><p>Die Arbeit mit LLMs ist komplex – sie tun nicht immer das, was man möchte. DSPy macht es einfacher, beeindruckende Anwendungen mit LLMs zu entwickeln. Definieren Sie einfach Ihre Eingaben und Ausgaben (Signatur), und ein effizienter Prompt wird automatisch generiert und verwendet. Verbinden Sie verschiedene Signaturen, um komplexe Systeme und Workflows mit LLMs zu bauen.</p><p>Und damit Sie Ax wirklich produktiv einsetzen können, liefern wir alles Weitere mit, was Sie benötigen: Observability, Streaming, Unterstützung für weitere Modalitäten (Bilder, Audio, etc.), Fehlerkorrektur, Multi-Step-Funktionsaufrufe, MCP, RAG und vieles mehr.</p><p><a href="https://www.npmjs.com/package/@ax-llm/ax" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green" alt="NPM Package"></a>
<a href="https://discord.gg/DSHg3dU7dW" target="_blank" rel="noopener noreferrer"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat"></a>
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red" alt="Twitter"></a></p><p><!-- header --></p><h2>Warum Ax verwenden?</h2></p><ul><li>Standardisierte Schnittstelle für alle führenden LLMs</li>
<li>Prompts aus einfachen Signaturen kompiliert</li>
<li>Volles, natives End-to-End-Streaming</li>
<li>Unterstützung für "Thinking Budget" und "Thought Tokens"</li>
<li>Baue Agenten, die andere Agenten aufrufen können</li>
<li>Eingebaute MCP (Model Context Protocol) Unterstützung</li>
<li>Dokumente beliebigen Formats in Text umwandeln</li>
<li>RAG, intelligentes Chunking, Embedding, Abfragen</li>
<li>Funktioniert mit Vercel AI SDK</li>
<li>Ausgabevalidierung während des Streamings</li>
<li>Multi-modales DSPy unterstützt</li>
<li>Automatisches Prompt-Tuning mit Optimierern</li>
<li>OpenTelemetry Tracing / Observability</li>
<li>Produktionsreifer Typescript-Code</li>
<li>Leichtgewichtig, keine Abhängigkeiten</li></p><p></ul><h2>Produktionsreif</h2></p><ul><li>Keine Breaking Changes (Minor Versions)</li>
<li>Hohe Testabdeckung</li>
<li>Eingebaute Open Telemetry <code>gen_ai</code> Unterstützung</li>
<li>Wird produktiv von vielen Startups genutzt</li></p><p></ul><h2>Was ist eine Prompt-Signatur?</h2></p><p><img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b"></p><p>Effiziente, typsichere Prompts werden aus einer einfachen Signatur automatisch generiert. Eine Prompt-Signatur besteht aus
<code>"task description" inputField:type "field description" -> "outputField:type</code>.
Die Idee der Prompt-Signaturen basiert auf Arbeiten aus dem Paper
"Demonstrate-Search-Predict".</p><p>Es sind mehrere Eingabe- und Ausgabefelder möglich, jedes Feld kann die Typen <code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>, <code>class "class1, class2"</code>, <code>JSON</code> oder ein Array davon, z.B. <code>string[]</code>, haben. Ist kein Typ definiert, wird <code>string</code> als Standard verwendet. Das Suffix <code>?</code> macht das Feld optional (Standard: erforderlich), <code>!</code> macht das Feld intern, was z. B. für Reasoning nützlich ist.</p><h2>Ausgabefeld-Typen</h2></p><p>| Typ                      | Beschreibung                         | Verwendung                  | Beispielausgabe                                     |
| ------------------------ | ------------------------------------ | --------------------------- | --------------------------------------------------- |
| <code>string</code>                 | Eine Zeichenkette                    | <code>fullName:string</code>           | <code>"example"</code>                                         |
| <code>number</code>                 | Ein numerischer Wert                 | <code>price:number</code>              | <code>42</code>                                                |
| <code>boolean</code>                | Wahrheitswert                        | <code>isEvent:boolean</code>           | <code>true</code>, <code>false</code>                                     |
| <code>date</code>                   | Ein Datum                            | <code>startDate:date</code>            | <code>"2023-10-01"</code>                                      |
| <code>datetime</code>               | Datum und Uhrzeit                    | <code>createdAt:datetime</code>        | <code>"2023-10-01T12:00:00Z"</code>                            |
| <code>class "class1,class2"</code>  | Klassifizierung von Items            | <code>category:class</code>            | <code>["class1", "class2", "class3"]</code>                    |
| <code>string[]</code>               | Array von Zeichenketten              | <code>tags:string[]</code>             | <code>["example1", "example2"]</code>                          |
| <code>number[]</code>               | Array von Zahlen                     | <code>scores:number[]</code>           | <code>[1, 2, 3]</code>                                         |
| <code>boolean[]</code>              | Array von Wahrheitswerten            | <code>permissions:boolean[]</code>     | <code>[true, false, true]</code>                               |
| <code>date[]</code>                 | Array von Daten                      | <code>holidayDates:date[]</code>       | <code>["2023-10-01", "2023-10-02"]</code>                      |
| <code>datetime[]</code>             | Array von Datum und Uhrzeit          | <code>logTimestamps:datetime[]</code>  | <code>["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]</code>  |
| <code>class[] "class1,class2"</code>| Mehrere Klassen                      | <code>categories:class[]</code>        | <code>["class1", "class2", "class3"]</code>                    |
| <code>code "language"</code>        | Codeblock in einer Sprache           | <code>code:code "python"</code>        | <code>print('Hello, world!')</code>                            |</p><h2>Unterstützte LLMs</h2></p><p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>, <code>Hugging Face</code></p><h2>Installation</h2></p><pre><code class="language-bash">npm install @ax-llm/ax
<h1>oder</h1>
yarn add @ax-llm/ax</code></pre></p><h2>Beispiel: Chain-of-Thought zum Zusammenfassen von Text</h2></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'</p><p>const textToSummarize = <code>
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...</code></p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>const gen = new AxChainOfThought(
  <code>textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward(ai, { textToSummarize })</p><p>console.log('>', res)</code></pre></p><h2>Beispiel: Einen Agenten bauen</h2></p><p>Nutzen Sie das Agent-Prompt-Framework, um Agenten zu bauen, die mit anderen Agenten zusammenarbeiten, um Aufgaben zu erledigen. Agenten sind mit Prompt-Signaturen einfach zu erstellen. Probieren Sie das Agenten-Beispiel aus.</p><pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts</p><p>const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: <code>physicsQuestion "physics questions" -> answer "reply in bullet points"</code>
});</p><p>const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: <code>text "text so summarize" -> shortSummary "summarize in 5 to 10 words"</code>
});</p><p>const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: <code>question -> answer</code>,
  agents: [researcher, summarizer]
});</p><p>agent.forward(ai, { questions: "How many atoms are there in the universe" })</code></pre></p><h2>Unterstützung für Thinking Models</h2></p><p>Ax bietet native Unterstützung für Modelle mit "Thinking"-Fähigkeiten, sodass Sie das Token-Budget für das Denken steuern und auf die Überlegungen des Modells zugreifen können. Diese Funktion hilft dabei, den Denkprozess des Modells nachzuvollziehen und den Token-Verbrauch zu optimieren.</p><pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})</p><p>// Oder Thinking-Budget pro Anfrage steuern
const gen = new AxChainOfThought(<code>question -> answer</code>)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', oder 'high'
)</p><p>// Gedanken aus der Antwort auslesen
console.log(res.thoughts) // Zeigt den Denkprozess des Modells</code></pre></p><h2>Unterstützte Vektor-Datenbanken</h2></p><p>Vektor-Datenbanken sind entscheidend für LLM-Workflows. Wir bieten saubere Abstraktionen über populäre Vektor-Datenbanken und eine schnelle In-Memory-Variante.</p><p>| Anbieter    | Getestet |
| ----------- | -------- |
| In Memory   | 🟢 100%  |
| Weaviate    | 🟢 100%  |
| Cloudflare  | 🟡 50%   |
| Pinecone    | 🟡 50%   |</p><pre><code class="language-typescript">// Erstellen von Embeddings aus Text mit einem LLM
const ret = await this.ai.embed({ texts: 'hello world' })</p><p>// In-Memory-Vektor-DB erstellen
const db = new axDB('memory')</p><p>// In die Vektor-DB einfügen
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})</p><p>// Ähnliche Einträge mit Embeddings abfragen
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})</code></pre></p><p>Alternativ können Sie den <code>AxDBManager</code> nutzen, der intelligentes Chunking, Embedding und Abfrage für Sie übernimmt.</p><pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)</code></pre></p><h2>RAG-Dokumente</h2></p><p>Dokumente wie PDF, DOCX, PPT, XLS, etc. mit LLMs zu nutzen ist mühsam. Wir machen es mit Apache Tika, einer Open-Source-Dokumenten-Engine, einfach.</p><p>Apache Tika starten:</p><pre><code class="language-shell">docker run -p 9998:9998 apache/tika</code></pre></p><p>Dokumente zu Text konvertieren und mit dem <code>AxDBManager</code> für Retrieval einbetten. Es sind auch ein Reranker und Query-Rewriter integriert (<code>AxDefaultResultReranker</code> und <code>AxDefaultQueryRewriter</code>).</p><pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')</p><p>const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query('Find some text')
console.log(matches)</code></pre></p><h2>Multi-modal DSPy</h2></p><p>Bei Modellen wie <code>GPT-4o</code> und <code>Gemini</code>, die Multi-Modal Prompts unterstützen, können Bildfelder verwendet werden – funktioniert mit der gesamten DSP-Pipeline.</p><pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')</p><p>const gen = new AxChainOfThought(<code>question, animalImage:image -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})</code></pre></p><p>Mit Modellen wie <code>gpt-4o-audio-preview</code>, die Audio unterstützen, sind auch Audiofelder möglich:</p><pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')</p><p>const gen = new AxGen(<code>question, commentAudio:audio -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})</code></pre></p><h2>DSPy Chat API</h2></p><p>Inspiriert von DSPys Demonstration Weaving bietet Ax <code>AxMessage</code> für nahtloses Management des Gesprächsverlaufs. So können Sie Chatbots bauen, die den Kontext über mehrere Turns behalten und die volle Power der Prompt-Signaturen nutzen. Details siehe Beispiel.</p><pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts</code></pre></p><pre><code class="language-typescript">const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  <code>message:string "A casual message from the user" -> reply:string "A friendly, casual response"</code>
)</p><p>await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])</code></pre></p><p>Die Gesprächshistorie wird automatisch in den Prompt eingebunden, wodurch das Modell den Kontext behält und zusammenhängende Antworten gibt. Funktioniert mit allen Ax-Features wie Streaming, Funktionsaufrufen, Chain-of-Thought.</p><h2>Streaming</h2></p><h3>Assertions</h3></p><p>Wir unterstützen das Parsen von Ausgabefeldern und Funktionsausführung während des Streamings. Das erlaubt Fail-Fast und Fehlerkorrektur, spart Tokens, Kosten und reduziert Latenz. Assertions sind eine mächtige Methode, um die Einhaltung Ihrer Anforderungen sicherzustellen – auch beim Streaming.</p><pre><code class="language-typescript">// Prompt-Programm aufsetzen
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>// Assertion hinzufügen, dass die Zahl 5 nicht enthalten sein darf
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')</p><p>// Programm mit aktiviertem Streaming ausführen
const res = await gen.forward({ startNumber: 1 }, { stream: true })</p><p>// Oder End-to-End-Streaming
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}</code></pre></p><p>Sie können ganze Ausgabefelder während des Streamings validieren. Die Validierung wird ausgelöst, sobald das Feld komplett ist. Für echte Validierung schon während des Streamings siehe folgendes Beispiel – das steigert Performance und spart Tokens in der Produktion.</p><pre><code class="language-typescript">// Assertion für jede Zeile: muss mit Zahl und Punkt beginnen
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./</p><p>    // Zeilen splitten, trimmen, leere Zeilen filtern, alle auf Regex prüfen
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)</p><p>// Programm mit aktiviertem Streaming ausführen
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)</code></pre></p><h3>Field Processors</h3></p><p>Field Processors verarbeiten Felder in einem Prompt vor der Übergabe an das LLM.</p><pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>const streamValue = false</p><p>const processorFunction = (value) => {
  return value.map((x) => x + 1)
}</p><p>// Feld-Processor zum Programm hinzufügen
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)</p><p>const res = await gen.forward({ startNumber: 1 })</code></pre></p><h2>Model Context Protocol (MCP)</h2></p><p>Ax integriert nahtlos das Model Context Protocol (MCP), sodass Ihre Agenten externe Tools und Ressourcen über eine standardisierte Schnittstelle erreichen.</p><h3>Verwendung von AxMCPClient</h3></p><p>Mit <code>AxMCPClient</code> können Sie sich mit jedem MCP-kompatiblen Server verbinden und dessen Funktionen in Ax-Agenten nutzen:</p><pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'</p><p>// MCP-Client mit Transport initialisieren
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})</p><p>// Client mit optionalem Debug-Modus erstellen
const client = new AxMCPClient(transport, { debug: true })</p><p>// Verbindung initialisieren
await client.init()</p><p>// Funktionen des Clients im Agenten verwenden
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Client als Funktionsanbieter angeben
})</p><p>// Oder Client mit AxGen verwenden
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})</code></pre></p><h3>AxMCPClient mit Remote-Server verwenden</h3></p><p>Einen Remote-MCP-Server mit Ax anzusprechen ist unkompliziert. Beispiel: DeepWiki MCP-Server für Fragen zu öffentlichen GitHub-Repos (<code>https://mcp.deepwiki.com/mcp</code>).</p><pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'</p><p>// 1. MCP-Transport zum DeepWiki-Server initialisieren
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)</p><p>// 2. MCP-Client erstellen
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Verbindung initialisieren</p><p>// 3. AI-Modell initialisieren (z.B. OpenAI)
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// 4. AxAgent, der den MCP-Client nutzt
const deepwikiAgent = new AxAgent<
  {
    // Input-Typen passend zur DeepWiki-Funktion
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // MCP-Client als Funktion liefern
})</p><p>// 5. Frage formulieren und Agenten aufrufen
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax',
})
console.log('DeepWiki Answer:', result.answer)</code></pre></p><p>Dieses Beispiel zeigt, wie Sie einen öffentlichen MCP-Server mit einem Ax-Agenten nutzen. Die Signatur (<code>questionAboutRepo, githubRepositoryUrl -> answer</code>) ist ein Beispiel. Die verfügbaren Funktionen und Signaturen erfahren Sie vom MCP-Server selbst (z.B. via <code>mcp.getFunctions</code> oder Dokumentation).</p><p>Für komplexere Beispiele mit Authentifizierung und benutzerdefinierten Headern siehe die Datei <code>src/examples/mcp-client-pipedream.ts</code> in diesem Repository.</p><h2>AI Routing und Load Balancing</h2></p><p>Ax bietet zwei leistungsstarke Methoden zur Nutzung mehrerer AI-Services: einen Load Balancer für hohe Verfügbarkeit und einen Router für modell-spezifisches Routing.</p><h3>Load Balancer</h3></p><p>Der Load Balancer verteilt Anfragen automatisch auf mehrere AI-Services basierend auf Performance und Verfügbarkeit. Fällt ein Service aus, wird automatisch zum nächsten gewechselt.</p><pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'</p><p>// Mehrere AI-Services aufsetzen
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// Load Balancer mit allen Services erstellen
const balancer = new AxBalancer([openai, ollama, gemini])</p><p>// Wie einen normalen AI-Service verwenden
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})</p><p>// Oder mit AxGen verwenden
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(balancer, { question: 'Hello!' })</code></pre></p><h3>Multi-Service Router</h3></p><p>Der Router ermöglicht die Nutzung mehrerer AI-Services über eine Schnittstelle und routet Anfragen modellbasiert.</p><pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'</p><p>// OpenAI mit Modellen
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})</p><p>// Gemini mit Modellen
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}</p><p>// Router mit allen Services erstellen
const router = new AxMultiServiceRouter([openai, gemini, secretService])</p><p>// Routing zu OpenAIs Expert-Modell
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})</p><p>// Oder mit AxGen verwenden
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(router, { question: 'Hello!' })</code></pre></p><p>Der Load Balancer ist ideal für Hochverfügbarkeit, der Router für modell-spezifische Aufgaben. Beide lassen sich mit allen Ax-Features wie Streaming, Funktionsaufrufen und Chain-of-Thought kombinieren.</p><p>Balancer und Router können auch zusammen eingesetzt werden: Mehrere Balancer mit dem Router, oder umgekehrt.</p><h2>OpenTelemetry-Unterstützung</h2></p><p>Tracing und Observability Ihres LLM-Workflows sind zentral für den Betrieb in der Produktion. OpenTelemetry ist Standard und wir unterstützen das <code>gen_ai</code>-Attribut-Namespace. Siehe <code>src/examples/telemetry.ts</code> für mehr Informationen.</p><pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'</p><p>const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)</p><p>const tracer = trace.getTracer('test')</p><p>const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})</p><p>const gen = new AxChainOfThought(
  ai,
  <code>text -> shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward({ text })</code></pre></p><pre><code class="language-json">{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}</code></pre></p><h2>Prompt-Tuning (Basic)</h2></p><p>Optimieren Sie Ihre Prompts mit einem größeren Modell für effizientere und bessere Ergebnisse. Dies geschieht mit einem Optimierer wie <code>AxBootstrapFewShot</code> und Beispielen, z.B. aus dem <code>HotPotQA</code>-Datensatz. Der Optimierer generiert Demonstrationen (<code>demos</code>), die die Effizienz der Prompts steigern.</p><pre><code class="language-typescript">// HotPotQA-Datensatz von Huggingface laden
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})</p><p>const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})</p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// Programm zum Tuning aufsetzen
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  <code>question -> answer "in short 2 or 3 words"</code>
)</p><p>// Bootstrap Few Shot Optimierer zum Tuning
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})</p><p>// Evaluations-Metrik em, f1 Score ist beliebt für Retrieval-Performance
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)</p><p>// Optimierer ausführen und Ergebnis speichern
const result = await optimize.compile(metricFn);</p><p>// Demos in Datei speichern
// import fs from 'fs';
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');</code></pre></p><p><img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
<pre><code class="language-">
<h2>Prompt-Tuning (Fortgeschritten, Mipro v2)</h2></p><p>MiPRO v2 ist ein fortgeschrittenes Prompt-Optimierungs-Framework, das Bayes'sche Optimierung nutzt, um automatisch die besten Instruktionen, Demonstrationen und Beispiele für Ihre LLM-Programme zu finden. Durch systematisches Testen verschiedener Prompt-Konfigurationen maximiert MiPRO v2 die Modell-Performance ohne manuelles Tuning. </p><h3>Hauptmerkmale</h3></p><ul><li><strong>Instruktionsoptimierung</strong>: Generiert und testet automatisch mehrere Instruktionskandidaten</li>
<li><strong>Few-Shot-Beispielauswahl</strong>: Findet optimale Demonstrationen aus Ihrem Datensatz</li>
<li><strong>Bayes'sche Optimierung</strong>: Nutzt UCB-Strategie für effiziente Exploration</li>
<li><strong>Early Stopping</strong>: Stoppt Optimierung, wenn Verbesserungen stagnieren</li>
<li><strong>Programm- und datenbewusst</strong>: Berücksichtigt Programmstruktur und Datensatz</li></p><p></ul><h3>Funktionsweise</h3></p><ul><li>Generiert verschiedene Instruktionskandidaten</li>
<li>Bootstrapped Few-Shot-Beispiele aus Ihren Daten</li>
<li>Wählt gelabelte Beispiele direkt aus dem Datensatz</li>
<li>Findet mit Bayes'scher Optimierung die optimale Kombination</li>
<li>Setzt die beste Konfiguration im Programm ein</li></p><p></ul><h3>Grundlegende Nutzung</h3>
</code></pre>typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'</p><p>// 1. AI-Service aufsetzen
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// 2. Programm erstellen
const program = new AxChainOfThought(<code>input -> output</code>)</p><p>// 3. Optimierer konfigurieren
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData,
  options: {
    numTrials: 20, // Anzahl Konfigurationen
    auto: 'medium',
  },
})</p><p>// 4. Evaluationsmetrik definieren
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}</p><p>// 5. Optimierung ausführen
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// 6. Optimiertes Programm nutzen
const result = await optimizedProgram.forward(ai, { input: 'test input' })
<pre><code class="language-">
<h3>Konfigurationsoptionen</h3></p><p>MiPRO v2 bietet umfangreiche Konfigurationsmöglichkeiten:</p><p>| Option                    | Beschreibung                                    | Standard |
| ------------------------- | ----------------------------------------------- | -------- |
| <code>numCandidates</code>           | Anzahl Instruktionskandidaten                   | 5        |
| <code>numTrials</code>               | Optimierungsdurchläufe                          | 30       |
| <code>maxBootstrappedDemos</code>    | Max. Anzahl bootstrapped Demos                  | 3        |
| <code>maxLabeledDemos</code>         | Max. Anzahl gelabelter Beispiele                | 4        |
| <code>minibatch</code>               | Minibatching für schnellere Auswertung          | true     |
| <code>minibatchSize</code>           | Größe der Minibatches                           | 25       |
| <code>earlyStoppingTrials</code>     | Stoppt nach N Durchläufen ohne Verbesserung     | 5        |
| <code>minImprovementThreshold</code> | Minimale Verbesserung für Score                 | 0.01     |
| <code>programAwareProposer</code>    | Programmstruktur für bessere Vorschläge nutzen  | true     |
| <code>dataAwareProposer</code>       | Berücksichtigt Datensatz-Charakteristika        | true     |
| <code>verbose</code>                 | Zeigt detaillierten Fortschritt                 | false    |
| abort-patterns.ts | Beispiel, wie Requests abgebrochen werden können      |</p><h3>Optimierungslevel</h3></p><p>Mit dem <code>auto</code>-Parameter lässt sich die Optimierungsintensität einstellen:
</code></pre>typescript
// Light (schneller, weniger gründlich)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })</p><p>// Medium (ausgewogen)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })</p><p>// Heavy (langsamer, sehr gründlich)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
<pre><code class="language-">
<h3>Fortgeschrittenes Beispiel: Sentimentanalyse</h3>
</code></pre>typescript
// Sentimentanalyse-Programm erstellen
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(<code>productReview -> label:string "positive" or "negative"</code>)</p><p>// Optimierer mit erweiterten Einstellungen
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})</p><p>// Optimierung ausführen und Ergebnis speichern
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// Konfiguration für später speichern
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
<pre><code class="language-">
<h2>Nutzung der getunten Prompts</h2></p><p>Sowohl der Bootstrap Few Shot Optimierer als auch der MiPRO v2 Optimierer erzeugen <strong>Demos</strong> (Demonstrationen), die die Leistung Ihres Programms deutlich verbessern. Diese Demos zeigen dem LLM, wie es ähnliche Aufgaben korrekt löst.</p><h3>Was sind Demos?</h3></p><p>Demos sind Input-Output-Beispiele, die automatisch in Ihren Prompts eingebunden werden, um dem LLM das erwartete Verhalten für Ihre Aufgabe zu zeigen.</p><h3>Laden und Nutzen von Demos</h3></p><p>Unabhängig davon, ob Sie Bootstrap Few Shot oder MiPRO v2 verwendet haben, ist die Nutzung gleich:
</code></pre>typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'</p><p>// 1. AI-Service aufsetzen
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>// 2. Programm mit derselben Signatur wie beim Tuning erstellen
const program = new AxChainOfThought(<code>question -> answer "in short 2 or 3 words"</code>)</p><p>// 3. Demos aus gespeicherter Datei laden
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))</p><p>// 4. Demos auf das Programm anwenden
program.setDemos(demos)</p><p>// 5. Programm nutzen
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})</p><p>console.log(result) // Dank Beispielen bessere Leistung
<pre><code class="language-">
<h3>Einfaches Beispiel: Textklassifikation</h3></p><p>Komplettes Beispiel, wie Demos die Klassifikation verbessern:
</code></pre>typescript
// Klassifikationsprogramm erstellen
const classifier = new AxGen(<code>text -> category:class "positive, negative, neutral"</code>)</p><p>// Demos laden (Bootstrap oder MiPRO)
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)</p><p>// Klassifikator nutzt erlernte Beispiele für bessere Ergebnisse
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})</p><p>console.log(result.category) // Genauere Klassifikation
<pre><code class="language-">
<h3>Vorteile der Nutzung von Demos</h3></p><ul><li><strong>Bessere Genauigkeit</strong>: Programme liefern mit Beispielen deutlich bessere Ergebnisse</li>
<li><strong>Konsistente Ausgaben</strong>: Demos sichern konsistente Ausgabeformate</li>
<li><strong>Weniger Halluzinationen</strong>: Beispiele lenken das Modell auf erwartetes Verhalten</li>
<li><strong>Kosteneffizient</strong>: Bessere Resultate ohne größere/teurere Modelle</li></p><p></ul><h3>Best Practices</h3></p><ul><li><strong>Demos speichern</strong>: Immer in Dateien sichern für Wiederverwendung</li>
<li><strong>Signaturen abgleichen</strong>: Exakt gleiche Signatur beim Laden verwenden</li>
<li><strong>Versionieren</strong>: Demo-Dateien im Versionskontrollsystem halten</li>
<li><strong>Regelmäßig aktualisieren</strong>: Mit neuen Daten regelmäßig neu tunen</li></p><p></ul>Sowohl Bootstrap Few Shot als auch MiPRO v2 liefern Demos im gleichen Format, sodass das Laden immer gleich funktioniert.</p><h2>Eingebaute Funktionen</h2></p><p>| Funktion            | Name               | Beschreibung                                   |
| ------------------- | ------------------ | ---------------------------------------------- |
| JS Interpreter      | AxJSInterpreter    | JS-Code in einer Sandbox ausführen             |
| Docker Sandbox      | AxDockerSession    | Kommandos in einer Docker-Umgebung ausführen   |
| Embeddings Adapter  | AxEmbeddingAdapter | Embedding abrufen und an Funktionen übergeben  |</p><h2>Alle Beispiele ansehen</h2></p><p>Mit dem <code>tsx</code>-Kommando Beispiele ausführen. Damit kann Node direkt TypeScript ausführen. <code>.env</code>-Datei für AI-API-Keys wird unterstützt.
</code></pre>shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
<pre><code class="language-">
| Beispiel                 | Beschreibung                                             |
| ------------------------ | -------------------------------------------------------- |
| customer-support.ts      | Extrahiert Details aus Kundennachrichten                 |
| function.ts              | Einfaches Beispiel für Funktionsaufruf                   |
| food-search.ts           | Multi-Step, Multi-Funktionsaufruf                        |
| marketing.ts             | Kurze, effektive Marketing-SMS generieren                |
| vectordb.ts              | Text chunken, embedden, durchsuchen                      |
| fibonacci.ts             | JS-Code-Interpreter für Fibonacci                        |
| summarize.ts             | Kurzzusammenfassung eines großen Textblocks              |
| chain-of-thought.ts      | Chain-of-Thought zum Beantworten von Fragen              |
| rag.ts                   | Multi-Hop-Retrieval für Fragenbeantwortung               |
| rag-docs.ts              | PDF in Text umwandeln und für Rag-Suche embedden         |
| react.ts                 | Funktionsaufruf und Reasoning für Antworten              |
| agent.ts                 | Agenten-Framework, Agenten nutzen andere Agenten, Tools  |
| streaming1.ts            | Validierung von Ausgabefeldern beim Streaming            |
| streaming2.ts            | Validierung pro Ausgabefeld beim Streaming               |
| streaming3.ts            | End-to-End Streaming Beispiel <code>streamingForward()</code>       |
| smart-hone.ts            | Agent sucht Hund im Smart Home                           |
| multi-modal.ts           | Bildinput plus weitere Textinputs                        |
| balancer.ts              | Balanciert verschiedene LLMs nach Kosten etc.            |
| docker.ts                | Docker-Sandbox sucht Dateien nach Beschreibung           |
| prime.ts                 | Field Processor für Promptfeld                           |
| simple-classify.ts       | Einfache Klassifikation                                  |
| mcp-client-memory.ts     | MCP-Server für Memory mit Ax                             |
| mcp-client-blender.ts    | MCP-Server für Blender mit Ax                            |
| mcp-client-pipedream.ts  | Integration mit Remote-MCP                               |
| tune-bootstrap.ts        | Bootstrap-Optimizer für Prompt-Effizienz                 |
| tune-mipro.ts            | MiPRO v2 Optimizer für Prompt-Effizienz                  |
| tune-usage.ts            | Optimierte Prompts nutzen                                |
| telemetry.ts             | Tracing und Jaeger-Integration                           |
| openai-responses.ts      | Nutzung der neuen OpenAI Responses API                   |
| use-examples.ts          | Verwendung von 'examples' zur Lenkung des LLM            |</p><h2>Unser Ziel</h2></p><p>Große Sprachmodelle (LLMs) werden immer leistungsfähiger und sind mittlerweile in der Lage, das Backend Ihres gesamten Produkts zu bilden. Dennoch bleibt viel Komplexität zu bewältigen: richtige Prompts, Modelle, Streaming, Funktionsaufrufe, Fehlerkorrektur usw. Unser Ziel ist es, all diese Komplexität in einer gepflegten, einfach nutzbaren Bibliothek zu bündeln, die mit allen modernen LLMs funktioniert. Außerdem bringen wir neue Features wie DSPy aus der Forschung direkt in die Bibliothek.</p><h2>Wie benutzt man diese Bibliothek?</h2></p><h3>1. Wählen Sie eine KI</h3>
</code></pre>ts
// LLM wählen
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
<pre><code class="language-">
<h3>2. Prompt-Signatur je nach Anwendungsfall erstellen</h3>
</code></pre>ts
// Die Signatur definiert Eingaben und Ausgaben des Prompt-Programms
const cot = new ChainOfThought(ai, <code>question:string -> answer:string</code>, { mem })
<pre><code class="language-">
<h3>3. Prompt-Programm ausführen</h3>
</code></pre>ts
// Die in der Signatur definierten Eingabefelder übergeben
const res = await cot.forward({ question: 'Are we in a simulation?' })
<pre><code class="language-">
<h3>4. Oder direkt das LLM nutzen</h3>
</code></pre>ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
<pre><code class="language-">
<h2>Wie nutzt man Funktionsaufrufe?</h2></p><h3>1. Funktionen definieren</h3>
</code></pre>ts
// Eine oder mehrere Funktionen und einen Handler definieren
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return <code>The weather in ${args.location} is 72 degrees</code>
    },
  },
]
<pre><code class="language-">
<h3>2. Funktionen an einen Prompt übergeben</h3>
</code></pre>ts
const cot = new AxGen(ai, <code>question:string -> answer:string</code>, { functions })
<pre><code class="language-">
<h2>Debug-Logs aktivieren</h2>
</code></pre>ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
<pre><code class="language-">
<h2>Kontakt</h2></p><p>Wir helfen gerne weiter – bei Fragen einfach melden oder auf Discord joinen:  
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer">twitter/dosco</a></p><h2>FAQ</h2></p><h3>1. Das LLM findet nicht die richtige Funktion</h3></p><p>Verbessern Sie die Funktionsnamen und Beschreibungen. Seien Sie sehr klar, was die Funktion tut. Auch die Parameter sollten gut beschrieben sein – kurz, aber präzise.</p><h3>2. Wie ändere ich die Konfiguration des verwendeten LLMs?</h3></p><p>Sie können beim Erstellen eines neuen LLM-Objekts ein Konfigurationsobjekt als zweiten Parameter übergeben.
</code></pre>ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
<pre><code class="language-">
<h2>3. Mein Prompt ist zu lang / Kann ich die max Tokens ändern?</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // oder OpenAIBestOptions()
conf.maxTokens = 2000
<pre><code class="language-">
<h2>4. Wie ändere ich das Modell? (z.B. GPT4 nutzen)</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // oder OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
``<code></p><h2>Monorepo-Tipps & Tricks</h2></p><p>Führen Sie </code>npm install<code> nur im Root-Verzeichnis aus. So verhindern Sie verschachtelte </code>package-lock.json<code> und nicht deduplizierte </code>node_modules<code>.</p><p>Neue Abhängigkeiten in Packages mit  
</code>npm install lodash --workspace=ax<code>  
hinzufügen (oder </code>package.json<code> anpassen und </code>npm install` im Root ausführen).

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/ax-llm/ax/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>