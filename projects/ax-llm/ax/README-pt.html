<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - Read ax documentation in Portuguese. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read ax documentation in Portuguese. This project has 0 stars on GitHub.">
    <meta name="keywords" content="ax, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ax",
  "description": "Read ax documentation in Portuguese. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "ax-llm"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/ax-llm/ax/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/ax-llm/ax/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/ax-llm/ax" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ax
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Portuguese</span>
                <span>by ax-llm</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Ax, DSPy para Typescript</h1></p><p>Trabalhar com LLMs é complexo — eles nem sempre fazem o que você deseja. O DSPy facilita a construção de soluções incríveis com LLMs. Basta definir suas entradas e saídas (assinatura) e um prompt eficiente é gerado automaticamente e utilizado. Conecte várias assinaturas para construir sistemas e fluxos de trabalho complexos usando LLMs.</p><p>E para te ajudar realmente a usar isso em produção, temos tudo o que você precisa, como observabilidade, streaming, suporte a outras modalidades (imagens, áudio, etc.), correção de erros, chamadas de função multi-etapas, MCP, RAG, etc.</p><p><a href="https://www.npmjs.com/package/@ax-llm/ax" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green" alt="NPM Package"></a>
<a href="https://discord.gg/DSHg3dU7dW" target="_blank" rel="noopener noreferrer"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat"></a>
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red" alt="Twitter"></a></p><p><!-- header --></p><h2>Por que usar Ax?</h2></p><ul><li>Interface padrão para todos os principais LLMs</li>
<li>Prompts compilados a partir de assinaturas simples</li>
<li>Streaming nativo de ponta a ponta</li>
<li>Suporte para orçamento de pensamento e tokens de raciocínio</li>
<li>Construa agentes que podem chamar outros agentes</li>
<li>Suporte nativo ao MCP, Model Context Protocol</li>
<li>Converta documentos de qualquer formato para texto</li>
<li>RAG, chunking inteligente, embedding, consulta</li>
<li>Funciona com Vercel AI SDK</li>
<li>Validação de saída durante o streaming</li>
<li>DSPy multimodal suportado</li>
<li>Ajuste automático de prompts usando otimizadores</li>
<li>Rastreamento / observabilidade OpenTelemetry</li>
<li>Código Typescript pronto para produção</li>
<li>Leve, sem dependências</li></p><p></ul><h2>Pronto para Produção</h2></p><ul><li>Sem mudanças incompatíveis (versões menores)</li>
<li>Grande cobertura de testes</li>
<li>Suporte nativo Open Telemetry <code>gen_ai</code></li>
<li>Amplamente usado por startups em produção</li></p><p></ul><h2>O que é uma assinatura de prompt?</h2></p><p><img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b"></p><p>Prompts eficientes e type-safe são gerados automaticamente a partir de uma assinatura simples. Uma assinatura de prompt é composta por uma
<code>"descrição da tarefa" campoEntrada:tipo "descrição do campo" -> campoSaída:tipo</code>.
A ideia por trás das assinaturas de prompt é baseada no trabalho feito no artigo
"Demonstrate-Search-Predict".</p><p>Você pode ter vários campos de entrada e saída, e cada campo pode ser dos tipos <code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>,
<code>class "class1, class2"</code>, <code>JSON</code>, ou um array de qualquer um destes, por exemplo, <code>string[]</code>.
Quando um tipo não é definido, o padrão é <code>string</code>. O sufixo <code>?</code> torna o campo opcional (obrigatório por padrão) e <code>!</code> torna o campo interno, útil para coisas como raciocínio.</p><h2>Tipos de Campo de Saída</h2></p><p>| Tipo                     | Descrição                             | Uso                        | Exemplo de Saída                                     |
| ------------------------ | ------------------------------------- | -------------------------- | ---------------------------------------------------- |
| <code>string</code>                 | Uma sequência de caracteres.          | <code>fullName:string</code>          | <code>"exemplo"</code>                                          |
| <code>number</code>                 | Um valor numérico.                    | <code>price:number</code>             | <code>42</code>                                                 |
| <code>boolean</code>                | Um valor verdadeiro ou falso.         | <code>isEvent:boolean</code>          | <code>true</code>, <code>false</code>                                      |
| <code>date</code>                   | Um valor de data.                     | <code>startDate:date</code>           | <code>"2023-10-01"</code>                                       |
| <code>datetime</code>               | Valor de data e hora.                 | <code>createdAt:datetime</code>       | <code>"2023-10-01T12:00:00Z"</code>                             |
| <code>class "class1,class2"</code>  | Classificação de itens.               | <code>category:class</code>           | <code>["class1", "class2", "class3"]</code>                     |
| <code>string[]</code>               | Array de strings.                     | <code>tags:string[]</code>            | <code>["exemplo1", "exemplo2"]</code>                           |
| <code>number[]</code>               | Array de números.                     | <code>scores:number[]</code>          | <code>[1, 2, 3]</code>                                          |
| <code>boolean[]</code>              | Array de valores booleanos.           | <code>permissions:boolean[]</code>    | <code>[true, false, true]</code>                                |
| <code>date[]</code>                 | Array de datas.                       | <code>holidayDates:date[]</code>      | <code>["2023-10-01", "2023-10-02"]</code>                       |
| <code>datetime[]</code>             | Array de datas e horários.            | <code>logTimestamps:datetime[]</code> | <code>["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]</code>   |
| <code>class[] "class1,class2"</code>| Várias classes                        | <code>categories:class[]</code>       | <code>["class1", "class2", "class3"]</code>                     |
| <code>code "language"</code>        | Bloco de código em linguagem específica| <code>code:code "python"</code>      | <code>print('Hello, world!')</code>                             |</p><h2>LLMs Suportados</h2></p><p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>,
<code>Hugging Face</code></p><h2>Instalação</h2></p><pre><code class="language-bash">npm install @ax-llm/ax
<h1>ou</h1>
yarn add @ax-llm/ax</code></pre></p><h2>Exemplo: Usando chain-of-thought para resumir texto</h2></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'</p><p>const textToSummarize = <code>
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...</code></p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>const gen = new AxChainOfThought(
  <code>textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward(ai, { textToSummarize })</p><p>console.log('>', res)</code></pre></p><h2>Exemplo: Construindo um agente</h2></p><p>Use o prompt de agente (framework) para construir agentes que trabalham com outros agentes para completar tarefas. Agentes são fáceis de criar com assinaturas de prompt. Experimente o exemplo de agente.</p><pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts</p><p>const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: <code>physicsQuestion "physics questions" -> answer "reply in bullet points"</code>
});</p><p>const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: <code>text "text so summarize" -> shortSummary "summarize in 5 to 10 words"</code>
});</p><p>const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: <code>question -> answer</code>,
  agents: [researcher, summarizer]
});</p><p>agent.forward(ai, { questions: "How many atoms are there in the universe" })</code></pre></p><h2>Suporte a Modelos de Pensamento</h2></p><p>Ax oferece suporte nativo para modelos com capacidades de pensamento, permitindo controlar o orçamento de tokens de pensamento e acessar o raciocínio do modelo. Esse recurso ajuda a entender o processo de raciocínio do modelo e otimizar o uso de tokens.</p><pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})</p><p>// Ou controle o orçamento de pensamento por requisição
const gen = new AxChainOfThought(<code>question -> answer</code>)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium' ou 'high'
)</p><p>// Acesse pensamentos na resposta
console.log(res.thoughts) // Mostra o processo de raciocínio do modelo</code></pre></p><h2>Bancos de Dados Vetoriais Suportados</h2></p><p>Bancos de dados vetoriais são essenciais para construir fluxos de trabalho com LLM. Temos abstrações limpas sobre bancos de dados vetoriais populares e nosso próprio banco de dados vetorial em memória.</p><p>| Provedor    | Testado |
| ----------- | ------- |
| Em Memória  | 🟢 100% |
| Weaviate    | 🟢 100% |
| Cloudflare  | 🟡 50%  |
| Pinecone    | 🟡 50%  |</p><pre><code class="language-typescript">// Crie embeddings de texto usando um LLM
const ret = await this.ai.embed({ texts: 'hello world' })</p><p>// Crie um banco vetorial em memória
const db = new axDB('memory')</p><p>// Insira no banco vetorial
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})</p><p>// Consulte por entradas similares usando embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})</code></pre></p><p>Alternativamente, você pode usar o <code>AxDBManager</code> que gerencia chunking, embedding e consultas automaticamente, facilitando tudo.</p><pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)</code></pre></p><h2>Documentos RAG</h2></p><p>Usar documentos como PDF, DOCX, PPT, XLS, etc., com LLMs é complicado. Facilitamos isso com o Apache Tika, um mecanismo open-source de processamento de documentos.</p><p>Inicie o Apache Tika</p><pre><code class="language-shell">docker run -p 9998:9998 apache/tika</code></pre></p><p>Converta documentos para texto e faça o embedding para recuperação usando o <code>AxDBManager</code>, que também suporta reranker e rewriter de consulta. Duas implementações padrão, <code>AxDefaultResultReranker</code> e <code>AxDefaultQueryRewriter</code>, estão disponíveis.</p><pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')</p><p>const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query('Find some text')
console.log(matches)</code></pre></p><h2>DSPy Multimodal</h2></p><p>Ao usar modelos como <code>GPT-4o</code> e <code>Gemini</code> que suportam prompts multimodais, é possível usar campos de imagem, funcionando com todo o pipeline DSP.</p><pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')</p><p>const gen = new AxChainOfThought(<code>question, animalImage:image -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})</code></pre></p><p>Ao usar modelos como <code>gpt-4o-audio-preview</code> que suportam prompts multimodais com áudio, também é possível usar campos de áudio, funcionando em todo o pipeline DSP.</p><pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')</p><p>const gen = new AxGen(<code>question, commentAudio:audio -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})</code></pre></p><h2>API de Chat DSPy</h2></p><p>Inspirado pelo demonstration weaving do DSPy, Ax fornece o <code>AxMessage</code> para gerenciamento de histórico de conversas. Isso permite criar chatbots e agentes conversacionais que mantêm contexto em múltiplas interações, aproveitando todo o poder das assinaturas de prompt. Veja o exemplo para mais detalhes.</p><pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts</code></pre></p><pre><code class="language-typescript">const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  <code>message:string "A casual message from the user" -> reply:string "A friendly, casual response"</code>
)</p><p>await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])</code></pre></p><p>O histórico da conversa é automaticamente incorporado ao prompt, permitindo que o modelo mantenha o contexto e forneça respostas coerentes. Isso funciona perfeitamente com todos os recursos do Ax, incluindo streaming, chamadas de função e chain-of-thought.</p><h2>Streaming</h2></p><h3>Asserções</h3></p><p>Suportamos parsing de campos de saída e execução de funções durante o streaming. Isso permite falhas rápidas e correção de erros sem esperar pela saída completa, economizando tokens e reduzindo latência. Asserções são uma forma poderosa de garantir que a saída atenda seus requisitos; elas também funcionam com streaming.</p><pre><code class="language-typescript">// configurar o programa de prompt
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>// adicionar uma asserção para garantir que o número 5 não está em um campo de saída
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')</p><p>// executar o programa com streaming habilitado
const res = await gen.forward({ startNumber: 1 }, { stream: true })</p><p>// ou execute o programa com streaming end-to-end
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}</code></pre></p><p>O exemplo acima permite validar campos inteiros de saída à medida que são transmitidos. Essa validação funciona tanto com streaming quanto sem, e é acionada quando o valor do campo inteiro está disponível. Para validação real durante o streaming, veja o exemplo abaixo. Isso melhora muito o desempenho e economiza tokens em escala de produção.</p><pre><code class="language-typescript">// adicionar uma asserção para garantir que todas as linhas começam com um número e um ponto
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./</p><p>    // divide o valor em linhas, remove espaços, filtra linhas vazias e verifica se todas começam com o regex
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)</p><p>// execute o programa com streaming habilitado
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)</code></pre></p><h3>Processadores de Campo</h3></p><p>Processadores de campo são uma forma poderosa de processar campos em um prompt antes de enviá-lo ao LLM.</p><pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>const streamValue = false</p><p>const processorFunction = (value) => {
  return value.map((x) => x + 1)
}</p><p>// Adicione um processador de campo ao programa
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)</p><p>const res = await gen.forward({ startNumber: 1 })</code></pre></p><h2>Model Context Protocol (MCP)</h2></p><p>Ax oferece integração perfeita com o Model Context Protocol (MCP), permitindo que seus agentes acessem ferramentas externas e recursos através de uma interface padronizada.</p><h3>Usando AxMCPClient</h3></p><p>O <code>AxMCPClient</code> permite conectar-se a qualquer servidor compatível com MCP e usar suas capacidades dentro dos agentes Ax:</p><pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'</p><p>// Inicialize um cliente MCP com um transporte
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})</p><p>// Crie o cliente com modo debug opcional
const client = new AxMCPClient(transport, { debug: true })</p><p>// Inicialize a conexão
await client.init()</p><p>// Use as funções do cliente em um agente
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Passe o cliente como provedor de função
})</p><p>// Ou use o cliente com AxGen
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})</code></pre></p><h3>Usando AxMCPClient com um Servidor Remoto</h3></p><p>Chamar um servidor MCP remoto com Ax é simples. Por exemplo, veja como usar o servidor DeepWiki MCP para perguntar sobre qualquer repositório público do GitHub. O servidor DeepWiki MCP está disponível em <code>https://mcp.deepwiki.com/mcp</code>.</p><pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'</p><p>// 1. Inicialize o transporte MCP para o servidor DeepWiki
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)</p><p>// 2. Crie o cliente MCP
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Inicialize a conexão</p><p>// 3. Inicialize seu modelo AI (ex: OpenAI)
// Garanta que a variável de ambiente OPENAI_APIKEY esteja definida
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// 4. Crie um AxAgent que usa o cliente MCP
const deepwikiAgent = new AxAgent<
  {
    // Defina tipos de entrada para clareza, combinando com uma função possível do DeepWiki
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // Forneça o cliente MCP ao agente
})</p><p>// 5. Formule uma pergunta e chame o agente
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // Exemplo: biblioteca Ax
})
console.log('DeepWiki Answer:', result.answer)</code></pre></p><p>Este exemplo mostra como conectar-se a um servidor MCP público e usá-lo dentro de um agente Ax. A assinatura do agente (<code>questionAboutRepo, githubRepositoryUrl -> answer</code>) é uma suposição de como interagir com o serviço DeepWiki; normalmente, você descobriria as funções disponíveis e suas assinaturas diretamente do servidor MCP (ex: via chamada <code>mcp.getFunctions</code>, se suportada, ou documentação).</p><p>Para um exemplo mais complexo envolvendo autenticação e cabeçalhos customizados com um servidor MCP remoto, consulte o arquivo <code>src/examples/mcp-client-pipedream.ts</code> neste repositório.</p><h2>Roteamento de IA e Balanceamento de Carga</h2></p><p>Ax oferece duas formas poderosas de trabalhar com múltiplos serviços de IA: um balanceador de carga para alta disponibilidade e um roteador para roteamento específico por modelo.</p><h3>Balanceador de Carga</h3></p><p>O balanceador de carga distribui automaticamente requisições entre vários serviços de IA com base em desempenho e disponibilidade. Se um serviço falhar, ele faz failover automaticamente para o próximo disponível.</p><pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'</p><p>// Configure múltiplos serviços de IA
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// Crie um balanceador com todos os serviços
const balancer = new AxBalancer([openai, ollama, gemini])</p><p>// Use como um serviço IA regular - utiliza automaticamente o melhor serviço disponível
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})</p><p>// Ou use o balanceador com AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(balancer, { question: 'Hello!' })</code></pre></p><h3>Roteador Multi-Serviço</h3></p><p>O roteador permite usar múltiplos serviços de IA por uma única interface, roteando automaticamente as requisições para o serviço correto baseado no modelo especificado.</p><pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'</p><p>// Configure OpenAI com lista de modelos
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Modelo para tarefas muito simples como responder perguntas rápidas e curtas',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Modelo para tarefas semi-complexas como resumir textos, escrever código e mais',
    },
  ],
})</p><p>// Configure Gemini com lista de modelos
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Modelo que pode pensar profundamente sobre uma tarefa, ideal para tarefas que exigem planejamento',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Modelo ideal para tarefas muito complexas como escrever ensaios longos, códigos complexos e mais',
    },
  ],
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Modelo para tarefas sensíveis',
}</p><p>// Crie um roteador com todos os serviços
const router = new AxMultiServiceRouter([openai, gemini, secretService])</p><p>// Roteie para o modelo expert do OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})</p><p>// Ou use o roteador com AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(router, { question: 'Hello!' })</code></pre></p><p>O balanceador é ideal para alta disponibilidade, enquanto o roteador é perfeito quando você precisa de modelos específicos para tarefas específicas. Ambos podem ser usados com qualquer recurso do Ax, como streaming, chamadas de função e prompting chain-of-thought.</p><p>Você também pode usar balanceador e roteador juntos — múltiplos balanceadores podem ser usados com o roteador ou vice-versa.</p><h2>Suporte OpenTelemetry</h2></p><p>A habilidade de rastrear e observar seu fluxo de trabalho LLM é fundamental para construir fluxos de produção. OpenTelemetry é o padrão da indústria, e suportamos o novo namespace de atributos <code>gen_ai</code>. Confira <code>src/examples/telemetry.ts</code> para mais informações.</p><pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'</p><p>const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)</p><p>const tracer = trace.getTracer('test')</p><p>const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})</p><p>const gen = new AxChainOfThought(
  ai,
  <code>text -> shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward({ text })</code></pre></p><pre><code class="language-json">{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}</code></pre></p><h2>Ajustando os Prompts (Básico)</h2></p><p>Você pode ajustar seus prompts usando um modelo maior para ajudá-los a funcionar de forma mais eficiente e gerar melhores resultados. Isso é feito usando um otimizador como <code>AxBootstrapFewShot</code> com exemplos do popular dataset <code>HotPotQA</code>. O otimizador gera demonstrações (<code>demos</code>) que, quando usadas com o prompt, ajudam a melhorar sua eficiência.</p><pre><code class="language-typescript">// Baixe o dataset HotPotQA do huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})</p><p>const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})</p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// Configure o programa para ajustar
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  <code>question -> answer "in short 2 or 3 words"</code>
)</p><p>// Configure um otimizador Bootstrap Few Shot para ajustar o programa acima
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})</p><p>// Configure uma métrica de avaliação em, f1 scores são formas populares de medir performance de recuperação.
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)</p><p>// Execute o otimizador e lembre-se de salvar o resultado para uso posterior
const result = await optimize.compile(metricFn);</p><p>// Salve as demos geradas em um arquivo
// import fs from 'fs'; // Certifique-se de importar fs em seu script
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');</code></pre></p><p><img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/githubusercontent.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74"></p><h2>Ajustando os Prompts (Avançado, Mipro v2)</h2></p><p>MiPRO v2 é um framework avançado de otimização de prompts que usa otimização Bayesiana para encontrar automaticamente as melhores instruções, demonstrações e exemplos para seus programas LLM. Ao explorar sistematicamente diferentes configurações de prompt, o MiPRO v2 ajuda a maximizar a performance do modelo sem ajustes manuais.</p><h3>Principais Recursos</h3></p><ul><li><strong>Otimização de instruções</strong>: Gera e testa automaticamente múltiplas instruções</li>
<li><strong>Seleção de exemplos few-shot</strong>: Encontra demonstrações ideais no seu dataset</li>
<li><strong>Otimização Bayesiana inteligente</strong>: Usa UCB para explorar configurações eficientemente</li>
<li><strong>Parada antecipada</strong>: Interrompe a otimização quando não há melhorias</li>
<li><strong>Ciente do programa e dos dados</strong>: Considera estrutura do programa e características do dataset</li></p><p></ul><h3>Como Funciona</h3></p><ul><li>Gera várias instruções candidatas</li>
<li>Bootstrapa exemplos few-shot a partir dos seus dados</li>
<li>Seleciona exemplos rotulados diretamente do seu dataset</li>
<li>Usa otimização Bayesiana para encontrar a combinação ideal</li>
<li>Aplica a melhor configuração ao seu programa</li></p><p></ul><h3>Uso Básico</h3></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'</p><p>// 1. Configure seu serviço AI
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// 2. Crie seu programa
const program = new AxChainOfThought(<code>input -> output</code>)</p><p>// 3. Configure o otimizador
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // Seus exemplos de treinamento
  options: {
    numTrials: 20, // Número de configurações a tentar
    auto: 'medium', // Nível de otimização
  },
})</p><p>// 4. Defina sua métrica de avaliação
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}</p><p>// 5. Execute a otimização
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // Conjunto de validação opcional
})</p><p>// 6. Use o programa otimizado
const result = await optimizedProgram.forward(ai, { input: 'test input' })</code></pre></p><h3>Opções de Configuração</h3></p><p>MiPRO v2 oferece opções de configuração extensivas:</p><p>| Opção                    | Descrição                                     | Padrão  |
| ------------------------ | --------------------------------------------- | ------- |
| <code>numCandidates</code>          | Número de instruções candidatas a gerar       | 5       |
| <code>numTrials</code>              | Número de tentativas de otimização            | 30      |
| <code>maxBootstrappedDemos</code>   | Máximo de demonstrações bootstrapped          | 3       |
| <code>maxLabeledDemos</code>        | Máximo de exemplos rotulados                  | 4       |
| <code>minibatch</code>              | Usa minibatches para avaliação mais rápida    | true    |
| <code>minibatchSize</code>          | Tamanho dos minibatches de avaliação          | 25      |
| <code>earlyStoppingTrials</code>    | Para se não houver melhoria após N tentativas | 5       |
| <code>minImprovementThreshold</code>| Limite mínimo de melhoria                     | 0.01    |
| <code>programAwareProposer</code>   | Usa estrutura do programa para propostas      | true    |
| <code>dataAwareProposer</code>      | Considera características do dataset          | true    |
| <code>verbose</code>                | Mostra progresso detalhado da otimização      | false   |
| abort-patterns.ts | Exemplo de como abortar requisições |</p><h3>Níveis de Otimização</h3></p><p>Você pode configurar rapidamente a intensidade da otimização com o parâmetro <code>auto</code>:</p><pre><code class="language-typescript">// Otimização leve (rápida, menos minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })</p><p>// Otimização média (balanceada)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })</p><p>// Otimização pesada (mais lenta, mais minuciosa)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })</code></pre></p><h3>Exemplo Avançado: Análise de Sentimento</h3></p><pre><code class="language-typescript">// Crie um programa de análise de sentimento
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(<code>productReview -> label:string "positive" or "negative"</code>)</p><p>// Configure o otimizador com opções avançadas
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})</p><p>// Execute a otimização e salve o resultado
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// Salve a configuração para uso futuro
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');</code></pre></p><h2>Usando os Prompts Ajustados</h2></p><p>Tanto o otimizador Bootstrap Few Shot quanto o avançado MiPRO v2 geram <strong>demos</strong> (demonstrações) que melhoram significativamente a performance do seu programa. Essas demos são exemplos que mostram ao LLM como lidar corretamente com tarefas similares.</p><h3>O que são Demos?</h3></p><p>Demos são exemplos de entrada-saída que são automaticamente incluídos nos seus prompts para guiar o LLM. Eles atuam como exemplos de few-shot learning, mostrando ao modelo o comportamento esperado para sua tarefa específica.</p><h3>Carregando e Usando Demos</h3></p><p>Seja usando Bootstrap Few Shot ou MiPRO v2, o processo de uso das demos geradas é o mesmo:</p><pre><code class="language-typescript">import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'</p><p>// 1. Configure seu serviço AI
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>// 2. Crie seu programa (mesma assinatura usada no ajuste)
const program = new AxChainOfThought(<code>question -> answer "in short 2 or 3 words"</code>)</p><p>// 3. Carregue as demos do arquivo salvo
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))</p><p>// 4. Aplique as demos ao seu programa
program.setDemos(demos)</p><p>// 5. Use seu programa aprimorado
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})</p><p>console.log(result) // Agora com melhor desempenho usando exemplos aprendidos</code></pre></p><h3>Exemplo Simples: Classificação de Texto</h3></p><p>Veja um exemplo completo mostrando como demos melhoram uma tarefa de classificação:</p><pre><code class="language-typescript">// Crie um programa de classificação
const classifier = new AxGen(<code>text -> category:class "positive, negative, neutral"</code>)</p><p>// Carregue demos geradas de Bootstrap ou MiPRO tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)</p><p>// Agora o classificador aprendeu com exemplos e tem melhor desempenho
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})</p><p>console.log(result.category) // Classificação mais precisa</code></pre></p><h3>Principais Benefícios do Uso de Demos</h3></p><ul><li><strong>Maior Precisão</strong>: Programas performam muito melhor com exemplos relevantes</li>
<li><strong>Saída Consistente</strong>: Demos ajudam a manter formatos de resposta consistentes</li>
<li><strong>Reduz Alucinações</strong>: Exemplos direcionam o modelo para comportamentos esperados</li>
<li><strong>Custo-efetivo</strong>: Melhores resultados sem precisar de modelos maiores/caros</li></p><p></ul><h3>Boas Práticas</h3></p><ul><li><strong>Salve suas Demos</strong>: Sempre salve demos geradas para reutilização</li>
<li><strong>Combine Assinaturas</strong>: Use exatamente a mesma assinatura ao carregar demos</li>
<li><strong>Controle de Versão</strong>: Mantenha arquivos de demos sob controle de versão para reprodutibilidade</li>
<li><strong>Atualize Regularmente</strong>: Reajuste periodicamente com novos dados para melhorar as demos</li></p><p></ul>Tanto Bootstrap Few Shot quanto MiPRO v2 geram demos no mesmo formato, então você pode usar o mesmo padrão de carregamento independentemente do otimizador.</p><h2>Funções Embutidas</h2></p><p>| Função              | Nome                | Descrição                                     |
| ------------------- | ------------------- | --------------------------------------------- |
| Interpretador JS    | AxJSInterpreter     | Executa código JS em ambiente isolado         |
| Docker Sandbox      | AxDockerSession     | Executa comandos em ambiente Docker           |
| Adaptador de Embeddings | AxEmbeddingAdapter | Busca e passa embedding para sua função    |</p><h2>Veja todos os exemplos</h2></p><p>Use o comando <code>tsx</code> para rodar os exemplos. Ele permite rodar código Typescript no Node. Também suporta uso de arquivo <code>.env</code> para passar as chaves da API de IA ao invés de colocar na linha de comando.</p><pre><code class="language-shell">OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts</code></pre></p><p>| Exemplo                  | Descrição                                               |
| ------------------------ | ------------------------------------------------------ |
| customer-support.ts      | Extrai detalhes de comunicações com clientes           |
| function.ts              | Exemplo simples de chamada de função                   |
| food-search.ts           | Exemplo multi-etapas e multi-funções                   |
| marketing.ts             | Gera mensagens SMS de marketing curtas e eficazes      |
| vectordb.ts              | Chunk, embed e busca de texto                          |
| fibonacci.ts             | Usa interpretador JS para calcular fibonacci           |
| summarize.ts             | Gera resumo curto de um texto grande                   |
| chain-of-thought.ts      | Usa prompting chain-of-thought para responder questões |
| rag.ts                   | Usa multi-hop retrieval para responder questões        |
| rag-docs.ts              | Converte PDF em texto e faz embedding para RAG search  |
| react.ts                 | Usa chamadas de função e raciocínio para responder     |
| agent.ts                 | Framework de agentes; agentes podem usar outros agentes|
| streaming1.ts            | Validação de campos de saída durante streaming         |
| streaming2.ts            | Validação por campo durante streaming                  |
| streaming3.ts            | Exemplo de streaming end-to-end <code>streamingForward()</code>   |
| smart-hone.ts            | Agente procura cachorro em casa inteligente            |
| multi-modal.ts           | Usa imagem como entrada junto com texto                |
| balancer.ts              | Balanceia entre vários LLMs por custo, etc             |
| docker.ts                | Usa sandbox Docker para encontrar arquivos por descrição|
| prime.ts                 | Usa processadores de campo em prompt                   |
| simple-classify.ts       | Usa classificador simples para classificar itens       |
| mcp-client-memory.ts     | Exemplo de MCP server para memória com Ax              |
| mcp-client-blender.ts    | Exemplo de MCP server para Blender com Ax              |
| mcp-client-pipedream.ts  | Exemplo de integração com MCP remoto                   |
| tune-bootstrap.ts        | Usa otimizador bootstrap para eficiência de prompt     |
| tune-mipro.ts            | Usa otimizador mipro v2 para eficiência de prompt      |
| tune-usage.ts            | Usa prompts otimizados                                 |
| telemetry.ts             | Trace e envia traces para serviço Jaeger               |
| openai-responses.ts      | Exemplo usando OpenAI Responses API                    |
| use-examples.ts | Exemplo de uso de 'examples' para direcionar o llm              |</p><h2>Nosso Objetivo</h2></p><p>Grandes modelos de linguagem (LLMs) estão cada vez mais poderosos e já podem funcionar como backend de um produto inteiro. Porém, ainda há muita complexidade a ser gerenciada, como prompts corretos, modelos, streaming, chamadas de função, correção de erros e muito mais. Nosso objetivo é empacotar toda essa complexidade em uma biblioteca bem mantida e fácil de usar, que funcione com todos os LLMs de ponta. Além disso, utilizamos as pesquisas mais recentes para adicionar novas capacidades como DSPy à biblioteca.</p><h2>Como usar esta biblioteca?</h2></p><h3>1. Escolha uma IA para trabalhar</h3></p><pre><code class="language-ts">// Escolha um LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)</code></pre></p><h3>2. Crie uma assinatura de prompt baseada no seu caso de uso</h3></p><pre><code class="language-ts">// A assinatura define as entradas e saídas do seu programa de prompt
const cot = new ChainOfThought(ai, <code>question:string -> answer:string</code>, { mem })</code></pre></p><h3>3. Execute este novo programa de prompt</h3></p><pre><code class="language-ts">// Passe os campos de entrada definidos na assinatura acima
const res = await cot.forward({ question: 'Are we in a simulation?' })</code></pre></p><h3>4. Ou se quiser usar o LLM diretamente</h3></p><pre><code class="language-ts">const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);</code></pre></p><h2>Como usar chamadas de função</h2></p><h3>1. Defina as funções</h3></p><pre><code class="language-ts">// defina uma ou mais funções e um handler de função
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return <code>The weather in ${args.location} is 72 degrees</code>
    },
  },
]</code></pre></p><h3>2. Passe as funções para um prompt</h3></p><pre><code class="language-ts">const cot = new AxGen(ai, <code>question:string -> answer:string</code>, { functions })</code></pre></p><h2>Ativar logs de debug</h2></p><pre><code class="language-ts">const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })</code></pre></p><h2>Entre em contato</h2></p><p>Estamos felizes em ajudar! Entre em contato se tiver dúvidas ou junte-se ao Discord
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer">twitter/dosco</a></p><h2>FAQ</h2></p><h3>1. O LLM não encontra a função correta</h3></p><p>Melhore o nome e a descrição da função. Seja claro sobre o que a função faz. Além disso, garanta que os parâmetros tenham boas descrições. As descrições podem ser curtas, mas precisam ser precisas.</p><h3>2. Como altero a configuração do LLM que estou usando?</h3></p><p>Você pode passar um objeto de configuração como segundo parâmetro ao criar um novo objeto LLM.</p><pre><code class="language-ts">const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)</code></pre></p><h2>3. Meu prompt está muito longo / posso alterar o max tokens?</h2></p><pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.maxTokens = 2000</code></pre></p><h2>4. Como altero o modelo? (ex: quero usar o GPT4)</h2></p><pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // ou OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo</code></pre></p><h2>Dicas & truques para Monorepo</h2></p><p>É essencial lembrar que devemos rodar <code>npm install</code> apenas no diretório raiz. Isso previne a criação de arquivos <code>package-lock.json</code> aninhados e evita <code>node_modules</code> não deduplicados.</p><p>Adicionar novas dependências em pacotes deve ser feito com, por exemplo,
<code>npm install lodash --workspace=ax</code> (ou apenas modificar o <code>package.json</code> apropriado e rodar <code>npm install</code> do root).

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/ax-llm/ax/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>