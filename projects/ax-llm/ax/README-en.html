<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - Read ax documentation in English. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read ax documentation in English. This project has 0 stars on GitHub.">
    <meta name="keywords" content="ax, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ax",
  "description": "Read ax documentation in English. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "ax-llm"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/ax-llm/ax/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/ax-llm/ax/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/ax-llm/ax" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ax
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">English</span>
                <span>by ax-llm</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Ax, DSPy for Typescript</h1></p><p>Working with LLMs is complex—they don't always do what you want. DSPy makes it easier to build amazing things with LLMs. Just define your inputs and outputs (signature), and an efficient prompt is auto-generated and used. Connect together various signatures to build complex systems and workflows using LLMs.</p><p>And to help you really use this in production, we have everything else you need, like observability, streaming, support for other modalities (images, audio, etc.), error-correction, multi-step function calling, MCP, RAG, etc.</p><p><a href="https://www.npmjs.com/package/@ax-llm/ax" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green" alt="NPM Package"></a>
<a href="https://discord.gg/DSHg3dU7dW" target="_blank" rel="noopener noreferrer"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat"></a>
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red" alt="Twitter"></a></p><p><!-- header --></p><h2>Why use Ax?</h2></p><ul><li>Standard interface across all top LLMs</li>
<li>Prompts compiled from simple signatures</li>
<li>Full native end-to-end streaming</li>
<li>Support for thinking budget and thought tokens</li>
<li>Build Agents that can call other agents</li>
<li>Built-in MCP, Model Context Protocol support</li>
<li>Convert docs of any format to text</li>
<li>RAG, smart chunking, embedding, querying</li>
<li>Works with Vercel AI SDK</li>
<li>Output validation while streaming</li>
<li>Multi-modal DSPy supported</li>
<li>Automatic prompt tuning using optimizers</li>
<li>OpenTelemetry tracing / observability</li>
<li>Production-ready Typescript code</li>
<li>Lightweight, zero dependencies</li></p><p></ul><h2>Production Ready</h2></p><ul><li>No breaking changes (minor versions)</li>
<li>Large test coverage</li>
<li>Built-in Open Telemetry <code>gen_ai</code> support</li>
<li>Widely used by startups in production</li></p><p></ul><h2>What's a prompt signature?</h2></p><p><img width="860" alt="shapes at 24-03-31 00 05 55" src="https://raw.githubusercontent.com/ax-llm/ax/main/https://github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b"></p><p>Efficient type-safe prompts are auto-generated from a simple signature. A prompt signature is made up of a
<code>"task description" inputField:type "field description" -> "outputField:type"</code>.
The idea behind prompt signatures is based on work done in the
"Demonstrate-Search-Predict" paper.</p><p>You can have multiple input and output fields, and each field can be of the
types <code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>,
<code>class "class1, class2"</code>, <code>JSON</code>, or an array of any of these, e.g., <code>string[]</code>.
When a type is not defined, it defaults to <code>string</code>. The suffix <code>?</code> makes the
field optional (required by default) and <code>!</code> makes the field internal, which is
good for things like reasoning.</p><h2>Output Field Types</h2></p><p>| Type                      | Description                         | Usage                      | Example Output                                     |
| ------------------------- | ----------------------------------- | -------------------------- | -------------------------------------------------- |
| <code>string</code>                  | A sequence of characters.           | <code>fullName:string</code>          | <code>"example"</code>                                        |
| <code>number</code>                  | A numerical value.                  | <code>price:number</code>             | <code>42</code>                                               |
| <code>boolean</code>                 | A true or false value.              | <code>isEvent:boolean</code>          | <code>true</code>, <code>false</code>                                    |
| <code>date</code>                    | A date value.                       | <code>startDate:date</code>           | <code>"2023-10-01"</code>                                     |
| <code>datetime</code>                | A date and time value.              | <code>createdAt:datetime</code>       | <code>"2023-10-01T12:00:00Z"</code>                           |
| <code>class "class1,class2"</code>   | A classification of items.          | <code>category:class</code>           | <code>["class1", "class2", "class3"]</code>                   |
| <code>string[]</code>                | An array of strings.                | <code>tags:string[]</code>            | <code>["example1", "example2"]</code>                         |
| <code>number[]</code>                | An array of numbers.                | <code>scores:number[]</code>          | <code>[1, 2, 3]</code>                                        |
| <code>boolean[]</code>               | An array of boolean values.         | <code>permissions:boolean[]</code>    | <code>[true, false, true]</code>                              |
| <code>date[]</code>                  | An array of dates.                  | <code>holidayDates:date[]</code>      | <code>["2023-10-01", "2023-10-02"]</code>                     |
| <code>datetime[]</code>              | An array of date and time values.   | <code>logTimestamps:datetime[]</code> | <code>["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]</code> |
| <code>class[] "class1,class2"</code> | Multiple classes                    | <code>categories:class[]</code>       | <code>["class1", "class2", "class3"]</code>                   |
| <code>code "language"</code>         | A code block in a specific language | <code>code:code "python"</code>       | <code>print('Hello, world!')</code>                           |</p><h2>LLMs Supported</h2></p><p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>,
<code>Hugging Face</code></p><h2>Install</h2></p><pre><code class="language-bash">npm install @ax-llm/ax
<h1>or</h1>
yarn add @ax-llm/ax</code></pre></p><h2>Example: Using chain-of-thought to summarize text</h2></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'</p><p>const textToSummarize = <code>
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...</code></p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>const gen = new AxChainOfThought(
  <code>textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward(ai, { textToSummarize })</p><p>console.log('>', res)</code></pre></p><h2>Example: Building an agent</h2></p><p>Use the agent prompt (framework) to build agents that work with other agents to
complete tasks. Agents are easy to make with prompt signatures. Try out the
agent example.</p><pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts</p><p>const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: <code>physicsQuestion "physics questions" -> answer "reply in bullet points"</code>
});</p><p>const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: <code>text "text so summarize" -> shortSummary "summarize in 5 to 10 words"</code>
});</p><p>const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: <code>question -> answer</code>,
  agents: [researcher, summarizer]
});</p><p>agent.forward(ai, { questions: "How many atoms are there in the universe" })</code></pre></p><h2>Thinking Models Support</h2></p><p>Ax provides native support for models with thinking capabilities, allowing you to control the thinking token budget and access the model's thoughts. This feature helps in understanding the model's reasoning process and optimizing token usage.</p><pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})</p><p>// Or control thinking budget per request
const gen = new AxChainOfThought(<code>question -> answer</code>)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', or 'high'
)</p><p>// Access thoughts in the response
console.log(res.thoughts) // Shows the model's reasoning process</code></pre></p><h2>Vector DBs Supported</h2></p><p>Vector databases are critical to building LLM workflows. We have clean
abstractions over popular vector databases and our own quick in-memory vector
database.</p><p>| Provider   | Tested  |
| ---------- | ------- |
| In Memory  | 🟢 100% |
| Weaviate   | 🟢 100% |
| Cloudflare | 🟡 50%  |
| Pinecone   | 🟡 50%  |</p><pre><code class="language-typescript">// Create embeddings from text using an LLM
const ret = await this.ai.embed({ texts: 'hello world' })</p><p>// Create an in memory vector db
const db = new axDB('memory')</p><p>// Insert into vector db
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})</p><p>// Query for similar entries using embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})</code></pre></p><p>Alternatively, you can use the <code>AxDBManager</code> which handles smart chunking,
embedding, and querying everything for you. It makes things almost too easy.</p><pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)</code></pre></p><h2>RAG Documents</h2></p><p>Using documents like PDF, DOCX, PPT, XLS, etc., with LLMs is a huge pain. We
make it easy with Apache Tika, an open-source document processing engine.</p><p>Launch Apache Tika</p><pre><code class="language-shell">docker run -p 9998:9998 apache/tika</code></pre></p><p>Convert documents to text and embed them for retrieval using the <code>AxDBManager</code>,
which also supports a reranker and query rewriter. Two default implementations,
<code>AxDefaultResultReranker</code> and <code>AxDefaultQueryRewriter</code>, are available.</p><pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')</p><p>const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query('Find some text')
console.log(matches)</code></pre></p><h2>Multi-modal DSPy</h2></p><p>When using models like <code>GPT-4o</code> and <code>Gemini</code> that support multi-modal prompts,
we support using image fields, and this works with the whole DSP pipeline.</p><pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')</p><p>const gen = new AxChainOfThought(<code>question, animalImage:image -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})</code></pre></p><p>When using models like <code>gpt-4o-audio-preview</code> that support multi-modal prompts
with audio support, we support using audio fields, and this works with the whole
DSP pipeline.</p><pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')</p><p>const gen = new AxGen(<code>question, commentAudio:audio -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})</code></pre></p><h2>DSPy Chat API</h2></p><p>Inspired by DSPy's demonstration weaving, Ax provides <code>AxMessage</code> for seamless conversation history management. This allows you to build chatbots and conversational agents that maintain context across multiple turns while leveraging the full power of prompt signatures. See the example for more details.</p><pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts</code></pre></p><pre><code class="language-typescript">const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  <code>message:string "A casual message from the user" -> reply:string "A friendly, casual response"</code>
)</p><p>await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])</code></pre></p><p>The conversation history is automatically woven into the prompt, allowing the model to maintain context and provide coherent responses. This works seamlessly with all Ax features including streaming, function calling, and chain-of-thought reasoning.</p><h2>Streaming</h2></p><h3>Assertions</h3></p><p>We support parsing output fields and function execution while streaming. This
allows for fail-fast and error correction without waiting for the whole output,
saving tokens and costs and reducing latency. Assertions are a powerful way to
ensure the output matches your requirements; they also work with streaming.</p><pre><code class="language-typescript">// setup the prompt program
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>// add a assertion to ensure that the number 5 is not in an output field
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')</p><p>// run the program with streaming enabled
const res = await gen.forward({ startNumber: 1 }, { stream: true })</p><p>// or run the program with end-to-end streaming
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}</code></pre></p><p>The above example allows you to validate entire output fields as they are
streamed in. This validation works with streaming and when not streaming and is
triggered when the whole field value is available. For true validation while
streaming, check out the example below. This will massively improve performance
and save tokens at scale in production.</p><pre><code class="language-typescript">// add a assertion to ensure all lines start with a number and a dot.
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./</p><p>    // split the value by lines, trim each line,
    // filter out empty lines and check if all lines match the regex
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)</p><p>// run the program with streaming enabled
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)</code></pre></p><h3>Field Processors</h3></p><p>Field processors are a powerful way to process fields in a prompt. They are used
to process fields in a prompt before the prompt is sent to the LLM.</p><pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>const streamValue = false</p><p>const processorFunction = (value) => {
  return value.map((x) => x + 1)
}</p><p>// Add a field processor to the program
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)</p><p>const res = await gen.forward({ startNumber: 1 })</code></pre></p><h2>Model Context Protocol (MCP)</h2></p><p>Ax provides seamless integration with the Model Context Protocol (MCP), allowing
your agents to access external tools, and resources through a standardized
interface.</p><h3>Using AxMCPClient</h3></p><p>The <code>AxMCPClient</code> allows you to connect to any MCP-compatible server and use its
capabilities within your Ax agents:</p><pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'</p><p>// Initialize an MCP client with a transport
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})</p><p>// Create the client with optional debug mode
const client = new AxMCPClient(transport, { debug: true })</p><p>// Initialize the connection
await client.init()</p><p>// Use the client's functions in an agent
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Pass the client as a function provider
})</p><p>// Or use the client with AxGen
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})</code></pre></p><h3>Using AxMCPClient with a Remote Server</h3></p><p>Calling a remote MCP server with Ax is straightforward. For example, here's how you can use the DeepWiki MCP server to ask questions about nearly any public GitHub repository. The DeepWiki MCP server is available at <code>https://mcp.deepwiki.com/mcp</code>.</p><pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'</p><p>// 1. Initialize the MCP transport to the DeepWiki server
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)</p><p>// 2. Create the MCP client
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Initialize the connection</p><p>// 3. Initialize your AI model (e.g., OpenAI)
// Ensure your OPENAI_APIKEY environment variable is set
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// 4. Create an AxAgent that uses the MCP client
const deepwikiAgent = new AxAgent<
  {
    // Define input types for clarity, matching a potential DeepWiki function
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // Provide the MCP client to the agent
})</p><p>// 5. Formulate a question and call the agent
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // Example: Ax library itself
})
console.log('DeepWiki Answer:', result.answer)</code></pre></p><p>This example shows how to connect to a public MCP server and use it within an Ax agent. The agent's signature (<code>questionAboutRepo, githubRepositoryUrl -> answer</code>) is an assumption of how one might interact with the DeepWiki service; you would typically discover the available functions and their signatures from the MCP server itself (e.g., via an <code>mcp.getFunctions</code> call if supported, or documentation).</p><p>For a more complex example involving authentication and custom headers with a remote MCP server, please refer to the <code>https://raw.githubusercontent.com/ax-llm/ax/main/src/examples/mcp-client-pipedream.ts</code> file in this repository.</p><h2>AI Routing and Load Balancing</h2></p><p>Ax provides two powerful ways to work with multiple AI services: a load balancer
for high availability and a router for model-specific routing.</p><h3>Load Balancer</h3></p><p>The load balancer automatically distributes requests across multiple AI services
based on performance and availability. If one service fails, it automatically
fails over to the next available service.</p><pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'</p><p>// Setup multiple AI services
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// Create a load balancer with all services
const balancer = new AxBalancer([openai, ollama, gemini])</p><p>// Use like a regular AI service - automatically uses the best available service
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})</p><p>// Or use the balance with AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(balancer, { question: 'Hello!' })</code></pre></p><h3>Multi-Service Router</h3></p><p>The router lets you use multiple AI services through a single interface,
automatically routing requests to the right service based on the model
specified.</p><pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'</p><p>// Setup OpenAI with model list
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})</p><p>// Setup Gemini with model list
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}</p><p>// Create a router with all services
const router = new AxMultiServiceRouter([openai, gemini, secretService])</p><p>// Route to OpenAI's expert model
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})</p><p>// Or use the router with AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(router, { question: 'Hello!' })</code></pre></p><p>The load balancer is ideal for high availability while the router is perfect
when you need specific models for specific tasks. Both can be used with any of
Ax's features like streaming, function calling, and chain-of-thought prompting.</p><p>You can also use the balancer and the router together—either the multiple
balancers can be used with the router, or the router can be used with the
balancer.</p><h2>OpenTelemetry support</h2></p><p>The ability to trace and observe your llm workflow is critical to building
production workflows. OpenTelemetry is an industry-standard, and we support the
new <code>gen_ai</code> attribute namespace. Checkout <code>https://raw.githubusercontent.com/ax-llm/ax/main/src/examples/telemetry.ts</code> for more information.</p><pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'</p><p>const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)</p><p>const tracer = trace.getTracer('test')</p><p>const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})</p><p>const gen = new AxChainOfThought(
  ai,
  <code>text -> shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward({ text })</code></pre></p><pre><code class="language-json">{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}</code></pre></p><h2>Tuning the prompts (Basic)</h2></p><p>You can tune your prompts using a larger model to help them run more efficiently
and give you better results. This is done by using an optimizer like
<code>AxBootstrapFewShot</code> with examples from the popular <code>HotPotQA</code> dataset. The
optimizer generates demonstrations (<code>demos</code>) which, when used with the prompt, help
improve its efficiency.</p><pre><code class="language-typescript">// Download the HotPotQA dataset from huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})</p><p>const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})</p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// Setup the program to tune
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  <code>question -> answer "in short 2 or 3 words"</code>
)</p><p>// Setup a Bootstrap Few Shot optimizer to tune the above program
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})</p><p>// Setup a evaluation metric em, f1 scores are a popular way measure retrieval performance.
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)</p><p>// Run the optimizer and remember to save the result to use later
const result = await optimize.compile(metricFn);</p><p>// Save the generated demos to a file
// import fs from 'fs'; // Ensure fs is imported in your actual script
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');</code></pre></p><p><img width="853" alt="tune-prompt" src="https://raw.githubusercontent.com/ax-llm/ax/main/https://github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
<pre><code class="language-">
<h2>Tuning the prompts (Advanced, Mipro v2)</h2></p><p>MiPRO v2 is an advanced prompt optimization framework that uses Bayesian
optimization to automatically find the best instructions, demonstrations, and
examples for your LLM programs. By systematically exploring different prompt
configurations, MiPRO v2 helps maximize model performance without manual tuning. </p><h3>Key Features</h3></p><ul><li><strong>Instruction optimization</strong>: Automatically generates and tests multiple</li>
  </ul>instruction candidates
<ul><li><strong>Few-shot example selection</strong>: Finds optimal demonstrations from your dataset</li>
<li><strong>Smart Bayesian optimization</strong>: Uses UCB (Upper Confidence Bound) strategy to</li>
  </ul>efficiently explore configurations
<ul><li><strong>Early stopping</strong>: Stops optimization when improvements plateau to save</li>
  </ul>compute
<ul><li><strong>Program and data-aware</strong>: Considers program structure and dataset</li>
  </ul>characteristics</p><h3>How It Works</h3></p><ul><li>Generates various instruction candidates</li>
<li>Bootstraps few-shot examples from your data</li>
<li>Selects labeled examples directly from your dataset</li>
<li>Uses Bayesian optimization to find the optimal combination</li>
<li>Applies the best configuration to your program</li></p><p></ul><h3>Basic Usage</h3>
</code></pre>typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'</p><p>// 1. Setup your AI service
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// 2. Create your program
const program = new AxChainOfThought(<code>input -> output</code>)</p><p>// 3. Configure the optimizer
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // Your training examples
  options: {
    numTrials: 20, // Number of configurations to try
    auto: 'medium', // Optimization level
  },
})</p><p>// 4. Define your evaluation metric
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}</p><p>// 5. Run the optimization
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // Optional validation set
})</p><p>// 6. Use the optimized program
const result = await optimizedProgram.forward(ai, { input: 'test input' })
<pre><code class="language-">
<h3>Configuration Options</h3></p><p>MiPRO v2 provides extensive configuration options:</p><p>| Option                    | Description                                   | Default |
| ------------------------- | --------------------------------------------- | ------- |
| <code>numCandidates</code>           | Number of instruction candidates to generate  | 5       |
| <code>numTrials</code>               | Number of optimization trials                 | 30      |
| <code>maxBootstrappedDemos</code>    | Maximum number of bootstrapped demonstrations | 3       |
| <code>maxLabeledDemos</code>         | Maximum number of labeled examples            | 4       |
| <code>minibatch</code>               | Use minibatching for faster evaluation        | true    |
| <code>minibatchSize</code>           | Size of evaluation minibatches                | 25      |
| <code>earlyStoppingTrials</code>     | Stop if no improvement after N trials         | 5       |
| <code>minImprovementThreshold</code> | Minimum score improvement threshold           | 0.01    |
| <code>programAwareProposer</code>    | Use program structure for better proposals    | true    |
| <code>dataAwareProposer</code>       | Consider dataset characteristics              | true    |
| <code>verbose</code>                 | Show detailed optimization progress           | false   |
| abort-patterns.ts | Example on how to abort requests |</p><h3>Optimization Levels</h3></p><p>You can quickly configure optimization intensity with the <code>auto</code> parameter:
</code></pre>typescript
// Light optimization (faster, less thorough)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })</p><p>// Medium optimization (balanced)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })</p><p>// Heavy optimization (slower, more thorough)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
<pre><code class="language-">
<h3>Advanced Example: Sentiment Analysis</h3>
</code></pre>typescript
// Create sentiment analysis program
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(<code>productReview -> label:string "positive" or "negative"</code>)</p><p>// Configure optimizer with advanced settings
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})</p><p>// Run optimization and save the result
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// Save configuration for future use
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
<pre><code class="language-">
<h2>Using the Tuned Prompts</h2></p><p>Both the basic Bootstrap Few Shot optimizer and the advanced MiPRO v2 optimizer generate <strong>demos</strong> (demonstrations) that significantly improve your program's performance. These demos are examples that show the LLM how to properly handle similar tasks.</p><h3>What are Demos?</h3></p><p>Demos are input-output examples that get automatically included in your prompts to guide the LLM. They act as few-shot learning examples, showing the model the expected behavior for your specific task.</p><h3>Loading and Using Demos</h3></p><p>Whether you used Bootstrap Few Shot or MiPRO v2, the process of using the generated demos is the same:
</code></pre>typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'</p><p>// 1. Setup your AI service
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>// 2. Create your program (same signature as used during tuning)
const program = new AxChainOfThought(<code>question -> answer "in short 2 or 3 words"</code>)</p><p>// 3. Load the demos from the saved file
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))</p><p>// 4. Apply the demos to your program
program.setDemos(demos)</p><p>// 5. Use your enhanced program
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})</p><p>console.log(result) // Now performs better with the learned examples
<pre><code class="language-">
<h3>Simple Example: Text Classification</h3></p><p>Here's a complete example showing how demos improve a classification task:
</code></pre>typescript
// Create a classification program
const classifier = new AxGen(<code>text -> category:class "positive, negative, neutral"</code>)</p><p>// Load demos generated from either Bootstrap or MiPRO tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)</p><p>// Now the classifier has learned from examples and performs better
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})</p><p>console.log(result.category) // More accurate classification
<pre><code class="language-">
<h3>Key Benefits of Using Demos</h3></p><ul><li><strong>Improved Accuracy</strong>: Programs perform significantly better with relevant examples</li>
<li><strong>Consistent Output</strong>: Demos help maintain consistent response formats</li>
<li><strong>Reduced Hallucination</strong>: Examples guide the model toward expected behaviors</li>
<li><strong>Cost Effective</strong>: Better results without needing larger/more expensive models</li></p><p></ul><h3>Best Practices</h3></p><ul><li><strong>Save Your Demos</strong>: Always save generated demos to files for reuse</li>
<li><strong>Match Signatures</strong>: Use the exact same signature when loading demos</li>
<li><strong>Version Control</strong>: Keep demo files in version control for reproducibility</li>
<li><strong>Regular Updates</strong>: Re-tune periodically with new data to improve demos</li></p><p></ul>Both Bootstrap Few Shot and MiPRO v2 generate demos in the same format, so you can use this same loading pattern regardless of which optimizer you used for tuning.</p><h2>Built-in Functions</h2></p><p>| Function           | Name               | Description                                  |
| ------------------ | ------------------ | -------------------------------------------- |
| JS Interpreter     | AxJSInterpreter    | Execute JS code in a sandboxed env           |
| Docker Sandbox     | AxDockerSession    | Execute commands within a docker environment |
| Embeddings Adapter | AxEmbeddingAdapter | Fetch and pass embedding to your function    |</p><h2>Check out all the examples</h2></p><p>Use the <code>tsx</code> command to run the examples. It makes node run typescript
code. It also supports using an <code>.env</code> file to pass the AI API Keys instead of
putting them in the command line.
</code></pre>shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
<pre><code class="language-">
| Example                 | Description                                             |
| ----------------------- | ------------------------------------------------------- |
| customer-support.ts     | Extract valuable details from customer communications   |
| function.ts             | Simple single function calling example                  |
| food-search.ts          | Multi-step, multi-function calling example              |
| marketing.ts            | Generate short effective marketing sms messages         |
| vectordb.ts             | Chunk, embed and search text                            |
| fibonacci.ts            | Use the JS code interpreter to compute fibonacci        |
| summarize.ts            | Generate a short summary of a large block of text       |
| chain-of-thought.ts     | Use chain-of-thought prompting to answer questions      |
| rag.ts                  | Use multi-hop retrieval to answer questions             |
| rag-docs.ts             | Convert PDF to text and embed for rag search            |
| react.ts                | Use function calling and reasoning to answer questions  |
| agent.ts                | Agent framework, agents can use other agents, tools etc |
| streaming1.ts           | Output fields validation while streaming                |
| streaming2.ts           | Per output field validation while streaming             |
| streaming3.ts           | End-to-end streaming example <code>streamingForward()</code>       |
| smart-hone.ts           | Agent looks for dog in smart home                       |
| multi-modal.ts          | Use an image input along with other text inputs         |
| balancer.ts             | Balance between various llm's based on cost, etc        |
| docker.ts               | Use the docker sandbox to find files by description     |
| prime.ts                | Using field processors to process fields in a prompt    |
| simple-classify.ts      | Use a simple classifier to classify stuff               |
| mcp-client-memory.ts    | Example of using an MCP server for memory with Ax       |
| mcp-client-blender.ts   | Example of using an MCP server for Blender with Ax      |
| mcp-client-pipedream.ts | Example of integrating with a remote MCP                |
| tune-bootstrap.ts       | Use bootstrap optimizer to improve prompt efficiency    |
| tune-mipro.ts           | Use mipro v2 optimizer to improve prompt efficiency     |
| tune-usage.ts           | Use the optimized tuned prompts                         |
| telemetry.ts            | Trace and push traces to a Jaeger service               |
| openai-responses.ts     | Example using the new OpenAI Responses API              |
| use-examples.ts | Example of using 'examples' to direct the llm |</p><h2>Our Goal</h2></p><p>Large language models (LLMs) are becoming really powerful and have reached a
point where they can work as the backend for your entire product. However,
there's still a lot of complexity to manage from using the correct prompts,
models, streaming, function calls, error correction, and much more. We aim to
package all this complexity into a well-maintained, easy-to-use library that can
work with all state-of-the-art LLMs. Additionally, we are using the latest
research to add new capabilities like DSPy to the library.</p><h2>How to use this library?</h2></p><h3>1. Pick an AI to work with</h3>
</code></pre>ts
// Pick a LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
<pre><code class="language-">
<h3>2. Create a prompt signature based on your usecase</h3>
</code></pre>ts
// Signature defines the inputs and outputs of your prompt program
const cot = new ChainOfThought(ai, <code>question:string -> answer:string</code>, { mem })
<pre><code class="language-">
<h3>3. Execute this new prompt program</h3>
</code></pre>ts
// Pass in the input fields defined in the above signature
const res = await cot.forward({ question: 'Are we in a simulation?' })
<pre><code class="language-">
<h3>4. Or if you just want to directly use the LLM</h3>
</code></pre>ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
<pre><code class="language-">
<h2>How do you use function calling</h2></p><h3>1. Define the functions</h3>
</code></pre>ts
// define one or more functions and a function handler
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return <code>The weather in ${args.location} is 72 degrees</code>
    },
  },
]
<pre><code class="language-">
<h3>2. Pass the functions to a prompt</h3>
</code></pre>ts
const cot = new AxGen(ai, <code>question:string -> answer:string</code>, { functions })
<pre><code class="language-">
<h2>Enable debug logs</h2>
</code></pre>ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
<pre><code class="language-">
<h2>Reach out</h2></p><p>We're happy to help. Reach out if you have questions or join the Discord
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer">twitter/dosco</a></p><h2>FAQ</h2></p><h3>1. The LLM can't find the correct function to use</h3></p><p>Improve the function naming and description. Be very clear about what the
function does. Also, ensure the function parameters have good descriptions. The
descriptions can be a little short but need to be precise.</p><h3>2. How do I change the configuration of the LLM I'm using?</h3></p><p>You can pass a configuration object as the second parameter when creating a new
LLM object.
</code></pre>ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
<pre><code class="language-">
<h2>3. My prompt is too long / can I change the max tokens?</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // or OpenAIBestOptions()
conf.maxTokens = 2000
<pre><code class="language-">
<h2>4. How do I change the model? (e.g., I want to use GPT4)</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // or OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
``<code></p><h2>Monorepo tips & tricks</h2></p><p>It is essential to remember that we should only run </code>npm install<code> from the root
directory. This prevents the creation of nested </code>package-lock.json<code> files and
avoids non-deduplicated </code>node_modules<code>.</p><p>Adding new dependencies in packages should be done with e.g.
</code>npm install lodash --workspace=ax<code> (or just modify the appropriate
</code>package.json<code> and run </code>npm install` from root).

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/ax-llm/ax/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>