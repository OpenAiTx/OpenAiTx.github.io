<!DOCTYPE html>
<html lang="th">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - Read ax documentation in Thai. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read ax documentation in Thai. This project has 0 stars on GitHub.">
    <meta name="keywords" content="ax, Thai, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ax",
  "description": "Read ax documentation in Thai. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "ax-llm"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/ax-llm/ax/README-th.html",
  "sameAs": "https://raw.githubusercontent.com/ax-llm/ax/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/ax-llm/ax" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ax
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Thai</span>
                <span>by ax-llm</span>
            </div>
        </div>
        
        <div class="content">
            <pre><code class="language-markdown"># Ax, DSPy สำหรับ Typescript</p><p>การทำงานกับ LLMs มีความซับซ้อนและมักจะไม่เป็นไปตามที่ต้องการ DSPy ช่วยให้การสร้างสิ่งที่ยอดเยี่ยมด้วย LLMs เป็นเรื่องง่าย เพียงแค่กำหนดอินพุตและเอาต์พุต (signature) แล้ว prompt ที่มีประสิทธิภาพจะถูกสร้างขึ้นโดยอัตโนมัติและนำไปใช้ เชื่อมต่อ signatures หลายๆ อันเข้าด้วยกันเพื่อสร้างระบบและ workflow ที่ซับซ้อนโดยใช้ LLMs</p><p>และเพื่อให้คุณนำไปใช้งานจริงได้ เรายังมีสิ่งที่จำเป็นอื่นๆ เช่น การสังเกตการณ์ (observability), สตรีมมิ่ง, รองรับมัลติโมดัล (ภาพ, เสียง ฯลฯ), การแก้ไขข้อผิดพลาด, การเรียกใช้งานฟังก์ชันหลายขั้นตอน, MCP, RAG และอื่นๆ</p><p><a href="https://www.npmjs.com/package/@ax-llm/ax" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green" alt="NPM Package"></a>
<a href="https://discord.gg/DSHg3dU7dW" target="_blank" rel="noopener noreferrer"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat"></a>
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red" alt="Twitter"></a></p><p><!-- header --></p><h2>ทำไมต้องใช้ Ax?</h2></p><ul><li>อินเทอร์เฟซมาตรฐานสำหรับ LLM ชั้นนำทั้งหมด</li>
<li>Prompt สร้างจาก signature ที่เรียบง่าย</li>
<li>สตรีมมิ่งแบบ native ครบวงจร</li>
<li>รองรับงบประมาณในการคิดและโทเค็นความคิด</li>
<li>สร้าง Agents ที่เรียกใช้ agent อื่นได้</li>
<li>มี MCP, รองรับ Model Context Protocol ในตัว</li>
<li>แปลงเอกสารรูปแบบใดก็ได้เป็นข้อความ</li>
<li>RAG, การ chunk อย่างชาญฉลาด, embedding, การค้นหา</li>
<li>ใช้งานร่วมกับ Vercel AI SDK</li>
<li>ตรวจสอบผลลัพธ์ระหว่างสตรีมมิ่ง</li>
<li>รองรับ Multi-modal DSPy</li>
<li>ปรับแต่ง prompt อัตโนมัติด้วย optimizer</li>
<li>รองรับ OpenTelemetry / observability</li>
<li>โค้ด Typescript พร้อมใช้งาน production</li>
<li>น้ำหนักเบา ไม่มี dependencies</li></p><p></ul><h2>พร้อมสำหรับ Production</h2></p><ul><li>ไม่มี breaking changes (minor versions)</li>
<li>มีการทดสอบครอบคลุมมาก</li>
<li>รองรับ Open Telemetry <code>gen_ai</code> ในตัว</li>
<li>ได้รับความนิยมใน startup หลายแห่ง</li></p><p></ul><h2>Prompt signature คืออะไร?</h2></p><p><img width="860" alt="shapes at 24-03-31 00 05 55" src="https://github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b"></p><p>Prompt ที่มี type-safe ถูกสร้างจาก signature อย่างง่ายโดยอัตโนมัติ Prompt signature ประกอบด้วย
<code>"task description" inputField:type "field description" -> "outputField:type</code> แนวคิดของ prompt signature นี้อ้างอิงจากงานวิจัยในหัวข้อ "Demonstrate-Search-Predict"</p><p>คุณสามารถมี input และ output หลายฟิลด์ โดยแต่ละฟิลด์เป็นชนิด <code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>, <code>class "class1, class2"</code>, <code>JSON</code> หรือ array ของชนิดเหล่านี้ เช่น <code>string[]</code> ถ้าไม่กำหนดชนิดจะเป็น <code>string</code> โดยปริยาย Suffix <code>?</code> ทำให้ฟิลด์เป็นทางเลือก (ปกติจำเป็น) และ <code>!</code> ทำให้ฟิลด์เป็น internal เหมาะสำหรับ reasoning</p><h2>Output Field Types</h2></p><p>| Type                      | คำอธิบาย                            | การใช้งาน                 | ตัวอย่างผลลัพธ์                                      |
| ------------------------- | ------------------------------------ | ------------------------- | ----------------------------------------------------- |
| <code>string</code>                  | ลำดับอักขระ                         | <code>fullName:string</code>         | <code>"example"</code>                                           |
| <code>number</code>                  | ค่าตัวเลข                            | <code>price:number</code>            | <code>42</code>                                                  |
| <code>boolean</code>                 | ค่าจริงหรือเท็จ                      | <code>isEvent:boolean</code>         | <code>true</code>, <code>false</code>                                       |
| <code>date</code>                    | ค่าวันที่                            | <code>startDate:date</code>          | <code>"2023-10-01"</code>                                        |
| <code>datetime</code>                | วันและเวลารวมกัน                     | <code>createdAt:datetime</code>      | <code>"2023-10-01T12:00:00Z"</code>                              |
| <code>class "class1,class2"</code>   | การจัดประเภท                         | <code>category:class</code>          | <code>["class1", "class2", "class3"]</code>                      |
| <code>string[]</code>                | อาร์เรย์ของ string                   | <code>tags:string[]</code>           | <code>["example1", "example2"]</code>                            |
| <code>number[]</code>                | อาร์เรย์ของ number                   | <code>scores:number[]</code>         | <code>[1, 2, 3]</code>                                           |
| <code>boolean[]</code>               | อาร์เรย์ของ boolean                  | <code>permissions:boolean[]</code>   | <code>[true, false, true]</code>                                 |
| <code>date[]</code>                  | อาร์เรย์ของวันที่                    | <code>holidayDates:date[]</code>     | <code>["2023-10-01", "2023-10-02"]</code>                        |
| <code>datetime[]</code>              | อาร์เรย์ของวันและเวลา                | <code>logTimestamps:datetime[]</code>| <code>["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]</code>    |
| <code>class[] "class1,class2"</code> | หลายคลาส                             | <code>categories:class[]</code>      | <code>["class1", "class2", "class3"]</code>                      |
| <code>code "language"</code>         | โค้ดบล็อกในภาษาที่ระบุ              | <code>code:code "python"</code>      | <code>print('Hello, world!')</code>                              |</p><h2>LLMs ที่รองรับ</h2></p><p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>, <code>Hugging Face</code></p><h2>ติดตั้ง</h2>
</code></pre>bash
npm install @ax-llm/ax
<h1>หรือ</h1>
yarn add @ax-llm/ax
<pre><code class="language-">
<h2>ตัวอย่าง: ใช้ chain-of-thought เพื่อสรุปข้อความ</h2>
</code></pre>typescript
import { AxAI, AxChainOfThought } from '@ax-llm/ax'</p><p>const textToSummarize = <code>
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...</code></p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>const gen = new AxChainOfThought(
  <code>textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward(ai, { textToSummarize })</p><p>console.log('>', res)
<pre><code class="language-">
<h2>ตัวอย่าง: สร้าง agent</h2></p><p>ใช้ agent prompt (framework) เพื่อสร้าง agent ที่สามารถประสานงานกับ agent อื่นในการทำงาน Agents สร้างได้ง่ายจาก prompt signature ลองดูตัวอย่าง agent
</code></pre>typescript
<h1>npm run tsx ./src/examples/agent.ts</h1></p><p>const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: <code>physicsQuestion "physics questions" -> answer "reply in bullet points"</code>
});</p><p>const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: <code>text "text so summarize" -> shortSummary "summarize in 5 to 10 words"</code>
});</p><p>const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: <code>question -> answer</code>,
  agents: [researcher, summarizer]
});</p><p>agent.forward(ai, { questions: "How many atoms are there in the universe" })
<pre><code class="language-">
<h2>รองรับ Thinking Models</h2></p><p>Ax รองรับโมเดลที่มีความสามารถในการคิดโดยตรง สามารถควบคุมงบโทเค็นความคิดและเข้าถึง reasoning ของโมเดล ช่วยเข้าใจ reasoning และปรับใช้โทเค็นอย่างเหมาะสม
</code></pre>typescript
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})</p><p>// หรือควบคุม budget ต่อ request
const gen = new AxChainOfThought(<code>question -> answer</code>)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', หรือ 'high'
)</p><p>// เข้าถึง thoughts ใน response
console.log(res.thoughts) // แสดง reasoning ของโมเดล
<pre><code class="language-">
<h2>Vector DBs ที่รองรับ</h2></p><p>Vector database สำคัญในการสร้าง workflow ของ LLM เรามี abstraction สำหรับฐานข้อมูลเวกเตอร์ยอดนิยม และฐานข้อมูลเวกเตอร์ในหน่วยความจำ</p><p>| ผู้ให้บริการ | ทดสอบแล้ว  |
| ------------ | ---------- |
| In Memory    | 🟢 100%    |
| Weaviate     | 🟢 100%    |
| Cloudflare   | 🟡 50%     |
| Pinecone     | 🟡 50%     |
</code></pre>typescript
// สร้าง embedding จากข้อความโดยใช้ LLM
const ret = await this.ai.embed({ texts: 'hello world' })</p><p>// สร้าง vector db ในหน่วยความจำ
const db = new axDB('memory')</p><p>// เพิ่มข้อมูลลง vector db
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})</p><p>// ค้นหาข้อมูลที่คล้ายกันด้วย embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})
<pre><code class="language-">
หรือใช้ <code>AxDBManager</code> ที่จัดการ chunk, embed, query ให้อัตโนมัติ
</code></pre>typescript
const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)
<pre><code class="language-">
<h2>RAG Documents</h2></p><p>ใช้งานเอกสารอย่าง PDF, DOCX, PPT, XLS ฯลฯ กับ LLM เป็นเรื่องยุ่งยาก เราทำให้ง่ายขึ้นด้วย Apache Tika ซึ่งเป็นเอนจินโอเพ่นซอร์สสำหรับประมวลผลเอกสาร</p><p>รัน Apache Tika
</code></pre>shell
docker run -p 9998:9998 apache/tika
<pre><code class="language-">
แปลงเอกสารเป็นข้อความและสร้าง embedding เพื่อค้นหาด้วย <code>AxDBManager</code> ที่รองรับ reranker และ query rewriter มี implementation เริ่มต้น 2 ตัว ได้แก่ <code>AxDefaultResultReranker</code> และ <code>AxDefaultQueryRewriter</code>
</code></pre>typescript
const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')</p><p>const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query('Find some text')
console.log(matches)
<pre><code class="language-">
<h2>Multi-modal DSPy</h2></p><p>เมื่อใช้โมเดลอย่าง <code>GPT-4o</code> และ <code>Gemini</code> ที่รองรับ multi-modal prompt สามารถใช้ image field ร่วมใน pipeline ได้
</code></pre>typescript
const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')</p><p>const gen = new AxChainOfThought(<code>question, animalImage:image -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})
<pre><code class="language-">
สำหรับโมเดลอย่าง <code>gpt-4o-audio-preview</code> ที่รองรับ audio สามารถใช้ audio field ได้
</code></pre>typescript
const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')</p><p>const gen = new AxGen(<code>question, commentAudio:audio -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})
<pre><code class="language-">
<h2>DSPy Chat API</h2></p><p>ได้รับแรงบันดาลใจจาก demonstration weaving ของ DSPy, Ax มี <code>AxMessage</code> สำหรับการจัดการประวัติการสนทนาอย่างไร้รอยต่อ สามารถสร้าง chatbot ที่รักษาบริบทการสนทนาได้เต็มรูปแบบด้วย prompt signature ดูตัวอย่างเพิ่มเติม
</code></pre>shell
GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts
<pre><code class="language-"></code></pre>typescript
const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  <code>message:string "A casual message from the user" -> reply:string "A friendly, casual response"</code>
)</p><p>await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])
<pre><code class="language-">
ประวัติการสนทนาจะถูก weave เข้าไปใน prompt อัตโนมัติ ทำให้โมเดลสามารถรักษาบริบทและตอบสนองได้อย่างต่อเนื่อง รองรับทุกฟีเจอร์ของ Ax เช่น สตรีมมิ่ง, การเรียกฟังก์ชัน, chain-of-thought reasoning</p><h2>Streaming</h2></p><h3>Assertions</h3></p><p>เรารองรับการ parse output fields และเรียกฟังก์ชันระหว่าง streaming ช่วยให้ fail-fast และแก้ไขข้อผิดพลาดได้ทันทีโดยไม่ต้องรอผลลัพธ์ทั้งหมด ประหยัดโทเค็นและลด latency Assertion ช่วยให้มั่นใจว่า output ตรงตามข้อกำหนด ใช้ได้กับ streaming
</code></pre>typescript
// setup the prompt program
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>// add a assertion ให้แน่ใจว่าไม่มีเลข 5 ใน output field
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')</p><p>// run the program พร้อม streaming
const res = await gen.forward({ startNumber: 1 }, { stream: true })</p><p>// หรือ run แบบ end-to-end streaming
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}
<pre><code class="language-">
ตัวอย่างข้างต้นช่วย validate output field แบบเต็มขณะ streaming สามารถใช้ทั้งแบบ streaming และไม่ streaming โดยจะ trigger เมื่อได้รับค่าครบ หากต้องการ validate ระหว่าง streaming จริงๆ ดูตัวอย่างด้านล่าง จะช่วยเพิ่ม performance และประหยัดโทเค็น
</code></pre>typescript
// assertion ตรวจแต่ละบรรทัดต้องขึ้นต้นด้วยตัวเลขและจุด
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./</p><p>    // แยกบรรทัด, trim, กรองบรรทัดว่าง, ตรวจ regex
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)</p><p>// run แบบ streaming
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)
<pre><code class="language-">
<h3>Field Processors</h3></p><p>Field processor ใช้ประมวลผล field ใน prompt ก่อนส่งเข้า LLM
</code></pre>typescript
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>const streamValue = false</p><p>const processorFunction = (value) => {
  return value.map((x) => x + 1)
}</p><p>// เพิ่ม field processor เข้าโปรแกรม
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)</p><p>const res = await gen.forward({ startNumber: 1 })
<pre><code class="language-">
<h2>Model Context Protocol (MCP)</h2></p><p>Ax รองรับ Model Context Protocol (MCP) อย่างไร้รอยต่อ ให้ agent ของคุณเข้าถึง tools ภายนอกผ่านอินเทอร์เฟซมาตรฐาน</p><h3>การใช้ AxMCPClient</h3></p><p><code>AxMCPClient</code> ช่วยเชื่อมต่อกับ MCP server ใดก็ได้และใช้งานผ่าน agent ของคุณ:
</code></pre>typescript
import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'</p><p>// สร้าง MCP client พร้อม transport
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})</p><p>// สร้าง client พร้อมโหมด debug
const client = new AxMCPClient(transport, { debug: true })</p><p>// เริ่มเชื่อมต่อ
await client.init()</p><p>// ใช้ฟังก์ชัน client ใน agent
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // ใส่ client เป็น function provider
})</p><p>// หรือใช้ client กับ AxGen
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})
<pre><code class="language-">
<h3>การใช้ AxMCPClient กับ Remote Server</h3></p><p>เรียก MCP server แบบ remote ได้ง่าย ตัวอย่าง การใช้ DeepWiki MCP server ถามข้อมูล repo สาธารณะบน GitHub DeepWiki MCP server อยู่ที่ <code>https://mcp.deepwiki.com/mcp</code>
</code></pre>typescript
import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'</p><p>// 1. เตรียม MCP transport ไป DeepWiki server
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)</p><p>// 2. สร้าง MCP client
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // เริ่มเชื่อมต่อ</p><p>// 3. เตรียม AI model (เช่น OpenAI)
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// 4. สร้าง AxAgent ที่ใช้ MCP client
const deepwikiAgent = new AxAgent<
  {
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // ใส่ MCP client ใน agent
})</p><p>// 5. ถามคำถามและเรียก agent
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax',
})
console.log('DeepWiki Answer:', result.answer)
<pre><code class="language-">
ตัวอย่างนี้แสดงการเชื่อมต่อ MCP server สาธารณะและใช้งานกับ agent ของ Ax signature (<code>questionAboutRepo, githubRepositoryUrl -> answer</code>) เป็นตัวอย่างเท่านั้น โดยปกติจะดูฟังก์ชันและ signature จาก MCP server โดยตรง (เช่น จาก <code>mcp.getFunctions</code> หรือเอกสาร)</p><p>ดูตัวอย่างที่ซับซ้อนกว่านี้เช่น authentication และ custom header ใน remote MCP server ได้ที่ <code>src/examples/mcp-client-pipedream.ts</code></p><h2>AI Routing และ Load Balancing</h2></p><p>Ax มี 2 วิธีหลักในการทำงานกับ AI หลายบริการ: load balancer เพื่อความเสถียร และ router สำหรับ routing ตามโมเดล</p><h3>Load Balancer</h3></p><p>load balancer แจกจ่าย request ไปยัง AI หลายๆ บริการตามประสิทธิภาพและสถานะ หากบริการใดล้มเหลว จะสลับอัตโนมัติ
</code></pre>typescript
import { AxAI, AxBalancer } from '@ax-llm/ax'</p><p>// ตั้งค่าหลาย AI service
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// สร้าง load balancer
const balancer = new AxBalancer([openai, ollama, gemini])</p><p>// ใช้งานเหมือน AI ปกติ - เลือกบริการที่ดีที่สุดให้อัตโนมัติ
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})</p><p>// หรือใช้กับ AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(balancer, { question: 'Hello!' })
<pre><code class="language-">
<h3>Multi-Service Router</h3></p><p>router ช่วยให้ใช้ AI หลายบริการผ่านอินเทอร์เฟซเดียว โดย route ตามโมเดลที่ระบุ
</code></pre>typescript
import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'</p><p>// ตั้งค่า OpenAI กับรายชื่อโมเดล
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})</p><p>// ตั้งค่า Gemini
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}</p><p>// สร้าง router
const router = new AxMultiServiceRouter([openai, gemini, secretService])</p><p>// route ไปที่ expert model ของ OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})</p><p>// หรือใช้กับ AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(router, { question: 'Hello!' })
<pre><code class="language-">
load balancer เหมาะกับ high availability ส่วน router เหมาะกับงานที่ต้องเลือกโมเดลเฉพาะ ทั้งสองใช้ร่วมกับฟีเจอร์ Ax เช่น streaming, function calling, chain-of-thought ได้</p><p>สามารถใช้ balancer กับ router ร่วมกันได้</p><h2>รองรับ OpenTelemetry</h2></p><p>การ trace และสังเกตการณ์ workflow LLM สำคัญสำหรับ production OpenTelemetry เป็นมาตรฐานอุตสาหกรรม และเรารองรับ namespace <code>gen_ai</code> ดูเพิ่มที่ <code>src/examples/telemetry.ts</code>
</code></pre>typescript
import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'</p><p>const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)</p><p>const tracer = trace.getTracer('test')</p><p>const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})</p><p>const gen = new AxChainOfThought(
  ai,
  <code>text -> shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward({ text })
<pre><code class="language-"></code></pre>json
{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}
<pre><code class="language-">
<h2>การปรับแต่ง prompt (พื้นฐาน)</h2></p><p>คุณสามารถปรับแต่ง prompt โดยใช้โมเดลขนาดใหญ่เพื่อเพิ่มประสิทธิภาพและผลลัพธ์ที่ดีขึ้น ด้วย optimizer เช่น <code>AxBootstrapFewShot</code> และตัวอย่างจากชุดข้อมูล <code>HotPotQA</code> optimizer จะสร้าง demo ซึ่งเมื่อใช้กับ prompt จะช่วยเพิ่มประสิทธิภาพ
</code></pre>typescript
// ดาวน์โหลด HotPotQA dataset จาก huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})</p><p>const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})</p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// ตั้งค่าโปรแกรมสำหรับ tuning
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  <code>question -> answer "in short 2 or 3 words"</code>
)</p><p>// ตั้งค่า Bootstrap Few Shot optimizer
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})</p><p>// ตั้งค่า evaluation metric เช่น em, f1 score
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)</p><p>// run optimizer และอย่าลืมบันทึกผลลัพธ์
const result = await optimize.compile(metricFn);</p><p>// บันทึก demo ที่สร้างไว้
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');
<pre><code class="language-">
<img width="853" alt="tune-prompt" src="https://github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74"></code></pre></p><h2>การปรับแต่ง prompt (ขั้นสูง, Mipro v2)</h2></p><p>MiPRO v2 เป็น framework ขั้นสูงสำหรับ optimization prompt ที่ใช้ Bayesian optimization เพื่อค้นหา instruction, demonstration, และ example ที่ดีที่สุดสำหรับโปรแกรม LLM ของคุณโดยอัตโนมัติ</p><h3>จุดเด่น</h3></p><ul><li><strong>Instruction optimization</strong>: สร้างและทดสอบ instruction อัตโนมัติ</li>
<li><strong>Few-shot example selection</strong>: เลือก demonstration ที่เหมาะสมจาก dataset</li>
<li><strong>Smart Bayesian optimization</strong>: ใช้ UCB strategy สำรวจ configuration อย่างมีประสิทธิภาพ</li>
<li><strong>Early stopping</strong>: หยุดเมื่อไม่มี improvement เพื่อประหยัดคอมพิวต์</li>
<li><strong>Program and data-aware</strong>: คำนึงถึงโครงสร้างโปรแกรมและ dataset</li></p><p></ul><h3>วิธีการทำงาน</h3></p><ul><li>สร้าง instruction candidate ต่างๆ</li>
<li>Bootstrap ตัวอย่าง few-shot จากข้อมูล</li>
<li>เลือก example จาก dataset</li>
<li>ใช้ Bayesian optimization หาค่าเหมาะสม</li>
<li>ใช้ configuration ที่ดีที่สุดกับโปรแกรม</li></p><p></ul><h3>การใช้งานเบื้องต้น</h3></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'</p><p>// 1. ตั้งค่า AI service
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// 2. สร้างโปรแกรม
const program = new AxChainOfThought(<code>input -> output</code>)</p><p>// 3. ตั้งค่า optimizer
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData,
  options: {
    numTrials: 20,
    auto: 'medium',
  },
})</p><p>// 4. กำหนด evaluation metric
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}</p><p>// 5. เริ่ม optimization
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// 6. ใช้โปรแกรมที่ปรับแต่งแล้ว
const result = await optimizedProgram.forward(ai, { input: 'test input' })</code></pre></p><h3>ตัวเลือกการตั้งค่า</h3></p><p>| Option                    | คำอธิบาย                                      | ค่าเริ่มต้น |
| ------------------------- | ---------------------------------------------- | ----------- |
| <code>numCandidates</code>           | จำนวน instruction candidate ที่จะสร้าง        | 5           |
| <code>numTrials</code>               | จำนวนรอบ optimization                        | 30          |
| <code>maxBootstrappedDemos</code>    | จำนวน bootstrapped demo สูงสุด                | 3           |
| <code>maxLabeledDemos</code>         | จำนวน labeled example สูงสุด                  | 4           |
| <code>minibatch</code>               | ใช้ minibatch เพื่อประเมินเร็วขึ้น            | true        |
| <code>minibatchSize</code>           | ขนาด minibatch สำหรับประเมิน                  | 25          |
| <code>earlyStoppingTrials</code>     | หยุดเมื่อไม่มี improvement หลัง N trial      | 5           |
| <code>minImprovementThreshold</code> | threshold การพัฒนาคะแนนขั้นต่ำ                | 0.01        |
| <code>programAwareProposer</code>    | ใช้โครงสร้างโปรแกรมเพื่อ proposal ที่ดีกว่า | true        |
| <code>dataAwareProposer</code>       | คำนึงถึง dataset                              | true        |
| <code>verbose</code>                 | แสดงความคืบหน้าอย่างละเอียด                  | false       |
| abort-patterns.ts | ตัวอย่างการ abort request |</p><h3>ระดับการ optimization</h3></p><p>ตั้งค่าความเข้มข้นการ optimize ด้วย <code>auto</code> parameter:</p><pre><code class="language-typescript">// light (เร็ว แต่ไม่ thorough)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })</p><p>// medium (สมดุล)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })</p><p>// heavy ( thorough แต่ช้า)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })</code></pre></p><h3>ตัวอย่างขั้นสูง: วิเคราะห์ความรู้สึก</h3></p><pre><code class="language-typescript">// สร้างโปรแกรม sentiment analysis
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(<code>productReview -> label:string "positive" or "negative"</code>)</p><p>// ตั้งค่า optimizer ขั้นสูง
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})</p><p>// run optimization และบันทึกผลลัพธ์
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// บันทึก config สำหรับใช้ต่อ
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');</code></pre></p><h2>การใช้ Tuned Prompts</h2></p><p>ทั้ง Bootstrap Few Shot optimizer และ MiPRO v2 optimizer จะสร้าง <strong>demos</strong> (demonstration) ที่ช่วยปรับปรุงประสิทธิภาพโปรแกรมของคุณอย่างมาก</p><h3>Demos คืออะไร?</h3></p><p>Demos คือ input-output example ที่ถูกเพิ่มเข้า prompt อัตโนมัติ เพื่อแนะนำ LLM ให้เห็นตัวอย่างการแก้ปัญหาแบบ few-shot</p><h3>โหลดและใช้ demo</h3></p><p>ไม่ว่าจะใช้ Bootstrap หรือ MiPRO การใช้ demo ที่สร้างเหมือนกัน:</p><pre><code class="language-typescript">import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'</p><p>// 1. ตั้งค่า AI service
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>// 2. สร้างโปรแกรม (signature เดียวกับตอน tuning)
const program = new AxChainOfThought(<code>question -> answer "in short 2 or 3 words"</code>)</p><p>// 3. โหลด demo ที่บันทึกไว้
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))</p><p>// 4. ใส่ demo เข้าโปรแกรม
program.setDemos(demos)</p><p>// 5. ใช้โปรแกรมที่ปรับปรุงแล้ว
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})</p><p>console.log(result) // แม่นยำขึ้นจากตัวอย่างที่เรียนรู้</code></pre></p><h3>ตัวอย่างง่าย: Text Classification</h3></p><p>ตัวอย่างการใช้ demo เพื่อเพิ่มประสิทธิภาพงาน classification</p><pre><code class="language-typescript">// สร้างโปรแกรม classifier
const classifier = new AxGen(<code>text -> category:class "positive, negative, neutral"</code>)</p><p>// โหลด demo ที่ได้จาก tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)</p><p>// classifier จะเรียนรู้จากตัวอย่างและแม่นยำขึ้น
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})</p><p>console.log(result.category) // classification แม่นยำขึ้น</code></pre></p><h3>ข้อดีของการใช้ demo</h3></p><ul><li><strong>แม่นยำขึ้น</strong>: โปรแกรมทำงานดีขึ้นมากด้วยตัวอย่างที่เกี่ยวข้อง</li>
<li><strong>ผลลัพธ์สม่ำเสมอ</strong>: demo ช่วยให้รูปแบบ output คงที่</li>
<li><strong>ลด Hallucination</strong>: ตัวอย่างช่วยชี้นำโมเดลให้ตอบตามที่ควร</li>
<li><strong>คุ้มค่า</strong>: ได้ผลลัพธ์ดีขึ้นโดยไม่ต้องใช้โมเดลใหญ่หรือแพงกว่า</li></p><p></ul><h3>Best Practices</h3></p><ul><li><strong>บันทึก demo</strong>: บันทึก demo ลงไฟล์ไว้ใช้ซ้ำ</li>
<li><strong>ใช้ signature เดิม</strong>: โหลด demo ด้วย signature เดียวกับตอน tuning</li>
<li><strong>ควบคุมเวอร์ชัน</strong>: เก็บไฟล์ demo ไว้ใน version control</li>
<li><strong>อัปเดตสม่ำเสมอ</strong>: tune ใหม่เป็นระยะด้วยข้อมูลใหม่</li></p><p></ul>ทั้ง Bootstrap และ MiPRO v2 สร้าง demo ในรูปแบบเดียวกัน ใช้ pattern นี้ได้กับทั้งสองแบบ</p><h2>ฟังก์ชันในตัว</h2></p><p>| Function           | Name               | คำอธิบาย                                            |
| ------------------ | ------------------ | --------------------------------------------------- |
| JS Interpreter     | AxJSInterpreter    | รัน JS ใน sandbox                                    |
| Docker Sandbox     | AxDockerSession    | รันคำสั่งในสภาพแวดล้อม docker                      |
| Embeddings Adapter | AxEmbeddingAdapter | รับ embedding และส่งต่อไปยังฟังก์ชันของคุณ          |</p><h2>ดูตัวอย่างทั้งหมด</h2></p><p>ใช้ <code>tsx</code> รันตัวอย่างต่างๆ ช่วยให้รัน typescript ได้บน node และรองรับ <code>.env</code> สำหรับ API Key</p><pre><code class="language-shell">OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts</code></pre></p><p>| Example                 | คำอธิบาย                                             |
| ----------------------- | ---------------------------------------------------- |
| customer-support.ts     | ดึงข้อมูลสำคัญจากการสื่อสารกับลูกค้า               |
| function.ts             | ตัวอย่าง function calling อย่างง่าย                  |
| food-search.ts          | ตัวอย่าง multi-step, multi-function calling         |
| marketing.ts            | สร้างข้อความ sms การตลาดสั้นๆ ที่มีประสิทธิภาพ      |
| vectordb.ts             | chunk, embed และค้นหาข้อความ                        |
| fibonacci.ts            | ใช้ interpreter JS คำนวณ fibonacci                  |
| summarize.ts            | สรุปข้อความขนาดใหญ่                                 |
| chain-of-thought.ts     | ใช้ chain-of-thought ในการตอบคำถาม                  |
| rag.ts                  | ใช้ multi-hop retrieval เพื่อตอบคำถาม               |
| rag-docs.ts             | แปลง PDF เป็นข้อความและฝังสำหรับ rag search        |
| react.ts                | ใช้ function calling และ reasoning ตอบคำถาม         |
| agent.ts                | agent framework, agent ใช้ agent อื่น/เครื่องมือได้ |
| streaming1.ts           | ตรวจสอบ output field ขณะ streaming                  |
| streaming2.ts           | ตรวจสอบ output field ทีละ field ขณะ streaming       |
| streaming3.ts           | ตัวอย่าง end-to-end streaming <code>streamingForward()</code>  |
| smart-hone.ts           | agent มองหาสุนัขในบ้านอัจฉริยะ                     |
| multi-modal.ts          | ใช้รูปภาพพร้อม input อื่น                           |
| balancer.ts             | balance หลาย llm ตาม cost ฯลฯ                       |
| docker.ts               | ใช้ docker sandbox หาไฟล์ตามคำอธิบาย                |
| prime.ts                | ใช้ field processor ประมวลผล field ใน prompt         |
| simple-classify.ts      | classifier อย่างง่าย                                 |
| mcp-client-memory.ts    | ใช้ MCP server สำหรับ memory กับ Ax                 |
| mcp-client-blender.ts   | ใช้ MCP server สำหรับ Blender กับ Ax                |
| mcp-client-pipedream.ts | ตัวอย่างเชื่อมต่อ MCP remote                        |
| tune-bootstrap.ts       | ใช้ bootstrap optimizer เพื่อเพิ่มประสิทธิภาพ prompt|
| tune-mipro.ts           | ใช้ mipro v2 optimizer เพื่อเพิ่มประสิทธิภาพ prompt|
| tune-usage.ts           | ใช้ tuned prompts ที่ปรับปรุงแล้ว                    |
| telemetry.ts            | trace และส่ง trace ไปยัง Jaeger service              |
| openai-responses.ts     | ตัวอย่างใช้ OpenAI Responses API ใหม่                |
| use-examples.ts | ตัวอย่างใช้ 'examples' เพื่อกำกับ llm |</p><h2>เป้าหมายของเรา</h2></p><p>LLM (Large language models) ทรงพลังมากขึ้นเรื่อยๆ และสามารถเป็น backend ให้ผลิตภัณฑ์ของคุณได้ทั้งระบบ แต่ยังมีความซับซ้อนทั้งเรื่อง prompt, model, streaming, function call, error correction ฯลฯ เราต้องการรวมความซับซ้อนเหล่านี้ไว้ในไลบรารีที่บำรุงรักษาและใช้งานง่าย รองรับ LLM ที่ล้ำหน้าทุกตัว และอัปเดตความสามารถใหม่ๆ ตามงานวิจัยล่าสุด เช่น DSPy</p><h2>ใช้ไลบรารีนี้อย่างไร</h2></p><h3>1. เลือก AI ที่ต้องการใช้</h3></p><pre><code class="language-ts">// เลือก LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)</code></pre></p><h3>2. สร้าง prompt signature ตาม usecase</h3></p><pre><code class="language-ts">// signature กำหนด input/output ของ prompt program
const cot = new ChainOfThought(ai, <code>question:string -> answer:string</code>, { mem })</code></pre></p><h3>3. รัน prompt program ที่สร้างขึ้น</h3></p><pre><code class="language-ts">// ส่ง input ที่กำหนดใน signature
const res = await cot.forward({ question: 'Are we in a simulation?' })</code></pre></p><h3>4. หรือใช้ LLM โดยตรง</h3></p><pre><code class="language-ts">const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);</code></pre></p><h2>ใช้ function calling อย่างไร</h2></p><h3>1. กำหนดฟังก์ชัน</h3></p><pre><code class="language-ts">// กำหนดฟังก์ชันและ handler
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return <code>The weather in ${args.location} is 72 degrees</code>
    },
  },
]</code></pre></p><h3>2. ส่งฟังก์ชันเข้า prompt</h3></p><pre><code class="language-ts">const cot = new AxGen(ai, <code>question:string -> answer:string</code>, { functions })</code></pre></p><h2>เปิด debug log</h2></p><pre><code class="language-ts">const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })</code></pre></p><h2>ติดต่อเรา</h2></p><p>ยินดีช่วยเหลือ ติดต่อสอบถามหรือเข้าร่วม Discord <a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer">twitter/dosco</a></p><h2>FAQ</h2></p><h3>1. LLM เลือกฟังก์ชันไม่ถูกต้อง</h3></p><p>ตั้งชื่อและอธิบายฟังก์ชันให้ชัดเจน รวมถึงอธิบาย parameter อย่างกระชับและแม่นยำ</p><h3>2. เปลี่ยนการตั้งค่า LLM ได้อย่างไร</h3></p><p>ส่ง object configuration เป็นพารามิเตอร์ที่สองขณะสร้าง LLM object</p><pre><code class="language-ts">const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)</code></pre></p><h2>3. Prompt ยาวเกินไป / จะเปลี่ยน max tokens ได้ไหม</h2></p><pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // หรือ OpenAIBestOptions()
conf.maxTokens = 2000</code></pre></p><h2>4. เปลี่ยนโมเดลอย่างไร (เช่น GPT4)</h2></p><pre><code class="language-ts">const conf = axOpenAIDefaultConfig() // หรือ OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo</code></pre></p><h2>Monorepo tips & tricks</h2></p><p>ควรติดตั้ง <code>npm install</code> เฉพาะที่ root directory เท่านั้นเพื่อป้องกันการสร้าง <code>package-lock.json</code> ซ้อนซ้อนและ node_modules ซ้ำซ้อน</p><p>ติดตั้ง dependencies ใหม่ใน package ให้ใช้ <code>npm install lodash --workspace=ax</code> (หรือแก้ไข <code>package.json</code> แล้วรัน <code>npm install</code> ที่ root)
``<code>
</code>``

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/ax-llm/ax/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>