<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ax - Read ax documentation in Italian. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read ax documentation in Italian. This project has 0 stars on GitHub.">
    <meta name="keywords" content="ax, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ax",
  "description": "Read ax documentation in Italian. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "ax-llm"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/ax-llm/ax/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/ax-llm/ax/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/ax-llm/ax" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ax
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Italian</span>
                <span>by ax-llm</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Ax, DSPy per Typescript</h1></p><p>Lavorare con i LLM è complesso: non fanno sempre quello che vuoi. DSPy rende più semplice costruire soluzioni straordinarie con i LLM. Basta definire i tuoi input e output (signature) e un prompt efficiente viene generato e utilizzato automaticamente. Collega tra loro diverse signature per costruire sistemi e workflow complessi usando i LLM.</p><p>E per aiutarti davvero nell’uso in produzione, abbiamo tutto il necessario: osservabilità, streaming, supporto per altre modalità (immagini, audio, ecc.), correzione degli errori, chiamate di funzioni multi-step, MCP, RAG, ecc.</p><p><a href="https://www.npmjs.com/package/@ax-llm/ax" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/npm/v/@ax-llm/ax?style=for-the-badge&color=green" alt="NPM Package"></a>
<a href="https://discord.gg/DSHg3dU7dW" target="_blank" rel="noopener noreferrer"><img src="https://dcbadge.vercel.app/api/server/DSHg3dU7dW?style=for-the-badge" alt="Discord Chat"></a>
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/dosco?style=for-the-badge&color=red" alt="Twitter"></a></p><p><!-- header --></p><h2>Perché usare Ax?</h2></p><ul><li>Interfaccia standard per tutti i migliori LLM</li>
<li>Prompt compilati da signature semplici</li>
<li>Streaming end-to-end nativo completo</li>
<li>Supporto per budget di pensiero e token di ragionamento</li>
<li>Costruisci Agenti che possono chiamare altri agenti</li>
<li>Supporto integrato per MCP, Model Context Protocol</li>
<li>Conversione di documenti di qualsiasi formato in testo</li>
<li>RAG, smart chunking, embedding, interrogazione</li>
<li>Funziona con Vercel AI SDK</li>
<li>Validazione degli output durante lo streaming</li>
<li>DSPy multi-modale supportato</li>
<li>Ottimizzazione automatica dei prompt tramite optimizer</li>
<li>Tracing/observabilità OpenTelemetry</li>
<li>Codice Typescript pronto per la produzione</li>
<li>Leggero, senza dipendenze</li></p><p></ul><h2>Pronto per la produzione</h2></p><ul><li>Nessun breaking change (versioni minori)</li>
<li>Copertura di test estesa</li>
<li>Supporto integrato per Open Telemetry <code>gen_ai</code></li>
<li>Ampiamente usato da startup in produzione</li></p><p></ul><h2>Cos’è una prompt signature?</h2></p><p><img width="860" alt="shapes at 24-03-31 00 05 55" src="https://github.com/dosco/llm-client/assets/832235/0f0306ea-1812-4a0a-9ed5-76cd908cd26b"></p><p>Prompt efficienti e type-safe sono auto-generati da una signature semplice. Una signature di prompt è composta da
<code>"descrizione del task" campoInput:tipo "descrizione campo" -> "campoOutput:tipo</code>.
L’idea delle signature di prompt si basa sul lavoro svolto nell’articolo "Demonstrate-Search-Predict".</p><p>Puoi avere più campi di input e output, e ogni campo può essere di tipo
<code>string</code>, <code>number</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>, <code>class "class1, class2"</code>, <code>JSON</code>, o un array di uno qualsiasi di questi, ad esempio <code>string[]</code>.
Se un tipo non è definito, il default è <code>string</code>. Il suffisso <code>?</code> rende il campo opzionale (obbligatorio di default) e <code>!</code> rende il campo interno, utile per attività come il ragionamento.</p><h2>Tipi di campo Output</h2></p><p>| Tipo                     | Descrizione                            | Uso                         | Esempio Output                                      |
| ------------------------ | -------------------------------------- | --------------------------- | --------------------------------------------------- |
| <code>string</code>                 | Sequenza di caratteri                  | <code>fullName:string</code>           | <code>"example"</code>                                         |
| <code>number</code>                 | Un valore numerico                     | <code>price:number</code>              | <code>42</code>                                                |
| <code>boolean</code>                | Valore vero o falso                    | <code>isEvent:boolean</code>           | <code>true</code>, <code>false</code>                                     |
| <code>date</code>                   | Un valore data                         | <code>startDate:date</code>            | <code>"2023-10-01"</code>                                      |
| <code>datetime</code>               | Data e ora                             | <code>createdAt:datetime</code>        | <code>"2023-10-01T12:00:00Z"</code>                            |
| <code>class "class1,class2"</code>  | Classificazione di elementi            | <code>category:class</code>            | <code>["class1", "class2", "class3"]</code>                    |
| <code>string[]</code>               | Array di stringhe                      | <code>tags:string[]</code>             | <code>["example1", "example2"]</code>                          |
| <code>number[]</code>               | Array di numeri                        | <code>scores:number[]</code>           | <code>[1, 2, 3]</code>                                         |
| <code>boolean[]</code>              | Array di valori booleani               | <code>permissions:boolean[]</code>     | <code>[true, false, true]</code>                               |
| <code>date[]</code>                 | Array di date                          | <code>holidayDates:date[]</code>       | <code>["2023-10-01", "2023-10-02"]</code>                      |
| <code>datetime[]</code>             | Array di date e ora                    | <code>logTimestamps:datetime[]</code>  | <code>["2023-10-01T12:00:00Z", "2023-10-02T12:00:00Z"]</code>  |
| <code>class[] "class1,class2"</code>| Più classi                             | <code>categories:class[]</code>        | <code>["class1", "class2", "class3"]</code>                    |
| <code>code "language"</code>        | Blocco di codice in un linguaggio      | <code>code:code "python"</code>        | <code>print('Hello, world!')</code>                            |</p><h2>LLM supportati</h2></p><p><code>Google Gemini</code>, <code>OpenAI</code>, <code>Azure OpenAI</code>, <code>Anthropic</code>, <code>X Grok</code>, <code>TogetherAI</code>, <code>Cohere</code>, <code>Mistral</code>, <code>Groq</code>, <code>DeepSeek</code>, <code>Ollama</code>, <code>Reka</code>, <code>Hugging Face</code></p><h2>Installazione</h2></p><pre><code class="language-bash">npm install @ax-llm/ax
<h1>oppure</h1>
yarn add @ax-llm/ax</code></pre></p><h2>Esempio: Usare chain-of-thought per riassumere testo</h2></p><pre><code class="language-typescript">import { AxAI, AxChainOfThought } from '@ax-llm/ax'</p><p>const textToSummarize = <code>
The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.[2][3] ...</code></p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>const gen = new AxChainOfThought(
  <code>textToSummarize -> textType:class "note, email, reminder", shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward(ai, { textToSummarize })</p><p>console.log('>', res)</code></pre></p><h2>Esempio: Costruire un agente</h2></p><p>Usa il prompt agent (framework) per costruire agenti che lavorano con altri agenti per completare task. Gli agenti sono facili da creare con le signature dei prompt. Prova l’esempio dell’agente.</p><pre><code class="language-typescript"># npm run tsx ./src/examples/agent.ts</p><p>const researcher = new AxAgent({
  name: 'researcher',
  description: 'Researcher agent',
  signature: <code>physicsQuestion "physics questions" -> answer "reply in bullet points"</code>
});</p><p>const summarizer = new AxAgent({
  name: 'summarizer',
  description: 'Summarizer agent',
  signature: <code>text "text so summarize" -> shortSummary "summarize in 5 to 10 words"</code>
});</p><p>const agent = new AxAgent({
  name: 'agent',
  description: 'A an agent to research complex topics',
  signature: <code>question -> answer</code>,
  agents: [researcher, summarizer]
});</p><p>agent.forward(ai, { questions: "How many atoms are there in the universe" })</code></pre></p><h2>Supporto per modelli di pensiero</h2></p><p>Ax offre supporto nativo per modelli con capacità di ragionamento, permettendoti di controllare il budget di token di pensiero e accedere ai pensieri del modello. Questa funzionalità aiuta a comprendere il processo di ragionamento del modello e ottimizzare l’uso dei token.</p><pre><code class="language-typescript">const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY as string,
  config: {
    model: AxAIGoogleGeminiModel.Gemini25Flash,
    thinking: { includeThoughts: true },
  },
})</p><p>// Oppure controlla il budget di pensiero per richiesta
const gen = new AxChainOfThought(<code>question -> answer</code>)
const res = await gen.forward(
  ai,
  { question: 'What is quantum entanglement?' },
  { thinkingTokenBudget: 'medium' } // 'minimal', 'low', 'medium', o 'high'
)</p><p>// Accedi ai pensieri nella risposta
console.log(res.thoughts) // Mostra il processo di ragionamento del modello</code></pre></p><h2>Vector DB supportati</h2></p><p>I database vettoriali sono fondamentali per costruire workflow con LLM. Abbiamo astrazioni pulite su popolari database vettoriali e un nostro database vettoriale in memoria.</p><p>| Provider    | Testato |
| ----------- | ------- |
| In Memory   | 🟢 100% |
| Weaviate    | 🟢 100% |
| Cloudflare  | 🟡 50%  |
| Pinecone    | 🟡 50%  |</p><pre><code class="language-typescript">// Crea embeddings da testo usando un LLM
const ret = await this.ai.embed({ texts: 'hello world' })</p><p>// Crea un vector db in memoria
const db = new axDB('memory')</p><p>// Inserisci nel vector db
await this.db.upsert({
  id: 'abc',
  table: 'products',
  values: ret.embeddings[0],
})</p><p>// Interroga per elementi simili usando embeddings
const matches = await this.db.query({
  table: 'products',
  values: embeddings[0],
})</code></pre></p><p>In alternativa puoi usare <code>AxDBManager</code> che gestisce smart chunking, embedding e query tutto per te, rendendo il processo semplicissimo.</p><pre><code class="language-typescript">const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query(
  'John von Neumann on human intelligence and singularity.'
)
console.log(matches)</code></pre></p><h2>Documenti RAG</h2></p><p>Utilizzare documenti come PDF, DOCX, PPT, XLS, ecc. con i LLM è molto complesso. Lo rendiamo facile con Apache Tika, un motore open-source di processamento documentale.</p><p>Avvia Apache Tika</p><pre><code class="language-shell">docker run -p 9998:9998 apache/tika</code></pre></p><p>Converti documenti in testo e inseriscili per retrieval usando <code>AxDBManager</code>, che supporta anche reranker e query rewriter. Sono disponibili due implementazioni predefinite: <code>AxDefaultResultReranker</code> e <code>AxDefaultQueryRewriter</code>.</p><pre><code class="language-typescript">const tika = new AxApacheTika()
const text = await tika.convert('/path/to/document.pdf')</p><p>const manager = new AxDBManager({ ai, db })
await manager.insert(text)</p><p>const matches = await manager.query('Find some text')
console.log(matches)</code></pre></p><h2>DSPy Multi-modale</h2></p><p>Quando si usano modelli come <code>GPT-4o</code> e <code>Gemini</code> che supportano prompt multi-modali, supportiamo l’uso di campi immagine, e funziona con l’intera pipeline DSP.</p><pre><code class="language-typescript">const image = fs
  .readFileSync('./src/examples/assets/kitten.jpeg')
  .toString('base64')</p><p>const gen = new AxChainOfThought(<code>question, animalImage:image -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  animalImage: { mimeType: 'image/jpeg', data: image },
})</code></pre></p><p>Quando si usano modelli come <code>gpt-4o-audio-preview</code> che supportano prompt multi-modali con audio, supportiamo l’uso di campi audio, e funziona con l’intera pipeline DSP.</p><pre><code class="language-typescript">const audio = fs
  .readFileSync('./src/examples/assets/comment.wav')
  .toString('base64')</p><p>const gen = new AxGen(<code>question, commentAudio:audio -> answer</code>)</p><p>const res = await gen.forward(ai, {
  question: 'What family does this animal belong to?',
  commentAudio: { format: 'wav', data: audio },
})</code></pre></p><h2>DSPy Chat API</h2></p><p>Ispirato al demonstration weaving di DSPy, Ax offre <code>AxMessage</code> per una gestione fluida della cronologia conversazionale. Questo ti permette di costruire chatbot e agenti conversazionali che mantengono il contesto su più turni sfruttando tutta la potenza delle signature dei prompt. Vedi l’esempio per maggiori dettagli.</p><pre><code class="language-shell">GOOGLE_APIKEY=api-key npm run tsx ./src/examples/chat.ts</code></pre></p><pre><code class="language-typescript">const chatBot = new AxGen<
  { message: string } | ReadonlyArray<ChatMessage>,
  { reply: string }
>(
  <code>message:string "A casual message from the user" -> reply:string "A friendly, casual response"</code>
)</p><p>await chatBot.forward(ai, [
  {
    role: 'user',
    values: { message: 'Hi! How are you doing today?' },
  },
  {
    role: 'assistant',
    values: { reply: 'I am doing great! How about you?' },
  },
  {
    role: 'user',
    values: { message: 'Thats great!' },
  },
])</code></pre></p><p>La cronologia della conversazione viene automaticamente intrecciata nel prompt, permettendo al modello di mantenere il contesto e fornire risposte coerenti. Questo funziona con tutte le funzionalità di Ax, incluso streaming, function calling e chain-of-thought reasoning.</p><h2>Streaming</h2></p><h3>Assert</h3></p><p>Supportiamo il parsing dei campi di output e l’esecuzione di funzioni durante lo streaming. Questo consente fail-fast e correzione degli errori senza aspettare tutto l’output, risparmiando token e costi e riducendo la latenza. Gli assert sono un modo potente per garantire che l’output soddisfi i tuoi requisiti; funzionano anche in streaming.</p><pre><code class="language-typescript">// setup del programma prompt
const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>// aggiungi un assert per assicurarti che il numero 5 non sia in un campo di output
gen.addAssert(({ next10Numbers }: Readonly<{ next10Numbers: number[] }>) => {
  return next10Numbers ? !next10Numbers.includes(5) : undefined
}, 'Numbers 5 is not allowed')</p><p>// esegui il programma con streaming abilitato
const res = await gen.forward({ startNumber: 1 }, { stream: true })</p><p>// oppure esegui il programma con streaming end-to-end
const generator = await gen.streamingForward(
  { startNumber: 1 },
  {
    stream: true,
  }
)
for await (const res of generator) {
}</code></pre></p><p>L’esempio sopra permette di validare interi campi di output mentre vengono trasmessi. Questa validazione funziona sia con che senza streaming e viene attivata quando l’intero valore del campo è disponibile. Per una vera validazione durante lo streaming, vedi l’esempio sotto. Questo migliorerà notevolmente le prestazioni e farà risparmiare token in produzione.</p><pre><code class="language-typescript">// aggiungi un assert per assicurarti che tutte le righe inizino con un numero e un punto.
gen.addStreamingAssert(
  'answerInPoints',
  (value: string) => {
    const re = /^\d+\./</p><p>    // suddividi il valore per righe, rimuovi spazi, filtra righe vuote e verifica che tutte rispettino la regex
    return value
      .split('\n')
      .map((x) => x.trim())
      .filter((x) => x.length > 0)
      .every((x) => re.test(x))
  },
  'Lines must start with a number and a dot. Eg: 1. This is a line.'
)</p><p>// esegui il programma con streaming abilitato
const res = await gen.forward(
  {
    question: 'Provide a list of optimizations to speedup LLM inference.',
  },
  { stream: true, debug: true }
)</code></pre></p><h3>Field Processor</h3></p><p>I field processor sono un modo potente per processare i campi in un prompt. Sono utilizzati per processare i campi di un prompt prima che venga inviato al LLM.</p><pre><code class="language-typescript">const gen = new AxChainOfThought(
  ai,
  <code>startNumber:number -> next10Numbers:number[]</code>
)</p><p>const streamValue = false</p><p>const processorFunction = (value) => {
  return value.map((x) => x + 1)
}</p><p>// Aggiungi un field processor al programma
const processor = new AxFieldProcessor(
  gen,
  'next10Numbers',
  processorFunction,
  streamValue
)</p><p>const res = await gen.forward({ startNumber: 1 })</code></pre></p><h2>Model Context Protocol (MCP)</h2></p><p>Ax fornisce integrazione senza soluzione di continuità con il Model Context Protocol (MCP), permettendo ai tuoi agenti di accedere a tool e risorse esterne tramite un’interfaccia standardizzata.</p><h3>Utilizzare AxMCPClient</h3></p><p><code>AxMCPClient</code> ti permette di connetterti a qualsiasi server compatibile MCP e usare le sue capacità all’interno dei tuoi agenti Ax:</p><pre><code class="language-typescript">import { AxMCPClient, AxMCPStdioTransport } from '@ax-llm/ax'</p><p>// Inizializza un client MCP con un transport
const transport = new AxMCPStdioTransport({
  command: 'npx',
  args: ['-y', '@modelcontextprotocol/server-memory'],
})</p><p>// Crea il client con modalità debug opzionale
const client = new AxMCPClient(transport, { debug: true })</p><p>// Inizializza la connessione
await client.init()</p><p>// Usa le funzioni del client in un agente
const memoryAgent = new AxAgent({
  name: 'MemoryAssistant',
  description: 'An assistant with persistent memory',
  signature: 'input, userId -> response',
  functions: [client], // Passa il client come provider di funzioni
})</p><p>// Oppure usa il client con AxGen
const memoryGen = new AxGen('input, userId -> response', {
  functions: [client],
})</code></pre></p><h3>Usare AxMCPClient con un server remoto</h3></p><p>Chiamare un server MCP remoto con Ax è semplice. Ad esempio, così puoi usare il server DeepWiki MCP per porre domande su quasi qualsiasi repository pubblico GitHub. Il server DeepWiki MCP è disponibile su <code>https://mcp.deepwiki.com/mcp</code>.</p><pre><code class="language-typescript">import {
  AxAgent,
  AxAI,
  AxAIOpenAIModel,
  AxMCPClient,
  AxMCPStreambleHTTPTransport,
} from '@ax-llm/ax'</p><p>// 1. Inizializza il transport MCP verso il server DeepWiki
const transport = new AxMCPStreambleHTTPTransport(
  'https://mcp.deepwiki.com/mcp'
)</p><p>// 2. Crea il client MCP
const mcpClient = new AxMCPClient(transport, { debug: false })
await mcpClient.init() // Inizializza la connessione</p><p>// 3. Inizializza il tuo modello AI (es: OpenAI)
// Assicurati che la variabile d’ambiente OPENAI_APIKEY sia impostata
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// 4. Crea un AxAgent che usa il client MCP
const deepwikiAgent = new AxAgent<
  {
    // Definisci i tipi di input per chiarezza, in linea con una funzione DeepWiki
    questionAboutRepo: string
    githubRepositoryUrl: string
  },
  {
    answer: string
  }
>({
  name: 'DeepWikiQueryAgent',
  description: 'Agent to query public GitHub repositories via DeepWiki MCP.',
  signature: 'questionAboutRepo, githubRepositoryUrl -> answer',
  functions: [mcpClient], // Fornisci il client MCP all’agente
})</p><p>// 5. Formula una domanda e chiama l’agente
const result = await deepwikiAgent.forward(ai, {
  questionAboutRepo: 'What is the main purpose of this library?',
  githubRepositoryUrl: 'https://github.com/dosco/ax', // Esempio: la libreria Ax stessa
})
console.log('DeepWiki Answer:', result.answer)</code></pre></p><p>Questo esempio mostra come connettersi a un server MCP pubblico e usarlo all’interno di un agente Ax. La signature dell’agente (<code>questionAboutRepo, githubRepositoryUrl -> answer</code>) è un’ipotesi su come si potrebbe interagire con il servizio DeepWiki; normalmente scopriresti le funzioni disponibili e le loro signature dal server MCP stesso (ad esempio tramite una chiamata <code>mcp.getFunctions</code> se supportata, o dalla documentazione).</p><p>Per un esempio più complesso che coinvolge autenticazione e header personalizzati con un server MCP remoto, consulta il file <code>src/examples/mcp-client-pipedream.ts</code> in questo repository.</p><h2>AI Routing e Load Balancing</h2></p><p>Ax offre due potenti modi per lavorare con più servizi AI: un load balancer per alta disponibilità e un router per routing specifico per modello.</p><h3>Load Balancer</h3></p><p>Il load balancer distribuisce automaticamente le richieste tra più servizi AI in base a prestazioni e disponibilità. Se un servizio fallisce, passa automaticamente al successivo disponibile.</p><pre><code class="language-typescript">import { AxAI, AxBalancer } from '@ax-llm/ax'</p><p>// Imposta più servizi AI
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// Crea un load balancer con tutti i servizi
const balancer = new AxBalancer([openai, ollama, gemini])</p><p>// Usalo come un normale servizio AI - usa automaticamente il servizio migliore
const response = await balancer.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
})</p><p>// Oppure usa il balancer con AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(balancer, { question: 'Hello!' })</code></pre></p><h3>Multi-Service Router</h3></p><p>Il router ti consente di usare più servizi AI attraverso un’unica interfaccia, instradando automaticamente le richieste al servizio giusto in base al modello specificato.</p><pre><code class="language-typescript">import { AxAI, AxAIOpenAIModel, AxMultiServiceRouter } from '@ax-llm/ax'</p><p>// Configura OpenAI con lista di modelli
const openai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
  models: [
    {
      key: 'basic',
      model: AxAIOpenAIModel.GPT4OMini,
      description:
        'Model for very simple tasks such as answering quick short questions',
    },
    {
      key: 'medium',
      model: AxAIOpenAIModel.GPT4O,
      description:
        'Model for semi-complex tasks such as summarizing text, writing code, and more',
    },
  ],
})</p><p>// Configura Gemini con lista di modelli
const gemini = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
  models: [
    {
      key: 'deep-thinker',
      model: 'gemini-2.0-flash-thinking',
      description:
        'Model that can think deeply about a task, best for tasks that require planning',
    },
    {
      key: 'expert',
      model: 'gemini-2.0-pro',
      description:
        'Model that is the best for very complex tasks such as writing large essays, complex coding, and more',
    },
  ],
})</p><p>const ollama = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
})</p><p>const secretService = {
  key: 'sensitive-secret',
  service: ollama,
  description: 'Model for sensitive secrets tasks',
}</p><p>// Crea un router con tutti i servizi
const router = new AxMultiServiceRouter([openai, gemini, secretService])</p><p>// Instrada verso il modello expert di OpenAI
const openaiResponse = await router.chat({
  chatPrompt: [{ role: 'user', content: 'Hello!' }],
  model: 'expert',
})</p><p>// Oppure usa il router con AxGen
const gen = new AxGen(<code>question -> answer</code>)
const res = await gen.forward(router, { question: 'Hello!' })</code></pre></p><p>Il load balancer è ideale per alta disponibilità, mentre il router è perfetto quando hai bisogno di modelli specifici per compiti specifici. Entrambi possono essere usati con tutte le funzionalità di Ax come streaming, function calling e chain-of-thought prompting.</p><p>Puoi anche usare balancer e router insieme: più balancer possono essere usati col router o viceversa.</p><h2>Supporto OpenTelemetry</h2></p><p>La capacità di tracciare e osservare il tuo workflow LLM è fondamentale per costruire workflow di produzione. OpenTelemetry è uno standard di settore, e supportiamo il nuovo namespace di attributi <code>gen_ai</code>. Consulta <code>src/examples/telemetry.ts</code> per maggiori informazioni.</p><pre><code class="language-typescript">import { trace } from '@opentelemetry/api'
import {
  BasicTracerProvider,
  ConsoleSpanExporter,
  SimpleSpanProcessor,
} from '@opentelemetry/sdk-trace-base'</p><p>const provider = new BasicTracerProvider()
provider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()))
trace.setGlobalTracerProvider(provider)</p><p>const tracer = trace.getTracer('test')</p><p>const ai = new AxAI({
  name: 'ollama',
  config: { model: 'nous-hermes2' },
  options: { tracer },
})</p><p>const gen = new AxChainOfThought(
  ai,
  <code>text -> shortSummary "summarize in 5 to 10 words"</code>
)</p><p>const res = await gen.forward({ text })</code></pre></p><pre><code class="language-json">{
  "traceId": "ddc7405e9848c8c884e53b823e120845",
  "name": "Chat Request",
  "id": "d376daad21da7a3c",
  "kind": "SERVER",
  "timestamp": 1716622997025000,
  "duration": 14190456.542,
  "attributes": {
    "gen_ai.system": "Ollama",
    "gen_ai.request.model": "nous-hermes2",
    "gen_ai.request.max_tokens": 500,
    "gen_ai.request.temperature": 0.1,
    "gen_ai.request.top_p": 0.9,
    "gen_ai.request.frequency_penalty": 0.5,
    "gen_ai.request.llm_is_streaming": false,
    "http.request.method": "POST",
    "url.full": "http://localhost:11434/v1/chat/completions",
    "gen_ai.usage.completion_tokens": 160,
    "gen_ai.usage.prompt_tokens": 290
  }
}</code></pre></p><h2>Ottimizzare i prompt (Base)</h2></p><p>Puoi ottimizzare i tuoi prompt usando un modello più grande per renderli più efficienti e ottenere risultati migliori. Questo si fa usando un optimizer come <code>AxBootstrapFewShot</code> con esempi dal popolare dataset <code>HotPotQA</code>. L’optimizer genera delle dimostrazioni (<code>demos</code>) che, usate con il prompt, ne migliorano l’efficienza.</p><pre><code class="language-typescript">// Scarica il dataset HotPotQA da huggingface
const hf = new AxHFDataLoader({
  dataset: 'hotpot_qa',
  split: 'train',
})</p><p>const examples = await hf.getData<{ question: string; answer: string }>({
  count: 100,
  fields: ['question', 'answer'],
})</p><p>const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY as string,
})</p><p>// Imposta il programma da ottimizzare
const program = new AxChainOfThought<{ question: string }, { answer: string }>(
  ai,
  <code>question -> answer "in short 2 or 3 words"</code>
)</p><p>// Imposta un optimizer Bootstrap Few Shot per ottimizzare il programma sopra
const optimize = new AxBootstrapFewShot<
  { question: string },
  { answer: string }
>({
  program,
  examples,
})</p><p>// Imposta una metrica di valutazione em, f1 score è un modo popolare di misurare la performance di retrieval.
const metricFn: AxMetricFn = ({ prediction, example }) =>
  emScore(prediction.answer as string, example.answer as string)</p><p>// Esegui l’optimizer e ricordati di salvare il risultato per uso futuro
const result = await optimize.compile(metricFn);</p><p>// Salva i demos generati su file
// import fs from 'fs'; // Assicurati di importare fs nello script reale
fs.writeFileSync('bootstrap-demos.json', JSON.stringify(result.demos, null, 2));
console.log('Demos saved to bootstrap-demos.json');</code></pre></p><p><img width="853" alt="tune-prompt" src="https://github.com/dosco/llm-client/assets/832235/f924baa7-8922-424c-9c2c-f8b2018d8d74">
<pre><code class="language-">
<h2>Ottimizzare i prompt (Avanzato, Mipro v2)</h2></p><p>MiPRO v2 è un framework avanzato di ottimizzazione dei prompt che usa l’ottimizzazione bayesiana per trovare automaticamente le migliori istruzioni, dimostrazioni ed esempi per i tuoi programmi LLM. Esplorando sistematicamente diverse configurazioni di prompt, MiPRO v2 aiuta a massimizzare le performance del modello senza tuning manuale.</p><h3>Caratteristiche principali</h3></p><ul><li><strong>Ottimizzazione delle istruzioni</strong>: genera e testa automaticamente molteplici candidati di istruzioni</li>
<li><strong>Selezione di esempi few-shot</strong>: trova le dimostrazioni ottimali dal tuo dataset</li>
<li><strong>Ottimizzazione bayesiana intelligente</strong>: usa la strategia UCB (Upper Confidence Bound) per esplorare efficientemente le configurazioni</li>
<li><strong>Early stopping</strong>: interrompe l’ottimizzazione quando i miglioramenti si stabilizzano, risparmiando risorse</li>
<li><strong>Consapevole di programma e dati</strong>: considera la struttura del programma e le caratteristiche del dataset</li></p><p></ul><h3>Come funziona</h3></p><ul><li>Genera diversi candidati di istruzioni</li>
<li>Bootstrap di esempi few-shot dai tuoi dati</li>
<li>Seleziona esempi etichettati direttamente dal dataset</li>
<li>Usa l’ottimizzazione bayesiana per trovare la combinazione ottimale</li>
<li>Applica la miglior configurazione al tuo programma</li></p><p></ul><h3>Uso base</h3>
</code></pre>typescript
import { AxAI, AxChainOfThought, AxMiPRO } from '@ax-llm/ax'</p><p>// 1. Imposta il tuo servizio AI
const ai = new AxAI({
  name: 'google-gemini',
  apiKey: process.env.GOOGLE_APIKEY,
})</p><p>// 2. Crea il tuo programma
const program = new AxChainOfThought(<code>input -> output</code>)</p><p>// 3. Configura l’optimizer
const optimizer = new AxMiPRO({
  ai,
  program,
  examples: trainingData, // I tuoi esempi di training
  options: {
    numTrials: 20, // Numero di configurazioni da provare
    auto: 'medium', // Livello di ottimizzazione
  },
})</p><p>// 4. Definisci la metrica di valutazione
const metricFn = ({ prediction, example }) => {
  return prediction.output === example.output
}</p><p>// 5. Esegui l’ottimizzazione
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData, // Set di validazione opzionale
})</p><p>// 6. Usa il programma ottimizzato
const result = await optimizedProgram.forward(ai, { input: 'test input' })
<pre><code class="language-">
<h3>Opzioni di configurazione</h3></p><p>MiPRO v2 offre opzioni di configurazione estese:</p><p>| Opzione                  | Descrizione                                      | Default |
| ------------------------ | ------------------------------------------------ | ------- |
| <code>numCandidates</code>          | Numero di istruzioni candidate da generare       | 5       |
| <code>numTrials</code>              | Numero di tentativi di ottimizzazione            | 30      |
| <code>maxBootstrappedDemos</code>   | Numero massimo di dimostrazioni bootstrappate    | 3       |
| <code>maxLabeledDemos</code>        | Numero massimo di esempi etichettati             | 4       |
| <code>minibatch</code>              | Usa minibatch per valutazione più veloce         | true    |
| <code>minibatchSize</code>          | Dimensione dei minibatch di valutazione          | 25      |
| <code>earlyStoppingTrials</code>    | Stop se nessun miglioramento dopo N tentativi    | 5       |
| <code>minImprovementThreshold</code>| Soglia minima di miglioramento                   | 0.01    |
| <code>programAwareProposer</code>   | Usa la struttura del programma per proposte      | true    |
| <code>dataAwareProposer</code>      | Considera le caratteristiche del dataset         | true    |
| <code>verbose</code>                | Mostra avanzamento dettagliato                   | false   |
| abort-patterns.ts | Esempio su come abortire richieste |</p><h3>Livelli di ottimizzazione</h3></p><p>Puoi configurare rapidamente l’intensità dell’ottimizzazione con il parametro <code>auto</code>:
</code></pre>typescript
// Ottimizzazione leggera (più veloce, meno approfondita)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'light' })</p><p>// Ottimizzazione media (bilanciata)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'medium' })</p><p>// Ottimizzazione pesante (più lenta, più approfondita)
const optimizedProgram = await optimizer.compile(metricFn, { auto: 'heavy' })
<pre><code class="language-">
<h3>Esempio avanzato: Sentiment Analysis</h3>
</code></pre>typescript
// Crea il programma di sentiment analysis
const classifyProgram = new AxChainOfThought<
  { productReview: string },
  { label: string }
>(<code>productReview -> label:string "positive" or "negative"</code>)</p><p>// Configura l’optimizer con impostazioni avanzate
const optimizer = new AxMiPRO({
  ai,
  program: classifyProgram,
  examples: trainingData,
  options: {
    numCandidates: 3,
    numTrials: 10,
    maxBootstrappedDemos: 2,
    maxLabeledDemos: 3,
    earlyStoppingTrials: 3,
    programAwareProposer: true,
    dataAwareProposer: true,
    verbose: true,
  },
})</p><p>// Esegui l’ottimizzazione e salva il risultato
const optimizedProgram = await optimizer.compile(metricFn, {
  valset: validationData,
})</p><p>// Salva la configurazione per uso futuro
const programConfig = JSON.stringify(optimizedProgram, null, 2);
await fs.promises.writeFile("./optimized-config.json", programConfig);
console.log('> Done. Optimized program config saved to optimized-config.json');
<pre><code class="language-">
<h2>Usare i prompt ottimizzati</h2></p><p>Sia l’optimizer Bootstrap Few Shot che il più avanzato MiPRO v2 generano <strong>demos</strong> (dimostrazioni) che migliorano significativamente le performance del tuo programma. Questi demos sono esempi che mostrano al LLM come gestire correttamente task simili.</p><h3>Cosa sono i Demos?</h3></p><p>I demos sono esempi input-output che vengono automaticamente inclusi nei tuoi prompt per guidare il LLM. Agiscono come esempi few-shot, mostrando al modello il comportamento atteso per il tuo task specifico.</p><h3>Caricamento e uso dei Demos</h3></p><p>Che tu abbia usato Bootstrap Few Shot o MiPRO v2, il processo di utilizzo dei demos generati è lo stesso:
</code></pre>typescript
import fs from 'fs'
import { AxAI, AxGen, AxChainOfThought } from '@ax-llm/ax'</p><p>// 1. Imposta il servizio AI
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
})</p><p>// 2. Crea il programma (stessa signature usata durante l’ottimizzazione)
const program = new AxChainOfThought(<code>question -> answer "in short 2 or 3 words"</code>)</p><p>// 3. Carica i demos dal file salvato
const demos = JSON.parse(fs.readFileSync('bootstrap-demos.json', 'utf8'))</p><p>// 4. Applica i demos al programma
program.setDemos(demos)</p><p>// 5. Usa il programma potenziato
const result = await program.forward(ai, {
  question: 'What castle did David Gregory inherit?'
})</p><p>console.log(result) // Ora funziona meglio con gli esempi appresi
<pre><code class="language-">
<h3>Esempio semplice: Classificazione Testo</h3></p><p>Ecco un esempio completo che mostra come i demos migliorano un task di classificazione:
</code></pre>typescript
// Crea un programma di classificazione
const classifier = new AxGen(<code>text -> category:class "positive, negative, neutral"</code>)</p><p>// Carica i demos generati da Bootstrap o MiPRO tuning
const savedDemos = JSON.parse(fs.readFileSync('classification-demos.json', 'utf8'))
classifier.setDemos(savedDemos)</p><p>// Ora il classificatore ha imparato dagli esempi e funziona meglio
const result = await classifier.forward(ai, {
  text: "This product exceeded my expectations!"
})</p><p>console.log(result.category) // Classificazione più accurata
<pre><code class="language-">
<h3>Vantaggi principali dell’uso dei Demos</h3></p><ul><li><strong>Migliore accuratezza</strong>: i programmi funzionano molto meglio con esempi rilevanti</li>
<li><strong>Output coerente</strong>: i demos aiutano a mantenere formati di risposta coerenti</li>
<li><strong>Meno allucinazioni</strong>: gli esempi guidano il modello verso comportamenti attesi</li>
<li><strong>Costi ridotti</strong>: risultati migliori senza bisogno di modelli più grandi/costosi</li></p><p></ul><h3>Best Practice</h3></p><ul><li><strong>Salva i tuoi demos</strong>: salva sempre i demos generati per riutilizzarli</li>
<li><strong>Signature coerente</strong>: usa la stessa signature quando carichi i demos</li>
<li><strong>Versiona i file</strong>: tieni i file demos sotto version control per riproducibilità</li>
<li><strong>Aggiorna regolarmente</strong>: riottimizza periodicamente con dati nuovi per migliorare i demos</li></p><p></ul>Sia Bootstrap Few Shot che MiPRO v2 generano demos nello stesso formato, quindi puoi usare lo stesso schema di caricamento a prescindere dall’optimizer.</p><h2>Funzioni integrate</h2></p><p>| Funzione            | Nome                | Descrizione                                   |
| ------------------- | ------------------- | --------------------------------------------- |
| Interprete JS       | AxJSInterpreter     | Esegue codice JS in ambiente sandbox          |
| Docker Sandbox      | AxDockerSession     | Esegue comandi in un ambiente docker          |
| Embeddings Adapter  | AxEmbeddingAdapter  | Ottieni e passa embedding alla tua funzione   |</p><h2>Guarda tutti gli esempi</h2></p><p>Usa il comando <code>tsx</code> per eseguire gli esempi. Fa sì che node esegua codice typescript. Supporta anche l’uso di un file <code>.env</code> per passare le API Key AI invece di metterle in linea di comando.
</code></pre>shell
OPENAI_APIKEY=api-key npm run tsx ./src/examples/marketing.ts
<pre><code class="language-">
| Esempio                 | Descrizione                                             |
| ----------------------- | ------------------------------------------------------- |
| customer-support.ts     | Estrai dettagli preziosi dalle comunicazioni clienti    |
| function.ts             | Esempio semplice di chiamata funzione singola           |
| food-search.ts          | Esempio multi-step, multi-funzione                      |
| marketing.ts            | Genera brevi messaggi sms di marketing efficaci         |
| vectordb.ts             | Chunk, embed e ricerca testo                            |
| fibonacci.ts            | Usa l’interprete JS per calcolare la serie di Fibonacci |
| summarize.ts            | Genera un breve riassunto di un testo lungo             |
| chain-of-thought.ts     | Usa chain-of-thought prompting per rispondere           |
| rag.ts                  | Retrieval multi-hop per rispondere a domande            |
| rag-docs.ts             | Converti PDF in testo e embeddalo per ricerca rag       |
| react.ts                | Usa chiamata funzioni e ragionamento per rispondere     |
| agent.ts                | Framework agenti, possono usare altri agenti e tool     |
| streaming1.ts           | Validazione campi output durante lo streaming           |
| streaming2.ts           | Validazione per campo output durante streaming          |
| streaming3.ts           | Streaming end-to-end esempio <code>streamingForward()</code>       |
| smart-hone.ts           | Agente cerca il cane in una smart home                  |
| multi-modal.ts          | Usa un input immagine con altri input testuali          |
| balancer.ts             | Bilancia tra vari llm in base a costi, ecc              |
| docker.ts               | Usa la sandbox docker per trovare file per descrizione  |
| prime.ts                | Usa field processor per processare campi in un prompt   |
| simple-classify.ts      | Classificatore semplice                                 |
| mcp-client-memory.ts    | Esempio di uso MCP server per memoria con Ax            |
| mcp-client-blender.ts   | Esempio di uso MCP server per Blender con Ax            |
| mcp-client-pipedream.ts | Esempio di integrazione MCP remoto                      |
| tune-bootstrap.ts       | Usa bootstrap optimizer per migliorare l’efficacia      |
| tune-mipro.ts           | Usa optimizer mipro v2 per migliorare prompt            |
| tune-usage.ts           | Usa i prompt ottimizzati                                |
| telemetry.ts            | Traccia e invia trace a un servizio Jaeger              |
| openai-responses.ts     | Esempio uso OpenAI Responses API                        |
| use-examples.ts         | Esempio di uso di 'examples' per guidare il llm         |</p><h2>Il nostro obiettivo</h2></p><p>I Large Language Model (LLM) stanno diventando molto potenti e hanno raggiunto un punto in cui possono essere il backend dell’intero prodotto. Tuttavia, resta molta complessità da gestire: prompt corretti, modelli, streaming, function call, correzione errori e molto altro. Il nostro obiettivo è incapsulare tutta questa complessità in una libreria ben mantenuta, facile da usare e compatibile con tutti i migliori LLM. Inoltre, usiamo la ricerca più avanzata per aggiungere nuove capacità come DSPy alla libreria.</p><h2>Come usare questa libreria?</h2></p><h3>1. Scegli un AI con cui lavorare</h3>
</code></pre>ts
// Scegli un LLM
const ai = new AxOpenAI({ apiKey: process.env.OPENAI_APIKEY } as AxOpenAIArgs)
<pre><code class="language-">
<h3>2. Crea una prompt signature secondo il tuo caso d’uso</h3>
</code></pre>ts
// La signature definisce input e output del tuo programma prompt
const cot = new ChainOfThought(ai, <code>question:string -> answer:string</code>, { mem })
<pre><code class="language-">
<h3>3. Esegui il nuovo programma prompt</h3>
</code></pre>ts
// Passa i campi input definiti nella signature sopra
const res = await cot.forward({ question: 'Are we in a simulation?' })
<pre><code class="language-">
<h3>4. Oppure se vuoi usare direttamente il LLM</h3>
</code></pre>ts
const res = await ai.chat([
  { role: "system", content: "Help the customer with his questions" }
  { role: "user", content: "I'm looking for a Macbook Pro M2 With 96GB RAM?" }
]);
<pre><code class="language-">
<h2>Come si usa la function calling</h2></p><h3>1. Definisci le funzioni</h3>
</code></pre>ts
// definisci una o più funzioni e un gestore di funzioni
const functions = [
  {
    name: 'getCurrentWeather',
    description: 'get the current weather for a location',
    parameters: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'location to get weather for',
        },
        units: {
          type: 'string',
          enum: ['imperial', 'metric'],
          default: 'imperial',
          description: 'units to use',
        },
      },
      required: ['location'],
    },
    func: async (args: Readonly<{ location: string; units: string }>) => {
      return <code>The weather in ${args.location} is 72 degrees</code>
    },
  },
]
<pre><code class="language-">
<h3>2. Passa le funzioni a un prompt</h3>
</code></pre>ts
const cot = new AxGen(ai, <code>question:string -> answer:string</code>, { functions })
<pre><code class="language-">
<h2>Abilita log di debug</h2>
</code></pre>ts
const ai = new AxAI({
  name: 'openai',
  apiKey: process.env.OPENAI_APIKEY,
} as AxOpenAIArgs)
ai.setOptions({ debug: true })
<pre><code class="language-">
<h2>Contattaci</h2></p><p>Siamo felici di aiutare, contattaci se hai domande o unisciti al Discord  
<a href="https://twitter.com/dosco" target="_blank" rel="noopener noreferrer">twitter/dosco</a></p><h2>FAQ</h2></p><h3>1. Il LLM non trova la funzione corretta da usare</h3></p><p>Migliora il nome e la descrizione della funzione. Sii molto chiaro su cosa fa la funzione. Assicurati che i parametri abbiano buone descrizioni. Le descrizioni possono essere brevi ma devono essere precise.</p><h3>2. Come cambio la configurazione del LLM che sto usando?</h3></p><p>Puoi passare un oggetto di configurazione come secondo parametro quando crei un nuovo oggetto LLM.
</code></pre>ts
const apiKey = process.env.OPENAI_APIKEY
const conf = AxOpenAIBestConfig()
const ai = new AxOpenAI({ apiKey, conf } as AxOpenAIArgs)
<pre><code class="language-">
<h2>3. Il mio prompt è troppo lungo / posso cambiare i max tokens?</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // o OpenAIBestOptions()
conf.maxTokens = 2000
<pre><code class="language-">
<h2>4. Come cambio il modello? (es: voglio usare GPT4)</h2>
</code></pre>ts
const conf = axOpenAIDefaultConfig() // o OpenAIBestOptions()
conf.model = OpenAIModel.GPT4Turbo
``<code></p><h2>Suggerimenti per Monorepo</h2></p><p>È essenziale ricordare che bisogna eseguire </code>npm install<code> solo dalla directory root. Questo evita la creazione di file </code>package-lock.json<code> annidati e previene node_modules non deduplicati.</p><p>Aggiungere nuove dipendenze nei pacchetti deve essere fatto con ad esempio  
</code>npm install lodash --workspace=ax<code> (o semplicemente modifica il </code>package.json<code> appropriato ed esegui </code>npm install` da root).

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/ax-llm/ax/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>