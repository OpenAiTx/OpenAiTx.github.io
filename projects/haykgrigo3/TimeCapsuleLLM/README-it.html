<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM addestrato solo su dati di determinati periodi temporali per ridurre i bias moderni</title>
    <meta name="description" content="Un LLM addestrato solo su dati di determinati periodi temporali per ridurre i bias moderni">
    <meta name="keywords" content="TimeCapsuleLLM, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM addestrato solo su dati di determinati periodi temporali per ridurre i bias moderni",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Italian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (presto disponibile)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (presto disponibile)</a> |
        | <a href="#" title="Coming soon">ไทย (presto disponibile)</a> |
        | <a href="#" title="Coming soon">Français (presto disponibile)</a>
        | <a href="#" title="Coming soon">Deutsch (presto disponibile)</a>
        | <a href="#" title="Coming soon">Español (presto disponibile)</a>
        | <a href="#" title="Coming soon">Italiano (presto disponibile)</a>
        | <a href="#" title="Coming soon">Русский (presto disponibile)</a>
        | <a href="#" title="Coming soon">Português (presto disponibile)</a>
        | <a href="#" title="Coming soon">Nederlands (presto disponibile)</a>
        | <a href="#" title="Coming soon">Polski (presto disponibile)</a>
        | <a href="#" title="Coming soon">العربية (presto disponibile)</a>
        | <a href="#" title="Coming soon">فارسی (presto disponibile)</a>
        | <a href="#" title="Coming soon">Türkçe (presto disponibile)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (presto disponibile)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (presto disponibile)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM addestrato solo su dati di determinati periodi storici per ridurre i bias moderni.</p><p>Immagina se un modello AI non si limitasse a fingere di essere storico, ma lo fosse davvero.</p><p>Basato su <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a>. Gli script di training principali e l’architettura del modello sono opera sua.</p><h1>Obiettivi del Progetto</h1></p><p>TimeCapsule LLM è un progetto sperimentale che verrà addestrato solo su testi scritti in determinati periodi storici. L’obiettivo è simulare la visione del mondo e il linguaggio di specifiche epoche storiche.</p><h1>Perché il fine-tuning non basta</h1></p><p>Se si esegue solo un fine-tuning su un modello pre-addestrato, il vostro LLM conoscerà comunque concetti moderni. Ovviamente raggiungere un bias moderno pari a zero è difficile, ma voglio avvicinarmi il più possibile a questo obiettivo. Ottenere l’assenza di bias moderni richiede un addestramento del modello da zero.</p><h1>Risultati attesi</h1></p><p>Spero che, una volta completato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ciò su cui è stato addestrato. Non dovrebbe riconoscere vocaboli/conoscenze moderne e spero che non “allucini” conoscenze moderne.</p><h1>Aggiornamenti sui progressi</h1></p><h2>9 luglio 2025</h2></p><p>Ho fissato il mio periodo di riferimento tra il 1800 e il 1850 e la regione: Londra</p><p>Ho raccolto un elenco di testi, libri, documenti</p><p>Finora ne ho ottenuti 50 in formato txt e inizierò presto il training di NanoGPT</p><p>Aggiornerò questa sezione man mano che progredisco</p><h2>13 luglio 2025</h2></p><p>Ho addestrato nanoGPT con 187MB di testi storici.</p><h2>15 luglio 2025</h2></p><p>Ho iniziato a scaricare testi per la seconda sessione di training. Sto prendendo tutto da Internet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi, puoi usare i filtri per soggetto e ricerca per luogo di pubblicazione, periodo e argomenti su Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtri di Ricerca"></p><h2>16 luglio 2025</h2></p><p>Ho scaricato circa 500 file txt da Internet Archive e dopo averli ripuliti (eliminando solo spazi bianchi, header Gutenberg, ecc.) ho circa 500MB di dati. È un dataset piccolo ma l’ultima volta ho addestrato su 187MB quindi dovrei vedere almeno qualche differenza nell’output dopo aver addestrato il secondo modello. Spero che questo modello riesca almeno a produrre frasi più coerenti che abbiano un senso. Ovviamente non è garantito dato che il dataset è ancora minuscolo, ma è più grande rispetto all’ultima volta.</p><p>Questo dovrebbe essere fattibile sul mio hardware personale, il che è positivo perché posso vedere qualche miglioramento prima di passare a un dataset più grande che richiederebbe il noleggio di una GPU. Ma non preoccuparti, ho comunque intenzione di noleggiare presto una GPU, ma prima voglio assicurarmi che il mio dataset sia il più possibile curato e pulito. Uno dei problemi è la pulizia; molti di questi file txt hanno dentro del “gibberish”. Gli script che ho usato per la pulizia funzionano, ma non sono efficaci al 100%.</p><p>Allenerò questo dataset oggi e dovrebbe richiedere circa 4-5 ore. Una volta terminato e testato, fornirò aggiornamenti. Grazie ancora a tutti coloro che stanno seguendo il mio progetto, alcune persone mi hanno anche fornito link a risorse OCR quindi grazie! Spero che più persone provino a sperimentare con i propri dataset.</p><h2>28 luglio 2025</h2></p><p>Ho caricato la versione v0.5 su Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Dai un’occhiata</a> se vuoi. Ora puoi scaricare il mio repository ed eseguirlo localmente. Purtroppo nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello in locale.</p><p>Inoltre inizierò a curare i dati per la prossima sessione di training, credo mi serviranno forse 5-10 volte più dati per raggiungere capacità di ragionamento.</p><h3>Aggiornamento Training</h3></p><p>Ho iniziato il training su un corpus di 435MB (108 M di token), al momento sta andando piuttosto bene. La train loss è scesa da 10.9 a 4.9 nelle prime 2800 iterazioni. Penso che ci vorranno circa 8 o 9 ore per completarlo. Pubblicherò un altro aggiornamento quando sarà finito.</p><h2>17 luglio 2025, ore 2:13</h2></p><p>Il training è terminato per il secondo modello, la mia 4060 ha impiegato circa 8 ore e 40 minuti (3.900 iter/ora) per 33.000 iterazioni (5 epoche). La train loss finale è stata 3.73. Gli output erano sorprendentemente buoni: ora genera frasi coerenti in stile XIX secolo.</p><h1>Comportamento & Limitazioni del Modello V0</h1></p><p>I primi prompt mostrano che il modello risponde con linguaggio e comportamento da anni 1800. Ad esempio, ho chiesto "Who art Henry?" e ha risposto "I know that man, I have did not a black, the storm." e sì, quella frase non ha senso ma l’LLM sta riconoscendo che sto chiedendo di una persona.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Esempio Output">
Non c'è alcun riferimento a concetti moderni, gli output contengono principalmente parole ed espressioni dell’Ottocento.</p><p>C'è ancora molto lavoro da fare, addestrare su 187MB non ti darà un modello che produca testo con ragionamento complesso.</p><p>Al momento genera frasi che mancano di una struttura completa e in generale non hanno senso, ma questo è normale per la dimensione del training.</p><h1>Comportamento e Limitazioni del Modello V0.5</h1></p><p>Questo è un bel miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi tutte le frasi sono grammaticalmente corrette con la punteggiatura giusta. E anche questo è addestrato da zero quindi si attiene ai temi dell’Ottocento.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Esempio di Output TimeLockLLM"></p><p>Ci sono molte allucinazioni fattuali. Molti (tipo il 100%) dei dettagli (date, eventi, personaggi storici) sono inventati. Inoltre le frasi non hanno davvero collegamento tra loro, a volte forse 2 frasi sono correlate ma oltre non lo sono. Un altro problema è che a volte compare un footer “Digitized by Google”, quindi la prossima volta dovrò assicurarmi di pulire bene i testi. Nel complesso sono molto contento dei risultati, non è ancora un LLM ma sicuramente è un generatore di frasi.</p><p>Sto imparando molto e nelle prossime settimane cercherò di capire cosa devo migliorare. Caricherò presto i file!</p><h1>Piani Futuri</h1></p><p>(Completato) Inizierò a lavorare sulla versione 0.5, invece di addestrare usando 50 libri, userò idealmente 500-600. Al momento sto addestrando nanoGPT usando libri dal 1800 al 1850 e specificatamente da Londra. Ci sono delle sfide, come assicurarsi che i libri trovati non siano aggiornati o abbiano interpretazioni moderne ma siano libri intatti pubblicati nel periodo scelto.</p><p>Voglio addestrare un nuovo modello (v1) con un corpus molto più grande, magari 5-10 volte più grande di quello usato per la v0.5. Il mio obiettivo è vedere se riesco a far emergere capacità di ragionamento dal Solo Addestramento Temporale Selettivo, sarà un compito più difficile e non sono nemmeno sicuro che sia possibile a causa dei limiti dei dati storici. Nelle prossime settimane cercherò di curare abbastanza dati per un corpus di 5-10GB. Credo che se riuscirò a ottenere dati puliti di alta qualità e noleggiare una GPU, ci saranno progressi.</p><h1>Come Usare Questo Progetto</h1></p><p>Questo progetto si concentra principalmente sulla raccolta di dati storici, la loro preparazione per l’addestramento e la costruzione di un tokenizer. Non tratterò l’intero processo di addestramento LLM, per quello fai riferimento a nanoGPT di Andrej Karpathy.</p><h1>Passo 1: Raccogli e Prepara Testi Storici</h1></p><p>Raccogli file .txt di libri di pubblico dominio, documenti, ecc dal periodo scelto (es: Londra 1800-1850)</p><p>Puoi usare download_texts_improved.py per scaricare libri se ne hai bisogno.</p><p>Pulisci i file di testo usando uno script o rimuovi manualmente intestazioni/footer di Project Gutenberg, annotazioni moderne o errori OCR.</p><p>prepare_dataset.py dovrebbe funzionare bene.</p><h1>Passo 2: Costruisci un Tokenizer Personalizzato</h1></p><p>Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Questo ti darà vocab.json e merges.txt</p><p>Questi file definiscono il vocabolario e le regole di unione per il tuo modello</p><h1>Passo 3: Addestra il Tuo Modello (nanoGPT)</h1></p><p>Fai riferimento a <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a> per il processo di addestramento.</p><p>Puoi addestrare un altro LLM se vuoi, ma io ho usato nanoGPT</p><h1>FAQ</h1></p><h2>Cos’è il Selective Temporal Training?</h2></p><p>Il Selective Temporal Training (STT) è una metodologia di machine learning in cui tutti i dati di addestramento sono curati specificamente per rientrare in un determinato periodo storico. Si fa per modellare il linguaggio e la conoscenza di quell’epoca senza l’influenza di concetti moderni. Ad esempio, il modello che ho ora (v0.5) è addestrato esclusivamente su dati dal 1800 al 1875, non è fine tuned ma addestrato da zero, producendo output che riflettono lo stile linguistico e il contesto storico di quel periodo.</p><h2>Perché non usare solo fine-tuning o LoRA?</h2></p><p>Per questo progetto sto cercando di creare un modello linguistico non influenzato da bias moderni. Se faccio fine-tuning su qualcosa come GPT-2, è già pre-addestrato e quell’informazione non andrà via. Se addestro da zero il modello non fingerà di essere antico, lo sarà davvero. L’obiettivo per ora è creare qualcosa che possa ragionare esclusivamente usando conoscenze tratte da libri londinesi pubblicati tra il 1800 e il 1850.</p><h2>Che tipo di dati hai usato per l’addestramento?</h2></p><p>Uso libri, documenti legali, giornali e altri scritti della Londra 1800–1850. La lista che ho linkato ne ha circa 200 ma per il primo addestramento ho usato solo 50 file per circa ~187 MB. Puoi vedere una lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Quanto è grande la Versione 0 del modello?</h2></p><p>Questo modello è molto piccolo al momento, lo sto facendo solo per divertimento e seguendo una regola ferrea di nessuna fonte moderna. Ha quasi 16 milioni di parametri ma inizierò a raccogliere altri testi antichi per iniziare un nuovo addestramento. Darò aggiornamenti strada facendo.</p><h2>Specifiche dell’addestramento?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>