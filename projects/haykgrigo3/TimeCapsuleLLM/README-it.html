<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM addestrato solo su dati di determinati periodi storici per ridurre i bias moderni</title>
    <meta name="description" content="Un LLM addestrato solo su dati di determinati periodi storici per ridurre i bias moderni">
    <meta name="keywords" content="TimeCapsuleLLM, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM addestrato solo su dati di determinati periodi storici per ridurre i bias moderni",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">Italian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM addestrato solo su dati di determinati periodi per ridurre i bias moderni.</p><p>Immagina se un modello AI non si limitasse a fingere di essere storico ma lo fosse davvero.</p><p>Basato su <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a>. Gli script di addestramento principali e l'architettura del modello sono opera sua.</p><h1>Obiettivi del Progetto</h1></p><p>TimeCapsule LLM è un progetto sperimentale che verrà addestrato solo su testi scritti in determinati periodi storici. L'obiettivo è simulare la visione del mondo e il linguaggio di specifiche epoche storiche.</p><h1>Perché il fine tuning non basta</h1></p><p>Se esegui solo un fine tuning su un modello pre-addestrato, il tuo LLM conoscerà comunque concetti moderni. Ovviamente ottenere zero bias moderno è difficile, ma voglio avvicinarmi il più possibile a questo risultato. Per non avere bias moderni è necessario addestrare un modello da zero.</p><h1>Risultati attesi</h1></p><p>Si spera che, una volta terminato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ciò su cui è stato addestrato. Non dovrebbe riconoscere concetti o vocabolario moderni e spero che non "allucini" conoscenze moderne.</p><h1>Aggiornamenti sui Progressi</h1></p><h2>9 luglio 2025</h2></p><p>Ho fissato il mio periodo di interesse: 1800-1850 e regione: Londra</p><p>Ho raccolto un elenco di testi, libri, documenti</p><p>Finora ne ho ottenuti 50 in formato txt e inizierò presto l’addestramento di NanoGPT</p><p>Aggiornerò questa sezione man mano che ci saranno progressi</p><h2>13 luglio 2025</h2></p><p>Ho addestrato nanoGPT con 187MB di dati testuali storici.</p><h2>15 luglio 2025</h2></p><p>Ho iniziato a scaricare testi per la seconda sessione di addestramento. Sto prendendo tutto da Internet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi, puoi usare i filtri per soggetto e ricerca relativi al luogo di pubblicazione, periodo e argomenti su Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtri di Ricerca"></p><h2>16 luglio 2025</h2></p><p>Ho scaricato circa 500 file txt da Internet Archive e dopo averli ripuliti (eliminando spazi bianchi, intestazioni Gutenberg, ecc.) ho circa 500MB di dati. È un dataset piccolo ma la volta scorsa ho addestrato con 187MB quindi dovrei vedere almeno qualche differenza nell’output dopo aver addestrato il secondo modello. Spero che questo modello riesca almeno a produrre frasi più coerenti che abbiano un certo senso. Ovviamente non è garantito visto che il dataset è ancora molto piccolo, ma è più di quanto usato la volta scorsa.</p><p>Dovrebbe essere fattibile con il mio hardware, ed è anche positivo perché spero di vedere qualche miglioramento prima di passare a un dataset più grande che richiederebbe di affittare una GPU. Ma non preoccupatevi, ho comunque intenzione di noleggiare una GPU a breve, ma prima voglio assicurarmi che il mio dataset sia il più curato e pulito possibile. Uno dei problemi che ho è la pulizia, molti di questi file txt contengono del testo insensato. Gli script che ho usato funzionano ma non sono efficaci al 100%.</p><p>Allenerò questo dataset oggi e dovrebbe richiedere circa 4-5 ore. Una volta completato e testato, fornirò aggiornamenti. Grazie ancora a tutti quelli che stanno seguendo il mio progetto, alcune persone mi hanno anche segnalato risorse OCR quindi grazie! Spero che altri provino a fare esperimenti con i propri dataset.</p><h3>Aggiornamento Addestramento</h3></p><p>Ho iniziato l’addestramento su un corpus di 435MB (108 milioni di token), sta andando abbastanza bene al momento. La train loss è scesa da 10.9 a 4.9 nelle prime 2800 iterazioni. Credo ci vorranno circa 8 o 9 ore per completare. Pubblicherò un altro aggiornamento una volta terminato.</p><h2>17 luglio 2025 2:13AM</h2></p><p>L’addestramento è terminato per il secondo modello, la mia 4060 ha impiegato circa 8 ore e 40 minuti (3.900 iter/ora) per 33.000 iterazioni (5 epoche). La train loss finale è stata 3.73. Gli output sono stati sorprendentemente buoni: ora genera davvero frasi in stile XIX secolo in modo coerente.</p><h2>28 luglio 2025</h2></p><p>Ho caricato la versione 0.5 su Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Dai un’occhiata</a> se vuoi. Ora puoi scaricare la mia repo ed eseguirla localmente. Sfortunatamente nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello localmente.</p><p>Inoltre inizierò a curare i dati per la prossima sessione di addestramento. Credo che avrò bisogno di almeno 5-10 volte più dati per ottenere capacità di ragionamento.</p><h1>Comportamento e Limiti del Modello V0</h1></p><p>I primi prompt mostrano che il modello risponde con linguaggio e comportamento da 1800. Ad esempio, l’ho interrogato con “Who art Henry?” e ha risposto “I know that man, I have did not a black, the storm.” e sì, quella frase non ha senso ma l’LLM riconosce che sto chiedendo di una persona.</p><p>
<img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Esempio di Output TimeLockLLM"></p><p>Non c'è alcun riferimento a concetti moderni, gli output contengono per lo più parole e formulazioni tipiche del 1800.</p><p>C'è ancora molto lavoro da fare, addestrarsi su 187MB non permette di ottenere un modello capace di produrre testo con ragionamento complesso.</p><p>Attualmente produce frasi che mancano di una struttura completa e in generale non hanno senso, ma ciò è normale per la dimensione del dataset di addestramento.</p><h1>Comportamento e Limitazioni del Modello V0.5</h1></p><p>Questo rappresenta un notevole miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi ogni frase è grammaticalmente corretta, con la punteggiatura appropriata. E di nuovo, essendo stato addestrato da zero, resta fedele agli argomenti dell’800.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Esempio di Output TimeLockLLM"></p><p>Sono presenti molte allucinazioni fattuali. Molti (come il 100%) dei dettagli (date, eventi, personaggi storici) sono inventati. Inoltre, le frasi spesso non sono collegate tra loro: a volte forse 2 frasi hanno relazione, ma oltre a ciò non lo sono. Un altro problema è che talvolta compare un footer errante “Digitized by Google”, quindi la prossima volta che addestro dovrò davvero assicurarmi che i testi siano ben puliti. In generale, sono molto soddisfatto dei risultati: non è ancora lontanamente un LLM ma sicuramente è un generatore di frasi.</p><p>Sto imparando molto e presto cercherò di capire cosa devo migliorare nelle prossime settimane. Caricherò i file a breve!</p><h1>Piani Futuri</h1></p><p>(Completato) Inizierò a lavorare sulla versione 0.5; invece di usare 50 libri per l’addestramento, ne userò idealmente 500-600. Attualmente sto addestrando nanoGPT usando libri dal 1800-1850 e specificamente da Londra. Ci sono alcune sfide, come assicurarsi che i libri trovati non siano stati aggiornati o abbiano interpretazioni moderne, ma siano libri intatti pubblicati nel periodo scelto.</p><p>Voglio addestrare un nuovo modello (v1) con un corpus molto più grande, magari 5-10 volte più grande di quello usato per v0.5. Il mio obiettivo è vedere se è possibile far emergere capacità di ragionamento solo dal Selective Temporal Training, sarà un compito più difficile e non sono nemmeno sicuro che sia possibile a causa delle limitazioni dei dati storici. Nelle prossime settimane cercherò di curare abbastanza dati per un corpus da 5-10GB. Credo che se riuscirò a ottenere dati puliti e di alta qualità e affittare una GPU, ci saranno progressi.</p><h1>Come Utilizzare Questo Progetto</h1></p><p>Questo progetto si concentra principalmente sulla raccolta di dati storici, sulla loro preparazione per l’addestramento e sulla costruzione di un tokenizer. Non tratterò l’intero processo di addestramento LLM; per quello, fare riferimento a nanoGPT di Andrej Karpathy.</p><h1>Passo 1: Raccogliere e Preparare Testi Storici</h1></p><p>Raccogli file .txt di libri, documenti, ecc. di pubblico dominio dal periodo scelto (es: Londra 1800-1850)</p><p>Puoi usare download_texts_improved.py per scaricare i libri, se necessario.</p><p>Pulisci i file di testo usando uno script o rimuovi manualmente header/footer di Project Gutenberg, annotazioni moderne o errori OCR.</p><p>prepare_dataset.py dovrebbe funzionare correttamente.</p><h1>Passo 2: Costruisci un Tokenizer Personalizzato</h1></p><p>Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Questo ti darà vocab.json e merges.txt</p><p>Questi file definiscono il vocabolario e le regole di unione per il tuo modello</p><h1>Passo 3: Addestra il Tuo Modello (nanoGPT)</h1></p><p>Fare riferimento a <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a> per il processo di addestramento.</p><p>Puoi addestrare un LLM diverso se vuoi, ma io ho usato nanoGPT</p><h1>FAQ</h1></p><h2>Cos'è il Selective Temporal Training?</h2></p><p>Il Selective Temporal Training (STT) è una metodologia di machine learning in cui tutti i dati di addestramento sono specificamente curati per ricadere in un determinato periodo storico. Serve a modellare il linguaggio e le conoscenze di quell’epoca senza influenze da concetti moderni. Per esempio, il modello attuale (v0.5) è addestrato esclusivamente su dati dal 1800 al 1875, non è fine-tuned ma addestrato da zero, producendo output che riflette lo stile linguistico e il contesto storico di quel periodo.</p><h2>Perché non usare semplicemente il fine-tuning o LoRA?</h2></p><p>Per questo progetto sto cercando di creare un modello linguistico privo di bias moderni. Se faccio il fine-tuning di qualcosa come GPT-2, è già pre-addestrato e quell’informazione non scompare. Se invece addestro da zero, il modello linguistico non fingerà di essere antico, lo sarà davvero. L'obiettivo per ora è creare qualcosa che possa ragionare esclusivamente usando conoscenze tratte da libri londinesi pubblicati tra il 1800 e il 1850.</p><h2>Che tipo di dati hai usato per l’addestramento?</h2></p><p>Sto usando libri, documenti legali, giornali e altri scritti da Londra 1800–1850. La lista che ho linkato ne contiene circa 200, ma per il primo addestramento ho usato solo 50 file per circa 187 MB. Puoi consultare la lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Quanto è grande il modello Versione 0?</h2></p><p>Questo modello è molto piccolo al momento, lo sto facendo solo per divertimento e seguo una regola rigorosa di non includere fonti moderne. Ha quasi 16 milioni di parametri ma inizierò a raccogliere altri testi antichi per cominciare un nuovo addestramento. Darò aggiornamenti man mano.</p><h2>Specifiche di Addestramento?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>