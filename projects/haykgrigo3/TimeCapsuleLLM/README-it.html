<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM addestrato solo su dati di determinati periodi per ridurre i bias moderni</title>
    <meta name="description" content="Un LLM addestrato solo su dati di determinati periodi per ridurre i bias moderni">
    <meta name="keywords" content="TimeCapsuleLLM, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM addestrato solo su dati di determinati periodi per ridurre i bias moderni",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 275
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-07",
  "dateModified": "2025-08-07"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 275 stars</span>
                <span class="language">Italian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM addestrato solo su dati di determinati periodi storici per ridurre il bias moderno.</p><p>Immagina se un modello AI non si limitasse a fingere di essere storico, ma lo fosse davvero.</p><p>Basato su <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a> Gli script di addestramento principali e l’architettura del modello sono opera sua.</p><h1>Obiettivi del Progetto</h1></p><p>TimeCapsule LLM è un progetto sperimentale che sarà addestrato solo su testi scritti in determinati periodi storici. L’obiettivo è simulare la visione del mondo e la lingua di specifiche epoche storiche.</p><h1>Perché il fine tuning non basta</h1></p><p>Se fai solo il fine tuning di un modello pre-addestrato, il tuo LLM conoscerà comunque concetti moderni. Ovviamente raggiungere zero bias moderno è difficile ma voglio avvicinarmi il più possibile a questo. Azzerare il bias moderno richiede l’addestramento di un modello da zero.</p><h1>Risultati attesi</h1></p><p>Si spera che, una volta completato, questo modello non conosca concetti moderni e non sia in grado di ragionare oltre ciò su cui è stato addestrato. Non dovrebbe riconoscere concetti/vocaboli moderni e spero che non “allucini” conoscenze moderne.</p><h1>Aggiornamenti sui progressi</h1></p><h2>9 luglio 2025</h2></p><p>Ho impostato il mio periodo di riferimento tra il 1800 e il 1850 e la regione: Londra</p><p>Ho raccolto una lista di testi, libri, documenti</p><p>Finora ne ho ottenuti 50 come file txt e inizierò presto l’addestramento di NanoGPT</p><p>Aggiornerò questa sezione man mano che farò progressi</p><h2>13 luglio 2025</h2></p><p>Ho addestrato nanoGPT con 187MB di testi storici.</p><h2>15 luglio 2025</h2></p><p>Ho iniziato a scaricare testi per la seconda sessione di addestramento. Sto prendendo tutto dall’Internet Archive e ho ampliato il periodo a 1800-1875. Per ottenere una gamma diversificata di testi puoi usare i filtri di ricerca per luogo di pubblicazione, periodo storico e argomenti su Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtri di ricerca"></p><h2>16 luglio 2025</h2></p><p>Ho scaricato circa 500 file txt da Internet Archive e, dopo averli ripuliti (solo spazi bianchi, intestazioni Gutenberg, ecc.), ho circa 500MB di dati. È un dataset minuscolo ma l’ultima volta ho addestrato con 187MB quindi dovrebbe esserci almeno qualche differenza rilevabile nell’output dopo aver addestrato il secondo modello. Spero che questo modello possa almeno produrre frasi più coerenti che abbiano senso. Ovviamente non è garantito, dato che il dataset è ancora minuscolo, ma è comunque più di quanto usato l’ultima volta.</p><p>Questo dovrebbe essere fattibile sull’hardware che ho, il che è positivo perché spero di vedere qualche miglioramento prima di passare a un dataset più grande che richiederebbe l’affitto di una GPU. Ma non preoccupatevi, ho comunque intenzione di noleggiare una GPU presto, ma prima voglio essere sicuro che il mio dataset sia il più curato e pulito possibile. Uno dei problemi che ho è la pulizia, molti di questi file txt hanno dentro del “gibberish”. Gli script che ho usato funzionano ma non sono efficaci al 100%.</p><p>Allenerò questo dataset oggi e dovrebbe richiedere circa 4-5 ore. Una volta terminato e testato, vi aggiornerò. Grazie ancora a tutti quelli che stanno seguendo il mio progetto, alcune persone mi hanno anche mandato link a risorse OCR quindi grazie! Spero che più persone provino a sperimentare con i propri dataset.</p><h3>Aggiornamento sull’addestramento</h3></p><p>Ho iniziato ad addestrare su un corpus di 435MB (108 M di token), al momento sta andando piuttosto bene. La train loss è scesa da 10,9 a 4,9 nelle prime 2800 iterazioni. Credo che ci vorranno circa 8 o 9 ore per completare. Pubblicherò un altro aggiornamento una volta finito.</p><h2>17 luglio 2025</h2></p><p>L’addestramento è terminato per il secondo modello, la mia 4060 ci ha messo circa 8 ore e 40 minuti (3.900 iteraz./ora) per 33.000 iterazioni (5 epoche). La train loss finale è stata di 3,73. Gli output erano sorprendentemente buoni, ora genera davvero frasi coerenti in stile XIX secolo.</p><h2>28 luglio 2025</h2></p><p>Ho caricato la versione v0.5 su Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Dai un’occhiata</a> se vuoi. Ora puoi scaricare il mio repo ed eseguirlo localmente. Sfortunatamente nanoGPT non funziona nativamente con HuggingFace, quindi dovrai scaricare ed eseguire il modello in locale.</p><p>Inoltre inizierò a curare dati per la prossima sessione di addestramento, credo che mi serviranno forse 5-10 volte più dati per ottenere capacità di ragionamento.</p><h2>2 agosto 2025</h2></p><p>Presto inizierò a lavorare sulla Versione 1. Dovrò passare dall’architettura di nanoGPT a qualcosa di più moderno. Ho in mente diverse architetture LLM open source, tra cui: OpenLLaMA v3, Phi-2 e Qwen 1.5B. E per supportare il salto a V1, dovrò curare con attenzione un dataset molto più grande e diversificato. Mi serviranno almeno 5GB di dati di addestramento puliti.</p><h1>Comportamento e Limitazioni del Modello V0</h1></p><p>I primi prompt mostrano il modello rispondere con linguaggio e comportamenti dell’Ottocento. Ad esempio, l’ho stimolato con "Who art Henry?" e ha risposto "I know that man, I have did not a black, the storm." e sì, quella frase non ha senso ma il LLM riconosce che sto chiedendo di una persona.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Esempio Output TimeLockLLM"></p><p>Non ci sono riferimenti a concetti moderni, gli output contengono per lo più parole e frasi dell’Ottocento.</p><p>Ha ancora bisogno di molto lavoro, l’addestramento su 187MB non produrrà un modello capace di ragionamenti complessi.</p><p>Al momento genera frasi prive di struttura completa e generalmente senza senso, ma questo è normale per la dimensione del training.</p><h1>Comportamento e Limitazioni del Modello V0.5</h1></p><p>Questo è un bel miglioramento rispetto al modello precedente. Lo stile di scrittura e il vocabolario sono vittoriani e quasi ogni frase è grammaticalmente corretta con la punteggiatura giusta. Anche questo è addestrato da zero quindi si attiene ai temi dell’Ottocento.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Esempio Output TimeLockLLM"></p><p>Ci sono molte allucinazioni fattuali. Molti (tipo il 100%) dei dettagli (date, eventi, figure storiche) sono inventati. Inoltre le frasi non hanno davvero connessioni tra loro, a volte forse 2 frasi si collegano ma oltre no. Un altro problema è che a volte appare un footer “Digitized by Google” fuori posto, quindi la prossima volta dovrò pulire meglio i testi. Nel complesso sono molto soddisfatto dei risultati, non è ancora un LLM ma sicuramente un generatore di frasi.</p><p>Sto imparando molto e nelle prossime settimane capirò cosa devo migliorare. Caricherò presto i file!</p><h1>Piani Futuri</h1></p><p>(Completato) Inizio a lavorare sulla versione 0.5, invece di usare 50 libri per il training, ne userò idealmente 500-600. Ora sto addestrando nanoGPT usando libri dal 1800 al 1850 e specificamente da Londra. Ci sono alcune sfide, come assicurarsi che i libri trovati non siano aggiornati o con interpretazioni moderne ma rimasti intatti pubblicati nel periodo scelto.</p><p>Voglio addestrare un nuovo modello (v1) con un corpus molto più grande, magari 5-10 volte quello usato per v0.5. Il mio obiettivo è vedere se riesco a far emergere capacità di ragionamento dal solo Selective Temporal Training, sarà più difficile e non sono neanche sicuro sia possibile dato che ci sono limitazioni storiche nei dati. Nelle prossime settimane cercherò di curare abbastanza dati per un corpus da 5-10GB. Credo che se riesco ad avere dati puliti e di alta qualità e ad affittare una GPU, ci sarà progresso.</p><h1>Come Usare Questo Progetto</h1></p><p>Questo progetto si concentra soprattutto sulla cura di dati storici, la loro preparazione per il training e la costruzione di un tokenizer. Non coprirò l’intero processo di training di un LLM, per quello fai riferimento a nanoGPT di Andrej Karpathy.</p><h1>Passo 1: Raccogli e Prepara Testi Storici</h1></p><p>Raccogli file .txt di libri di dominio pubblico, documenti, ecc. dal periodo scelto (es. Londra 1800-1850)</p><p>Puoi usare download_texts_improved.py per scaricare libri se necessario.</p><p>Pulisci i file di testo tramite script o manualmente rimuovendo header/footer di Project Gutenberg, annotazioni moderne o errori OCR.</p><p>prepare_dataset.py dovrebbe andare bene.</p><h1>Passo 2: Costruisci un Tokenizer Personalizzato</h1></p><p>Esegui train_tokenizer.py o train_tokenizer_hf.py sui dati puliti.
Otterrai vocab.json e merges.txt</p><p>Questi file definiscono vocaboli e regole di merge per il tuo modello</p><h1>Passo 3: Addestra il Tuo Modello (nanoGPT)</h1></p><p>Fai riferimento a <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT di Andrej Karpathy</a> per il processo di training.</p><p>Puoi addestrare un altro LLM se vuoi, ma io ho usato nanoGPT</p><h1>FAQ</h1></p><h2>Cos’è il Selective Temporal Training?</h2></p><p>Il Selective Temporal Training (STT) è una metodologia di machine learning in cui tutti i dati di training sono curati specificamente per rientrare in un determinato periodo storico. Serve a modellare il linguaggio e la conoscenza di quell’epoca senza influenze moderne. Ad esempio, il modello attuale (v0.5) è addestrato esclusivamente su dati dal 1800 al 1875, non è fine-tuned ma addestrato da zero producendo output che riflettono stile linguistico e contesto storico di quel periodo.</p><h2>Perché non usare solo fine-tuning o LoRA?</h2></p><p>Per questo progetto sto cercando di creare un modello linguistico privo di bias moderno. Se faccio fine-tuning su qualcosa tipo GPT-2, è già pre-addestrato e quell’informazione non sparirà. Se addestro da zero il modello non fingerà di essere antico, lo sarà davvero. L’obiettivo ora è creare qualcosa che possa ragionare solo usando conoscenza da libri londinesi pubblicati tra il 1800 e il 1850.</p><h2>Che tipo di dati hai usato per l’addestramento?</h2></p><p>Uso libri, documenti legali, giornali e altri scritti dalla Londra del 1800–1850. La lista che ho linkato ne ha circa 200 ma per il primo training ho usato solo 50 file, circa ~187 MB. Puoi vedere la lista dei documenti:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Quanto è grande il modello Versione 0?</h2></p><p>Questo modello è molto piccolo al momento, lo faccio solo per divertimento e seguendo la regola ferrea di nessuna fonte moderna. Ha quasi 16 milioni di parametri ma inizierò a raccogliere altri testi antichi per un nuovo training. Aggiornerò man mano.</p><h2>Specifiche di Training?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-07 
    </div>
    
</body>
</html>