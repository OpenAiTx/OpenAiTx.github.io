<!DOCTYPE html>
<html lang="ar">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - نموذج لغة كبير (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل التحيز الحديث</title>
    <meta name="description" content="نموذج لغة كبير (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل التحيز الحديث">
    <meta name="keywords" content="TimeCapsuleLLM, Arabic, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "نموذج لغة كبير (LLM) تم تدريبه فقط على بيانات من فترات زمنية معينة لتقليل التحيز الحديث",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 1248
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-ar.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2026-01-13",
  "dateModified": "2026-01-13"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 1248 stars</span>
                <span class="language">Arabic</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 اللغة</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-TW">繁體中文</a> 
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=as">हिन्दी</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=th">ไทย</a> 
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=es">Español</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=ar">العربية</a>
        | <a href="#" title="قريباً">فارسی (قريباً)</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/#/view?user=haykgrigo3&project=TimeCapsuleLLM&lang=vi">Tiếng Việt</a>
        | <a href="#" title="قريباً">Bahasa Indonesia (قريباً)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1></p><p><em>نموذج لغوي تم تدريبه <strong>من الصفر</strong> حصرياً على بيانات من أماكن وفترات زمنية معينة للحد من التحيز العصري وتقليد صوت ومفردات ورؤية العصر.</em></p><p>تخيل لو أن نموذج الذكاء الاصطناعي لم يكن يتظاهر فقط بأنه تاريخي، بل كان كذلك فعلاً.</p><p>تم بناء الإصدار v0 وv0.5 على <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT بواسطة Andrej Karpathy</a> نصوص التدريب الأساسية وبنية النموذج هي من عمله.</p><p>v1 مبني على <a href="https://huggingface.co/microsoft/phi-1_5" target="_blank" rel="noopener noreferrer">Phi 1.5 من مايكروسوفت</a></p><p>v2 مبني على llamaforcausallm</p><p><a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">رابط Hugging Face</a></p><h2> سلوك النموذج والقيود</h2></p><h3><strong>v0</strong>  </h3></p><p>العينات المبكرة أظهرت استجابة النموذج بلغة وسلوك القرن التاسع عشر.
مثال: السؤال: "من هو هنري؟" وأجاب: "أعرف هذا الرجل، لم أفعل شيئًا أسودًا، العاصفة."</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="مخرجات نموذج TimeLockLLM"></p><ul><li>لا ذكر للمفاهيم الحديثة  </li>
<li>مفردات دقيقة لعصرها في الغالب  </li>
<li>الجمل غالبًا غير مترابطة (متوقع لبيانات تدريب ~187 ميغابايت)</li></p><p></ul><h3><strong>v0.5</strong></h3></p><p>تحسن كبير مقارنة بالإصدار v0.
<ul><li>أسلوب كتابة فيكتوري، علامات ترقيم مناسبة، جمل سليمة نحويًا في الغالب  </li>
<li>لا زال هناك معدل عالٍ من الهلوسة الواقعية  </li>
<li>ضوضاء OCR ("Digitized by Google") ما زالت تظهر في النتائج</li></p><p></ul><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="مخرجات نموذج TimeLockLLM"></p><h3><strong>v1</strong></h3></p><p>أول نموذج يستدعي ويربط حدثًا تاريخيًا حقيقيًا مع شخصية فعلية من مجموعة البيانات.</p><p>مثال: السؤال: "كان عام 1834 للميلاد"</p><p>الناتج: "كان عام 1834 للميلاد وكانت شوارع لندن مليئة بالاحتجاجات والعرائض. والسبب، كما ذكر كثيرون، لم يكن مرتبطًا بالخصوصية، لكن تم اتخاذ نفس اليوم في يوم اللورد بالمرستون، وسيتلقى الجمهور بيانًا موجزًا عن الصعوبات التي وصلت إليها قوانين اليوم. إنه لأمر مؤسف جدًا أن الأحداث الجارية في تاريخ العالم واضحة، وبالتالي ستُعرف. ليس صحيحًا أن الرجال الذين استقروا أولاً في الإنجيل في القدس يجب أن يكون لديهم سجل واسع ومثير للاهتمام عن الازدهار والرخاء"</p><p>في البداية افترضت أن احتجاجًا ربما حدث بالصدفة في نفس السنة، ولكن انظر إلى هذا: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3><strong>v2mini-eval1</strong></h3>
تم التدريب باستخدام عينة بحجم 15 جيجابايت من مجموعة بيانات الإصدار الثاني التي تبلغ 90 جيجابايت.
تم تدريب النموذج حتى 10 آلاف خطوة فقط.</p><p>كانت هناك مشكلة في الترميز تسبب في ظهور المخرجات بهذا الشكل:</p><p>المطالبة:
من هو تشارلز ديكنز؟</p><p>W ho is Charles D ic ens ? D oes that work more of h ise x cell ent st ir ring , in his pl ays , int he G reat C omp any 's f arm ? What I have y et to qu ote from J ack P ick ett ? D oy oun ot know th att hey were as pe cies of g all oping , ors aw ing oft heir bre asts , or what w ast ob ed one about the t im ew hen J acks on was looking ont hew ind ow ? What ist hes uccess of an English man , and which h isson has not be end is cover ing t ome , where by to acc omp lish such a we ight ? D idy ou ever m ake ap ass age int othe old road way , ort o an anch or - b ree ze att he foot of our boat , which you m ust le ave us ? The fact is , that whe ther the wind would rise up from the pl ain on S atur day night orn o ont ide , ort ill then orth , or other wise , wes hall be com pelled t od esc ribe af orm idable bar rier , with t hes ame e ffect s ast he p resent . In th iss itu ation , at le ast , itis not to omuch t os ay that we have le ft that room . I bel ieve there are three c op ies int he " F ive H undred - fold ," to bere fer red to , ast he first n umber of our read ers wh ow is ht
مصَحح:</p><p>"من هو تشارلز ديكنز؟ هل يعمل بذلك بشكل أكثر إثارة في مسرحياته، في مزرعة الشركة الكبرى؟ ما الذي لم أقتبسه بعد من جاك بيكيت؟ ألا تعلم أنهم كانوا نوعًا من الجري السريع، أو نشر صدورهم، أو ما الذي كان يجب فعله في الوقت الذي كان فيه جاكسون ينظر من النافذة؟ ما هو نجاح الإنجليزي، والذي لم يكتشفه ابنه لي بعد، والذي من خلاله يمكن تحقيق مثل هذا العبء؟ هل سبق لك أن عبرت الطريق القديم، أو إلى نسيم المرسى عند قدم قاربنا، والذي يجب عليك أن تتركنا عنده؟ الواقع هو، أنه سواء ارتفعت الرياح من السهل مساء السبت أو في الظهيرة، أو حتى الشمال، أو غير ذلك، سنضطر لوصف حاجز هائل، مع نفس التأثيرات كما هي الآن. في هذا الوضع، على الأقل، ليس من المبالغة القول أننا تركنا تلك الغرفة. أعتقد أن هناك ثلاث نسخ في 'خمسمئة ضعف'، يمكن الرجوع إليها، كأول عدد من قرائنا الذين يرغبون ف[ـ]..."</p><h3><strong>v2mini-eval2</strong></h3></p><p>تم التدريب باستخدام نفس العينة بحجم 15 غيغابايت المستخدمة في تدريب eval1.
النموذج تم تدريبه مجددًا حتى 10,000 خطوة فقط.</p><p>المُحَفِّز: تشارلز داروين</p><p>تشارلز داروينECCEMACY. سيدي، — الحالة التالية تهمني: — كنت في لندن لمدة أسبوعين، وتأثرت كثيرًا بنوبة من الروماتيزم. كانت أول نوبة روماتيزم قبل أسبوع من رؤيتك، والثانية عندما رأيتك، والثالثة عندما رأيتك، والثالثة في نفس الوقت. لكن النوبة الثانية من النقرس لم تكن مصحوبة بأي أعراض حمى، بل صاحبها زيادة في تدفق البول، وتفريغ أكثر غزارة للبول. النوبة الثالثة جاءت بعد ساعة من رؤيتك، وتبعتها عودة نوبة من النقرس، وعودة أسرع للنقرس. النوبة الرابعة صاحبتها أيضًا حمى، لكنها لم تكن دائمًا مصحوبة بأي أعراض حمى. النوبة الثالثة من النقرس كانت بعد أسبوعين من مرضك، والرابعة تبعتها نوبة من النقرس. النوبة الرابعة جاءت بعد أسبوعين من إصابتك، وصاحبتها إحساس</p><h2>مجموعات البيانات</h2></p><h3><strong>v2</strong></h3></p><ul><li>90 غيغابايت من نصوص لندن بين 1800-1875</li>
<li>136,344 وثيقة</li>
<li>لم تتوفر الـ 90 غيغابايت كاملة بعد لأنها لم تُحوّل إلى رموز بعد، لكن يمكنك إيجاد عينة 15 غيغابايت هنا: https://huggingface.co/datasets/haykgrigorian/TimeCapsuleLLM-London-1800-1875-v2-15GB</li></p><p> </ul>### إحصائيات التحيز 
  <img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/pronoun_bias.png" alt="تحيز الضمائر"></p><p>  <img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/geographic_bias.png" alt="التحيز الجغرافي"></p><p>  <img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/temporal_bias.png" alt="التحيز الزمني"></p><p>يرجى الرجوع إلى <a href="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v2mini_eval1/v2_bias_report.json" target="_blank" rel="noopener noreferrer">تقرير تحيز v2</a> لمزيد من المعلومات.</p><h2>كيفية الاستخدام</h2></p><p>يركز هذا المشروع بشكل أساسي على تنسيق البيانات التاريخية، وتجهيزها للتدريب وبناء محلل الرموز tokenizer. لن أغطي هنا عملية تدريب LLM الكاملة، لذلك راجع nanoGPT بواسطة أندريه كارباثي.</p><h3>الخطوة 1: جمع وتحضير النصوص التاريخية</h3></p><ul><li>جمع ملفات .txt من الكتب، الوثائق، وغيرها من المصادر المتاحة للاستخدام العام من الفترة الزمنية التي تختارها (مثل لندن 1800-1850)</li></p><p><li>احتفظ بها ضمن نافذة الوقت/المكان التي اخترتها  </li>
<li>نظف ملفات النصوص باستخدام برنامج نصي أو قم بإزالة الرؤوس/التذييلات يدوياً من مشروع غوتنبرغ، أو التعليقات الحديثة أو أشياء مثل أخطاء OCR.</li></p><p></ul><h3>الخطوة 2: بناء محلل رموز مخصص</h3></p><ul><li>شغل train_tokenizer.py أو train_tokenizer_hf.py على البيانات المنظفة.</li>
<li>سيعطيك ذلك vocab.json و merges.txt</li>
<li>هذه الملفات تحدد المفردات وقواعد الدمج لنموذجك</li></p><p></ul><h3>الخطوة 3: درب نموذجك </h3></p><ul><li>ارجع إلى <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT بواسطة Andrej Karpathy</a> لعملية التدريب أو وثائق البنية التي اخترتها.</li></p><p></ul><h1>الأسئلة الشائعة</h1></p><h2>ما هو التدريب الزمني الانتقائي ؟</h2></p><p>التدريب الزمني الانتقائي (STT) هو منهجية تعلم آلي حيث يتم اختيار كل بيانات التدريب لتندرج ضمن فترة زمنية تاريخية محددة. يتم ذلك من أجل نمذجة اللغة والمعرفة لتلك الحقبة دون التأثر بالمفاهيم الحديثة. على سبيل المثال، النموذج الحالي (v0.5) تم تدريبه حصرياً على بيانات من 1800-1875، ولم يتم تحسينه بل تم تدريبه من الصفر، مما أدى إلى إنتاج يعكس الأسلوب اللغوي والسياق التاريخي لتلك الفترة.</p><h2>لماذا لا تستخدم فقط الضبط الدقيق أو LoRA؟</h2></p><p>لهذا المشروع أحاول إنشاء نموذج لغة غير متأثر بالانحياز الحديث. إذا قمت بضبط شيء مثل GPT-2، فهو مدرب مسبقاً بالفعل وهذه المعلومات لن تختفي. إذا دربت من الصفر، لن يتظاهر نموذج اللغة بأنه قديم، بل سيكون كذلك فعلاً. الهدف لهذا المشروع الآن هو إنشاء شيء يمكنه الاستنتاج حصرياً باستخدام المعرفة من كتب لندن المنشورة بين 1800 و 1875.</p><h2>ما نوع البيانات التي استخدمتها للتدريب؟</h2></p><p>أستخدم كتباً، وثائق قانونية، صحفاً، وكتابات أخرى من لندن بين 1800–1875. القائمة التي ربطتها (لـ v0) تحتوي على حوالي 200 ملف لكن للتدريب الأول استخدمت فقط 50 ملفاً بحجم ~187 ميغابايت. يمكنك عرض قائمة الوثائق هنا:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><p>أحجام قواعد البيانات:
<ul><li>v0: ~187 ميغابايت</li>
<li>v0.5: ~435 ميغابايت </li>
<li>v1: ~6.25 غيغابايت </li>
<li>v2mini-eval1: 15 غيغابايت</li></p><p></ul><h2>ما هو حجم النماذج ؟</h2></p><p>v0: 16 مليون معامل</p><p>v0.5 123 مليون معامل</p><p>v1: 700M Parameters</p><p>v2mini-eval1: 300M Parameters</p><h1>Training Specs ? </h1></p><h1>v0/v0.5</h1>
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.</p><h1>v1</h1>
GPU: A100 SXM rented</p><h1>v2mini-eval1</h1></p><p>GPU: A100 SXM rented</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2026-01-13

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2026-01-13 
    </div>
    
</body>
</html>