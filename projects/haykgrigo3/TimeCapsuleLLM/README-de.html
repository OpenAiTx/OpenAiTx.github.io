<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.</title>
    <meta name="description" content="Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.">
    <meta name="keywords" content="TimeCapsuleLLM, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Ein LLM, das ausschließlich mit Daten aus bestimmten Zeiträumen trainiert wurde, um moderne Verzerrungen zu reduzieren.",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">German</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (demnächst verfügbar)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">हिन्दी (demnächst verfügbar)</a> |
        | <a href="#" title="Coming soon">ไทย (demnächst verfügbar)</a> |
        | <a href="#" title="Coming soon">Französisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Deutsch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Spanisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Italienisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Russisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Portugiesisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Niederländisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Polnisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">العربية (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">فارسی (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Türkisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (demnächst verfügbar)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (demnächst verfügbar)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Ein LLM, der ausschließlich mit Daten aus bestimmten Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren.</p><p>Stellen Sie sich vor, ein KI-Modell würde nicht nur so tun, als wäre es historisch, sondern wäre es tatsächlich.</p><p>Basiert auf <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a>. Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm.</p><h1>Projektziele</h1></p><p>TimeCapsule LLM ist ein experimentelles Projekt, das ausschließlich mit Texten aus bestimmten Zeitabschnitten trainiert wird. Ziel ist es, die Weltsicht und Sprache spezifischer historischer Epochen zu simulieren.</p><h1>Warum Fine-Tuning nicht ausreicht</h1></p><p>Wenn man nur ein vortrainiertes Modell feinabstimmt, kennt das LLM trotzdem moderne Konzepte. Natürlich ist es schwierig, einen völlig modernen Bias zu vermeiden, aber ich möchte diesem Ziel so nahe wie möglich kommen. Um keinen modernen Bias zu haben, muss ein Modell von Grund auf neu trainiert werden.</p><h1>Erwartete Ergebnisse</h1></p><p>Hoffentlich weiß das Modell am Ende nichts von modernen Konzepten und kann nicht über das hinaus argumentieren, worauf es trainiert wurde. Es sollte moderne Begriffe/Vokabeln nicht erkennen und keine modernen Kenntnisse halluzinieren.</p><h1>Fortschrittsupdates</h1></p><h2>9. Juli 2025</h2></p><p>Ich habe meinen Zeitraum auf 1800-1850 und die Region: London festgelegt</p><p>Ich habe eine Liste von Texten, Büchern, Dokumenten zusammengestellt</p><p>Bisher habe ich 50 als txt-Dateien und werde bald mit dem Training von NanoGPT beginnen</p><p>Ich werde dies aktualisieren, solange Fortschritte gemacht werden</p><h2>13. Juli 2025</h2></p><p>NanoGPT mit 187MB historischer Textdaten trainiert.</p><h2>15. Juli 2025</h2></p><p>Ich habe begonnen, Texte für den zweiten Trainingslauf herunterzuladen. Ich bekomme alles vom Internet Archive und habe den Zeitraum auf 1800-1875 erweitert. Um eine breite Palette von Texten zu erhalten, kann man auf Internet Archive nach Veröffentlichungsort, Zeitraum und Themen filtern. </p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Suchfilter"></p><h2>16. Juli 2025</h2></p><p>Ich habe etwa 500 txt-Dateien vom Internet Archive heruntergeladen und nach dem Bereinigen (nur Leerzeichen löschen, Gutenberg-Header etc.) habe ich etwa 500MB Daten. Es ist ein winziger Datensatz, aber letztes Mal habe ich mit 187MB trainiert, also sollte es zumindest einen bemerkbaren Unterschied im Output nach dem Training des zweiten Modells geben. Ich hoffe, dieses Modell kann zumindest kohärentere Sätze produzieren, die einigermaßen Sinn ergeben. Das ist natürlich keine Garantie, denn der Datensatz ist immer noch winzig, aber es ist mehr als beim letzten Mal.</p><p>Das sollte auf meiner eigenen Hardware machbar sein, was gut ist, weil ich hoffentlich einige Verbesserungen sehe, bevor ich zu einem größeren Datensatz springe, für den ich eine GPU mieten müsste. Aber keine Sorge, ich plane weiterhin, bald eine GPU zu mieten, aber bevor ich das tue, möchte ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie möglich ist. Eines der Probleme ist das Bereinigen, viele dieser txt-Dateien enthalten Kauderwelsch. Die Skripte, die ich zum Bereinigen verwendet habe, funktionieren, sind aber nicht zu 100% effektiv.</p><p>Ich werde diesen Datensatz heute trainieren und es sollte etwa 4-5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Vielen Dank nochmals an alle, die sich mein Projekt ansehen, manche haben mir sogar Links zu OCR-Ressourcen geschickt, also Danke! Ich hoffe, dass mehr Leute das ausprobieren und mit eigenen Datensätzen experimentieren.</p><h2>12. August 2025</h2></p><p>Version 1 ist fertiggestellt, diese Version verwendet Phi 1.5 (700M Parameter) und wurde mit einem Datensatz von fast 7000 Texten trainiert. Die Hauptverbesserung bei diesem Modell ist seine Fähigkeit, gelegentlich echte Namen und historische Ereignisse zu referenzieren. Version 0 und 0.5 konnten dies nicht, wenn ich diese Modelle mit etwas wie „Im Jahr 1821“ aufforderte, waren die Ausgaben nur Halluzinationen, Dinge, die nie passiert sind. Ein Beispiel dafür mit Version 1:</p><p>Mein Prompt: Es war das Jahr unseres Herrn 1834,</p><p>Die Ausgabe: „Es war das Jahr unseres Herrn 1834 und die Straßen Londons waren voller Protest und Petition. Der Grund, wie viele berichteten, lag nicht im Privaten, sondern wurde am selben Tag im Hause von Lord Palmerston aufgenommen, die Öffentlichkeit wird eine kurze Darstellung der Schwierigkeiten erhalten, unter denen das Gesetz uns erreicht hat. Es ist sehr bedauerlich, dass die aktuellen Ereignisse der Weltgeschichte klar sind und folglich bekannt sein werden. Es ist nicht wahr, dass gerade die Männer, die zuerst im Evangelium zu Jerusalem tätig wurden, so umfassende und interessante Aufzeichnungen über den Wohlstand und das Gedeihen besitzen.“</p><p>Zuerst dachte ich, das sei Zufall, aber schaut euch das an: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>Trainings-Update</h3></p><p>Ich habe mit dem Training auf einem 435MB (108 Mio. Token) Korpus begonnen, es läuft gerade ziemlich reibungslos. Der Trainingsverlust fiel von 10,9 auf 4,9 in den ersten 2800 Iterationen. Ich rechne damit, dass es etwa 8 oder 9 Stunden dauern wird. Ich poste ein weiteres Update, sobald es fertig ist.</p><h2>17. Juli 2025</h2></p><p>Das Training für das zweite Modell ist abgeschlossen, meine 4060 hat etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) für 33.000 Iterationen (5 Epochen). Der finale Trainingsverlust lag bei 3,73. Die Ausgaben waren überraschend gut, es generiert jetzt tatsächlich zusammenhängende Sätze im Stil des 19. Jahrhunderts.</p><h2>28. Juli 2025</h2></p><p>Ich habe v0.5 auf Hugging Face hochgeladen, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Schaut es euch an</a>, wenn ihr möchtet. Ihr könnt jetzt mein Repo herunterladen und lokal ausführen. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, daher müsst ihr das Modell lokal herunterladen und ausführen.</p><p>Außerdem beginne ich mit der Auswahl von Daten für meinen nächsten Trainingslauf, ich glaube, ich werde 5-10x mehr Daten benötigen, um Fähigkeiten zum logischen Schlussfolgern zu erreichen.</p><h2>2. August 2025</h2></p><p>Ich werde bald mit der Arbeit an Version 1 beginnen. Ich muss von der Architektur von nanoGPT zu etwas Modernerem wechseln. Ich habe mehrere Open-Source-LLM-Architekturen im Blick, darunter: OpenLLaMA v3, Phi-2 und Qwen 1.5B. Und um den Sprung zu V1 zu unterstützen, muss ich einen viel größeren und vielfältigeren Datensatz sorgfältig zusammenstellen. Ich brauche mindestens 5GB saubere Trainingsdaten.</p><h1>V0 Modellverhalten & Einschränkungen</h1></p><p>Frühe Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er reagiert. Zum Beispiel habe ich es mit „Who art Henry?“ aufgefordert und es antwortete „I know that man, I have did not a black, the storm.“ und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt, dass ich nach einer Person frage.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt keine Erwähnung moderner Konzepte, die Ausgaben enthalten hauptsächlich Wörter und Formulierungen aus den 1800er Jahren.</p><p>Es braucht noch viel Arbeit, das Training mit 187MB reicht nicht aus, um ein Modell zu bekommen, das Texte mit komplexer Argumentation erzeugt.</p><p>Im Moment erzeugt das Modell Sätze, die keine vollständige Satzstruktur aufweisen und insgesamt einfach keinen Sinn ergeben, aber das ist bei der Trainingsgröße normal.</p><h1>V0.5 Modellverhalten & Einschränkungen</h1></p><p>Dies ist eine schöne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und Wortschatz ist viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und wieder wurde das Modell von Grund auf neu trainiert, daher bezieht es sich auf Themen aus dem 19. Jahrhundert.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt viele faktische Halluzinationen. Viele (etwa 100%) der Details (Daten, Ereignisse, historische Figuren) sind erfunden. Außerdem haben die Sätze untereinander keine wirklichen Verbindungen, manchmal beziehen sich vielleicht 2 Sätze aufeinander, aber darüber hinaus nicht. Ein weiteres Problem ist, dass manchmal ein fremder „Digitized by Google“-Footer auftaucht, daher muss ich beim nächsten Training unbedingt darauf achten, dass die Texte gut bereinigt sind. Insgesamt bin ich sehr zufrieden mit den Ergebnissen, es ist zwar noch lange kein LLM, aber definitiv ein Satzgenerator.</p><p>Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich werde bald Dateien hochladen!</p><h1>V1 Modellverhalten & Einschränkungen</h1></p><p>Ich werde bald einige Beispielausgaben hochladen und auch Vergleiche zwischen den 3 Modellen mit dem gleichen Prompt machen. Ich werde V1 auch auf Huggingface hochladen, wie ich es bei meiner letzten Version gemacht habe, meinen Huggingface-Account findet ihr hier: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>Kommende Pläne</h1></p><p>(Abgeschlossen) Ich werde mit der Arbeit an Version 0.5 beginnen. Statt mit 50 Büchern zu trainieren, werde ich idealerweise mit 500-600 Büchern trainieren. Im Moment trainiere ich nanoGPT mit Büchern aus 1800-1850 und speziell aus London. Es gibt einige Herausforderungen, wie sicherzustellen, dass die gefundenen Bücher nicht aktualisiert oder modern interpretiert wurden, sondern unveränderte Bücher, die innerhalb meines gewählten Zeitraums veröffentlicht wurden.</p><p>Ich möchte ein neues Modell (v1) mit einem viel größeren Korpus trainieren, vielleicht 5-10x größer als der für v0.5 verwendete. Mein Ziel ist es zu sehen, ob durch Selective Temporal Training allein Fähigkeiten zum logischen Denken entstehen können. Dies wird eine schwierigere Aufgabe und ich bin mir nicht einmal sicher, ob es möglich ist, da es Einschränkungen bei historischen Daten gibt. In den nächsten Wochen werde ich versuchen, genug Daten für einen Korpus von 5-10GB zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.</p><h1>Wie man dieses Projekt verwendet</h1></p><p>Dieses Projekt konzentriert sich hauptsächlich auf das Kuratieren historischer Daten, deren Vorbereitung für das Training und den Aufbau eines Tokenizers. Ich werde den vollständigen LLM-Trainingsprozess nicht abdecken, dafür verweise ich auf nanoGPT von Andrej Karpathy.</p><h1>Schritt 1: Historische Texte sammeln und vorbereiten</h1></p><p>Sammle .txt-Dateien von gemeinfreien Büchern, Dokumenten usw. aus deinem gewählten Zeitraum (z.B. London 1800-1850)</p><p>Du kannst download_texts_improved.py verwenden, um Bücher für dich herunterzuladen, falls du das benötigst.</p><p>Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footer von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.</p><p>prepare_dataset.py sollte einwandfrei funktionieren.</p><h1>Schritt 2: Einen eigenen Tokenizer bauen</h1></p><p>Führe train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
Dadurch erhältst du vocab.json und merges.txt</p><p>Diese Dateien definieren Vokabular und Merge-Regeln für Ihr Modell</p><h1>Schritt 3: Trainieren Sie Ihr Modell (nanoGPT)</h1></p><p>Beziehen Sie sich auf <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a> für den Trainingsprozess.</p><p>Sie können ein anderes LLM trainieren, wenn Sie möchten, aber ich habe nanoGPT verwendet</p><h1>FAQ</h1></p><h2>Was ist Selektives Temporales Training?</h2></p><p>Selektives Temporales Training (STT) ist eine Methode des maschinellen Lernens, bei der alle Trainingsdaten gezielt auf einen bestimmten historischen Zeitraum beschränkt werden. Dies dient dazu, die Sprache und das Wissen jener Epoche ohne Einfluss moderner Konzepte zu modellieren. Zum Beispiel wurde das aktuelle Modell (v0.5) ausschließlich mit Daten von 1800–1875 trainiert, nicht feinabgestimmt, sondern von Grund auf neu, wodurch die Ausgabe den Sprachstil und Kontext jener Zeit widerspiegelt.</p><h2>Warum nicht einfach Fine-Tuning oder LoRA verwenden?</h2></p><p>Für dieses Projekt möchte ich ein Sprachmodell erschaffen, das frei von modernen Einflüssen ist. Wenn ich etwa GPT-2 feinabstimme, ist es bereits vortrainiert, und diese Informationen bleiben erhalten. Wenn ich von Grund auf trainiere, tut das Sprachmodell nicht so, als wäre es alt, sondern es ist es wirklich. Das Ziel des Projekts ist aktuell, etwas zu schaffen, das ausschließlich mit Wissen aus Londoner Büchern zwischen 1800 und 1850 argumentieren kann.</p><h2>Welche Daten wurden für das Training verwendet?</h2></p><p>Ich verwende Bücher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800–1850. Die verlinkte Liste enthält etwa 200, aber für das erste Training habe ich nur 50 Dateien mit ca. 187 MB genutzt. Eine Liste der Dokumente finden Sie unter:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Wie groß sind die Modelle?</h2></p><p>V0: 16M Parameter</p><p>V0.5: 123M Parameter</p><p>V1: 700M Parameter</p><h1>Trainingsspezifikationen?</h1></p><p>#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.</p><p>#V1
GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>