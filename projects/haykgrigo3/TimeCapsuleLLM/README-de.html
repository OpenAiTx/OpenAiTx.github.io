<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren</title>
    <meta name="description" content="Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren">
    <meta name="keywords" content="TimeCapsuleLLM, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Ein LLM, das ausschließlich mit Daten aus bestimmten Zeiträumen trainiert wurde, um moderne Verzerrungen zu reduzieren",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 275
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-07",
  "dateModified": "2025-08-07"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 275 stars</span>
                <span class="language">German</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (bald verfügbar)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">हिन्दी (bald verfügbar)</a> |
        | <a href="#" title="Coming soon">ไทย (bald verfügbar)</a> |
        | <a href="#" title="Coming soon">Französisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Deutsch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Spanisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Italienisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Russisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Portugiesisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Niederländisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Polnisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">العربية (bald verfügbar)</a>
        | <a href="#" title="Coming soon">فارسی (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Türkisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bald verfügbar)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Ein LLM, das ausschließlich mit Daten aus bestimmten Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren.</p><p>Stellen Sie sich vor, ein KI-Modell würde nicht nur so tun, als sei es historisch, sondern es tatsächlich sein.</p><p>Basierend auf <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a>. Die Kern-Trainingsskripte und die Modellarchitektur sind seine Arbeit.</p><h1>Projektziele </h1></p><p>TimeCapsule LLM ist ein experimentelles Projekt, das nur mit Texten trainiert wird, die in bestimmten Zeitabschnitten geschrieben wurden. Das Ziel ist es, die Weltanschauung und Sprache spezifischer historischer Epochen zu simulieren.</p><h1>Warum Fine-Tuning nicht ausreicht </h1></p><p>Wenn Sie nur ein vortrainiertes Modell feinabstimmen, kennt Ihr LLM trotzdem moderne Konzepte. Natürlich ist es schwierig, eine vollständige Freiheit von modernen Verzerrungen zu erreichen, aber ich möchte dem so nahe wie möglich kommen. Um keinerlei moderne Einflüsse zu haben, muss ein Modell von Grund auf neu trainiert werden.</p><h1>Erwartete Ergebnisse </h1></p><p>Hoffentlich wird dieses Modell, wenn es fertig ist, keine modernen Konzepte kennen und nicht über das hinaus denken können, worauf es trainiert wurde. Es sollte keine modernen Begriffe/Konzepte erkennen und ich hoffe, dass es kein modernes Wissen halluziniert.</p><h1>Fortschrittsberichte</h1></p><h2>9. Juli 2025</h2></p><p>Ich habe meine Zeitperiode auf 1800-1850 und die Region: London festgelegt.</p><p>Ich habe eine Liste von Texten, Büchern, Dokumenten gesammelt.</p><p>Bisher habe ich 50 als .txt-Dateien und werde bald mit dem Training von NanoGPT beginnen.</p><p>Ich werde dies weiter aktualisieren, solange Fortschritte gemacht werden.</p><h2>13. Juli 2025</h2></p><p>NanoGPT mit 187MB historischen Textdaten trainiert.</p><h2>15. Juli 2025</h2></p><p>Ich habe begonnen, Texte für den zweiten Trainingslauf herunterzuladen. Ich beziehe alles aus dem Internet Archive und habe die Zeitspanne auf 1800-1875 erweitert. Um eine vielfältige Auswahl an Texten zu bekommen, kann man im Internet Archive Such- und Themenfilter für Publikationsort, Zeitraum und Themen verwenden.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Suchfilter"></p><h2>16. Juli 2025</h2></p><p>Ich habe etwa 500 txt-Dateien aus dem Internet Archive heruntergeladen und nach der Bereinigung (nur Leerzeichen, Gutenberg-Kopfzeilen usw. gelöscht) habe ich rund 500MB an Daten. Es ist ein kleines Datenset, aber das letzte Mal habe ich mit 187MB trainiert, also sollte es zumindest einen spürbaren Unterschied in den Ergebnissen nach dem zweiten Training geben. Ich hoffe, dass dieses Modell wenigstens kohärentere Sätze produzieren kann, die einigermaßen Sinn ergeben. Das ist natürlich keine Garantie, denn das Datenset ist immer noch sehr klein, aber es ist mehr als beim letzten Mal.</p><p>Das sollte auf meiner eigenen Hardware machbar sein, was gut ist, weil ich hoffentlich Verbesserungen sehe, bevor ich auf ein größeres Datenset umsteige, für das ich eine GPU mieten müsste. Keine Sorge, ich plane trotzdem bald eine GPU zu mieten, aber bevor ich das tue, möchte ich mein Datenset so sorgfältig wie möglich kuratieren und bereinigen. Ein Problem ist das Bereinigen, viele dieser txt-Dateien enthalten Kauderwelsch. Die Skripte, die ich zur Bereinigung verwendet habe, funktionieren, sind aber nicht 100% effektiv.</p><p>Ich werde dieses Datenset heute trainieren, und es sollte etwa 4-5 Stunden dauern. Wenn es fertig ist und ich es getestet habe, gebe ich Updates. Nochmals vielen Dank an alle, die sich mein Projekt ansehen; einige Leute haben mir sogar Links zu OCR-Ressourcen geschickt, also Danke! Ich hoffe, mehr Leute probieren das aus und experimentieren mit eigenen Datensets.</p><h3>Trainings-Update</h3></p><p>Ich habe mit dem Training eines 435MB (108 Mio. Tokens) Korpus begonnen, es läuft gerade ziemlich reibungslos. Der Trainingsverlust ist von 10,9 auf 4,9 in den ersten 2800 Iterationen gesunken. Ich schätze, es wird etwa 8 oder 9 Stunden dauern. Ich poste ein weiteres Update, wenn es fertig ist.</p><h2>17. Juli 2025</h2></p><p>Das Training für das zweite Modell ist abgeschlossen, meine 4060 brauchte etwa 8 Stunden und 40 Minuten (3.900 Iters/Stunde) für 33.000 Iterationen (5 Epochen). Der finale Trainingsverlust lag bei 3,73. Die Ergebnisse waren überraschend gut – es erzeugt jetzt tatsächlich kohärente Sätze im Stil des 19. Jahrhunderts.</p><h2>28. Juli 2025 </h2></p><p>Ich habe Version 0.5 auf Hugging Face hochgeladen, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">schau es dir an</a>, wenn du möchtest. Du kannst mein Repository jetzt herunterladen und lokal ausführen. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, du musst das Modell also lokal herunterladen und ausführen.</p><p>Außerdem werde ich beginnen, Daten für meinen nächsten Trainingslauf zu kuratieren; ich denke, ich brauche vielleicht 5-10x mehr Daten, um echte Schlussfolgerungsfähigkeiten zu erreichen.</p><h2>2. August 2025</h2></p><p>Ich werde bald mit der Arbeit an Version 1 beginnen. Ich muss von der Architektur von nanoGPT zu etwas Modernerem übergehen. Ich habe mehrere Open-Source-LLM-Architekturen im Blick, darunter: OpenLLaMA v3, Phi-2 und Qwen 1.5B. Und um den Sprung zu V1 zu schaffen, muss ich ein viel größeres und vielfältigeres Datenset sorgfältig zusammenstellen. Ich benötige mindestens 5GB an sauberen Trainingsdaten.</p><h1>V0 Modellverhalten & Einschränkungen</h1></p><p>Frühe Prompts zeigen, dass das Modell mit Sprache und Verhalten aus dem 19. Jahrhundert antwortet. Zum Beispiel habe ich es mit "Who art Henry?" gefragt und es antwortete: "I know that man, I have did not a black, the storm." und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt, dass ich nach einer Person frage.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt keine Erwähnung moderner Konzepte, die Ausgaben enthalten hauptsächlich Wörter und Formulierungen aus dem 19. Jahrhundert.</p><p>Es ist noch viel Arbeit nötig, das Training mit 187MB reicht nicht aus, um ein Modell zu bekommen, das Texte mit komplexem Denken erzeugt.</p><p>Im Moment produziert es Sätze, denen eine vollständige Satzstruktur fehlt und die insgesamt einfach keinen Sinn ergeben, aber das ist bei der Trainingsgröße normal.</p><h1>V0.5 Modellverhalten & Einschränkungen</h1></p><p>Dies ist eine deutliche Verbesserung gegenüber dem letzten Modell. Der Schreibstil und der Wortschatz sind viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und auch das ist von Grund auf trainiert, daher bleibt es bei Themen des 19. Jahrhunderts.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt viele faktische Halluzinationen. Viele (ca. 100%) der Details (Daten, Ereignisse, historische Persönlichkeiten) sind erfunden. Außerdem haben die Sätze oft keinen Zusammenhang, manchmal beziehen sich vielleicht 2 Sätze aufeinander, aber darüber hinaus nicht. Ein weiteres Problem ist, dass manchmal ein irrtümliches “Digitized by Google”-Fußzeile auftaucht, daher muss ich beim nächsten Training wirklich darauf achten, dass die Texte gut bereinigt sind. Insgesamt bin ich mit den Ergebnissen sehr zufrieden, es ist zwar noch weit von einem LLM entfernt, aber definitiv ein Satzgenerator.</p><p>Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich lade bald Dateien hoch!</p><h1>Zukünftige Pläne</h1></p><p>(Abgeschlossen) Ich werde mit der Arbeit an Version 0.5 beginnen. Anstatt 50 Bücher zum Training zu verwenden, werde ich idealerweise 500-600 verwenden. Momentan trainiere ich nanoGPT mit Büchern von 1800-1850, speziell aus London. Es gibt Herausforderungen, wie sicherzustellen, dass die gefundenen Bücher nicht überarbeitet wurden oder moderne Interpretationen enthalten, sondern originale Bücher sind, die innerhalb meines gewählten Zeitraums veröffentlicht wurden.</p><p>Ich möchte ein neues Modell (v1) mit einem viel größeren Korpus trainieren, vielleicht 5-10x größer als der, den ich für v0.5 verwendet habe. Mein Ziel ist zu sehen, ob sich aus rein selektivem temporalen Training Denkfähigkeiten entwickeln. Das wird schwieriger und ich bin mir nicht einmal sicher, ob es möglich ist, da es historische Datenbeschränkungen gibt. In den kommenden Wochen werde ich versuchen, genug Daten für einen 5-10GB-Korpus zu kuratieren. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.</p><h1>Wie man dieses Projekt nutzt</h1></p><p>Dieses Projekt konzentriert sich hauptsächlich auf das Kuratieren historischer Daten, deren Aufbereitung für das Training und den Bau eines Tokenizers. Ich werde den kompletten LLM-Trainingsprozess nicht abdecken, dafür siehe nanoGPT von Andrej Karpathy.</p><h1>Schritt 1: Historische Texte sammeln und vorbereiten</h1></p><p>Sammle .txt-Dateien gemeinfreier Bücher, Dokumente usw. aus deinem gewählten Zeitraum (z.B. London 1800-1850)</p><p>Du kannst download_texts_improved.py verwenden, um Bücher herunterzuladen, falls du welche benötigst.</p><p>Bereinige die Textdateien mit einem Skript oder entferne manuell Kopf-/Fußzeilen von Project Gutenberg, moderne Anmerkungen oder Dinge wie OCR-Fehler.</p><p>prepare_dataset.py sollte problemlos funktionieren.</p><h1>Schritt 2: Einen eigenen Tokenizer bauen</h1></p><p>Führe train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.
Dadurch erhältst du vocab.json und merges.txt</p><p>Diese Dateien definieren Vokabular und Merge-Regeln für dein Modell</p><h1>Schritt 3: Trainiere dein Modell (nanoGPT)</h1></p><p>Siehe <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a> für den Trainingsprozess.</p><p>Du kannst ein anderes LLM trainieren, wenn du möchtest, aber ich habe nanoGPT verwendet</p><h1>FAQ</h1></p><h2>Was ist Selective Temporal Training?</h2></p><p>Selective Temporal Training (STT) ist eine maschinelle Lernmethode, bei der alle Trainingsdaten gezielt so kuratiert werden, dass sie in einen bestimmten historischen Zeitraum fallen. Das wird gemacht, um die Sprache und das Wissen dieser Epoche zu modellieren, ohne Einflüsse moderner Konzepte. Zum Beispiel ist das aktuelle Modell (v0.5) ausschließlich mit Daten von 1800-1875 trainiert, nicht feinjustiert, sondern von Grund auf, sodass die Ausgaben den sprachlichen Stil und Kontext dieser Zeit widerspiegeln.</p><h2>Warum nicht einfach Fine-Tuning oder LoRA verwenden?</h2></p><p>Für dieses Projekt versuche ich, ein Sprachmodell zu erstellen, das nicht von modernen Vorurteilen beeinflusst ist. Wenn ich zum Beispiel ein GPT-2 feinjustiere, ist es bereits vortrainiert und diese Informationen verschwinden nicht mehr. Wenn ich von Grund auf trainiere, tut das Modell nicht so, als wäre es alt, sondern ist es tatsächlich. Das Ziel ist momentan, etwas zu schaffen, das ausschließlich mit Wissen aus Londoner Büchern von 1800 bis 1850 argumentieren kann.</p><h2>Welche Daten hast du fürs Training benutzt?</h2></p><p>Ich verwende Bücher, juristische Dokumente, Zeitungen und andere Schriften aus London von 1800–1850. Die verlinkte Liste enthält etwa 200, aber für das erste Training habe ich nur 50 Dateien mit insgesamt etwa 187 MB verwendet. Eine Liste der Dokumente findest du hier:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Wie groß ist das Version 0 Modell?</h2></p><p>Dieses Modell ist derzeit sehr klein, ich mache das nur zum Spaß und halte mich strikt an die Trainingsregel, keine modernen Quellen zu verwenden. Es hat fast 16 Millionen Parameter, aber ich werde bald mehr alte Texte sammeln, um ein weiteres Modelltraining zu beginnen. Updates folgen!</p><h2>Trainingsspezifikationen?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-07 
    </div>
    
</body>
</html>