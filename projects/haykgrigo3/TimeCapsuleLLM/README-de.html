<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Ein LLM, der ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.</title>
    <meta name="description" content="Ein LLM, der ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.">
    <meta name="keywords" content="TimeCapsuleLLM, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Ein LLM, der ausschließlich mit Daten aus bestimmten Zeiträumen trainiert wurde, um moderne Verzerrungen zu reduzieren.",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 545
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-09-30",
  "dateModified": "2025-09-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 545 stars</span>
                <span class="language">German</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">Englisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (bald verfügbar)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">Japanisch</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">Koreanisch</a>
        | <a href="#" title="Coming soon">हिन्दी (bald verfügbar)</a> |
        | <a href="#" title="Coming soon">ไทย (bald verfügbar)</a> |
        | <a href="#" title="Coming soon">Französisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Deutsch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Spanisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Italienisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Russisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Portugiesisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Niederländisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Polnisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">العربية (bald verfügbar)</a>
        | <a href="#" title="Coming soon">فارسی (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Türkisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Vietnamesisch (bald verfügbar)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bald verfügbar)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1></p><p><em>Ein Sprachmodell, das <strong>von Grund auf</strong> ausschließlich mit Daten aus bestimmten Orten und Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren und die Stimme, den Wortschatz und die Weltanschauung der jeweiligen Epoche zu verkörpern.</em></p><p>Stellen Sie sich vor, ein KI-Modell würde nicht nur historisch erscheinen, sondern tatsächlich historisch sein.</p><p>v0 und v0.5 basieren auf <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a>. Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm. </p><p>v1 basiert auf <a href="https://huggingface.co/microsoft/phi-1_5" target="_blank" rel="noopener noreferrer">Phi 1.5 von Microsoft</a></p><p><a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Hugging Face Link</a></p><h2> Modellverhalten & Einschränkungen</h2></p><h3><strong>v0</strong>  </h3></p><p>Frühe Prompts zeigen, dass das Modell mit Sprache und Verhalten aus dem 19. Jahrhundert antwortet.
Beispiel: Prompt: "Who art Henry?" und es antwortete "I know that man, I have did not a black, the storm."</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><ul><li>Keine Erwähnung moderner Konzepte  </li>
<li>Meistens zeitgemäßer Wortschatz  </li>
<li>Sätze sind größtenteils unzusammenhängend (erwartet bei ~187MB Trainingsdaten)</li></p><p></ul><h3><strong>v0.5</strong> </h3></p><p>Eine deutliche Verbesserung gegenüber v0.  
<ul><li>Viktorianischer Schreibstil, korrekte Zeichensetzung, überwiegend grammatikalisch richtige Sätze  </li>
<li>Weiterhin hohe Rate an faktischen Halluzinationen  </li>
<li>OCR-Rauschen („Digitized by Google“) weiterhin in den Ausgaben vorhanden</li></p><p></ul><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><h3><strong>v1</strong></h3></p><p>Erstes Modell, das ein reales historisches Ereignis mit einer tatsächlichen Figur aus dem Datensatz verknüpft.</p><p>Beispiel: Prompt: "It was the year of our Lord 1834"</p><p>Die Ausgabe: "Es war das Jahr unseres Herrn 1834 und die Straßen Londons waren erfüllt von Protest und Petition. Die Ursache, wie viele berichteten, war nicht privat gebunden, sondern hatte am selben Tag im Hause Lord Palmerston stattgefunden; die Öffentlichkeit wird eine kurze Darstellung der Schwierigkeiten erhalten, unter denen das Gesetz uns erreicht hat. Es ist sehr bedauerlich, dass die aktuellen Ereignisse in der Geschichte der Welt klar sind und folglich bekannt sein werden. Es stimmt nicht, dass gerade die Männer, die das Evangelium zuerst in Jerusalem verbreitet haben, einen so umfangreichen und interessanten Bericht über Wohlstand und Prosperität vorweisen können."</p><p>Zunächst nahm ich an, dass ein Protest zufällig im selben Jahr stattgefunden haben könnte, aber sehen Sie sich das an: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>Warum das wichtig ist:</h3></p><p>Dies ist das erste Beispiel, in dem eines meiner Modelle ein Jahr mit einem realen historischen Ereignis und einer realen Person, die mit diesem Ereignis verbunden ist (Lord Palmerston), verknüpft. Frühere Modelle (v0 und v0.5) konnten zwar den Schreibstil des 19. Jahrhunderts nachahmen, halluzinierten jedoch stets Ereignisse, Personen und Fakten. Dies zeigt, dass das Modell beginnt, sich an Inhalte aus dem Datensatz zu erinnern.</p><h2>Kommende Pläne</h2></p><ul><li>Es gibt fast 175.000 Texte, die zwischen 1800 und 1875 in London veröffentlicht wurden, auf dem Internet Archive</li>
<li>Ich plane, das Korpus zu erweitern und weiter zu bereinigen, um bessere Argumentationsfähigkeiten zu ermöglichen</li>
<li>Erweiterung auf andere Regionen und Zeiträume für weitere historische Modelle</li></p><p></ul><h2>Verwendungshinweise</h2></p><p>Dieses Projekt konzentriert sich hauptsächlich auf die Sammlung historischer Daten, deren Vorbereitung für das Training und den Aufbau eines Tokenizers. Ich decke nicht den vollständigen LLM-Trainingsprozess ab, dafür verweise ich auf nanoGPT von Andrej Karpathy.</p><h3>Schritt 1: Historische Texte sammeln und vorbereiten</h3></p><ul><li>Sammeln Sie .txt-Dateien von gemeinfreien Büchern, Dokumenten usw. aus Ihrem gewählten Zeitraum (z. B. London 1800-1850)</li>
<li>Halten Sie sich innerhalb Ihres gewählten Zeit-/Ortsfensters</li>
<li>Bereinigen Sie die Textdateien mithilfe eines Skripts oder entfernen Sie manuell Kopf-/Fußzeilen von Project Gutenberg, moderne Anmerkungen oder beispielsweise OCR-Fehler.</li></p><p></ul><h3>Schritt 2: Eigener Tokenizer erstellen</h3></p><ul><li>Führen Sie train_tokenizer.py oder train_tokenizer_hf.py auf den bereinigten Daten aus.</li>
<li>Dadurch erhalten Sie vocab.json und merges.txt</li>
<li>Diese Dateien definieren Vokabular und Merge-Regeln für Ihr Modell</li></p><p></ul><h3>Schritt 3: Modell trainieren</h3></p><ul><li>Siehe <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a> für den Trainingsprozess oder die Dokumentation Ihrer gewählten Architektur.</li></p><p></ul><h1>FAQ</h1></p><h2>Was ist Selective Temporal Training?</h2></p><p>Selective Temporal Training (STT) ist eine maschinelle Lernmethodik, bei der alle Trainingsdaten gezielt aus einem bestimmten historischen Zeitraum stammen. Dies dient dazu, die Sprache und das Wissen dieser Epoche zu modellieren, ohne Einfluss moderner Konzepte. Zum Beispiel wurde das aktuelle Modell (v0.5) ausschließlich mit Daten von 1800-1875 trainiert, nicht feinabgestimmt, sondern komplett neu, sodass die Ausgabe den sprachlichen Stil und den historischen Kontext dieses Zeitraums widerspiegelt.</p><h2>Warum nicht einfach Fine-Tuning oder LoRA verwenden?</h2></p><p>Für dieses Projekt möchte ich ein Sprachmodell erstellen, das frei von modernen Vorurteilen ist. Wenn ich zum Beispiel GPT-2 feinabstimme, ist es bereits vortrainiert und diese Informationen verschwinden nicht. Wenn ich von Grund auf trainiere, wird das Sprachmodell nicht vorgeben, alt zu sein, sondern es wird es wirklich sein. Das Ziel dieses Projekts ist es derzeit, etwas zu schaffen, das ausschließlich mit Wissen aus Londoner Büchern von 1800 bis 1875 argumentieren kann.</p><h2>Welche Art von Daten wurde für das Training verwendet?</h2></p><p>
Ich verwende Bücher, Rechtsdokumente, Zeitungen und andere Schriften aus London von 1800–1875. Die verlinkte Liste (für v0) enthält etwa 200, aber für das erste Training habe ich nur 50 Dateien mit ca. ~187 MB verwendet. Eine Liste der Dokumente kann hier eingesehen werden:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><p>Datensatzgrößen:
v0: ~187MB
v0.5: ~435MB
v1: ~6,25GB</p><h2>Wie groß sind die Modelle?</h2></p><p>V0: 16 Mio. Parameter</p><p>V0.5: 123 Mio. Parameter</p><p>V1: 700 Mio. Parameter</p><h1>Trainingsspezifikationen?</h1></p><h1>V0/V0.5</h1>
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.</p><h1>V1</h1>
GPU: A100 gemietet</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-09-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-30 
    </div>
    
</body>
</html>