<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.</title>
    <meta name="description" content="Ein LLM, das ausschlie&#223;lich mit Daten aus bestimmten Zeitr&#228;umen trainiert wurde, um moderne Verzerrungen zu reduzieren.">
    <meta name="keywords" content="TimeCapsuleLLM, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Ein LLM, das ausschließlich mit Daten aus bestimmten Zeiträumen trainiert wurde, um moderne Verzerrungen zu reduzieren.",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">German</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Ein LLM, das nur mit Daten aus bestimmten Zeitperioden trainiert wurde, um moderne Verzerrungen zu reduzieren.</p><p>Stell dir vor, ein KI-Modell würde nicht nur so tun, als wäre es historisch, sondern wäre es tatsächlich.</p><p>Basierend auf <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a> Die zentralen Trainingsskripte und die Modellarchitektur stammen von ihm.</p><h1>Projektziele</h1></p><p>TimeCapsule LLM ist ein experimentelles Projekt, das ausschließlich mit Texten aus bestimmten Zeiträumen trainiert wird. Ziel ist es, die Weltanschauung und Sprache bestimmter historischer Epochen zu simulieren.</p><h1>Warum Feintuning nicht ausreicht</h1></p><p>Wenn man nur ein vortrainiertes Modell finetuned, kennt das LLM trotzdem moderne Konzepte. Natürlich ist es schwierig, eine Verzerrung durch moderne Einflüsse vollständig zu vermeiden, aber ich möchte so nah wie möglich daran kommen. Keine moderne Verzerrung zu erreichen, erfordert ein Training des Modells von Grund auf.</p><h1>Erwartete Ergebnisse</h1></p><p>Hoffentlich wird dieses Modell, wenn es fertig ist, keine modernen Konzepte kennen und nicht über das hinaus denken können, was es gelernt hat. Es sollte keine modernen Begriffe oder Vokabeln erkennen und ich hoffe, es halluziniert kein modernes Wissen.</p><h1>Fortschritts-Updates</h1></p><h2>9. Juli 2025</h2></p><p>Ich habe meinen Zeitraum auf 1800–1850 und die Region London festgelegt.</p><p>Ich habe eine Liste von Texten, Büchern, Dokumenten zusammengestellt.</p><p>Bisher habe ich 50 als txt-Dateien erhalten und werde NanoGPT bald mit dem Training beginnen.</p><p>Ich werde dies aktualisieren, solange Fortschritte gemacht werden.</p><h2>13. Juli 2025</h2></p><p>NanoGPT mit 187MB historischen Textdaten trainiert.</p><h2>15. Juli 2025</h2></p><p>Ich habe begonnen, Texte für den zweiten Trainingslauf herunterzuladen. Ich bekomme alles aus dem Internet Archive und habe den Zeitraum auf 1800–1875 erweitert. Um eine breite Palette an Texten zu erhalten, kann man im Internet Archive die Filter für Veröffentlichungsort, Zeitraum und Themen verwenden.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Suchfilter"></p><h2>16. Juli 2025</h2></p><p>Ich habe etwa 500 txt-Dateien aus dem Internet Archive heruntergeladen und nach dem Bereinigen (nur Leerzeichen löschen, Gutenberg-Kopfzeilen usw.) habe ich etwa 500MB Daten. Es ist ein winziger Datensatz, aber letztes Mal habe ich mit 187MB trainiert, also sollte es zumindest einen merklichen Unterschied im Output geben, nachdem ich das zweite Modell trainiert habe. Ich hoffe, dieses Modell kann zumindest zusammenhängendere Sätze produzieren, die einigermaßen Sinn ergeben. Das ist natürlich keine Garantie, da es immer noch ein sehr kleiner Datensatz ist, aber mehr als beim letzten Mal.</p><p>Das sollte auf meiner eigenen Hardware machbar sein, was gut ist, weil ich hoffentlich Verbesserungen sehe, bevor ich zu einem größeren Datensatz wechsle, für den ich eine GPU mieten müsste. Keine Sorge, ich plane trotzdem bald eine GPU zu mieten, aber vorher möchte ich sicherstellen, dass mein Datensatz so kuratiert und sauber wie möglich ist. Eines der Probleme ist das Bereinigen – viele dieser txt-Dateien enthalten Unsinn. Die Skripte, die ich für das Bereinigen verwendet habe, funktionieren, sind aber nicht 100% effektiv.</p><p>Ich werde diesen Datensatz heute trainieren und es sollte etwa 4–5 Stunden dauern. Sobald es fertig ist und ich es getestet habe, gebe ich Updates. Danke nochmal an alle, die sich mein Projekt ansehen, einige haben mir sogar Links zu OCR-Ressourcen geschickt – vielen Dank! Ich hoffe, mehr Leute probieren das aus und experimentieren mit eigenen Datensätzen.</p><h3>Trainings-Update</h3></p><p>Ich habe mit dem Training auf einem 435MB (108 Mio. Tokens) Korpus begonnen, es läuft aktuell ziemlich reibungslos. Der Trainingsverlust ist von 10,9 auf 4,9 in den ersten 2800 Iterationen gefallen. Ich rechne damit, dass es etwa 8 oder 9 Stunden dauern wird. Ich poste ein weiteres Update, sobald es fertig ist.</p><h2>17. Juli 2025 2:13 Uhr</h2></p><p>Das Training für das zweite Modell ist abgeschlossen, meine 4060 hat dafür etwa 8 Stunden und 40 Minuten gebraucht (3.900 Iterationen/Stunde) für 33.000 Iterationen (5 Epochen). Der endgültige Trainingsverlust lag bei 3,73. Die Ergebnisse waren überraschend gut, es generiert jetzt tatsächlich zusammenhängende Sätze im Stil des 19. Jahrhunderts.</p><h2>28. Juli 2025</h2></p><p>Ich habe v0.5 bei Hugging Face hochgeladen, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">schau es dir an</a>, wenn du möchtest. Du kannst jetzt mein Repo herunterladen und lokal ausführen. Leider funktioniert nanoGPT nicht nativ mit HuggingFace, daher musst du das Modell lokal herunterladen und ausführen.</p><p>Außerdem werde ich jetzt Daten für meinen nächsten Trainingslauf kuratieren. Ich denke, ich brauche vielleicht 5- bis 10-mal mehr Daten, um echte Argumentationsfähigkeiten zu erreichen.</p><h1>V0 Modellverhalten & Einschränkungen</h1></p><p>Frühe Prompts zeigen, dass das Modell mit Sprache und Verhalten der 1800er reagiert. Zum Beispiel fragte ich: "Who art Henry?" und es antwortete: "I know that man, I have did not a black, the storm." Und ja, dieser Satz ergibt keinen Sinn, aber das LLM erkennt immerhin, dass ich nach einer Person frage.</p><p>
<img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt keine Erwähnung moderner Konzepte, die Ausgaben enthalten hauptsächlich Wörter und Formulierungen aus dem 19. Jahrhundert.</p><p>Es muss noch viel Arbeit geleistet werden, das Training mit 187MB reicht nicht aus, um ein Modell zu erhalten, das Texte mit komplexem Denken produziert.</p><p>Momentan erzeugt es Sätze, die keine vollständige Satzstruktur haben und insgesamt einfach keinen Sinn ergeben, aber das ist bei dieser Trainingsgröße normal.</p><h1>V0.5 Modellverhalten & Einschränkungen</h1></p><p>Dies ist eine schöne Verbesserung im Vergleich zum letzten Modell. Der Schreibstil und der Wortschatz sind viktorianisch und fast jeder Satz ist grammatikalisch korrekt mit richtiger Zeichensetzung. Und erneut: Das Modell wurde von Grund auf trainiert, sodass es bei Themen des 19. Jahrhunderts bleibt.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Beispielausgabe"></p><p>Es gibt viele faktische Halluzinationen. Sehr viele (fast 100%) der Details (Daten, Ereignisse, historische Figuren) sind erfunden. Außerdem stehen die Sätze nicht wirklich im Zusammenhang zueinander, manchmal beziehen sich vielleicht 2 Sätze aufeinander, aber darüber hinaus nicht. Ein weiteres Problem ist, dass manchmal eine zufällige „Digitized by Google“-Fußzeile auftaucht, deshalb muss ich beim nächsten Training wirklich darauf achten, dass die Texte gut gesäubert sind. Insgesamt bin ich mit den Ergebnissen sehr zufrieden, es ist zwar noch kein LLM, aber definitiv ein Satzgenerator.</p><p>Ich lerne viel und werde in den kommenden Wochen herausfinden, was ich besser machen muss. Ich werde bald Dateien hochladen!</p><h1>Zukünftige Pläne</h1></p><p>(Abgeschlossen) Ich beginne mit der Arbeit an Version 0.5. Anstatt 50 Bücher zu verwenden, werde ich idealerweise 500–600 verwenden. Momentan trainiere ich nanoGPT mit Büchern aus den Jahren 1800–1850, speziell aus London. Es gibt Herausforderungen, wie sicherzustellen, dass die gefundenen Bücher nicht überarbeitet oder modern interpretiert sind, sondern unberührte Bücher, die innerhalb meines gewählten Zeitraums veröffentlicht wurden.</p><p>Ich möchte ein neues Modell (v1) mit einem viel größeren Korpus trainieren, vielleicht 5–10x so groß wie der für v0.5 verwendete. Mein Ziel ist es zu sehen, ob sich allein durch Selective Temporal Training Denkfähigkeiten herausbilden können. Das wird schwieriger und ich bin nicht ganz sicher, ob es wegen der historischen Datenbegrenzungen überhaupt möglich ist. In den nächsten Wochen werde ich versuchen, genug Daten für einen 5–10GB großen Korpus zusammenzustellen. Ich glaube, wenn ich saubere, hochwertige Daten bekomme und eine GPU miete, wird es Fortschritte geben.</p><h1>So benutzt du dieses Projekt</h1></p><p>Dieses Projekt konzentriert sich hauptsächlich auf das Sammeln historischer Daten, deren Aufbereitung für das Training und den Bau eines Tokenizers. Ich werde nicht den gesamten LLM-Trainingsprozess abdecken, dafür verweise ich auf nanoGPT von Andrej Karpathy.</p><h1>Schritt 1: Historische Texte sammeln und vorbereiten</h1></p><p>Sammle .txt-Dateien von gemeinfreien Büchern, Dokumenten usw. aus deinem gewählten Zeitraum (z. B. London 1800–1850).</p><p>Du kannst download_texts_improved.py verwenden, um Bücher herunterzuladen, falls du welche brauchst.</p><p>Bereinige die Textdateien mit einem Skript oder entferne manuell Header/Footers von Project Gutenberg, moderne Anmerkungen oder Fehler wie OCR-Fehler.</p><p>prepare_dataset.py sollte gut funktionieren.</p><h1>Schritt 2: Einen eigenen Tokenizer bauen</h1></p><p>Führe train_tokenizer.py oder train_tokenizer_hf.py auf den gesäuberten Daten aus.
Dadurch erhältst du vocab.json und merges.txt</p><p>Diese Dateien definieren Wortschatz und Merge-Regeln für dein Modell.</p><h1>Schritt 3: Dein Modell trainieren (nanoGPT)</h1></p><p>Siehe <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT von Andrej Karpathy</a> für den Trainingsprozess.</p><p>Du kannst auch ein anderes LLM trainieren, aber ich habe nanoGPT verwendet.</p><h1>FAQ</h1></p><h2>Was ist Selective Temporal Training?</h2></p><p>Selective Temporal Training (STT) ist eine Machine Learning-Methode, bei der alle Trainingsdaten gezielt aus einem bestimmten historischen Zeitraum stammen. Das geschieht, um Sprache und Wissen jener Epoche ohne Einfluss moderner Konzepte zu modellieren. Das aktuelle Modell (v0.5) wurde beispielsweise ausschließlich mit Daten von 1800–1875 trainiert, nicht feinabgestimmt, sondern von Grund auf. Das Ergebnis spiegelt den Sprachstil und den historischen Kontext jener Zeit wider.</p><h2>Warum nicht einfach Fine-Tuning oder LoRA verwenden?</h2></p><p>Für dieses Projekt versuche ich, ein Sprachmodell zu erschaffen, das nicht von modernen Einflüssen getrübt ist. Wenn ich z. B. GPT-2 feinabstimme, ist es bereits vortrainiert und dieses Wissen bleibt erhalten. Wenn ich von Grund auf trainiere, tut das Sprachmodell nicht so, als wäre es alt – es ist es einfach. Das Ziel ist, ein Modell zu bauen, das ausschließlich mit Wissen aus Londoner Büchern von 1800 bis 1850 argumentieren kann.</p><h2>Welche Daten hast du zum Training verwendet?</h2></p><p>Ich verwende Bücher, juristische Dokumente, Zeitungen und andere Schriften aus dem London der Jahre 1800–1850. Die verlinkte Liste enthält etwa 200, aber für das erste Training habe ich nur 50 Dateien mit rund 187 MB verwendet. Du kannst die Dokumentenliste hier ansehen:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Wie groß ist das Version 0 Modell?</h2></p><p>Dieses Modell ist momentan sehr klein, ich mache das nur zum Spaß und folge strikt der Regel, keine modernen Quellen zu verwenden. Es hat fast 16 Millionen Parameter, aber ich beginne bald, mehr alte Texte zu sammeln, um ein weiteres Modell zu trainieren. Updates folgen!</p><h2>Trainingsspezifikationen?</h2></p><p>GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>