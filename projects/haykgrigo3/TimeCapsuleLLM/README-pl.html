<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - LLM wytrenowany wyłącznie na danych z określonych okres&#243;w, aby ograniczyć wsp&#243;łczesne uprzedzenia</title>
    <meta name="description" content="LLM wytrenowany wyłącznie na danych z określonych okres&#243;w, aby ograniczyć wsp&#243;łczesne uprzedzenia">
    <meta name="keywords" content="TimeCapsuleLLM, Polish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "LLM wytrenowany wyłącznie na danych z określonych okresów, aby ograniczyć współczesne uprzedzenia",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-pl.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">Polish</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Język</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (wkrótce)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (wkrótce)</a> |
        | <a href="#" title="Coming soon">ไทย (wkrótce)</a> |
        | <a href="#" title="Coming soon">Français (wkrótce)</a>
        | <a href="#" title="Coming soon">Deutsch (wkrótce)</a>
        | <a href="#" title="Coming soon">Español (wkrótce)</a>
        | <a href="#" title="Coming soon">Italiano (wkrótce)</a>
        | <a href="#" title="Coming soon">Русский (wkrótce)</a>
        | <a href="#" title="Coming soon">Português (wkrótce)</a>
        | <a href="#" title="Coming soon">Nederlands (wkrótce)</a>
        | <a href="#" title="Coming soon">Polski (wkrótce)</a>
        | <a href="#" title="Coming soon">العربية (wkrótce)</a>
        | <a href="#" title="Coming soon">فارسی (wkrótce)</a>
        | <a href="#" title="Coming soon">Türkçe (wkrótce)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (wkrótce)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (wkrótce)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
LLM trenowany wyłącznie na danych z określonych okresów historycznych, by zredukować współczesne uprzedzenia.</p><p>Wyobraź sobie, że model AI nie tylko udaje historyczny, ale rzeczywiście nim jest.</p><p>Zbudowany na <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT autorstwa Andreja Karpathy’ego</a>. Podstawowe skrypty treningowe i architektura modelu to jego praca.</p><h1>Cele projektu</h1></p><p>TimeCapsule LLM to eksperymentalny projekt, który będzie trenowany wyłącznie na tekstach napisanych w określonych okresach historycznych. Celem jest symulacja światopoglądu i języka charakterystycznych dla wybranych epok.</p><h1>Dlaczego samo dostrajanie nie wystarcza</h1></p><p>Jeśli tylko dostroisz wstępnie wytrenowany model, twój LLM i tak będzie znał współczesne pojęcia. Oczywiście osiągnięcie całkowitego braku współczesnych uprzedzeń jest trudne, ale chcę być jak najbliżej tego ideału. Brak współczesnych wpływów wymaga trenowania modelu od zera.</p><h1>Oczekiwane rezultaty</h1></p><p>Mam nadzieję, że po zakończeniu ten model nie będzie znał współczesnych pojęć i nie będzie w stanie rozumować poza tym, na czym był trenowany. Nie powinien rozpoznawać współczesnych pojęć/słownictwa i mam nadzieję, że nie będzie halucynował współczesnej wiedzy.</p><h1>Aktualizacje postępów</h1></p><h2>9 lipca 2025</h2></p><p>Wybrałem okres: 1800-1850 oraz region: Londyn</p><p>Zebrałem listę tekstów, książek, dokumentów</p><p>Jak dotąd mam 50 plików txt i wkrótce rozpocznę trening NanoGPT</p><p>Będę aktualizować ten wpis wraz z postępami</p><h2>13 lipca 2025</h2></p><p>Wytrenowałem nanoGPT na 187MB historycznych danych tekstowych.</p><h2>15 lipca 2025</h2></p><p>Rozpocząłem pobieranie tekstów do drugiego treningu. Wszystko ściągam z Internet Archive i poszerzyłem okres do 1800-1875. Aby uzyskać zróżnicowany zestaw tekstów, można używać filtrów tematycznych i wyszukiwania według miejsca publikacji, okresu i tematów na Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtry wyszukiwania"></p><h2>16 lipca 2025</h2></p><p>Pobrałem około 500 plików txt z Internet Archive i po ich oczyszczeniu (usuwanie białych znaków, nagłówków Gutenberg itp.) mam około 500MB danych. To wciąż mały zbiór, ale poprzednio trenowałem na 187MB, więc powinno być przynajmniej trochę zauważalnej różnicy w wynikach po treningu drugiego modelu. Mam nadzieję, że ten model będzie przynajmniej generował bardziej spójne zdania, które mają sens. Oczywiście nie jest to gwarantowane, bo to nadal bardzo mały zbiór, ale większy niż poprzednio.</p><p>Powinienem być w stanie zrobić to na własnym sprzęcie, co jest dobre, bo mam szansę zobaczyć jakieś ulepszenia zanim przejdę do większego zbioru, który wymagałby wynajęcia GPU. Ale nie martw się, planuję niedługo wynająć GPU, ale zanim to zrobię, chcę mieć możliwie najlepiej przygotowany i oczyszczony zbiór. Jednym z problemów jest czyszczenie, wiele tych plików txt zawiera bełkot. Skrypty, których użyłem do czyszczenia, działają, ale nie są w 100% skuteczne.</p><p>Dzisiaj wytrenuję ten zbiór i powinno to zająć około 4-5 godzin. Gdy skończę i przetestuję, dam znać o efektach. Dziękuję wszystkim, którzy śledzą mój projekt, dostałem nawet kilka linków do zasobów OCR, więc dziękuję! Mam nadzieję, że więcej osób spróbuje tego podejścia i poeksperymentuje z własnymi zbiorami danych.</p><h3>Aktualizacja treningu</h3></p><p>Rozpocząłem trening na korpusie 435MB (108 mln tokenów), na razie idzie całkiem gładko. Strata trenowania spadła z 10.9 do 4.9 w pierwszych 2800 iteracjach. Przewiduję, że całość zajmie 8-9 godzin. Po zakończeniu wrzucę kolejną aktualizację.</p><h2>17 lipca 2025, 2:13</h2></p><p>Trening drugiego modelu zakończony, moja 4060 potrzebowała około 8 godzin i 40 minut (3900 iteracji/godz.) na 33 000 iteracji (5 epok). Końcowa strata treningowa wyniosła 3.73. Wyniki były zaskakująco dobre – model autentycznie generuje spójne zdania w stylu XIX wieku.</p><h2>28 lipca 2025</h2></p><p>Wrzuciłem v0.5 na Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">sprawdź tutaj</a> jeśli chcesz. Możesz teraz pobrać moje repo i uruchomić lokalnie. Niestety nanoGPT nie działa natywnie z HuggingFace, więc trzeba pobrać i uruchomić model lokalnie.</p><p>Wkrótce zacznę kuratorować dane do kolejnego treningu, myślę że będę potrzebował 5-10x więcej danych, by uzyskać możliwości rozumowania.</p><h1>Zachowanie modelu V0 i ograniczenia</h1></p><p>Wstępne podpowiedzi pokazują, że model odpowiada językiem i zachowaniem z XIX wieku. Na przykład, zapytałem "Who art Henry?" i odpowiedział "I know that man, I have did not a black, the storm." – i tak, to zdanie nie ma sensu, ale LLM rozpoznaje, że pytam o osobę.</p><p>
<img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Przykładowe wyjście TimeLockLLM"></p><p>Nie ma tu wzmianki o nowoczesnych pojęciach, wyniki zawierają głównie słowa i zwroty z XIX wieku.</p><p>Nadal wymaga to dużo pracy, trenowanie na 187 MB nie da modelu, który generuje tekst o złożonym rozumowaniu.</p><p>Obecnie generuje zdania, które nie mają pełnej struktury zdaniowej i ogólnie nie mają sensu, ale to normalne przy takiej wielkości zbioru treningowego.</p><h1>Zachowanie modelu V0.5 i ograniczenia</h1></p><p>To duży postęp w porównaniu do poprzedniego modelu. Styl pisania i słownictwo są wiktoriańskie, a prawie każde zdanie jest gramatycznie poprawne i posiada właściwą interpunkcję. Model ten został wytrenowany od zera, więc trzyma się tematów z XIX wieku.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Przykładowe wyjście TimeLockLLM"></p><p>Jest dużo halucynacji faktów. Bardzo dużo (praktycznie 100%) szczegółów (daty, wydarzenia, postaci historyczne) jest zmyślonych. Zdania nie są ze sobą powiązane, czasem może dwa zdania się łączą, ale poza tym już nie. Kolejnym problemem jest czasem pojawiający się losowy podpis „Digitized by Google”, więc przy następnym treningu muszę lepiej oczyścić teksty. Ogólnie jestem bardzo zadowolony z efektów, do LLM to jeszcze daleko, ale na pewno to już generator zdań.</p><p>Dużo się uczę i w najbliższych tygodniach zacznę analizować, co muszę zrobić lepiej. Wkrótce wrzucę pliki!</p><h1>Nadchodzące plany</h1></p><p>(Zakończone) Zaczynam pracę nad wersją 0.5, zamiast trenować na 50 książkach, będę trenował na 500-600. Obecnie trenuję nanoGPT na książkach z lat 1800-1850, szczególnie z Londynu. Są wyzwania, jak upewnić się, że książki nie są zaktualizowane czy mają nowoczesne interpretacje, ale tylko oryginalne wydania z wybranego okresu.</p><p>Chcę wytrenować nowy model (v1) na znacznie większym korpusie, może 5-10x większym niż użyty dla v0.5. Moim celem jest sprawdzić, czy umiejętności rozumowania mogą się pojawić tylko dzięki Selektywnemu Treningowi Czasowemu, to będzie trudniejsze zadanie i nie mam pewności, czy to możliwe z powodu ograniczeń historycznych danych. W nadchodzących tygodniach postaram się zebrać dane na korpus 5-10 GB. Wierzę, że jeśli zdobędę czyste, wysokiej jakości dane i wynajmę GPU, będzie postęp.</p><h1>Jak korzystać z tego projektu</h1></p><p>Projekt skupia się głównie na gromadzeniu historycznych danych, przygotowaniu ich do treningu i budowie tokenizera. Nie opisuję całego procesu trenowania LLM, do tego odsyłam do nanoGPT Andreja Karpathy.</p><h1>Krok 1: Zbierz i przygotuj historyczne teksty</h1></p><p>Zbierz pliki .txt książek, dokumentów itp. z wybranego okresu historycznego (np. Londyn 1800-1850)</p><p>Możesz użyć download_texts_improved.py do pobrania książek, jeśli chcesz.</p><p>Oczyść pliki tekstowe używając skryptu lub ręcznie usuń nagłówki/stopki z Project Gutenberg, nowoczesne adnotacje lub błędy OCR.</p><p>prepare_dataset.py powinien działać dobrze.</p><h1>Krok 2: Zbuduj własny tokenizer</h1></p><p>Uruchom train_tokenizer.py lub train_tokenizer_hf.py na oczyszczonych danych.
Dostaniesz vocab.json i merges.txt</p><p>Te pliki definiują słownictwo i reguły łączenia dla twojego modelu</p><h1>Krok 3: Trenuj swój model (nanoGPT)</h1></p><p>Zapoznaj się z <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT by Andrej Karpathy</a> w celu przeprowadzenia treningu.</p><p>Możesz trenować inny LLM, jeśli chcesz, ale ja użyłem nanoGPT</p><h1>FAQ</h1></p><h2>Czym jest Selektywny Trening Czasowy?</h2></p><p>Selective Temporal Training (STT) to metodologia uczenia maszynowego, w której wszystkie dane treningowe są specjalnie kuratorowane, by pochodziły z określonego okresu historycznego. Robi się to, aby modelować język i wiedzę tej epoki bez wpływu współczesnych pojęć. Na przykład obecny model (v0.5) jest trenowany wyłącznie na danych z lat 1800-1875, nie jest to fine-tuning, tylko trening od zera, dzięki czemu wynik odzwierciedla styl językowy i kontekst historyczny tego okresu.</p><h2>Dlaczego nie po prostu fine-tuning lub LoRA?</h2></p><p>W tym projekcie próbuję stworzyć model językowy pozbawiony współczesnych uprzedzeń. Jeśli zrobię fine-tuning np. GPT-2, to już jest wytrenowany i tych informacji nie da się usunąć. Jeśli wytrenuję od zera, model językowy nie będzie udawał starego, tylko taki będzie. Celem projektu jest stworzenie czegoś, co rozumuje wyłącznie na podstawie wiedzy z londyńskich książek z lat 1800-1850.</p><h2>Jakie dane wykorzystałeś do treningu?</h2></p><p>Używam książek, dokumentów prawnych, gazet i innych pism z Londynu z lat 1800–1850. Lista, którą podlinkowałem, zawiera ok. 200 pozycji, ale do pierwszego treningu użyłem tylko 50 plików o łącznej wielkości ~187 MB. Możesz zobaczyć listę dokumentów:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Jak duży jest model w wersji 0?</h2></p><p>Model jest teraz bardzo mały, robię to dla zabawy, trzymając się zasady braku współczesnych źródeł. Ma prawie 16 milionów parametrów, ale zaczynam zbierać więcej starych tekstów, by rozpocząć nowy trening modelu. Będę informować na bieżąco.</p><h2>Specyfikacja treningu?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>