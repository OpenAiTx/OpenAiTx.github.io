<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد</title>
    <meta name="description" content="یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد">
    <meta name="keywords" content="TimeCapsuleLLM, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 275
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-07",
  "dateModified": "2025-08-07"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 275 stars</span>
                <span class="language">Persian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">انگلیسی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">ژاپنی</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">کره‌ای</a>
        | <a href="#" title="Coming soon">هندی (به زودی)</a> |
        | <a href="#" title="Coming soon">تایلندی (به زودی)</a> |
        | <a href="#" title="Coming soon">فرانسوی (به زودی)</a>
        | <a href="#" title="Coming soon">آلمانی (به زودی)</a>
        | <a href="#" title="Coming soon">اسپانیایی (به زودی)</a>
        | <a href="#" title="Coming soon">ایتالیایی (به زودی)</a>
        | <a href="#" title="Coming soon">روسی (به زودی)</a>
        | <a href="#" title="Coming soon">پرتغالی (به زودی)</a>
        | <a href="#" title="Coming soon">هلندی (به زودی)</a>
        | <a href="#" title="Coming soon">لهستانی (به زودی)</a>
        | <a href="#" title="Coming soon">عربی (به زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به زودی)</a>
        | <a href="#" title="Coming soon">ترکی (به زودی)</a>
        | <a href="#" title="Coming soon">ویتنامی (به زودی)</a>
        | <a href="#" title="Coming soon">اندونزیایی (به زودی)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>تایم‌کپسول ال‌ال‌ام</h1>
یک مدل زبانی بزرگ که فقط با داده‌های دوره‌های زمانی خاص آموزش داده شده تا سوگیری مدرن را کاهش دهد.</p><p>تصور کنید اگر یک مدل هوش مصنوعی فقط نقش تاریخی را بازی نمی‌کرد، بلکه واقعاً تاریخی بود.</p><p>بر پایه <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT اثر آندری کارپاتی</a> اسکریپت‌های اصلی آموزش و معماری مدل از ایشان است.</p><h1>اهداف پروژه</h1></p><p>تایم‌کپسول ال‌ال‌ام یک پروژه آزمایشی است که فقط با متونی آموزش داده می‌شود که در دوره‌های زمانی خاص نوشته شده‌اند. هدف، شبیه‌سازی جهان‌بینی و زبان دوره‌های تاریخی معین است.</p><h1>چرا تنظیم دقیق کافی نیست</h1></p><p>اگر فقط یک مدل آموزش‌دیده را تنظیم دقیق کنید، مدل شما همچنان مفاهیم مدرن را خواهد دانست. البته رسیدن به صفر سوگیری مدرن دشوار است اما من می‌خواهم تا حد ممکن به این هدف نزدیک شوم. برای حذف کامل سوگیری مدرن باید مدل را از ابتدا آموزش داد.</p><h1>نتایج مورد انتظار</h1></p><p>امیدوارم وقتی مدل کامل شد، دانشی از مفاهیم مدرن نداشته باشد و نتواند فراتر از آموزش‌هایش استدلال کند. نباید مفاهیم/واژگان مدرن را تشخیص دهد و امیدوارم دانش مدرن را جعل نکند.</p><h1>به‌روزرسانی‌های پیشرفت</h1></p><h2>۹ ژوئیه ۲۰۲۵</h2></p><p>دوره زمانی من ۱۸۰۰-۱۸۵۰ و منطقه: لندن تعیین شد</p><p>فهرستی از متون، کتاب‌ها و اسناد جمع‌آوری کرده‌ام</p><p>تاکنون ۵۰ متن را به صورت فایل txt تهیه کرده‌ام و به زودی آموزش NanoGPT را آغاز خواهم کرد</p><p>تا زمانی که پیشرفت حاصل شود، این بخش را به‌روزرسانی می‌کنم</p><h2>۱۳ ژوئیه ۲۰۲۵</h2></p><p>مدل nanoGPT را با ۱۸۷ مگابایت داده متنی تاریخی آموزش دادم.</p><h2>۱۵ ژوئیه ۲۰۲۵</h2></p><p>برای دور دوم آموزش شروع به دانلود متون کردم. همه چیز را از Internet Archive دریافت می‌کنم و بازه زمانی را به ۱۸۰۰-۱۸۷۵ گسترش دادم. برای تنوع بیشتر متون، می‌توانید از فیلترهای موضوع و جستجو برای محل انتشار، بازه زمانی و موضوعات در Internet Archive استفاده کنید.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="فیلترهای جستجو"></p><h2>۱۶ ژوئیه ۲۰۲۵</h2></p><p>حدود ۵۰۰ فایل txt از Internet Archive دانلود کردم و پس از پاک‌سازی (حذف فاصله‌های اضافی، هدرهای گوتنبرگ و غیره) حدود ۵۰۰ مگابایت داده دارم. این یک مجموعه داده کوچک است، اما دفعه قبل فقط با ۱۸۷ مگابایت آموزش دادم، پس باید دست‌کم تفاوت قابل توجهی در خروجی پس از آموزش مدل دوم مشاهده شود. امیدوارم این مدل بتواند جملات منسجم‌تری تولید کند که تا حدی معنادار باشند. البته تضمینی نیست چون هنوز هم داده‌ها خیلی کم هستند، اما از دفعه قبل بیشتر است.</p><p>این کار باید روی سخت‌افزار خودم قابل انجام باشد، این هم خوب است چون می‌توانم قبل از رفتن به سمت مجموعه داده بزرگ‌تر که نیاز به اجاره GPU دارد، برخی بهبودها را ببینم. اما نگران نباشید، هنوز برنامه دارم به زودی GPU اجاره کنم، ولی قبل از آن می‌خواهم مطمئن شوم داده‌هایم تا حد امکان پاک و گزینش شده‌اند. یکی از مشکلاتم پاک‌سازی است، بسیاری از این فایل‌های txt مخلوطی از آشفتگی دارند. اسکریپت‌هایی که برای پاک‌سازی استفاده کردم تا حدی کار می‌کنند اما صد درصد موثر نیستند.</p><p>امروز این مجموعه داده را آموزش می‌دهم و باید حدود ۴-۵ ساعت طول بکشد. وقتی تمام شد و تست کردم، به‌روزرسانی خواهم داد. دوباره از همه کسانی که پروژه‌ام را بررسی می‌کنند ممنونم، حتی بعضی‌ها منابع OCR را به من معرفی کردند، پس متشکرم! امیدوارم افراد بیشتری این کار را امتحان کنند و با داده‌های خودشان آزمایش کنند.</p><h3>به‌روزرسانی آموزش</h3></p><p>آموزش را با یک مجموعه ۴۳۵ مگابایتی (۱۰۸ میلیون توکن) آغاز کردم، فعلاً روند خوبی دارد. خطای آموزش از ۱۰.۹ به ۴.۹ در ۲۸۰۰ تکرار اول کاهش یافت. انتظار دارم حدود ۸ یا ۹ ساعت طول بکشد تا کامل شود. پس از اتمام، یک به‌روزرسانی دیگر منتشر می‌کنم.</p><h2>۱۷ ژوئیه ۲۰۲۵</h2></p><p>آموزش مدل دوم تمام شد، کارت گرافیک ۴۰۶۰ من در حدود ۸ ساعت و ۴۰ دقیقه (۳۹۰۰ تکرار/ساعت) برای ۳۳,۰۰۰ تکرار (۵ اپوک) آن را انجام داد. خطای آموزش نهایی ۳.۷۳ بود. خروجی‌ها به طرز شگفت‌انگیزی خوب بودند و واقعاً جملات منسجم به سبک قرن نوزدهم تولید می‌کند.</p><h2>۲۸ ژوئیه ۲۰۲۵</h2></p><p>نسخه ۰.۵ را در Hugging Face آپلود کردم، <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">اینجا ببینید</a> اگر دوست دارید. حالا می‌توانید مخزن من را دانلود و به صورت محلی اجرا کنید. متأسفانه nanoGPT به طور بومی با HuggingFace کار نمی‌کند، پس باید مدل را دانلود و به صورت محلی اجرا کنید.</p><p>همچنین به زودی شروع به گزینش داده برای دور بعدی آموزش می‌کنم، فکر می‌کنم برای دستیابی به قابلیت استدلال به ۵ تا ۱۰ برابر داده بیشتر نیاز دارم.</p><h2>۲ اوت ۲۰۲۵</h2></p><p>به زودی کار روی نسخه ۱ را آغاز خواهم کرد. باید از معماری nanoGPT به چیزی مدرن‌تر مهاجرت کنم. چند معماری LLM متن‌باز مد نظر دارم، از جمله: OpenLLaMA v3، Phi-2 و Qwen 1.5B. و برای جهش به نسخه ۱، باید یک مجموعه داده بسیار بزرگ‌تر و متنوع‌تر را با دقت گزینش کنم. دست‌کم به ۵ گیگابایت داده آموزشی پاک نیاز دارم.</p><h1>رفتار و محدودیت‌های مدل V0</h1></p><p>دستورهای اولیه نشان می‌دهند که مدل با زبان و رفتار قرن ۱۸۰۰ پاسخ می‌دهد. برای مثال، من با جمله «چه کسی هنری است؟» از آن پرسیدم و جواب داد «من آن مرد را می‌شناسم، هرگز سیاه نکرده‌ام، طوفان.» و بله این جمله بی‌معنی است اما مدل LLM متوجه می‌شود که درباره یک شخص سؤال پرسیده‌ام.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="نمونه خروجی TimeLockLLM"></p><p>هیچ اشاره‌ای به مفاهیم مدرن وجود ندارد، خروجی‌ها عمدتاً شامل واژگان و عبارات متعلق به قرن ۱۸۰۰ هستند.</p><p>هنوز به کار زیادی نیاز دارد، آموزش روی ۱۸۷ مگابایت داده مدلی نمی‌سازد که متنی با استدلال پیچیده تولید کند.</p><p>در حال حاضر جملاتی تولید می‌کند که ساختار جمله کامل ندارند و در کل بی‌معنی هستند، اما این برای اندازه مجموعه آموزش طبیعی است.</p><h1>رفتار و محدودیت‌های مدل V0.5</h1></p><p>این نسبت به مدل قبلی پیشرفت خوبی است. سبک نگارش و واژگان ویکتوریایی است و تقریباً هر جمله از لحاظ گرامری صحیح و با نشانه‌گذاری درست است. و باز هم چون از ابتدا آموزش دیده، به موضوعات قرن ۱۸۰۰ وفادار است.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="نمونه خروجی TimeLockLLM"></p><p>تعداد زیادی خطاهای واقعی (hallucination) وجود دارد. بسیاری (تقریباً ۱۰۰٪) از جزئیات (تاریخ‌ها، رویدادها، شخصیت‌های تاریخی) ساختگی هستند. همچنین جملات واقعاً به هم مرتبط نیستند، شاید فقط دو جمله به هم مربوط باشند اما بیشتر از آن نه. مشکل دیگر این است که گاهی یک فوتر اضافی مانند «Digitized by Google» ظاهر می‌شود، پس دفعه بعد باید واقعاً مطمئن شوم که متون به‌خوبی پاک‌سازی شده‌اند. در کل از نتایج راضی هستم، هنوز به LLM نزدیک نیست اما قطعاً یک جمله‌ساز است.</p><p>خیلی چیزها یاد می‌گیرم و در هفته‌های آینده سعی می‌کنم بفهمم چه کارهایی باید بهتر انجام دهم. به‌زودی فایل‌ها را بارگذاری می‌کنم!</p><h1>برنامه‌های آینده</h1></p><p>(انجام شد) قصد دارم کار روی نسخه ۰.۵ را شروع کنم، به‌جای آموزش با ۵۰ کتاب، ترجیحاً با ۵۰۰ تا ۶۰۰ کتاب آموزش می‌دهم. در حال حاضر دارم nanoGPT را با کتاب‌هایی از سال ۱۸۰۰ تا ۱۸۵۰ و به طور خاص از لندن آموزش می‌دهم. چالش‌هایی وجود دارد مثل اینکه مطمئن شوم کتاب‌هایی که پیدا می‌کنم نسخه به‌روزرسانی شده یا تفسیرهای مدرن ندارند و کتاب‌های دست‌نخورده‌ای هستند که در بازه زمانی انتخابی من منتشر شده‌اند.</p><p>می‌خواهم یک مدل جدید (v1) با مجموعه داده بسیار بزرگ‌تر، شاید ۵ تا ۱۰ برابر بزرگ‌تر از v0.5 آموزش دهم. هدفم این است ببینم آیا می‌توانم صرفاً با آموزش زمانی انتخابی (Selective Temporal Training) توانایی استدلال را ظاهر کنم، این کار دشوارتر خواهد بود و حتی مطمئن نیستم به دلیل محدودیت داده‌های تاریخی امکان‌پذیر باشد. در هفته‌های آینده سعی می‌کنم داده کافی برای یک مجموعه ۵ تا ۱۰ گیگابایتی جمع‌آوری کنم. معتقدم اگر بتوانم داده‌های تمیز و با کیفیت بالا تهیه کنم و یک GPU اجاره کنم، پیشرفت حاصل خواهد شد.</p><h1>چگونه از این پروژه استفاده کنیم</h1></p><p>این پروژه عمدتاً بر جمع‌آوری داده‌های تاریخی، آماده‌سازی آن برای آموزش و ساخت یک توکنایزر متمرکز است. من کل فرایند آموزش LLM را پوشش نمی‌دهم، برای آن به nanoGPT از آندری کارپاتی مراجعه کنید.</p><h1>گام ۱: جمع‌آوری و آماده‌سازی متون تاریخی</h1></p><p>فایل‌های .txt کتاب‌ها، اسناد و غیره را از بازه زمانی مورد نظر خود (مثلاً لندن ۱۸۰۰-۱۸۵۰) جمع‌آوری کنید.</p><p>در صورت نیاز می‌توانید از download_texts_improved.py برای دانلود کتاب‌ها استفاده کنید.</p><p>فایل‌های متنی را با یک اسکریپت یا به صورت دستی از هدر/فوتر پروژه گوتنبرگ، یادداشت‌های مدرن یا خطاهای OCR پاک کنید.</p><p>prepare_dataset.py باید به‌خوبی کار کند.</p><h1>گام ۲: ساخت توکنایزر سفارشی</h1></p><p>train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاک‌شده اجرا کنید.
این کار vocab.json و merges.txt را به شما می‌دهد.</p><p>این فایل‌ها واژگان و قواعد ادغام توکن‌ها برای مدل شما را تعریف می‌کنند.</p><h1>گام ۳: آموزش مدل خود (nanoGPT)</h1></p><p>برای فرایند آموزش به <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT اثر آندری کارپاتی</a> مراجعه کنید.</p><p>اگر بخواهید می‌توانید یک LLM متفاوت آموزش دهید، اما من از nanoGPT استفاده کردم.</p><h1>پرسش‌های متداول</h1></p><h2>آموزش زمانی انتخابی (Selective Temporal Training) چیست؟</h2></p><p>آموزش زمانی انتخابی (STT) یک روش یادگیری ماشین است که در آن تمام داده‌های آموزشی به طور خاص در یک بازه زمانی تاریخی خاص جمع‌آوری می‌شوند. این کار به منظور مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن انجام می‌شود. برای مثال، مدل فعلی من (v0.5) فقط با داده‌های سال‌های ۱۸۰۰-۱۸۷۵ آموزش دیده است، ریزتنظیم نشده بلکه از ابتدا آموزش داده شده و خروجی آن بازتاب‌دهنده سبک زبانی و زمینه تاریخی همان زمان است.</p><h2>چرا فقط از ریزتنظیم یا LoRA استفاده نمی‌کنید؟</h2></p><p>در این پروژه سعی دارم یک مدل زبانی بسازم که از سوگیری مدرن دور باشد. اگر چیزی مثل GPT-2 را ریزتنظیم کنم، از قبل آموزش دیده و آن اطلاعات حذف نمی‌شود. اگر از ابتدا آموزش دهم، مدل زبانی تظاهر به قدیمی بودن نمی‌کند، بلکه واقعاً قدیمی خواهد بود. هدف فعلی این پروژه ساخت مدلی است که فقط با دانش کتاب‌های لندن بین سال‌های ۱۸۰۰ تا ۱۸۵۰ استدلال کند.</p><h2>از چه داده‌هایی برای آموزش استفاده کردید؟</h2></p><p>من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن ۱۸۰۰-۱۸۵۰ استفاده می‌کنم. لیست پیوند داده شده حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط از ۵۰ فایل با حجم حدود ۱۸۷ مگابایت استفاده کردم. می‌توانید فهرست اسناد را مشاهده کنید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>مدل نسخه ۰ چقدر بزرگ است؟</h2></p><p>این مدل در حال حاضر بسیار کوچک است، این کار را صرفاً برای سرگرمی انجام می‌دهم و قانون آموزشی سختگیرانه‌ای برای عدم استفاده از منابع مدرن دارم. تقریباً ۱۶ میلیون پارامتر دارد اما قصد دارم متون قدیمی بیشتری جمع‌آوری کنم تا آموزش مدل دیگری را آغاز کنم. به‌مرور به‌روزرسانی خواهم داد.</p><h2>مشخصات آموزش؟</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
رم: ۱۶ گیگابایت DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-07 
    </div>
    
</body>
</html>