<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد</title>
    <meta name="description" content="یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد">
    <meta name="keywords" content="TimeCapsuleLLM, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "یک مدل زبان بزرگ (LLM) که فقط با داده‌های مربوط به دوره‌های زمانی خاص آموزش دیده تا سوگیری‌های مدرن را کاهش دهد",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">Persian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (به زودی)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (به زودی)</a> |
        | <a href="#" title="Coming soon">ไทย (به زودی)</a> |
        | <a href="#" title="Coming soon">Français (به زودی)</a>
        | <a href="#" title="Coming soon">Deutsch (به زودی)</a>
        | <a href="#" title="Coming soon">Español (به زودی)</a>
        | <a href="#" title="Coming soon">Italiano (به زودی)</a>
        | <a href="#" title="Coming soon">Русский (به زودی)</a>
        | <a href="#" title="Coming soon">Português (به زودی)</a>
        | <a href="#" title="Coming soon">Nederlands (به زودی)</a>
        | <a href="#" title="Coming soon">Polski (به زودی)</a>
        | <a href="#" title="Coming soon">العربية (به زودی)</a>
        | <a href="#" title="Coming soon">فارسی (به زودی)</a>
        | <a href="#" title="Coming soon">Türkçe (به زودی)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (به زودی)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (به زودی)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>تایم‌کپسول LLM</h1>
یک مدل LLM که فقط بر اساس داده‌هایی از دوره‌های زمانی خاص آموزش دیده تا سوگیری مدرن را کاهش دهد.</p><p>تصور کنید مدلی هوشمند نه تنها وانمود به تاریخی بودن کند، بلکه واقعا چنین باشد.</p><p>ساخته شده بر پایه <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT نوشته آندره کارپاتی</a> اسکریپت‌های اصلی آموزش و معماری مدل متعلق به اوست.</p><h1>اهداف پروژه</h1></p><p>تایم‌کپسول LLM یک پروژه آزمایشی است که فقط بر روی متون نوشته شده در بازه‌های زمانی معین آموزش داده می‌شود. هدف شبیه‌سازی جهان‌بینی و زبان دوره‌های تاریخی خاص است.</p><h1>چرا تنظیم دقیق کافی نیست</h1></p><p>اگر فقط یک مدل از پیش آموزش دیده را تنظیم دقیق کنید، مدل LLM شما همچنان مفاهیم مدرن را خواهد دانست. البته رسیدن به صفر سوگیری مدرن دشوار است اما من می‌خواهم تا حد ممکن به این هدف نزدیک شوم. رسیدن به نبود سوگیری مدرن نیازمند آموزش مدل از ابتدا است.</p><h1>نتایج مورد انتظار</h1></p><p>امیدوارم پس از تکمیل، این مدل هیچ دانشی از مفاهیم مدرن نداشته باشد و نتواند فراتر از داده‌هایی که آموزش دیده است، استدلال کند. نباید مفاهیم/واژگان مدرن را بشناسد و امیدوارم دانش مدرن را به اشتباه تولید نکند.</p><h1>به‌روزرسانی‌های پیشرفت</h1></p><h2>۹ ژوئیه ۲۰۲۵</h2></p><p>دوره زمانی من ۱۸۰۰-۱۸۵۰ و منطقه: لندن تعیین شده است</p><p>یک لیست از متون، کتاب‌ها، اسناد گردآوری کرده‌ام</p><p>تا الان ۵۰ فایل به صورت txt جمع‌آوری کرده‌ام و به زودی آموزش NanoGPT را آغاز می‌کنم</p><p>تا زمانی که پیشرفتی حاصل شود این بخش را به‌روزرسانی خواهم کرد</p><h2>۱۳ ژوئیه ۲۰۲۵</h2></p><p>nanoGPT را با ۱۸۷ مگابایت داده متنی تاریخی آموزش دادم.</p><h2>۱۵ ژوئیه ۲۰۲۵</h2></p><p>شروع به دانلود متون برای دومین دوره آموزش کردم. همه چیز را از Internet Archive می‌گیرم و بازه زمانی را به ۱۸۰۰-۱۸۷۵ گسترش داده‌ام. برای به دست آوردن طیف متنوعی از متون، می‌توانید از فیلترهای موضوع و جستجو برای محل انتشار، دوره زمانی و موضوعات در Internet Archive استفاده کنید.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="فیلترهای جستجو"></p><h2>۱۶ ژوئیه ۲۰۲۵</h2></p><p>حدود ۵۰۰ فایل txt از Internet Archive دانلود کردم و پس از پاک‌سازی آنها (حذف فاصله‌ها، هدرهای گوتنبرگ و غیره) حدود ۵۰۰ مگابایت داده دارم. این داده‌کاوی بسیار کوچک است اما دفعه قبل با ۱۸۷ مگابایت آموزش دادم، پس باید حداقل تفاوت قابل توجهی در خروجی پس از آموزش مدل دوم مشاهده شود. امیدوارم این مدل بتواند جملات منسجم‌تری تولید کند که تا حدی معنا داشته باشند. البته تضمینی وجود ندارد چون همچنان داده‌ها بسیار کم است، اما نسبت به دفعه قبل بیشتر است.</p><p>این حجم داده روی سخت‌افزار خودم قابل انجام است، که خوب است چون می‌توانم قبل از رفتن به سراغ داده‌کاوی بزرگ‌تر و اجاره GPU، امیدوار باشم مقداری بهبود ببینم. اما نگران نباشید، به زودی قصد اجاره GPU را دارم، اما قبل از آن می‌خواهم مطمئن شوم داده‌کاوی من تا جای ممکن سرراست و پاک است. یکی از مشکلاتم پاک‌سازی است، بسیاری از این فایل‌های txt شامل داده‌های بی‌معنی هستند. اسکریپت‌هایی که برای پاک‌سازی استفاده کردم موثرند اما ۱۰۰٪ کارآمد نیستند.</p><p>امروز این داده‌کاوی را آموزش می‌دهم و باید حدود ۴-۵ ساعت طول بکشد. پس از اتمام و تست آن، به‌روزرسانی خواهم داد. دوباره از همه کسانی که پروژه‌ام را بررسی می‌کنند تشکر می‌کنم، حتی برخی افراد منابع OCR هم معرفی کردند، پس ممنونم! امیدوارم افراد بیشتری این کار را امتحان کنند و با داده‌کاوی‌های خودشان تجربه کنند.</p><h3>به‌روزرسانی آموزش</h3></p><p>آموزش را روی داده‌کاوی ۴۳۵ مگابایت (۱۰۸ میلیون توکن) شروع کردم، فعلا روند به خوبی پیش می‌رود. خطای آموزش از ۱۰.۹ به ۴.۹ در ۲۸۰۰ تکرار اول کاهش یافت. انتظار دارم حدود ۸ یا ۹ ساعت طول بکشد. پس از اتمام به‌روزرسانی دیگری خواهم داد.</p><h2>۱۷ ژوئیه ۲۰۲۵ ساعت ۲:۱۳ بامداد</h2></p><p>آموزش مدل دوم به پایان رسید، کارت گرافیک ۴۰۶۰ من حدود ۸ ساعت و ۴۰ دقیقه (۳۹۰۰ تکرار/ساعت) برای ۳۳,۰۰۰ تکرار (۵ اپوک) زمان برد. خطای آموزش نهایی ۳.۷۳ بود. خروجی‌ها شگفت‌انگیز بود و اکنون واقعا جملات منسجم به سبک قرن ۱۹ تولید می‌کند.</p><h2>۲۸ ژوئیه ۲۰۲۵</h2></p><p>نسخه v0.5 را در Hugging Face آپلود کرده‌ام، <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">اینجا ببینید</a> اگر دوست دارید. اکنون می‌توانید مخزن من را دانلود و به صورت محلی اجرا کنید. متاسفانه nanoGPT به صورت بومی با HuggingFace کار نمی‌کند، پس باید مدل را دانلود و به صورت محلی اجرا کنید.</p><p>همچنین شروع به گردآوری داده برای آموزش بعدی کردم، فکر می‌کنم شاید به ۵ تا ۱۰ برابر داده بیشتر برای رسیدن به قابلیت استدلال نیاز داشته باشم.</p><h1>رفتار مدل V0 و محدودیت‌ها</h1></p><p>در پرسش‌های اولیه مدل با زبان و رفتار قرن ۱۸۰۰ واکنش نشان می‌دهد. مثلا، با پرسیدن "Who art Henry?" پاسخ داد: "I know that man, I have did not a black, the storm." و بله، این جمله معنی ندارد اما LLM متوجه شده که درباره یک شخص پرسیده‌ام.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="نمونه خروجی TimeLockLLM"></p><p>هیچ اشاره‌ای به مفاهیم مدرن نشده و خروجی‌ها عمدتاً شامل واژه‌ها و عبارات متعلق به دهه ۱۸۰۰ هستند.</p><p>هنوز به کار زیادی نیاز دارد، آموزش با ۱۸۷ مگابایت داده هرگز مدلی تولید نمی‌کند که متنی با استدلال پیچیده ارائه دهد.</p><p>در حال حاضر جملاتی تولید می‌کند که ساختار کامل جمله را ندارند و به طور کلی بی‌معنی هستند، اما این برای این حجم آموزش طبیعی است.</p><h1>رفتار و محدودیت‌های مدل V0.5</h1></p><p>این نسبت به مدل قبلی بهبود خوبی است. سبک نوشتار و واژگان ویکتوریایی است و تقریباً هر جمله از نظر دستوری صحیح و با نقطه‌گذاری مناسب نوشته شده است. و مجدداً این مدل از ابتدا آموزش داده شده بنابراین به موضوعات دهه ۱۸۰۰ پایبند است.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="نمونه خروجی TimeLockLLM"></p><p>خطاهای واقعیت‌پنداری زیادی وجود دارد. بسیاری (تقریباً ۱۰۰٪) از جزئیات (تاریخ‌ها، رویدادها، شخصیت‌های تاریخی) ساختگی هستند. همچنین جملات واقعاً با هم ارتباط ندارند، گاهی شاید دو جمله به هم مربوط باشند اما بیشتر از آن خیر. مشکل دیگر این است که گاهی پاورقی «Digitized by Google» به طور تصادفی ظاهر می‌شود، بنابراین دفعه بعدی که آموزش می‌دهم باید مطمئن شوم که متون به خوبی پاکسازی شده‌اند. به طور کلی از نتایج بسیار راضی‌ام، هنوز به یک LLM نزدیک نیست اما قطعاً یک جمله‌ساز است.</p><p>خیلی چیزها یاد می‌گیرم و در هفته‌های آینده شروع می‌کنم به فهمیدن اینکه چه کارهایی باید بهتر انجام دهم. به زودی فایل‌ها را بارگذاری می‌کنم!</p><h1>برنامه‌های آینده</h1></p><p>(انجام شد) قصد دارم روی نسخه ۰.۵ کار کنم، به جای آموزش با ۵۰ کتاب، ایده‌آل این است که با ۵۰۰-۶۰۰ کتاب آموزش دهم. در حال حاضر دارم nanoGPT را با کتاب‌هایی از ۱۸۰۰ تا ۱۸۵۰ و به طور خاص از لندن آموزش می‌دهم. چالش‌هایی مانند اطمینان از عدم به‌روزرسانی یا تفسیر مدرن کتاب‌هایی که پیدا می‌کنم وجود دارد، بلکه باید کتاب‌هایی باشند که در بازه زمانی انتخابی من منتشر و دست‌نخورده باشند.</p><p>می‌خواهم یک مدل جدید (v1) با مجموعه داده بسیار بزرگ‌تر، شاید ۵ تا ۱۰ برابر بزرگ‌تر از مدل v0.5 آموزش دهم. هدفم این است که ببینم آیا می‌توانم تنها از طریق آموزش زمانی انتخابی، قابلیت استدلال را به وجود بیاورم یا نه، این کار دشوارتر است و حتی مطمئن نیستم که به خاطر محدودیت‌های داده‌های تاریخی ممکن باشد. در هفته‌های آینده سعی خواهم کرد داده کافی برای یک مجموعه ۵ تا ۱۰ گیگابایتی گردآوری کنم. معتقدم اگر بتوانم داده‌های تمیز و باکیفیت پیدا کنم و یک GPU اجاره کنم، پیشرفت حاصل خواهد شد.</p><h1>نحوه استفاده از این پروژه</h1></p><p>این پروژه عمدتاً بر گردآوری داده‌های تاریخی، آماده‌سازی برای آموزش و ساخت یک توکنایزر تمرکز دارد. من فرآیند کامل آموزش LLM را پوشش نمی‌دهم، برای این منظور به nanoGPT از Andrej Karpathy مراجعه کنید.</p><h1>گام ۱: جمع‌آوری و آماده‌سازی متون تاریخی</h1></p><p>فایل‌های .txt از کتاب‌ها، اسناد و غیره در مالکیت عمومی را از بازه زمانی مورد نظر خود (مثلاً لندن ۱۸۰۰-۱۸۵۰) جمع‌آوری کنید.</p><p>در صورت نیاز می‌توانید از download_texts_improved.py برای دانلود کتاب‌ها استفاده کنید.</p><p>فایل‌های متنی را با اسکریپت یا به صورت دستی پاکسازی کنید و سربرگ/پاورقی پروژه گوتنبرگ، توضیحات مدرن یا اشتباهات OCR را حذف کنید.</p><p>prepare_dataset.py باید به خوبی کار کند.</p><h1>گام ۲: ساخت توکنایزر اختصاصی</h1></p><p>train_tokenizer.py یا train_tokenizer_hf.py را روی داده‌های پاکسازی شده اجرا کنید.
این کار vocab.json و merges.txt را به شما می‌دهد.</p><p>این فایل‌ها واژگان و قوانین ادغام را برای مدل شما تعریف می‌کنند.</p><h1>گام ۳: آموزش مدل خود (nanoGPT)</h1></p><p>برای فرآیند آموزش به <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT توسط Andrej Karpathy</a> مراجعه کنید.</p><p>اگر بخواهید می‌توانید یک LLM دیگر آموزش دهید، اما من از nanoGPT استفاده کردم.</p><h1>سوالات متداول</h1></p><h2>آموزش زمانی انتخابی چیست؟</h2></p><p>آموزش زمانی انتخابی (Selective Temporal Training یا STT) یک روش یادگیری ماشین است که در آن تمام داده‌های آموزشی به طور خاص از یک بازه زمانی تاریخی خاص گردآوری می‌شود. این کار به منظور مدل‌سازی زبان و دانش آن دوره بدون تأثیر مفاهیم مدرن انجام می‌شود. برای مثال، مدل فعلی من (v0.5) فقط با داده‌های ۱۸۰۰ تا ۱۸۷۵ آموزش دیده، به جای تنظیم دقیق، از ابتدا آموزش داده شده و خروجی منعکس‌کننده سبک زبانی و زمینه تاریخی همان دوره است.</p><h2>چرا فقط تنظیم دقیق یا LoRA را استفاده نمی‌کنید؟</h2></p><p>در این پروژه سعی دارم یک مدل زبانی بسازم که فاقد سوگیری مدرن باشد. اگر چیزی مثل GPT-2 را تنظیم دقیق کنم، قبلاً آموزش دیده و این اطلاعات پاک نمی‌شود. اگر مدل را از ابتدا آموزش دهم، مدل زبانی وانمود نمی‌کند قدیمی است، واقعاً خواهد بود. هدف فعلی این پروژه ساخت مدلی است که فقط با دانش کتاب‌های لندن منتشرشده بین ۱۸۰۰ تا ۱۸۵۰ استدلال کند.</p><h2>چه نوع داده‌ای برای آموزش استفاده کردید؟</h2></p><p>من از کتاب‌ها، اسناد حقوقی، روزنامه‌ها و سایر نوشته‌های لندن از ۱۸۰۰ تا ۱۸۵۰ استفاده کردم. لیستی که پیوست کرده‌ام حدود ۲۰۰ مورد دارد اما برای اولین آموزش فقط از ۵۰ فایل به حجم تقریبی ۱۸۷ مگابایت استفاده کردم. می‌توانید لیست اسناد را در اینجا مشاهده کنید:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>اندازه مدل نسخه ۰ چقدر است؟</h2></p><p>این مدل در حال حاضر بسیار کوچک است، فقط برای سرگرمی این کار را انجام می‌دهم و به قانون سختگیرانه آموزش بدون منابع مدرن پایبندم. تقریباً ۱۶ میلیون پارامتر دارد اما قصد دارم متون قدیمی بیشتری جمع‌آوری کنم تا آموزش مدل بعدی را شروع کنم. در طول مسیر به‌روزرسانی ارائه خواهم داد.</p><h2>مشخصات سخت‌افزاری آموزش؟</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
رم: ۱۶ گیگابایت DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>