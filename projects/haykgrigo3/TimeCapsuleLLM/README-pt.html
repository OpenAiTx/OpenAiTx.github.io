<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Um LLM treinado apenas com dados de determinados per&#237;odos para reduzir o vi&#233;s moderno</title>
    <meta name="description" content="Um LLM treinado apenas com dados de determinados per&#237;odos para reduzir o vi&#233;s moderno">
    <meta name="keywords" content="TimeCapsuleLLM, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Um LLM treinado apenas com dados de determinados períodos para reduzir o viés moderno",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">Portuguese</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Em breve">繁體中文 (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Em breve">हिन्दी (em breve)</a> |
        | <a href="#" title="Em breve">ไทย (em breve)</a> |
        | <a href="#" title="Em breve">Français (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Español (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">Русский (em breve)</a>
        | <a href="#" title="Em breve">Português (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">العربية (em breve)</a>
        | <a href="#" title="Em breve">فارسی (em breve)</a>
        | <a href="#" title="Em breve">Türkçe (em breve)</a>
        | <a href="#" title="Em breve">Tiếng Việt (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Um LLM treinado apenas com dados de determinados períodos históricos para reduzir o viés moderno.</p><p>Imagine se um modelo de IA não apenas fingisse ser histórico, mas realmente fosse.</p><p>Baseado no <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT de Andrej Karpathy</a> Os scripts principais de treinamento e a arquitetura do modelo são de autoria dele. </p><h1>Objetivos do Projeto</h1>
TimeCapsule LLM é um projeto experimental que será treinado apenas com textos escritos durante determinados períodos históricos. O objetivo é simular a visão de mundo e a linguagem de eras históricas específicas.</p><h1>Por que o fine tuning não é suficiente</h1></p><p>Se você apenas fizer o fine tuning de um modelo pré-treinado, seu LLM ainda vai conhecer conceitos modernos. Claro que alcançar zero viés moderno é difícil, mas quero chegar o mais próximo possível disso. Para eliminar o viés moderno, é necessário treinar um modelo do zero.</p><h1>Resultados esperados</h1></p><p>Espero que, quando finalizado, este modelo não conheça conceitos modernos e não consiga raciocinar além do que foi treinado. Ele não deve reconhecer vocabulário/conceitos modernos e não deve alucinar conhecimento moderno.</p><h1>Atualizações de progresso</h1></p><h2>9 de julho de 2025</h2></p><p>Defini meu período como 1800-1850 e região: Londres</p><p>Reuni uma lista de textos, livros, documentos</p><p>Até agora consegui 50 arquivos txt e começarei a treinar o NanoGPT em breve</p><p>Atualizarei isto enquanto houver progresso</p><h2>13 de julho de 2025</h2></p><p>Treinei o nanoGPT com 187MB de dados históricos em texto.</p><h2>15 de julho de 2025</h2></p><p>Comecei a baixar textos para a segunda rodada de treinamento. Estou pegando tudo do Internet Archive e ampliei o período para 1800-1875. Para obter uma gama diversificada de textos, você pode usar filtros de assunto e pesquisa por local de publicação, período e temas no Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtros de Pesquisa"></p><h2>16 de julho de 2025</h2></p><p>Baixei cerca de 500 arquivos txt do Internet Archive e após limpá-los (removendo espaços em branco, cabeçalhos do Gutenberg, etc) fiquei com cerca de 500MB de dados. É um conjunto de dados pequeno, mas da última vez treinei com 187MB então deve haver pelo menos alguma diferença notável na saída após treinar o segundo modelo. Espero que este modelo consiga produzir frases mais coerentes que façam algum sentido. Não é garantido, claro, pois ainda é um conjunto de dados minúsculo, mas é mais do que usei da última vez.</p><p>Deve ser possível treinar isso no meu próprio hardware, o que é bom porque espero ver algum tipo de melhoria antes de migrar para um conjunto maior de dados, o que exigiria alugar uma GPU. Mas não se preocupe, ainda pretendo alugar uma GPU em breve, mas antes disso quero garantir que meu conjunto de dados esteja o mais curado e limpo possível. Um dos problemas que tenho é a limpeza, muitos desses arquivos txt têm textos sem sentido misturados. Os scripts que usei para limpeza funcionam, mas não são 100% eficazes.</p><p>Vou treinar este conjunto de dados hoje e deve levar cerca de 4-5 horas. Quando terminar e eu testar, darei atualizações. Obrigado novamente a todos que estão acompanhando meu projeto, até recebi algumas pessoas me enviando links para recursos de OCR então obrigado! Espero que mais pessoas tentem isso e experimentem com seus próprios conjuntos de dados.</p><h2>12 de agosto de 2025</h2></p><p>A Versão 1 está completa, esta versão utiliza Phi 1.5 (700M parâmetros) e foi treinada em um conjunto de dados com quase 7000 textos. As principais melhorias deste modelo são sua capacidade de às vezes referenciar nomes reais e eventos históricos. As Versões 0 e 0.5 não conseguiam fazer isso; sempre que eu pedia algo como "No ano de 1821", a saída era apenas alucinações, coisas que nunca aconteceram. Um exemplo disso com a Versão 1:</p><p>Meu prompt: Era o ano de nosso Senhor de 1834,</p><p>A saída: "Era o ano de nosso Senhor de 1834 e as ruas de Londres estavam cheias de protestos e petições. A causa, como muitos relataram, não estava vinculada ao privado, mas tendo ocupado o mesmo dia no dia de Lord Palmerston, o público receberá uma breve declaração das dificuldades sob as quais o dia da lei nos alcançou. É motivo de profundo pesar que os atuais eventos na história do mundo sejam claros e, consequentemente, serão conhecidos. Não é verdade que os mesmos homens que primeiro se estabeleceram no Evangelho em Jerusalém tenham um registro tão extenso e interessante da prosperidade e prosperidade"</p><p>A princípio pensei que fosse coincidência, mas veja isso: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>Atualização do Treinamento</h3></p><p>Comecei a treinar em um corpus de 435MB (108 M tokens), está indo bem até agora. A perda de treinamento caiu de 10,9 para 4,9 nas primeiras 2800 iterações. Espero que leve cerca de 8 ou 9 horas para completar. Vou postar outra atualização quando terminar.</p><h2>17 de julho de 2025</h2></p><p>O treinamento está concluído para o segundo modelo, minha 4060 levou cerca de 8 horas e 40 minutos (3.900 iterações/h) para 33.000 iterações (5 épocas). A perda final de treinamento foi 3,73. As saídas foram surpreendentemente boas, realmente gera frases coerentes no estilo do século XIX agora.</p><h2>28 de julho de 2025</h2></p><p>Fui em frente e enviei a v0.5 para o Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Confira aqui</a> se quiser. Agora você pode baixar meu repositório e rodar localmente. Infelizmente o nanoGPT não funciona nativamente com o HuggingFace, então você terá que baixar e rodar o modelo localmente.</p><p>Também vou começar a curar dados para minha próxima rodada de treinamento, acredito que vou precisar de talvez 5-10x mais dados para atingir capacidade de raciocínio.</p><h2>2 de agosto de 2025</h2></p><p>Vou começar a trabalhar na Versão 1 em breve. Será necessário migrar da arquitetura do nanoGPT para algo mais moderno. Tenho várias arquiteturas LLM de código aberto em mente, incluindo: OpenLLaMA v3, Phi-2 e Qwen 1.5B. E para suportar o salto para a V1, precisarei curar cuidadosamente um conjunto de dados muito maior e diverso. Vou precisar de pelo menos 5GB de dados limpos de treinamento.</p><h1>Comportamento e Limitações do Modelo V0</h1></p><p>Prompts iniciais mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, pedi "Quem é Henry?" e respondeu "Eu conheço esse homem, eu não fiz um preto, a tempestade." e sim, essa frase não faz sentido, mas o LLM reconhece que estou perguntando sobre uma pessoa.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Saída de Amostra do TimeLockLLM"></p><p>Não há menção a conceitos modernos, as saídas contêm principalmente palavras e frases dos anos 1800.</p><p>Ainda precisa de muito trabalho, treinar com 187MB não vai te dar um modelo que produza texto com raciocínio complexo.</p><p>Atualmente, ele produz sentenças que não possuem estrutura completa e, no geral, não fazem sentido, mas isso é normal para o tamanho do treinamento.</p><h1>Comportamento & Limitações do Modelo V0.5</h1></p><p>Esta é uma boa melhoria comparada ao último modelo. O estilo de escrita e o vocabulário são vitorianos e quase todas as frases estão gramaticalmente corretas, com pontuação adequada. E novamente, este foi treinado do zero, então mantém os temas dos anos 1800.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>Há muitas alucinações factuais. Muitas (como 100%) das informações (datas, eventos, figuras históricas) são inventadas. Além disso, as frases não têm realmente conexão umas com as outras, às vezes talvez 2 frases se relacionem, mas além disso não. Outro problema é que às vezes aparece um rodapé “Digitized by Google” perdido, então da próxima vez que treinar realmente preciso garantir que os textos estejam bem limpos. No geral estou muito satisfeito com os resultados, ainda está longe de ser um LLM, mas definitivamente é um gerador de sentenças.</p><p>Estou aprendendo muito e começarei a descobrir o que preciso fazer melhor nas próximas semanas. Em breve vou fazer upload dos arquivos!</p><h1>Comportamento & Limitações do Modelo V1</h1></p><p>Em breve vou fazer upload de alguns exemplos de saída e também realizar comparações entre os 3 modelos com o mesmo prompt. Também vou fazer upload do V1 para o Huggingface, como fiz com minha última versão; você pode encontrar minha conta Huggingface aqui: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>Planos Futuros</h1></p><p>(Concluído) Vou começar a trabalhar na versão 0.5, em vez de treinar usando 50 livros, vou treinar idealmente usando 500-600. Agora estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios como garantir que os livros que encontro não foram atualizados ou possuem interpretações modernas, mas sim livros intactos publicados dentro do período escolhido.</p><p>Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o que usei para o v0.5. Meu objetivo é ver se consigo fazer habilidades de raciocínio emergirem apenas do Treinamento Temporal Seletivo, esta será uma tarefa mais difícil e nem tenho certeza se é possível devido às limitações dos dados históricos. Nas próximas semanas tentarei curar dados suficientes para um corpus de 5-10GB. Acredito que se conseguir dados limpos e de alta qualidade e alugar uma GPU, haverá progresso.</p><h1>Como Usar Este Projeto</h1></p><p>Este projeto foca principalmente em curar dados históricos, prepará-los para treinamento e construir um tokenizador. Não vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.</p><h1>Passo 1: Coletar e Preparar Textos Históricos</h1></p><p>Colete arquivos .txt de livros de domínio público, documentos, etc, do período escolhido (ex: Londres 1800-1850)</p><p>Você pode usar o download_texts_improved.py para baixar livros caso precise.</p><p>Limpe os arquivos de texto usando um script ou remova manualmente cabeçalhos/rodapés do Project Gutenberg, anotações modernas ou coisas como erros de OCR.</p><p>prepare_dataset.py deve funcionar bem.</p><h1>Passo 2: Construir um Tokenizador Personalizado</h1></p><p>Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso irá gerar vocab.json e merges.txt
Esses arquivos definem o vocabulário e as regras de mesclagem para o seu modelo</p><h1>Passo 3: Treine Seu Modelo (nanoGPT)</h1></p><p>Consulte <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT por Andrej Karpathy</a> para o processo de treinamento.</p><p>Você pode treinar um LLM diferente se quiser, mas eu usei o nanoGPT</p><h1>FAQ</h1></p><h2>O que é Treinamento Temporal Seletivo?</h2></p><p>O Treinamento Temporal Seletivo (STT) é uma metodologia de aprendizado de máquina onde todos os dados de treinamento são especificamente selecionados para se enquadrar em um determinado período histórico. Isso é feito para modelar a linguagem e o conhecimento daquela era sem influência de conceitos modernos. Por exemplo, o modelo que tenho agora (v0.5) foi treinado exclusivamente com dados de 1800–1875, não foi apenas ajustado, mas treinado do zero, resultando em uma saída que reflete o estilo linguístico e o contexto histórico daquele período.</p><h2>Por que não usar apenas fine-tuning ou LoRA?</h2></p><p>Para este projeto estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu fizer fine-tuning em algo como GPT-2, ele já foi pré-treinado e essa informação não será eliminada. Se eu treinar do zero, o modelo de linguagem não vai fingir ser antigo, ele realmente será. O objetivo deste projeto agora é criar algo que possa raciocinar exclusivamente usando o conhecimento de livros publicados em Londres entre 1800 e 1850.</p><h2>Que tipo de dados você usou para o treinamento?</h2></p><p>Estou usando livros, documentos legais, jornais e outros escritos de Londres entre 1800–1850. A lista que eu linkei tem cerca de 200, mas para o primeiro treinamento eu usei apenas 50 arquivos, cerca de ~187 MB. Você pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Qual o tamanho dos modelos?</h2></p><p>V0: 16M Parâmetros</p><p>V0.5: 123M Parâmetros</p><p>V1: 700M Parâmetros</p><h1>Especificações de Treinamento?</h1></p><p>#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: 16GB DDR5.</p><p>#V1</p><p>GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>