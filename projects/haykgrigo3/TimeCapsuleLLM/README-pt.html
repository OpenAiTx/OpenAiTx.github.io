<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Um LLM treinado apenas com dados de determinados per&#237;odos para reduzir o vi&#233;s moderno</title>
    <meta name="description" content="Um LLM treinado apenas com dados de determinados per&#237;odos para reduzir o vi&#233;s moderno">
    <meta name="keywords" content="TimeCapsuleLLM, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Um LLM treinado apenas com dados de determinados períodos para reduzir o viés moderno",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Portuguese</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Em breve">繁體中文 (em breve)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Em breve">हिन्दी (em breve)</a> |
        | <a href="#" title="Em breve">ไทย (em breve)</a> |
        | <a href="#" title="Em breve">Français (em breve)</a>
        | <a href="#" title="Em breve">Deutsch (em breve)</a>
        | <a href="#" title="Em breve">Español (em breve)</a>
        | <a href="#" title="Em breve">Italiano (em breve)</a>
        | <a href="#" title="Em breve">Русский (em breve)</a>
        | <a href="#" title="Em breve">Português (em breve)</a>
        | <a href="#" title="Em breve">Nederlands (em breve)</a>
        | <a href="#" title="Em breve">Polski (em breve)</a>
        | <a href="#" title="Em breve">العربية (em breve)</a>
        | <a href="#" title="Em breve">فارسی (em breve)</a>
        | <a href="#" title="Em breve">Türkçe (em breve)</a>
        | <a href="#" title="Em breve">Tiếng Việt (em breve)</a>
        | <a href="#" title="Em breve">Bahasa Indonesia (em breve)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Um LLM treinado apenas com dados de determinados períodos para reduzir o viés moderno.</p><p>Imagine se um modelo de IA não apenas fingisse ser histórico, mas realmente fosse.</p><p>Baseado no <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT de Andrej Karpathy</a>. Os scripts principais de treinamento e arquitetura do modelo são de sua autoria. </p><h1>Objetivos do Projeto </h1></p><p>TimeCapsule LLM é um projeto experimental que será treinado apenas com textos escritos durante determinados períodos históricos. O objetivo é simular a visão de mundo e a linguagem de eras históricas específicas.</p><h1>Por que o fine tuning não é suficiente </h1></p><p>Se você apenas fizer fine tuning em um modelo pré-treinado, seu LLM ainda conhecerá conceitos modernos. Claro que atingir zero viés moderno é difícil, mas quero chegar o mais próximo possível disso. Obter ausência de viés moderno requer treinar um modelo do zero.</p><h1>Resultados esperados </h1></p><p>Espero que, quando finalizado, este modelo não conheça conceitos modernos e não consiga raciocinar além do que foi treinado. Ele não deve reconhecer conceitos/vocabulário modernos e espero que não alucine conhecimentos atuais.</p><h1>Atualizações de Progresso</h1></p><h2>9 de julho de 2025</h2></p><p>Defini meu período histórico para 1800-1850 e região: Londres </p><p>Reuni uma lista de textos, livros, documentos </p><p>Até agora consegui 50 arquivos txt e em breve iniciarei o treinamento do NanoGPT </p><p>Atualizarei isso sempre que houver progresso</p><h2>13 de julho de 2025</h2></p><p>Treinei o nanoGPT com 187MB de dados textuais históricos. </p><h2>15 de julho de 2025</h2></p><p>Comecei a baixar textos para a segunda rodada de treinamento. Estou buscando tudo no Internet Archive e ampliei o período para 1800-1875. Para obter uma variedade maior de textos, você pode usar filtros de assunto e pesquisa por local de publicação, período e temas no Internet Archive. </p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtros de Pesquisa"></p><h2>16 de julho de 2025</h2></p><p>Baixei cerca de 500 arquivos txt do Internet Archive e após limpá-los (apenas removendo espaços em branco, cabeçalhos do Gutenberg, etc.) tenho cerca de 500MB de dados. É um conjunto de dados pequeno, mas da última vez treinei com 187MB, então deve haver pelo menos alguma diferença perceptível no resultado após treinar o segundo modelo. Espero que este modelo consiga pelo menos produzir frases mais coerentes que façam algum sentido. Não é garantia, claro, pois ainda é um conjunto de dados muito pequeno, mas é mais do que usei da última vez. </p><p>Isso deve ser viável no meu próprio hardware, e é bom pois espero ver algum tipo de melhoria antes de partir para um conjunto de dados maior, o que exigiria alugar uma GPU. Mas não se preocupe, ainda pretendo alugar uma GPU em breve, mas antes quero garantir que meu conjunto de dados esteja o mais curado e limpo possível. Um dos problemas que tenho é a limpeza, muitos desses arquivos txt têm informações sem sentido misturadas. Os scripts que usei para limpeza funcionam, mas não são 100% eficazes. </p><p>Vou treinar esse conjunto de dados hoje e deve levar cerca de 4-5 horas. Assim que terminar e eu testar, darei atualizações. Obrigado novamente a todos que estão acompanhando meu projeto, inclusive já recebi links de recursos de OCR, então obrigado! Espero que mais pessoas tentem isso e experimentem com seus próprios conjuntos de dados. </p><h2>28 de julho de 2025 </h2></p><p>Já subi a versão v0.5 no Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Confira aqui</a> se quiser. Agora você pode baixar meu repositório e rodar localmente. Infelizmente, o nanoGPT não funciona nativamente com o HuggingFace, então você precisará baixar e rodar o modelo localmente. </p><p>Também vou começar a curar dados para minha próxima rodada de treinamento, acredito que precisarei de 5 a 10 vezes mais dados para atingir capacidade de raciocínio. </p><h3>Atualização de Treinamento </h3></p><p>Comecei o treinamento com um corpus de 435MB (108 milhões de tokens), está indo bem até agora. O train loss caiu de 10.9 para 4.9 nas primeiras 2800 iterações. Espero que leve cerca de 8 ou 9 horas para completar. Postarei outra atualização quando terminar.</p><h2>17 de julho de 2025 2:13AM</h2></p><p>O treinamento está concluído para o segundo modelo, levou cerca de 8 horas e 40 minutos no meu 4060 (3.900 iters/hora) para 33.000 iterações (5 épocas). O train loss final foi 3,73. Os resultados foram surpreendentemente bons, agora ele gera frases coerentes no estilo do século XIX. </p><h1>Comportamento do Modelo V0 & Limitações </h1></p><p>Os primeiros prompts mostram o modelo respondendo com linguagem e comportamento dos anos 1800. Por exemplo, solicitei "Who art Henry?" e ele respondeu "I know that man, I have did not a black, the storm." e sim, essa frase não faz sentido, mas o LLM reconheceu que estou perguntando sobre uma pessoa. </p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Saída de Exemplo do TimeLockLLM"></p><p>Não há menção de conceitos modernos, as saídas contêm principalmente palavras e frases do século XIX.</p><p>Ainda precisa de muito trabalho, treinar com 187MB não lhe dará um modelo que produza textos com raciocínio complexo.</p><p>Neste momento, ele produz frases que não possuem estrutura completa e, no geral, simplesmente não fazem sentido, mas isso é normal para o tamanho do treinamento.</p><h1>Comportamento do Modelo V0.5 & Limitações</h1></p><p>Esta é uma boa melhoria em comparação com o modelo anterior. O estilo de escrita e o vocabulário são vitorianos e quase todas as frases estão gramaticalmente corretas, com pontuação adequada. E novamente, foi treinado do zero, então se mantém em assuntos do século XIX.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Saída de Exemplo TimeLockLLM"></p><p>Há muitas alucinações factuais. Muitas (tipo 100%) das informações (datas, eventos, figuras históricas) são inventadas. Além disso, as frases realmente não têm conexão umas com as outras, às vezes talvez 2 frases se relacionem, mas fora isso não se conectam. Outro problema é que às vezes aparece um rodapé “Digitized by Google”, então na próxima vez que treinar preciso garantir que os textos estejam bem limpos. No geral, estou muito satisfeito com os resultados, ainda está longe de ser um LLM, mas definitivamente é um gerador de frases.</p><p>Estou aprendendo muito e começarei a descobrir o que preciso melhorar nas próximas semanas. Em breve vou fazer upload dos arquivos!</p><h1>Planos Futuros</h1></p><p>(Concluído) Vou começar a trabalhar na versão 0.5, ao invés de treinar usando 50 livros, vou treinar usando idealmente 500-600. Neste momento estou treinando o nanoGPT usando livros de 1800-1850 e especificamente de Londres. Existem alguns desafios como garantir que os livros encontrados não foram atualizados ou possuem interpretações modernas, mas livros intocados publicados dentro do período escolhido.</p><p>Quero treinar um novo modelo (v1) com um corpus muito maior, talvez 5-10x maior que o usado no v0.5. Meu objetivo é ver se consigo fazer habilidades de raciocínio emergirem apenas do Treinamento Temporal Seletivo, será uma tarefa mais difícil e nem tenho certeza se é possível devido às limitações dos dados históricos. Nas próximas semanas tentarei selecionar dados suficientes para um corpus de 5-10GB. Acredito que se eu conseguir dados limpos e de alta qualidade e alugar uma GPU, haverá progresso.</p><h1>Como Usar Este Projeto</h1></p><p>Este projeto foca principalmente na curadoria de dados históricos, preparação para treinamento e construção de um tokenizador. Não vou cobrir o processo completo de treinamento de LLM, para isso consulte o nanoGPT de Andrej Karpathy.</p><h1>Passo 1: Coletar e Preparar Textos Históricos</h1></p><p>Colete arquivos .txt de livros de domínio público, documentos, etc. do período escolhido (por exemplo, Londres 1800-1850)</p><p>Você pode usar o download_texts_improved.py para baixar os livros, se precisar.</p><p>Limpe os arquivos de texto usando um script ou remova manualmente cabeçalhos/rodapés do Project Gutenberg, anotações modernas ou coisas como erros de OCR.</p><p>prepare_dataset.py deve funcionar bem.</p><h1>Passo 2: Construir um Tokenizador Personalizado</h1></p><p>Execute train_tokenizer.py ou train_tokenizer_hf.py nos dados limpos.
Isso irá gerar vocab.json e merges.txt</p><p>Esses arquivos definem o vocabulário e as regras de mesclagem para o seu modelo</p><h1>Passo 3: Treinar Seu Modelo (nanoGPT)</h1></p><p>Consulte o <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT de Andrej Karpathy</a> para o processo de treinamento.</p><p>Você pode treinar outro LLM se quiser, mas eu usei o nanoGPT</p><h1>FAQ</h1></p><h2>O que é Treinamento Temporal Seletivo?</h2></p><p>Treinamento Temporal Seletivo (STT) é uma metodologia de machine learning onde todos os dados de treinamento são cuidadosamente selecionados para pertencer a um período histórico específico. Isso é feito para modelar a linguagem e o conhecimento daquela época sem influência de conceitos modernos. Por exemplo, o modelo atual (v0.5) é treinado exclusivamente com dados de 1800-1875, não é fine tuned, mas treinado do zero, resultando em uma saída que reflete o estilo linguístico e o contexto histórico daquele período.</p><h2>Por que não usar apenas fine-tuning ou LoRA?</h2></p><p>Para este projeto, estou tentando criar um modelo de linguagem livre de vieses modernos. Se eu fizer fine-tuning em algo como o GPT-2, ele já está pré-treinado e essa informação não desaparecerá. Se eu treinar do zero, o modelo de linguagem não vai fingir ser antigo, ele será de fato. O objetivo deste projeto agora é criar algo que possa raciocinar exclusivamente com o conhecimento de livros de Londres publicados entre 1800 e 1850.</p><h2>Que tipo de dados você usou para o treinamento?</h2></p><p>Estou usando livros, documentos legais, jornais e outros escritos de Londres de 1800–1850. A lista que linkei tem cerca de 200, mas para o primeiro treinamento usei apenas 50 arquivos, cerca de ~187 MB. Você pode ver uma lista dos documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Qual o tamanho do modelo da Versão 0?</h2></p><p>Este modelo é bem pequeno no momento, estou fazendo por diversão e seguindo uma regra rígida de não usar fontes modernas. Ele tem quase 16 milhões de parâmetros, mas vou começar a reunir mais textos antigos para começar outro treinamento. Vou atualizando conforme avançar.</p><h2>Especificações de Treinamento?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>