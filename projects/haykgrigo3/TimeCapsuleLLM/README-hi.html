<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - आधुनिक पक्षपात को कम करने के लिए केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM</title>
    <meta name="description" content="आधुनिक पक्षपात को कम करने के लिए केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM">
    <meta name="keywords" content="TimeCapsuleLLM, Hindi, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "आधुनिक पक्षपात को कम करने के लिए केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-hi.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">Hindi</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 भाषा</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>टाइमकैप्सूल एलएलएम</h1>
एक एलएलएम जिसे केवल कुछ निश्चित समय अवधि के डेटा पर प्रशिक्षित किया गया है ताकि आधुनिक पूर्वाग्रह को कम किया जा सके।</p><p>कल्पना कीजिए अगर एक एआई मॉडल केवल ऐतिहासिक होने का दिखावा नहीं करता बल्कि वास्तव में ऐतिहासिक होता।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy द्वारा nanoGPT</a> पर आधारित। मुख्य प्रशिक्षण स्क्रिप्ट्स और मॉडल आर्किटेक्चर उन्हीं का कार्य है।</p><h1>परियोजना के उद्देश्य</h1></p><p>टाइमकैप्सूल एलएलएम एक प्रायोगिक परियोजना है जिसे केवल कुछ निश्चित ऐतिहासिक समय में लिखे गए ग्रंथों पर प्रशिक्षित किया जाएगा। इसका उद्देश्य विशिष्ट ऐतिहासिक युगों की विश्वदृष्टि और भाषा का अनुकरण करना है।</p><h1>क्यों फाइन-ट्यूनिंग पर्याप्त नहीं है</h1></p><p>यदि आप केवल एक पूर्व-प्रशिक्षित मॉडल को फाइन-ट्यून करते हैं, तब भी आपका एलएलएम आधुनिक अवधारणाओं को जानता रहेगा। निश्चित रूप से शून्य आधुनिक पूर्वाग्रह प्राप्त करना कठिन है, लेकिन मैं इसके जितना निकट जा सकता हूं जाना चाहता हूं। बिल्कुल भी आधुनिक पूर्वाग्रह न होना तभी संभव है जब मॉडल को शुरू से प्रशिक्षित किया जाए।</p><h1>अपेक्षित परिणाम</h1></p><p>आशा है कि जब यह मॉडल पूरा होगा, तब यह आधुनिक अवधारणाओं को नहीं जानेगा और जिस पर इसे प्रशिक्षित किया गया है, उसके आगे तर्क नहीं कर पाएगा। इसे आधुनिक शब्दावली/अवधारणाओं को नहीं पहचानना चाहिए और मुझे उम्मीद है कि यह आधुनिक ज्ञान की कल्पना नहीं करेगा।</p><h1>प्रगति अद्यतन</h1></p><h2>9 जुलाई, 2025</h2></p><p>मैंने अपनी समयावधि 1800-1850 और क्षेत्र: लंदन निर्धारित कर दी है</p><p>मैंने ग्रंथों, पुस्तकों, दस्तावेजों की एक सूची एकत्रित की है</p><p>अब तक मैंने 50 टेक्स्ट फाइलें प्राप्त कर ली हैं और जल्द ही NanoGPT पर प्रशिक्षण शुरू करूंगा</p><p>जब तक प्रगति होती रहेगी, मैं इसे अपडेट करता रहूंगा</p><h2>13 जुलाई, 2025</h2></p><p>nanoGPT को 187MB ऐतिहासिक टेक्स्ट डेटा से प्रशिक्षित किया।</p><h2>15 जुलाई, 2025</h2></p><p>मैंने दूसरे प्रशिक्षण रन के लिए ग्रंथ डाउनलोड करने शुरू कर दिए हैं। मैं इंटरनेट आर्काइव से सब कुछ ले रहा हूं और समयावधि को 1800-1875 तक बढ़ा दिया है। विविध प्रकार के ग्रंथ प्राप्त करने के लिए आप इंटरनेट आर्काइव पर विषय और खोज फिल्टर का उपयोग कर सकते हैं, जैसे प्रकाशन स्थान, समयावधि और विषय।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="खोज फ़िल्टर"></p><h2>16 जुलाई, 2025</h2></p><p>मैंने इंटरनेट आर्काइव से लगभग 500 टेक्स्ट फाइलें डाउनलोड कीं और उन्हें साफ करने (सिर्फ व्हाइटस्पेसेस, गुटेनबर्ग हेडर आदि हटाने) के बाद मेरे पास लगभग 500MB डेटा है। यह एक बहुत छोटा डेटासेट है लेकिन पिछली बार मैंने 187MB से प्रशिक्षण लिया था, तो इस बार मॉडल के आउटपुट में कम से कम कुछ स्पष्ट अंतर दिखना चाहिए। मुझे उम्मीद है कि यह मॉडल कम से कम अधिक सुसंगत वाक्य बना सकेगा जो किसी हद तक समझ में आएं। यह निश्चित नहीं है क्योंकि यह अब भी बहुत ही छोटा डेटासेट है, लेकिन पिछली बार से ज्यादा है।</p><p>यह मेरे अपने हार्डवेयर पर संभव है, यह अच्छा है क्योंकि मैं शायद बड़े डेटासेट पर जाने से पहले कुछ सुधार देख सकूं, जिसके लिए मुझे GPU किराए पर लेना पड़ेगा। लेकिन चिंता न करें मैं जल्द ही GPU किराए पर लेने की योजना बना रहा हूं, पर उससे पहले मैं चाहता हूं कि मेरा डेटासेट जितना संभव हो सके उतना क्यूरेटेड और साफ हो। मेरी एक समस्या है सफाई, इन टेक्स्ट फाइलों में काफी बकवास मिली-जुली होती है। सफाई के लिए जो स्क्रिप्ट्स इस्तेमाल की हैं वे काम करती हैं लेकिन 100% प्रभावी नहीं हैं।</p><p>मैं आज इस डेटासेट को प्रशिक्षित करूंगा और इसमें लगभग 4-5 घंटे लगने चाहिए। जब यह पूरा हो जाएगा और मैं इसका परीक्षण करूंगा, मैं अपडेट दूंगा। एक बार फिर धन्यवाद सभी को जो मेरी परियोजना देख रहे हैं, कुछ लोगों ने मुझे तो OCR संसाधनों के लिंक भी दिए हैं, इसलिए धन्यवाद! मुझे उम्मीद है और लोग भी इसे आजमाएं और अपने खुद के डेटासेट्स के साथ प्रयोग करें।</p><h3>प्रशिक्षण अद्यतन</h3></p><p>मैंने 435MB (108 मिलियन टोकन) के कॉर्पस पर प्रशिक्षण शुरू किया, अभी तक यह काफी स्मूद चल रहा है। ट्रेन लॉस 10.9 से गिरकर पहले 2800 इटरेशनों में 4.9 हो गया। मुझे उम्मीद है कि पूरा होने में लगभग 8 या 9 घंटे लगेंगे। पूरा होने पर एक और अपडेट पोस्ट करूंगा।</p><h2>17 जुलाई, 2025 2:13AM</h2></p><p>दूसरे मॉडल का प्रशिक्षण पूरा हो गया है, मेरे 4060 GPU को इसमें लगभग 8 घंटे 40 मिनट (3,900 इटर्स/घंटा) लगे 33,000 इटरेशनों (5 इपॉक्स) के लिए। अंतिम ट्रेन लॉस 3.73 था। आउटपुट्स आश्चर्यजनक रूप से अच्छे थे, अब यह वास्तव में 19वीं सदी के अंदाज में सुसंगत वाक्य बना रहा है।</p><h2>28 जुलाई, 2025</h2></p><p>मैंने v0.5 को Hugging Face पर अपलोड कर दिया है, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">यहां देखें</a> अगर आप चाहें। अब आप मेरी रेपो डाउनलोड कर सकते हैं और लोकली चला सकते हैं। दुर्भाग्यवश nanoGPT हगिंगफेस के साथ नेटिवली काम नहीं करता, तो आपको मॉडल डाउनलोड करके लोकली चलाना होगा।</p><p>साथ ही मैं अपने अगले प्रशिक्षण रन के लिए डेटा क्यूरेट करना शुरू करूंगा, मुझे लगता है कि तर्क क्षमताएं पाने के लिए शायद 5-10 गुना अधिक डेटा चाहिए।</p><h1>V0 मॉडल व्यवहार और सीमाएँ</h1></p><p>प्रारंभिक प्रॉम्प्ट्स दिखाते हैं कि मॉडल 1800 के दशक की भाषा और व्यवहार में उत्तर दे रहा है। उदाहरण के लिए, मैंने इसे पूछा "Who art Henry?" और इसने जवाब दिया "I know that man, I have did not a black, the storm." और हां, उस वाक्य का कोई अर्थ नहीं बनता लेकिन एलएलएम समझ रहा है कि मैं किसी व्यक्ति के बारे में पूछ रहा हूं।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM नमूना आउटपुट"></p><p>यहाँ आधुनिक अवधारणाओं का कोई उल्लेख नहीं है, आउटपुट में अधिकांश शब्द और वाक्य संरचना 1800 के दशक की है।</p><p>इसे अभी भी बहुत काम की जरूरत है, 187MB से प्रशिक्षण करने पर आपको ऐसा मॉडल नहीं मिलेगा जो जटिल तर्क के साथ पाठ उत्पन्न करे।</p><p>अभी यह ऐसे वाक्य बनाता है जिनमें पूर्ण वाक्य संरचना की कमी है और कुल मिलाकर कोई मतलब नहीं बनता, लेकिन यह प्रशिक्षण आकार के लिए सामान्य है।</p><h1>V0.5 मॉडल व्यवहार और सीमाएँ</h1></p><p>यह पिछले मॉडल की तुलना में एक अच्छा सुधार है। लेखन शैली और शब्दावली विक्टोरियन है और लगभग हर वाक्य व्याकरणिक रूप से सही है और उचित विराम चिह्नों के साथ है। और फिर से, यह शुरू से प्रशिक्षित किया गया है इसलिए यह 1800 के दशक के विषयों तक ही सीमित रहता है।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM नमूना आउटपुट"></p><p>यहाँ बहुत सारे तथ्यात्मक भ्रम हैं। बहुत सारे (लगभग 100%) विवरण (तिथियाँ, घटनाएँ, ऐतिहासिक व्यक्ति) मनगढ़ंत हैं। साथ ही वाक्य एक-दूसरे से सच में नहीं जुड़ते, कभी-कभी शायद 2 वाक्य आपस में संबंधित हों लेकिन उससे अधिक नहीं। एक और समस्या यह है कि कभी-कभी “Digitized by Google” फुटर आ जाता है, इसलिए अगली बार प्रशिक्षण में मुझे यह सुनिश्चित करना होगा कि पाठ्य सामग्री अच्छी तरह से साफ की जाए। कुल मिलाकर मैं परिणामों से बहुत खुश हूँ, यह अभी LLM नहीं है लेकिन निश्चित रूप से वाक्य उत्पन्न करने वाला है।</p><p>मैं बहुत कुछ सीख रहा हूँ और अगले कुछ हफ्तों में समझने की कोशिश करूँगा कि क्या बेहतर करना है। मैं जल्द ही फाइलें अपलोड करूँगा!</p><h1>आगामी योजनाएँ</h1></p><p>(पूरा हो गया) मैं संस्करण 0.5 पर काम शुरू करूँगा, 50 पुस्तकों का उपयोग करने के बजाय, मैं आदर्श रूप से 500-600 का उपयोग करूँगा। अभी मैं 1800-1850 की पुस्तकों का उपयोग करके nanoGPT को प्रशिक्षित कर रहा हूँ, और विशेष रूप से लंदन से। कुछ चुनौतियाँ हैं, जैसे यह सुनिश्चित करना कि जो पुस्तकें मिल रही हैं वे अद्यतन नहीं हैं या उनमें आधुनिक व्याख्याएँ नहीं हैं, बल्कि मेरी चुनी गई समयावधि में प्रकाशित मूल पुस्तकें हैं।</p><p>मैं एक नया मॉडल (v1) एक बहुत बड़े कॉर्पस के साथ प्रशिक्षित करना चाहता हूँ, शायद v0.5 के मुकाबले 5-10 गुना बड़ा। मेरा लक्ष्य है देखना कि क्या केवल Selective Temporal Training से तर्क क्षमताएँ उभर सकती हैं, यह अधिक कठिन कार्य होगा और मैं पूरी तरह से निश्चित नहीं हूँ कि यह संभव है क्योंकि ऐतिहासिक डेटा की सीमाएँ हैं। आने वाले हफ्तों में मैं 5-10GB डेटा संग्रह करने की कोशिश करूँगा। मुझे विश्वास है कि यदि मुझे साफ और उच्च गुणवत्ता वाला डेटा मिल गया और एक GPU किराए पर ले लिया, तो प्रगति होगी।</p><h1>इस परियोजना का उपयोग कैसे करें</h1></p><p>यह परियोजना मुख्य रूप से ऐतिहासिक डेटा संग्रह, प्रशिक्षण के लिए उसकी तैयारी और एक टोकनाइज़र बनाने पर केंद्रित है। मैं पूरी LLM प्रशिक्षण प्रक्रिया को कवर नहीं कर रहा, उसके लिए Andrej Karpathy की nanoGPT देखें।</p><h1>चरण 1: ऐतिहासिक ग्रंथ एकत्र करें और तैयार करें</h1></p><p>अपने चुने हुए समय काल (जैसे, लंदन 1800-1850) से सार्वजनिक डोमेन की पुस्तकों, दस्तावेज़ों आदि की .txt फाइलें इकट्ठा करें।</p><p>यदि आपको आवश्यकता हो तो download_texts_improved.py का उपयोग करके पुस्तकें डाउनलोड कर सकते हैं।</p><p>पाठ्य फाइलों को स्क्रिप्ट से या मैन्युअली साफ करें, जैसे Project Gutenberg के हेडर/फुटर, आधुनिक एनोटेशन या OCR त्रुटियाँ हटाएँ।</p><p>prepare_dataset.py ठीक से काम करेगा।</p><h1>चरण 2: एक कस्टम टोकनाइज़र बनाएं</h1></p><p>साफ किए गए डेटा पर train_tokenizer.py या train_tokenizer_hf.py चलाएँ।
यह आपको vocab.json और merges.txt देगा।</p><p>ये फाइलें आपके मॉडल के लिए शब्दावली और मर्ज नियम परिभाषित करती हैं।</p><h1>चरण 3: अपने मॉडल को प्रशिक्षित करें (nanoGPT)</h1></p><p>प्रशिक्षण प्रक्रिया के लिए <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy का nanoGPT</a> देखें।</p><p>यदि चाहें तो आप कोई अन्य LLM प्रशिक्षित कर सकते हैं, लेकिन मैंने nanoGPT का उपयोग किया।</p><h1>अक्सर पूछे जाने वाले प्रश्न (FAQ)</h1></p><h2>Selective Temporal Training क्या है?</h2></p><p>Selective Temporal Training (STT) एक मशीन लर्निंग पद्धति है जिसमें सभी प्रशिक्षण डेटा को विशेष रूप से एक विशिष्ट ऐतिहासिक कालखंड के भीतर चुना जाता है। इसका उद्देश्य उस युग की भाषा और ज्ञान का मॉडल तैयार करना है, आधुनिक अवधारणाओं के प्रभाव के बिना। उदाहरण के लिए, मेरे पास जो वर्तमान मॉडल है (v0.5) वह विशेष रूप से 1800-1875 के डेटा पर प्रशिक्षित है, यह फाइन ट्यून नहीं किया गया बल्कि शुरू से प्रशिक्षित किया गया है, जिससे आउटपुट उस समय की भाषाई शैली और ऐतिहासिक संदर्भ को दर्शाता है।</p><h2>बस फाइन-ट्यूनिंग या LoRA क्यों नहीं?</h2></p><p>इस परियोजना के लिए मैं एक ऐसा भाषा मॉडल बनाना चाहता हूँ जो आधुनिक पक्षपात से मुक्त हो। यदि मैं GPT-2 जैसी किसी चीज को फाइन-ट्यून करता हूँ, तो वह पहले से प्रशिक्षित है और वह जानकारी हटाई नहीं जा सकती। यदि मैं शुरू से प्रशिक्षण करूँ तो भाषा मॉडल पुराने जैसा बनने का नाटक नहीं करेगा, वह सचमुच वैसा ही होगा। इस परियोजना का उद्देश्य फिलहाल यही है कि वह केवल 1800 से 1850 के बीच प्रकाशित लंदन की पुस्तकों के ज्ञान का उपयोग करके तर्क कर सके।</p><h2>आपने प्रशिक्षण के लिए किस प्रकार का डेटा उपयोग किया?</h2></p><p>मैं 1800-1850 के लंदन की पुस्तकें, कानूनी दस्तावेज, समाचारपत्र और अन्य लेखन सामग्री उपयोग कर रहा हूँ। मैंने जो सूची लिंक की है उसमें लगभग 200 हैं, लेकिन पहली बार प्रशिक्षण के लिए मैंने केवल 50 फाइलें उपयोग कीं जिनका आकार लगभग ~187 MB था। आप दस्तावेजों की सूची यहाँ देख सकते हैं:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>संस्करण 0 मॉडल कितना बड़ा है?</h2></p><p>यह मॉडल अभी बहुत छोटा है, मैं यह सिर्फ मजे के लिए कर रहा हूँ और एक सख्त प्रशिक्षण नियम का पालन कर रहा हूँ कि कोई आधुनिक स्रोत न हो। इसमें लगभग 16 मिलियन पैरामीटर हैं लेकिन मैं और पुराने ग्रंथ इकट्ठा कर रहा हूँ ताकि अगला मॉडल प्रशिक्षण शुरू कर सकूँ। जैसे-जैसे प्रगति होगी मैं अपडेट दूँगा।</p><h2>प्रशिक्षण विनिर्देश?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>