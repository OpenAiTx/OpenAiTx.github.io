<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - आधुनिक पक्षपात को कम करने के लिए केवल निश्चित समय अवधियों के डेटा पर प्रशिक्षित किया गया एक LLM</title>
    <meta name="description" content="आधुनिक पक्षपात को कम करने के लिए केवल निश्चित समय अवधियों के डेटा पर प्रशिक्षित किया गया एक LLM">
    <meta name="keywords" content="TimeCapsuleLLM, Hindi, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "आधुनिक पक्षपात को कम करने के लिए केवल निश्चित समय अवधियों के डेटा पर प्रशिक्षित किया गया एक LLM",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-hi.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">Hindi</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 भाषा</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (जल्द आ रहा है)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (जल्द आ रहा है)</a> |
        | <a href="#" title="Coming soon">ไทย (जल्द आ रहा है)</a> |
        | <a href="#" title="Coming soon">Français (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Deutsch (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Español (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Italiano (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Русский (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Português (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Nederlands (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Polski (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">العربية (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">فارسی (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Türkçe (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (जल्द आ रहा है)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (जल्द आ रहा है)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>टाइमकैप्सूल LLM</h1>
एक LLM जिसे केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित किया गया है ताकि आधुनिक पक्षपात को कम किया जा सके।</p><p>कल्पना कीजिए कि अगर कोई एआई मॉडल केवल ऐतिहासिक होने का नाटक नहीं करता बल्कि वास्तव में ऐतिहासिक होता।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy द्वारा nanoGPT</a> पर आधारित। मुख्य प्रशिक्षण स्क्रिप्ट्स और मॉडल संरचना उन्हीं का कार्य है। </p><h1>परियोजना के लक्ष्य</h1></p><p>TimeCapsule LLM एक प्रयोगात्मक परियोजना है जिसे केवल निश्चित समय अवधि के दौरान लिखे गए पाठों पर प्रशिक्षित किया जाएगा। इसका उद्देश्य विशिष्ट ऐतिहासिक युगों की विश्वदृष्टि और भाषा का अनुकरण करना है।</p><h1>क्यों फाइन ट्यूनिंग पर्याप्त नहीं है</h1></p><p>यदि आप सिर्फ एक प्री-ट्रेंड मॉडल को फाइन ट्यून करते हैं, तो आपका LLM अभी भी आधुनिक अवधारणाएँ जानता रहेगा। बिल्कुल शून्य आधुनिक पक्षपात प्राप्त करना कठिन है, लेकिन मैं इसके जितना करीब हो सकूं उतना चाहता हूं। बिना आधुनिक पक्षपात के लिए मॉडल को शुरू से प्रशिक्षित करना आवश्यक है।</p><h1>अपेक्षित परिणाम</h1></p><p>आशा है कि जब यह पूरा हो जाएगा, तब यह मॉडल आधुनिक अवधारणाएँ नहीं जानेगा और वह अपनी प्रशिक्षण सीमा से आगे तर्क नहीं कर सकेगा। इसे आधुनिक शब्दावली/अवधारणाओं को नहीं पहचानना चाहिए और न ही आधुनिक ज्ञान का भ्रम उत्पन्न करना चाहिए।</p><h1>प्रगति अपडेट</h1></p><h2>9 जुलाई, 2025</h2></p><p>मैंने अपनी समय अवधि 1800-1850 और क्षेत्र: लंदन निर्धारित किया है</p><p>मैंने पाठ, पुस्तकों, दस्तावेजों की एक सूची इकट्ठा की है</p><p>अब तक मुझे 50 txt फाइलें मिल गई हैं और जल्द ही NanoGPT का प्रशिक्षण शुरू करूंगा</p><p>प्रगति होने तक इसे अपडेट करता रहूंगा</p><h2>13 जुलाई, 2025</h2></p><p>187MB ऐतिहासिक पाठ डेटा के साथ nanoGPT को प्रशिक्षित किया।</p><h2>15 जुलाई, 2025</h2></p><p>मैंने दूसरे प्रशिक्षण रन के लिए पाठ डाउनलोड करना शुरू किया। मैं सब कुछ Internet Archive से ले रहा हूं और मैंने समय अवधि को 1800-1875 तक बढ़ा दिया है। विविध प्रकार के पाठ प्राप्त करने के लिए आप Internet Archive पर विषय और खोज फिल्टर का उपयोग कर सकते हैं जैसे प्रकाशन स्थान, समय अवधि और विषय।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="खोज फ़िल्टर"></p><h2>16 जुलाई, 2025</h2></p><p>मैंने Internet Archive से लगभग 500 txt फाइलें डाउनलोड कीं और उन्हें साफ करने के बाद (सिर्फ व्हाइटस्पेस, गुटेनबर्ग हेडर आदि हटाकर) मेरे पास लगभग 500MB डेटा है। यह एक छोटा डेटासेट है लेकिन पिछली बार मैंने 187MB से प्रशिक्षण लिया था तो दूसरे मॉडल को प्रशिक्षित करने के बाद आउटपुट में कुछ न कुछ फर्क जरूर दिखना चाहिए। मुझे उम्मीद है कि यह मॉडल कम से कम अधिक सुसंगत वाक्य उत्पन्न कर सकेगा जो कुछ हद तक समझ में आएं। यह निश्चित नहीं है क्योंकि यह अभी भी बहुत छोटा डेटासेट है, लेकिन पिछले बार से ज्यादा है।</p><p>यह मेरे खुद के हार्डवेयर पर किया जा सकता है, यह अच्छा है क्योंकि शायद मुझे कुछ सुधार देखने को मिलेंगे इससे पहले कि मैं बड़े डेटासेट पर जाऊं जिसके लिए मुझे GPU किराए पर लेना पड़ेगा। लेकिन चिंता न करें, मैं जल्द ही GPU किराए पर लेने की योजना बना रहा हूं, लेकिन उससे पहले मैं सुनिश्चित करना चाहता हूं कि मेरा डेटासेट पूरी तरह से चुना और साफ हो। मेरी एक समस्या सफाई है, इन txt फाइलों में बहुत सी बेमतलब चीजें मिली हुई हैं। सफाई के लिए जो स्क्रिप्ट मैंने इस्तेमाल की हैं, वे काम करती हैं लेकिन 100% प्रभावी नहीं हैं।</p><p>मैं आज इस डेटासेट को प्रशिक्षित करूंगा और इसमें लगभग 4-5 घंटे लगेंगे। एक बार जब यह हो जाए और मैं इसका परीक्षण करूं, तो अपडेट दूंगा। फिर से धन्यवाद उन सभी को जो मेरी परियोजना देख रहे हैं, कुछ लोगों ने मुझे OCR संसाधनों के लिंक भी दिए हैं तो बहुत धन्यवाद! मुझे आशा है कि और लोग भी इसे आजमाएं और अपने खुद के डेटासेट के साथ प्रयोग करें।</p><h2>12 अगस्त, 2025</h2></p><p>संस्करण 1 पूरा हो गया है, यह संस्करण Phi 1.5 (700M पैरामीटर) का उपयोग करता है और इसे लगभग 7000 ग्रंथों के डेटासेट पर प्रशिक्षित किया गया है। इस मॉडल की मुख्य सुधार इसकी क्षमता है कि यह कभी-कभी वास्तविक नामों और ऐतिहासिक घटनाओं का उल्लेख कर सकता है। संस्करण 0 और 0.5 ऐसा नहीं कर सकते थे, जब भी मैंने उन मॉडलों को "साल 1821 में" जैसा कुछ पूछा तो आउटपुट सिर्फ कल्पनाएँ होती थी, ऐसी बातें जो कभी हुई ही नहीं। इसका उदाहरण संस्करण 1 के साथ:</p><p>मेरा प्रॉम्प्ट: यह हमारे प्रभु का वर्ष 1834 था,</p><p>आउटपुट: "यह हमारे प्रभु का वर्ष 1834 था और लंदन की गलियाँ विरोध और याचिका से भरी थीं। कारण, जैसा कि कई लोगों ने गिनाया, निजी तरीके से नहीं बंधा था, बल्कि उसी दिन लॉर्ड पामरस्टन के दिन में उठाया गया था, जनता को कानून के दिन की कठिनाइयों का संक्षिप्त विवरण प्राप्त होगा। यह गहरी खेद की बात है, कि वर्तमान घटनाएँ दुनिया के इतिहास में स्पष्ट हैं, और तदनुसार ज्ञात होंगी। यह सच नहीं है कि वही लोग जिन्होंने पहली बार यरूशलेम में सुसमाचार बसाया था, उनके पास समृद्धि और समृद्धि का इतना व्यापक और दिलचस्प रिकॉर्ड होना चाहिए।"</p><p>शुरू में मैंने सोचा कि यह संयोग हो सकता है लेकिन इसे देखिए: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>प्रशिक्षण अपडेट</h3></p><p>मैंने 435MB (108 M टोकन) के कॉर्पस पर प्रशिक्षण शुरू किया है, अभी सब कुछ काफी सहज चल रहा है। ट्रेन लॉस पहले 2800 इटरेशन में 10.9 से घटकर 4.9 हो गया है। मुझे उम्मीद है कि इसे पूरा होने में लगभग 8 या 9 घंटे लगेंगे। पूरा होते ही एक और अपडेट पोस्ट करूंगा।</p><h2>17 जुलाई, 2025</h2></p><p>दूसरे मॉडल का प्रशिक्षण पूरा हो गया है, मेरे 4060 को लगभग 8 घंटे और 40 मिनट (3,900 इटर/घंटा) लगे 33,000 इटरेशन (5 युग) के लिए। अंतिम ट्रेन लॉस 3.73 था। आउटपुट आश्चर्यजनक रूप से अच्छे थे, अब यह वास्तव में 19वीं सदी की शैली के वाक्य स्पष्ट रूप से उत्पन्न करता है।</p><h2>28 जुलाई, 2025</h2></p><p>मैंने v0.5 को Hugging Face पर अपलोड कर दिया है, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">यहाँ देखें</a> अगर आप देखना चाहें। अब आप मेरा रिपॉजिटरी डाउनलोड करके इसे लोकली चला सकते हैं। दुर्भाग्यवश nanoGPT HuggingFace के साथ मूल रूप से काम नहीं करता, तो आपको मॉडल डाउनलोड करके लोकली चलाना होगा।</p><p>साथ ही मैं अपनी अगली प्रशिक्षण रन के लिए डेटा क्यूरेट करना शुरू करूंगा, मुझे लगता है कि तर्क क्षमताएँ पाने के लिए मुझे शायद 5-10 गुना अधिक डेटा चाहिए होगा।</p><h2>2 अगस्त, 2025</h2></p><p>मैं जल्द ही संस्करण 1 पर काम शुरू करने जा रहा हूँ। मुझे nanoGPT की आर्किटेक्चर से कुछ आधुनिक में स्थानांतरित होना होगा। मेरे मन में कई ओपन-सोर्स LLM आर्किटेक्चर हैं, जैसे: OpenLLaMA v3, Phi-2 और Qwen 1.5B। और V1 तक की छलांग के लिए, मुझे बहुत बड़ा और विविध डेटासेट सावधानीपूर्वक क्यूरेट करना होगा। मुझे कम से कम 5GB स्वच्छ प्रशिक्षण डेटा की आवश्यकता होगी।</p><h1>V0 मॉडल व्यवहार एवं सीमाएँ</h1></p><p>प्रारंभिक प्रॉम्प्ट्स दिखाते हैं कि मॉडल 1800 के दशक की भाषा और व्यवहार के साथ प्रतिक्रिया देता है। उदाहरण के लिए, मैंने इसे "Who art Henry?" पूछा और उसने जवाब दिया "I know that man, I have did not a black, the storm." और हाँ, उस वाक्य का कोई अर्थ नहीं है लेकिन LLM पहचान रहा है कि मैं किसी व्यक्ति के बारे में पूछ रहा हूँ।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>आधुनिक अवधारणाओं का कोई उल्लेख नहीं है, आउटपुट में ज्यादातर 1800 के दशक के शब्द और वाक्यांश होते हैं।</p><p>अब भी इसमें बहुत काम बाकी है, 187MB डेटा पर प्रशिक्षण देने से आपको ऐसा मॉडल नहीं मिलेगा जो जटिल तर्क के साथ पाठ उत्पन्न करे।</p><p>
अभी यह ऐसे वाक्य बनाता है जिनमें पूरी वाक्य संरचना नहीं होती है और कुल मिलाकर कोई अर्थ नहीं निकलता, लेकिन यह प्रशिक्षण आकार के लिए सामान्य है।</p><h1>V0.5 मॉडल व्यवहार और सीमाएँ</h1></p><p>यह पिछले मॉडल की तुलना में एक अच्छा सुधार है। लेखन शैली और शब्दावली विक्टोरियन है और लगभग हर वाक्य व्याकरण की दृष्टि से सही है, उचित विराम चिह्नों के साथ। और फिर से यह पूरी तरह से नई ट्रेनिंग से है, इसलिए यह 1800 के विषयों से जुड़ा रहता है।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>बहुत सारी तथ्यात्मक भ्रांतियाँ हैं। अधिकांश (लगभग 100%) विवरण (तिथियाँ, घटनाएँ, ऐतिहासिक व्यक्ति) काल्पनिक हैं। साथ ही वाक्य एक-दूसरे से जुड़े नहीं होते हैं, कभी-कभी शायद 2 वाक्य आपस में संबंधित हो सकते हैं, लेकिन उससे आगे नहीं। एक और समस्या यह है कि कभी-कभी "Digitized by Google" जैसा फुटर आ जाता है, तो अगली बार ट्रेनिंग करते समय मुझे सुनिश्चित करना होगा कि टेक्स्ट अच्छी तरह से साफ किए जाएं। कुल मिलाकर मैं परिणामों से काफी खुश हूँ, यह अभी LLM के पास नहीं है लेकिन निश्चित रूप से एक वाक्य जेनरेटर है।</p><p>मैं बहुत कुछ सीख रहा हूँ और आने वाले हफ्तों में यह समझना शुरू करूँगा कि मुझे क्या बेहतर करना है। मैं जल्द ही फाइलें अपलोड करूँगा!</p><h1>V1 मॉडल व्यवहार और सीमाएँ</h1></p><p>मैं जल्द ही कुछ उदाहरण आउटपुट अपलोड करूँगा और एक ही प्रॉम्प्ट के साथ तीनों मॉडलों की तुलना भी करूँगा। मैं अपने पिछले संस्करण की तरह V1 को भी huggingface पर अपलोड करूँगा, मेरा huggingface अकाउंट यहाँ है: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>आगामी योजनाएँ</h1></p><p>(पूरा हुआ) मैं वर्शन 0.5 पर काम शुरू करने जा रहा हूँ, 50 पुस्तकों के बजाय, मैं आदर्श रूप से 500-600 पुस्तकों का उपयोग करके ट्रेनिंग करूँगा। अभी मैं nanoGPT को 1800-1850 की लंदन की पुस्तकों का उपयोग करके ट्रेनिंग कर रहा हूँ। इसमें कुछ चुनौतियाँ हैं जैसे यह सुनिश्चित करना कि जो किताबें मैं चुन रहा हूँ वे आधुनिक संशोधन वाली न हों, बल्कि मेरे चुने हुए समय अवधि में प्रकाशित असंपादित किताबें हों।</p><p>मैं एक नया मॉडल (v1) बहुत बड़े कॉर्पस के साथ ट्रेन करना चाहता हूँ, शायद वह v0.5 के मुकाबले 5-10 गुना बड़ा हो। मेरा लक्ष्य देखना है कि क्या केवल Selective Temporal Training से तर्क क्षमता उत्पन्न हो सकती है, यह एक कठिन कार्य होगा और मुझे पूरी तरह यकीन नहीं है कि यह संभव भी है क्योंकि ऐतिहासिक डेटा की सीमाएँ हैं। आने वाले हफ्तों में मैं 5-10GB कॉर्पस के लिए पर्याप्त डेटा जुटाने की कोशिश करूँगा। मुझे विश्वास है कि अगर मुझे उच्च गुणवत्ता का साफ डेटा मिल जाए और GPU किराए पर मिल जाए, तो प्रगति होगी।</p><h1>इस प्रोजेक्ट का उपयोग कैसे करें</h1></p><p>यह प्रोजेक्ट मुख्य रूप से ऐतिहासिक डेटा को इकट्ठा करने, प्रशिक्षण के लिए तैयार करने और टोकनाइज़र बनाने पर केंद्रित है। मैं पूरे LLM प्रशिक्षण प्रक्रिया को नहीं कवर करूँगा, उसके लिए Andrej Karpathy का nanoGPT देखें।</p><h1>चरण 1: ऐतिहासिक ग्रंथ एकत्र और तैयार करें</h1></p><p>अपने चुने हुए समय अवधि (जैसे, लंदन 1800-1850) से सार्वजनिक डोमेन की किताबें, दस्तावेज़ आदि के .txt फाइलें एकत्र करें।</p><p>यदि आपको ज़रूरत हो तो download_texts_improved.py का उपयोग करके किताबें डाउनलोड कर सकते हैं।</p><p>टेक्स्ट फाइलों को स्क्रिप्ट या मैन्युअली साफ करें, जिसमें Project Gutenberg के हेडर/फुटर, आधुनिक टीका-टिप्पणियाँ या OCR त्रुटियाँ हटाना शामिल है।</p><p>prepare_dataset.py ठीक काम करना चाहिए।</p><h1>चरण 2: कस्टम टोकनाइज़र बनाएं</h1></p><p>साफ किए गए डेटा पर train_tokenizer.py या train_tokenizer_hf.py चलाएँ।
यह आपको vocab.json और merges.txt देगा।
ये फाइलें आपके मॉडल के लिए शब्दावली और विलय नियम परिभाषित करती हैं</p><h1>चरण 3: अपना मॉडल प्रशिक्षित करें (nanoGPT)</h1></p><p>प्रशिक्षण प्रक्रिया के लिए <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy का nanoGPT</a> देखें।</p><p>अगर आप चाहें तो कोई अन्य LLM प्रशिक्षित कर सकते हैं, लेकिन मैंने nanoGPT का उपयोग किया है</p><h1>अक्सर पूछे जाने वाले प्रश्न (FAQ)</h1></p><h2>चयनात्मक कालिक प्रशिक्षण (Selective Temporal Training) क्या है?</h2></p><p>चयनात्मक कालिक प्रशिक्षण (STT) एक मशीन लर्निंग पद्धति है जिसमें सभी प्रशिक्षण डेटा विशेष रूप से एक ऐतिहासिक समय अवधि के भीतर चुना जाता है। इसे उस युग की भाषा और ज्ञान का मॉडल बनाने के लिए किया जाता है, जिसमें आधुनिक अवधारणाओं का प्रभाव नहीं होता। उदाहरण के लिए, जो वर्तमान मॉडल मेरे पास है (v0.5) वह विशेष रूप से 1800-1875 के डेटा पर प्रशिक्षित है, यह फाइन ट्यून नहीं है बल्कि शुरू से प्रशिक्षित है, जिससे आउटपुट उस समय की भाषाई शैली और ऐतिहासिक संदर्भ को दर्शाता है।</p><h2>सिर्फ फाइन-ट्यूनिंग या LoRA का उपयोग क्यों नहीं किया?</h2></p><p>इस परियोजना के लिए मैं एक ऐसा भाषा मॉडल बनाना चाहता हूँ जिसमें आधुनिक पक्षपात न हो। अगर मैं GPT-2 जैसी किसी चीज़ को फाइन-ट्यून करता हूँ, तो वह पहले से ही पूर्व-प्रशिक्षित है और वह जानकारी कभी नहीं जाएगी। अगर मैं शुरू से प्रशिक्षण करता हूँ तो भाषा मॉडल पुराना होने का दिखावा नहीं करेगा, वह वास्तव में पुराना होगा। इस परियोजना का उद्देश्य अभी यही है कि कुछ ऐसा बनाया जाए जो सिर्फ 1800 से 1850 के बीच लंदन की पुस्तकों के ज्ञान का उपयोग करके तर्क कर सके।</p><h2>प्रशिक्षण के लिए किस प्रकार का डेटा इस्तेमाल किया गया?</h2></p><p>मैं 1800–1850 लंदन की किताबें, कानूनी दस्तावेज, समाचार पत्र और अन्य लेखन का उपयोग कर रहा हूँ। लिंक की गई सूची में लगभग 200 हैं लेकिन पहले प्रशिक्षण के लिए मैंने केवल 50 फाइलें (~187 MB) का उपयोग किया। आप दस्तावेज़ों की सूची देख सकते हैं:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>मॉडल कितने बड़े हैं?</h2></p><p>V0: 16M पैरामीटर</p><p>V0.5: 123M पैरामीटर</p><p>V1: 700M पैरामीटर</p><h1>प्रशिक्षण विनिर्देश ?</h1></p><p>#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>#V1</p><p>GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>