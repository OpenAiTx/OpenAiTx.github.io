<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM, ताकि आधुनिक पक्षपात को कम किया जा सके।</title>
    <meta name="description" content="केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM, ताकि आधुनिक पक्षपात को कम किया जा सके।">
    <meta name="keywords" content="TimeCapsuleLLM, Hindi, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित एक LLM, ताकि आधुनिक पक्षपात को कम किया जा सके।",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-hi.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Hindi</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 भाषा</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>टाइमकैप्सूल LLM</h1>
एक LLM जिसे केवल कुछ निश्चित समय अवधियों के डेटा पर प्रशिक्षित किया गया है ताकि आधुनिक पूर्वाग्रह को कम किया जा सके।</p><p>कल्पना करें यदि एक एआई मॉडल केवल ऐतिहासिक दिखावा न करे, बल्कि वास्तव में वैसा ही हो।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy के nanoGPT</a> पर आधारित। मुख्य प्रशिक्षण स्क्रिप्ट और मॉडल आर्किटेक्चर उन्हीं का कार्य है।</p><h1>परियोजना के लक्ष्य</h1></p><p>टाइमकैप्सूल LLM एक प्रयोगात्मक परियोजना है जिसे केवल कुछ निश्चित समय अवधि के दौरान लिखे गए ग्रंथों पर प्रशिक्षित किया जाएगा। लक्ष्य है विशिष्ट ऐतिहासिक युगों की विश्वदृष्टि और भाषा का अनुकरण करना।</p><h1>केवल फाइन ट्यूनिंग पर्याप्त क्यों नहीं है</h1></p><p>यदि आप केवल प्री-ट्रेंड मॉडल को फाइन ट्यून करते हैं, तो आपका LLM अभी भी आधुनिक अवधारणाओं को जानता रहेगा। बिल्कुल शून्य आधुनिक पूर्वाग्रह पाना कठिन है, लेकिन मैं इसके जितना करीब संभव हो सकूं, पहुंचना चाहता हूं। बिना आधुनिक पूर्वाग्रह के लिए मॉडल को पूरी तरह से शुरू से प्रशिक्षित करना आवश्यक है।</p><h1>अपेक्षित परिणाम</h1></p><p>आशा है कि जब यह मॉडल तैयार होगा, तो यह आधुनिक अवधारणाओं को नहीं जानेगा और केवल उसी पर विचार कर पाएगा जिस पर इसे प्रशिक्षित किया गया है। इसे आधुनिक अवधारणाओं/शब्दावली को नहीं पहचानना चाहिए और मैं चाहता हूं कि यह आधुनिक ज्ञान की गढ़ना न करे।</p><h1>प्रगति अपडेट्स</h1></p><h2>9 जुलाई, 2025</h2></p><p>मैंने अपनी समयावधि 1800-1850 और क्षेत्र: लंदन सेट किया है।</p><p>मैंने ग्रंथों, पुस्तकों, दस्तावेजों की एक सूची एकत्र की है।</p><p>अब तक मुझे 50 txt फाइलें मिल गई हैं और जल्द ही NanoGPT पर प्रशिक्षण शुरू करूंगा।</p><p>जैसे-जैसे प्रगति होती है, मैं इसे अपडेट करता रहूंगा।</p><h2>13 जुलाई, 2025</h2></p><p>मैंने nanoGPT को 187MB ऐतिहासिक टेक्स्ट डेटा के साथ प्रशिक्षित किया है।</p><h2>15 जुलाई, 2025</h2></p><p>मैंने दूसरे प्रशिक्षण रन के लिए ग्रंथ डाउनलोड करना शुरू कर दिया है। मैं सब कुछ Internet Archive से ले रहा हूं और समयावधि को 1800-1875 तक बढ़ा दिया है। ग्रंथों की विविधता के लिए, आप Internet Archive पर विषय और खोज फ़िल्टर का उपयोग कर सकते हैं जैसे कि प्रकाशन स्थान, समयावधि और विषय।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>16 जुलाई, 2025</h2></p><p>मैंने Internet Archive से लगभग 500 txt फाइलें डाउनलोड कीं और उन्हें साफ करने के बाद (केवल whitespaces, Gutenberg हेडर आदि हटाए) मेरे पास लगभग 500MB डेटा है। यह एक छोटा सा डेटासेट है, लेकिन पिछली बार मैंने 187MB पर प्रशिक्षण दिया था, इसलिए इस बार प्रशिक्षण के बाद आउटपुट में कम से कम कुछ उल्लेखनीय फर्क आना चाहिए। मुझे उम्मीद है यह मॉडल कम से कम अधिक सुसंगत वाक्य उत्पन्न कर पाएगा जो कुछ हद तक समझ में आते हों। ज़रूरी नहीं है कि यह गारंटी हो, क्योंकि यह अभी भी बहुत छोटा डेटासेट है, लेकिन पिछली बार से अधिक है।</p><p>यह मेरे अपने हार्डवेयर पर संभव होना चाहिए, यह भी अच्छा है क्योंकि मुझे उम्मीद है कि कोई सुधार दिखे इससे पहले कि मैं बड़े डेटासेट पर जाऊं, जिसके लिए मुझे GPU किराए पर लेना पड़ेगा। लेकिन चिंता न करें, मैं जल्द ही GPU किराए पर लेने की योजना बना रहा हूं, लेकिन उससे पहले मैं चाहता हूं कि मेरा डेटासेट जितना संभव हो उतना क्यूरेटेड और साफ हो। मेरी एक समस्या साफ-सफाई है, इनमें से कई txt फाइलों में बकवास भी मिली होती है। मैंने जो स्क्रिप्ट्स सफाई के लिए इस्तेमाल की हैं, वे काम करती हैं, लेकिन 100% प्रभावी नहीं हैं।</p><p>मैं आज इस डेटासेट पर प्रशिक्षण करूंगा और इसमें लगभग 4-5 घंटे लगेंगे। एक बार यह हो गया और मैंने इसका परीक्षण किया, तो मैं अपडेट दूंगा। मेरे प्रोजेक्ट को देखने वाले सभी लोगों का फिर से धन्यवाद, मुझे कुछ लोगों ने तो OCR संसाधनों के लिंक भी दिए हैं, धन्यवाद! मैं चाहता हूं कि और लोग भी इसे आजमाएं और अपने खुद के डेटासेट के साथ प्रयोग करें।</p><h2>28 जुलाई, 2025</h2></p><p>मैंने v0.5 को Hugging Face पर अपलोड कर दिया है, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">यहां देखें</a> अगर आप चाहें। अब आप मेरी रिपॉजिटरी डाउनलोड कर सकते हैं और इसे लोकली चला सकते हैं। दुर्भाग्यवश nanoGPT HuggingFace के साथ मूल रूप से काम नहीं करता है, इसलिए आपको मॉडल डाउनलोड करके लोकली चलाना होगा।</p><p>साथ ही मैं अपने अगले प्रशिक्षण रन के लिए डेटा क्यूरेट करना शुरू करूंगा, मुझे लगता है कि मुझे तर्क क्षमताओं के लिए शायद 5-10 गुना अधिक डेटा की आवश्यकता होगी।</p><h3>प्रशिक्षण अपडेट</h3></p><p>मैंने 435MB (108 M टोकन) कॉर्पस पर प्रशिक्षण शुरू किया है, अभी यह बहुत स्मूथ चल रहा है। ट्रेन लॉस 10.9 से गिरकर पहले 2800 इटरेशन में 4.9 पर आ गया। मुझे उम्मीद है कि इसे पूरा होने में लगभग 8 या 9 घंटे लगेंगे। एक और अपडेट पोस्ट करूंगा जब यह हो जाएगा।</p><h2>17 जुलाई, 2025 2:13AM</h2></p><p>दूसरे मॉडल के लिए प्रशिक्षण पूरा हो गया, मेरे 4060 ने इसे लगभग 8 घंटे 40 मिनट (3,900 इटर्स/घंटा) में 33,000 इटरेशन (5 युग) के लिए किया। अंतिम ट्रेन लॉस 3.73 था। आउटपुट हैरान करने वाले अच्छे थे, अब यह सच में 19वीं सदी की शैली के सुसंगत वाक्य उत्पन्न करता है।</p><h1>V0 मॉडल का व्यवहार और सीमाएँ</h1></p><p>प्रारंभिक संकेतों पर मॉडल 1800 की भाषा और व्यवहार के साथ प्रतिक्रिया देता है। उदाहरण के लिए, मैंने इसे "Who art Henry?" पूछा और इसने उत्तर दिया "I know that man, I have did not a black, the storm." और हां, उस वाक्य का कोई मतलब नहीं है लेकिन LLM समझ रहा है कि मैं किसी व्यक्ति के बारे में पूछ रहा हूं।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>आधुनिक अवधारणाओं का कोई उल्लेख नहीं है, आउटपुट में अधिकांश शब्द और वाक्यांश 1800 के दशक के हैं।</p><p>इसे अभी भी बहुत काम की जरूरत है, 187MB से प्रशिक्षण करने पर आपको ऐसा मॉडल नहीं मिलेगा जो जटिल तर्क के साथ पाठ उत्पन्न कर सके।</p><p>अभी यह ऐसे वाक्य बनाता है जिनमें पूरा वाक्य संरचना नहीं होती और कुल मिलाकर कोई अर्थ नहीं बनता, लेकिन प्रशिक्षण आकार के लिए यह सामान्य है।</p><h1>V0.5 मॉडल व्यवहार और सीमाएँ</h1></p><p>यह पिछले मॉडल की तुलना में अच्छा सुधार है। लेखन शैली और शब्दावली विक्टोरियन है और लगभग हर वाक्य व्याकरणिक रूप से सही है और उचित विराम चिह्न के साथ है। और फिर से, यह शुरू से प्रशिक्षित किया गया है इसलिए यह 1800 के दशक के विषयों तक ही सीमित है।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>बहुत सारी तथ्यात्मक भ्रांतियाँ हैं। अधिकांश (लगभग 100%) विवरण (तिथियाँ, घटनाएँ, ऐतिहासिक व्यक्ति) गढ़े हुए हैं। साथ ही वाक्यों के बीच में कोई संबंध नहीं है, कभी-कभी 2 वाक्य जुड़े होते हैं लेकिन उससे आगे नहीं। एक और समस्या है कि कभी-कभी "Digitized by Google" फुटर आ जाता है, इसलिए अगली बार प्रशिक्षण के पहले मुझे सुनिश्चित करना होगा कि ग्रंथ अच्छी तरह से साफ किए गए हों। कुल मिलाकर मैं परिणामों से बहुत खुश हूँ, यह अभी LLM नहीं है लेकिन निश्चित ही एक वाक्य जनरेटर है।</p><p>मैं बहुत कुछ सीख रहा हूँ और आने वाले हफ्तों में जानने की कोशिश करूँगा कि मुझे क्या बेहतर करना है। मैं जल्द ही फाइलें अपलोड करूँगा!</p><h1>आगामी योजनाएँ</h1></p><p>(पूरा) मैं वर्शन 0.5 पर काम शुरू करने जा रहा हूँ, 50 पुस्तकों के बजाय मैं आदर्श रूप से 500-600 पुस्तकों से प्रशिक्षण करूँगा। अभी मैं nanoGPT का प्रशिक्षण 1800-1850 की पुस्तकों से और विशेष रूप से लंदन से कर रहा हूँ। कुछ चुनौतियाँ हैं जैसे यह सुनिश्चित करना कि जो पुस्तकें मैं ढूँढ रहा हूँ वे अद्यतन या आधुनिक व्याख्याओं के साथ नहीं हैं, बल्कि मेरी चुनी अवधि में प्रकाशित अप्रभावित पुस्तकें हैं।</p><p>मैं एक नया मॉडल (v1) एक बहुत बड़े कार्पस के साथ प्रशिक्षित करना चाहता हूँ, शायद v0.5 के मुकाबले 5-10 गुना बड़ा। मेरा लक्ष्य है देखना कि केवल Selective Temporal Training से तर्क क्षमताएँ उभर सकती हैं या नहीं, यह एक कठिन कार्य होगा और मैं पूरी तरह से सुनिश्चित नहीं हूँ कि यह संभव है क्योंकि ऐतिहासिक डेटा सीमाएँ हैं। आने वाले हफ्तों में मैं 5-10GB के कार्पस के लिए पर्याप्त डेटा एकत्र करने की कोशिश करूँगा। मुझे विश्वास है कि अगर मुझे साफ, उच्च गुणवत्ता वाला डेटा मिल गया और GPU किराए पर ले लिया, तो प्रगति होगी।</p><h1>इस प्रोजेक्ट का उपयोग कैसे करें</h1></p><p>यह प्रोजेक्ट मुख्य रूप से ऐतिहासिक डेटा संकलन, प्रशिक्षण के लिए उसकी तैयारी और एक टोकनाइज़र बनाने पर केंद्रित है। मैं पूर्ण LLM प्रशिक्षण प्रक्रिया को कवर नहीं करूँगा, उसके लिए Andrej Karpathy का nanoGPT देखें।</p><h1>चरण 1: ऐतिहासिक ग्रंथों को एकत्र करें और तैयार करें</h1></p><p>अपने चुने हुए समय अवधि (जैसे, लंदन 1800-1850) के सार्वजनिक डोमेन पुस्तकों, दस्तावेज़ों आदि की .txt फाइलें एकत्र करें</p><p>यदि आपको आवश्यकता है तो आप download_texts_improved.py का उपयोग पुस्तकों को डाउनलोड करने के लिए कर सकते हैं।</p><p>टेक्स्ट फाइलों को स्क्रिप्ट या मैन्युअल रूप से साफ करें, प्रोजेक्ट गुटेनबर्ग के हेडर/फुटर, आधुनिक टिप्पणियाँ या OCR त्रुटियाँ हटाएँ।</p><p>prepare_dataset.py ठीक काम करेगा।</p><h1>चरण 2: एक कस्टम टोकनाइज़र बनाएँ</h1></p><p>साफ किए गए डेटा पर train_tokenizer.py या train_tokenizer_hf.py चलाएँ।
यह आपको vocab.json और merges.txt देगा</p><p>ये फाइलें आपके मॉडल के लिए शब्दावली और मर्ज नियमों को परिभाषित करती हैं</p><h1>चरण 3: अपने मॉडल को प्रशिक्षित करें (nanoGPT)</h1></p><p>प्रशिक्षण प्रक्रिया के लिए <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy का nanoGPT</a> देखें।</p><p>यदि आप चाहें तो कोई अन्य LLM प्रशिक्षित कर सकते हैं, लेकिन मैंने nanoGPT का उपयोग किया</p><h1>FAQ</h1></p><h2>Selective Temporal Training क्या है?</h2></p><p>Selective Temporal Training (STT) एक मशीन लर्निंग पद्धति है जिसमें सभी प्रशिक्षण डेटा को विशेष रूप से एक ऐतिहासिक समय अवधि के भीतर सीमित किया जाता है। यह उस युग की भाषा और ज्ञान का मॉडल बनाने के लिए किया जाता है, ताकि आधुनिक अवधारणाओं का प्रभाव न हो। उदाहरण के लिए, मेरा वर्तमान मॉडल (v0.5) केवल 1800-1875 के डेटा पर प्रशिक्षित है, यह फाइन-ट्यून नहीं किया गया बल्कि शुरू से प्रशिक्षित है, जिससे आउटपुट उस समय की भाषाई शैली और ऐतिहासिक संदर्भ को दर्शाता है।</p><h2>बस फाइन-ट्यूनिंग या LoRA का उपयोग क्यों नहीं?</h2></p><p>इस प्रोजेक्ट में मैं एक ऐसा भाषा मॉडल बनाने की कोशिश कर रहा हूँ जो आधुनिक पक्षपात से मुक्त हो। अगर मैं GPT-2 जैसा कुछ फाइन-ट्यून करता हूँ, तो वह पहले से ही प्री-ट्रेंड है और उसकी जानकारी हटाई नहीं जा सकती। अगर मैं शुरू से प्रशिक्षण करता हूँ तो भाषा मॉडल केवल पुराना प्रतीत नहीं होगा, बल्कि वह वास्तव में पुराना होगा। इस प्रोजेक्ट का अभी का लक्ष्य है कि केवल 1800 से 1850 के बीच लंदन में प्रकाशित पुस्तकों के ज्ञान का उपयोग कर तर्क करने वाला मॉडल बनाना।</p><h2>प्रशिक्षण के लिए किस तरह के डेटा का उपयोग किया?</h2></p><p>मैं लंदन 1800-1850 की किताबें, कानूनी दस्तावेज़, समाचार पत्र और अन्य लेखन का उपयोग कर रहा हूँ। मैंने लिंक की गई सूची में लगभग 200 हैं लेकिन पहले प्रशिक्षण के लिए मैंने केवल 50 फाइलें ~187 MB के आसपास लीं। आप दस्तावेज़ों की सूची देख सकते हैं:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Version 0 मॉडल कितना बड़ा है?</h2></p><p>यह मॉडल अभी बहुत छोटा है, मैं यह केवल मज़े के लिए कर रहा हूँ और कोई आधुनिक स्रोत न लेने के सख्त प्रशिक्षण नियम का पालन कर रहा हूँ। इसमें लगभग 16 मिलियन पैरामीटर हैं लेकिन मैं अब और पुरानी किताबें एकत्र करने जा रहा हूँ ताकि अगला मॉडल प्रशिक्षण शुरू कर सकूँ। जैसे-जैसे आगे बढ़ूँगा, अपडेट दूँगा।</p><h2>प्रशिक्षण विनिर्देश?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>