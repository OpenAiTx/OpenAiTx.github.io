<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - LLM, обученная только на данных из определённых временных периодов для снижения современного смещения</title>
    <meta name="description" content="LLM, обученная только на данных из определённых временных периодов для снижения современного смещения">
    <meta name="keywords" content="TimeCapsuleLLM, Russian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "LLM, обученная только на данных из определённых временных периодов для снижения современного смещения",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 275
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-ru.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-07",
  "dateModified": "2025-08-07"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 275 stars</span>
                <span class="language">Russian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Язык</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
LLM, обученная только на данных определённых исторических периодов, чтобы уменьшить современное влияние.</p><p>Представьте, если бы ИИ-модель не просто притворялась исторической, а действительно такой была.</p><p>Основано на <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT от Андрея Карпаты</a> Основные тренировочные скрипты и архитектура модели — его работа.</p><h1>Цели проекта</h1></p><p>TimeCapsule LLM — это экспериментальный проект, который будет обучаться только на текстах, написанных в определённые исторические периоды. Цель — смоделировать мировоззрение и язык конкретных эпох.</p><h1>Почему дообучения недостаточно</h1></p><p>Если вы просто дообучите уже предобученную модель, ваш LLM всё равно будет знать современные концепции. Конечно, полностью избавиться от современного влияния сложно, но я хочу максимально приблизиться к этому. Для этого нужно обучать модель с нуля.</p><h1>Ожидаемые результаты</h1></p><p>Надеюсь, после завершения эта модель не будет знать современных понятий и не сможет рассуждать о том, что ей не известно. Она не должна узнавать современную лексику/понятия, и я надеюсь, что она не будет "галлюцинировать" современные знания.</p><h1>Хронология прогресса</h1></p><h2>9 июля 2025</h2></p><p>Я выбрал временной диапазон 1800-1850 и регион: Лондон</p><p>Я собрал список текстов, книг, документов</p><p>Пока что у меня есть 50 файлов в формате txt, скоро начну обучение NanoGPT</p><p>Буду обновлять этот раздел по мере продвижения</p><h2>13 июля 2025</h2></p><p>Обучил nanoGPT на 187 МБ исторических текстовых данных.</p><h2>15 июля 2025</h2></p><p>Я начал скачивать тексты для второго этапа обучения. Беру всё с Internet Archive и расширил временной диапазон до 1800-1875. Чтобы получить разнообразные тексты, можно использовать фильтры по месту публикации, периоду и темам на Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Фильтры поиска"></p><h2>16 июля 2025</h2></p><p>Я скачал около 500 txt-файлов с Internet Archive и после чистки (удаление пробелов, заголовков Гутенберга и т.д.) у меня получилось около 500 МБ данных. Это крохотный датасет, но в прошлый раз я обучал на 187 МБ, так что теперь, вероятно, будет заметная разница на выходе после обучения второй модели. Надеюсь, эта версия хотя бы будет выдавать более связные предложения. Конечно, это не гарантия — датасет всё ещё крошечный, но он больше, чем был.</p><p>Это можно реализовать на моём оборудовании, и это хорошо — надеюсь, увижу улучшения до того, как перейду к большему датасету, для которого придётся арендовать GPU. Не волнуйтесь, я всё равно планирую арендовать GPU, но прежде хочу, чтобы мой датасет был максимально чистым и отобранным. Одна из проблем — очистка, во многих txt-файлах встречается "мусор". Скрипты для очистки работают, но не на 100%.</p><p>Я буду обучать этот датасет сегодня, это займёт примерно 4-5 часов. Как только закончу и протестирую, дам знать о результатах. Спасибо всем, кто следит за проектом, некоторые даже присылали ссылки на OCR-ресурсы — спасибо! Надеюсь, больше людей попробует и поэкспериментирует с собственными наборами данных.</p><h3>Обновление по обучению</h3></p><p>Я начал обучение на корпусе 435 МБ (108 млн токенов), всё пока идёт гладко. Train loss упал с 10.9 до 4.9 за первые 2800 итераций. Думаю, обучение займёт 8-9 часов. После окончания выложу обновление.</p><h2>17 июля 2025</h2></p><p>Обучение второй модели завершено, моя 4060 справилась за 8 часов 40 минут (3 900 итераций/час) за 33 000 итераций (5 эпох). Итоговый train loss — 3.73. Результаты удивили: теперь модель действительно генерирует связные предложения в стиле XIX века.</p><h2>28 июля 2025</h2></p><p>Я загрузил версию v0.5 на Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">ознакомиться можно здесь</a>. Теперь вы можете скачать мой репозиторий и запускать его локально. К сожалению, nanoGPT не работает нативно с HuggingFace, поэтому придётся скачать и запускать модель локально.</p><p>Также начинаю собирать данные для следующего этапа обучения — думаю, потребуется в 5-10 раз больше данных для достижения уровня рассуждения.</p><h2>2 августа 2025</h2></p><p>Скоро начну работу над Версией 1. Мне нужно будет перейти с архитектуры nanoGPT на что-то более современное. Я рассматриваю несколько open-source LLM-архитектур, включая: OpenLLaMA v3, Phi-2 и Qwen 1.5B. А для перехода к V1 потребуется тщательно отобранный, гораздо более крупный и разнообразный датасет. Нужно как минимум 5ГБ чистых обучающих данных.</p><h1>Поведение и ограничения модели V0</h1></p><p>Ранние подсказки показывают, что модель отвечает на языке и с поведением 1800-х годов. Например, я задал ей вопрос "Who art Henry?" и она ответила "I know that man, I have did not a black, the storm." — да, это предложение не имеет смысла, но LLM распознаёт, что я спрашиваю о человеке.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Пример вывода TimeLockLLM"></p><p>Нет упоминаний современных концепций, выводы содержат в основном слова и обороты из 1800-х годов.</p><p>Ей всё ещё требуется много доработки, обучение на 187 МБ не даст модели, генерирующей текст со сложным рассуждением.</p><p>Сейчас она генерирует предложения с неполной структурой и в целом бессмысленные, но это нормально для такого объёма обучения.</p><h1>Поведение и ограничения модели V0.5</h1></p><p>Это значительное улучшение по сравнению с предыдущей моделью. Стиль письма и словарный запас викторианские, почти каждое предложение грамматически правильно с корректной пунктуацией. И опять же — модель обучалась с нуля, так что придерживается тематики 1800-х годов.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Пример вывода TimeLockLLM"></p><p>Много фактических галлюцинаций. Очень много (практически 100%) деталей (дат, событий, исторических личностей) вымышлены. Также предложения зачастую не связаны между собой, иногда 2 предложения могут быть связаны, но дальше — нет. Ещё одна проблема — иногда появляется посторонний футер “Digitized by Google”, поэтому в следующий раз при обучении мне обязательно нужно лучше очищать тексты. В целом я очень доволен результатом, до настоящей LLM далеко, но это определённо генератор предложений.</p><p>Я многое изучаю и в ближайшие недели начну разбираться, что нужно улучшить. Файлы скоро выложу!</p><h1>Ближайшие планы</h1></p><p>(Выполнено) Начну работу над версией 0.5, вместо обучения на 50 книгах буду использовать 500-600. Сейчас я обучаю nanoGPT на книгах 1800-1850 годов, преимущественно из Лондона. Есть трудности с тем, чтобы убедиться, что найденные книги не обновлены и не содержат современных интерпретаций, а являются подлинными изданными в выбранный период.</p><p>Я хочу обучить новую модель (v1) на гораздо большем корпусе, возможно, в 5-10 раз больше, чем использованный для v0.5. Моя цель — проверить, могут ли способности к рассуждению появиться только за счёт Selective Temporal Training; это более сложная задача, и я не уверен, возможно ли это из-за ограниченности исторических данных. В ближайшие недели постараюсь собрать достаточно данных для корпуса на 5-10 ГБ. Думаю, если смогу получить чистые качественные данные и арендовать GPU, будет прогресс.</p><h1>Как использовать этот проект</h1></p><p>Проект в основном посвящён сбору исторических данных, их подготовке к обучению и созданию токенизатора. Я не буду рассматривать полный процесс обучения LLM, для этого обратитесь к nanoGPT Андрея Карпати.</p><h1>Шаг 1: Сбор и подготовка исторических текстов</h1></p><p>Соберите .txt-файлы книг, документов и пр. из вашего выбранного периода (например, Лондон 1800-1850).</p><p>Вы можете использовать download_texts_improved.py для загрузки книг, если нужно.</p><p>Очистите текстовые файлы с помощью скрипта или вручную удалите заголовки/футеры с Project Gutenberg, современные аннотации и ошибки OCR.</p><p>prepare_dataset.py должен подойти.</p><h1>Шаг 2: Построение собственного токенизатора</h1></p><p>Запустите train_tokenizer.py или train_tokenizer_hf.py на очищенных данных.
Вы получите vocab.json и merges.txt</p><p>Эти файлы определяют словарь и правила слияния для вашей модели</p><h1>Шаг 3: Обучение вашей модели (nanoGPT)</h1></p><p>См. <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT от Андрея Карпати</a> для процесса обучения.</p><p>Вы можете обучать другую LLM, если хотите, но я использовал nanoGPT</p><h1>FAQ</h1></p><h2>Что такое Selective Temporal Training?</h2></p><p>Selective Temporal Training (STT) — это методология машинного обучения, при которой все обучающие данные строго отбираются в рамках определённого исторического периода. Это делается для моделирования языка и знаний той эпохи без влияния современных концепций. Например, моя текущая модель (v0.5) обучена исключительно на данных 1800-1875 годов, не дообучена, а обучена с нуля, благодаря чему результат отражает лингвистический стиль и исторический контекст того времени.</p><h2>Почему не использовать просто дообучение или LoRA?</h2></p><p>В этом проекте я пытаюсь создать языковую модель, не затуманенную современными предубеждениями. Если я дообучу что-то вроде GPT-2, она уже предварительно обучена, и эта информация не исчезнет. Если обучать с нуля, языковая модель не будет притворяться старой — она такой и будет. Цель проекта — создать нечто, что сможет рассуждать исключительно на основе знаний из книг Лондона, изданных между 1800 и 1850.</p><h2>Какие данные использовались для обучения?</h2></p><p>Я использую книги, юридические документы, газеты и другие тексты Лондона 1800–1850 годов. В приведённом списке около 200, но для первого обучения я использовал только 50 файлов, примерно ~187 МБ. Список документов можно посмотреть по ссылке:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Какой размер модели версии 0?</h2></p><p>Эта модель сейчас очень мала, я делаю это для удовольствия и строго придерживаюсь правила обучения только на старых источниках. В ней почти 16 миллионов параметров, но я начну собирать больше старых текстов для следующего обучения. Буду делиться обновлениями по ходу дела.</p><h2>Характеристики для обучения?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
ОЗУ: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-07 
    </div>
    
</body>
</html>