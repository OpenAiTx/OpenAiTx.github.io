<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - LLM, обученная только на данных из определённых временных периодов для снижения современного уклона</title>
    <meta name="description" content="LLM, обученная только на данных из определённых временных периодов для снижения современного уклона">
    <meta name="keywords" content="TimeCapsuleLLM, Russian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "LLM, обученная только на данных из определённых временных периодов для снижения современного уклона",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-ru.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Russian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Язык</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (coming soon)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (coming soon)</a> |
        | <a href="#" title="Coming soon">ไทย (coming soon)</a> |
        | <a href="#" title="Coming soon">Français (coming soon)</a>
        | <a href="#" title="Coming soon">Deutsch (coming soon)</a>
        | <a href="#" title="Coming soon">Español (coming soon)</a>
        | <a href="#" title="Coming soon">Italiano (coming soon)</a>
        | <a href="#" title="Coming soon">Русский (coming soon)</a>
        | <a href="#" title="Coming soon">Português (coming soon)</a>
        | <a href="#" title="Coming soon">Nederlands (coming soon)</a>
        | <a href="#" title="Coming soon">Polski (coming soon)</a>
        | <a href="#" title="Coming soon">العربية (coming soon)</a>
        | <a href="#" title="Coming soon">فارسی (coming soon)</a>
        | <a href="#" title="Coming soon">Türkçe (coming soon)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (coming soon)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (coming soon)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
LLM, обученная только на данных определённых исторических периодов для уменьшения современного смещения.</p><p>Представьте, если бы ИИ-модель не просто притворялась исторической, а действительно такой была.</p><p>Построено на основе <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT Андрея Карпаты</a>. Основные обучающие скрипты и архитектура модели — его работа.</p><h1>Цели проекта</h1></p><p>TimeCapsule LLM — это экспериментальный проект, который будет обучаться только на текстах, написанных в определённые исторические периоды. Цель — смоделировать мировоззрение и язык конкретных исторических эпох.</p><h1>Почему дообучения недостаточно</h1></p><p>Если просто дообучить предварительно обученную модель, ваша LLM всё равно будет знать современные концепции. Конечно, достичь нулевого современного смещения сложно, но я хочу максимально к этому приблизиться. Отсутствие современного смещения требует обучения модели с нуля.</p><h1>Ожидаемые результаты</h1></p><p>Надеюсь, когда модель будет готова, она не будет знать современных концепций и не сможет рассуждать за пределами того, чему её обучали. Она не должна распознавать современную лексику/понятия и, надеюсь, не будет генерировать современные знания.</p><h1>Ход проекта</h1></p><h2>9 июля 2025</h2></p><p>Я выбрал временной период: 1800-1850 и регион: Лондон</p><p>Я собрал список текстов, книг, документов</p><p>На данный момент у меня есть 50 файлов в формате txt, скоро начну обучение NanoGPT</p><p>Буду обновлять это описание по мере продвижения</p><h2>13 июля 2025</h2></p><p>Обучил nanoGPT на 187 МБ исторических текстовых данных.</p><h2>15 июля 2025</h2></p><p>Я начал скачивать тексты для второго этапа обучения. Всё беру с Internet Archive и расширил временной период до 1800-1875. Чтобы получить разнообразные тексты, можно использовать фильтры по тематике, месту публикации, периоду и темам на Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>16 июля 2025</h2></p><p>Я скачал около 500 txt-файлов с Internet Archive и после очистки (удаление пробелов, заголовков Gutenberg и т.п.) получил около 500 МБ данных. Это очень маленький датасет, но в прошлый раз я обучал на 187 МБ, так что должна быть хотя бы какая-то заметная разница в результате после обучения второй модели. Надеюсь, что эта модель будет хотя бы создавать более связные предложения, которые имеют смысл. Конечно, гарантий нет, ведь датасет всё ещё очень мал, но это больше, чем в прошлый раз.</p><p>Это должно быть выполнимо на моём собственном оборудовании, что хорошо, потому что я надеюсь увидеть хоть какие-то улучшения до того, как перейду к большему датасету, для которого придётся арендовать GPU. Но не волнуйтесь, я всё равно планирую арендовать GPU в ближайшее время, просто хочу быть уверен, что мой датасет максимально курирован и чист. Одна из проблем — очистка, во многих txt-файлах есть мусор. Скрипты, которые я использовал для очистки, работают, но не на 100%.</p><p>Я буду обучать этот датасет сегодня, это должно занять около 4-5 часов. Как только всё будет готово и я протестирую модель, дам обновления. Спасибо всем, кто следит за проектом, некоторые даже скидывали мне ссылки на OCR-ресурсы, так что спасибо! Надеюсь, больше людей попробует такой подход и поэкспериментирует со своими датасетами.</p><h2>28 июля 2025</h2></p><p>Я загрузил v0.5 на Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">посмотреть здесь</a>, если интересно. Теперь вы можете скачать мой репозиторий и запускать его локально. К сожалению, nanoGPT не работает нативно с HuggingFace, поэтому модель нужно скачивать и запускать локально.</p><p>Также я начну собирать данные для следующей обучающей сессии, думаю, потребуется в 5-10 раз больше данных, чтобы добиться способности к рассуждению.</p><h3>Обновление по обучению</h3></p><p>Я начал обучение на корпусе 435 МБ (108 млн токенов), сейчас всё идёт довольно гладко. Потери на обучении снизились с 10.9 до 4.9 за первые 2800 итераций. Ожидаю, что всё займет около 8-9 часов. Как только всё закончится, выложу ещё одно обновление.</p><h2>17 июля 2025 2:13AM</h2></p><p>Обучение второй модели завершено, мой 4060 справился за 8 часов 40 минут (3,900 итераций/ч) для 33,000 итераций (5 эпох). Итоговая потеря на обучении составила 3.73. Результаты были удивительно хорошими — модель теперь действительно генерирует связные предложения в стиле XIX века.</p><h1>Поведение и ограничения модели V0</h1></p><p>Ранние запросы показывают, что модель отвечает на языке и в манере 1800-х. Например, я спросил "Who art Henry?" и она ответила "I know that man, I have did not a black, the storm." — да, это бессмысленная фраза, но LLM распознаёт, что я спрашиваю о человеке.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>
Современные концепции не упоминаются, выводы содержат в основном слова и формулировки из 1800-х годов.</p><p>Модели ещё предстоит много доработки, обучение на 187 МБ не даст вам модель, способную генерировать тексты со сложным рассуждением.</p><p>Сейчас она генерирует предложения, в которых нет полноценной структуры, и в целом они не имеют смысла, но это нормально для такого объёма обучения.</p><h1>Поведение и ограничения модели V0.5</h1></p><p>Это хороший прогресс по сравнению с предыдущей версией. Стиль письма и словарный запас викторианские, и почти каждое предложение грамматически правильно с корректной пунктуацией. И опять же, модель обучена с нуля, поэтому ограничена темами 1800-х годов.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Пример вывода"></p><p>Присутствует много фактологических галлюцинаций. Большинство (практически 100%) деталей (даты, события, исторические личности) вымышлены. Кроме того, предложения не особо связаны между собой, иногда 2 предложения могут перекликаться, но не более. Ещё одна проблема — иногда появляется случайный футер “Digitized by Google”, так что в следующий раз нужно будет особенно тщательно чистить тексты. В целом я очень доволен результатом, пока это далеко не LLM, но уже вполне генератор предложений.</p><p>Я много учусь и в ближайшие недели начну понимать, что нужно улучшать. Файлы скоро выложу!</p><h1>Ближайшие планы</h1></p><p>(Выполнено) Я начинаю работу над версией 0.5, вместо обучения на 50 книгах, буду использовать по возможности 500-600. Сейчас я обучаю nanoGPT на книгах 1800-1850 годов, в частности из Лондона. Есть определённые сложности, например, найти книги без современных правок и интерпретаций, только оригинальные из выбранного периода.</p><p>Я хочу обучить новую модель (v1) на гораздо большем корпусе, возможно в 5-10 раз большем, чем у v0.5. Моя цель — проверить, появятся ли способности к рассуждению только за счёт выборочного временного обучения, это будет сложнее, и я не уверен, возможно ли вообще из-за ограничений исторических данных. В ближайшие недели попытаюсь собрать достаточно данных для корпуса 5-10 ГБ. Думаю, если удастся найти чистые и качественные данные и арендовать GPU, прогресс будет.</p><h1>Как использовать этот проект</h1></p><p>Проект в основном посвящён сбору исторических данных, их подготовке для обучения и созданию токенизатора. Полный процесс обучения LLM не описываю — за этим обращайтесь к nanoGPT Андрея Карпати.</p><h1>Шаг 1: Сбор и подготовка исторических текстов</h1></p><p>Соберите .txt файлы книг, документов и т.д. из выбранного периода (например, Лондон 1800-1850).</p><p>Можно воспользоваться download_texts_improved.py для скачивания книг при необходимости.</p><p>Очистите файлы скриптом или вручную: уберите заголовки/футеры из Project Gutenberg, современные примечания или ошибки OCR.</p><p>prepare_dataset.py должен подойти.</p><h1>Шаг 2: Построение собственного токенизатора</h1></p><p>Запустите train_tokenizer.py или train_tokenizer_hf.py на очищенных данных.
Это даст вам vocab.json и merges.txt</p><p>Эти файлы определяют словарь и правила слияния для вашей модели</p><h1>Шаг 3: Обучение модели (nanoGPT)</h1></p><p>См. <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT от Андрея Карпати</a> для процесса обучения.</p><p>Можно обучать и другую LLM, но я использовал nanoGPT</p><h1>FAQ</h1></p><h2>Что такое Selective Temporal Training?</h2></p><p>Selective Temporal Training (STT) — методика машинного обучения, при которой все данные для обучения тщательно отбираются из определённого исторического периода. Это делается для моделирования языка и знаний той эпохи без влияния современных концепций. Например, текущая модель (v0.5) обучена только на данных 1800–1875 годов, не дообучалась, а сразу училась с нуля, поэтому выдаёт текст, отражающий стиль и исторический контекст того времени.</p><h2>Почему не просто использовать дообучение или LoRA?</h2></p><p>В этом проекте я пытаюсь создать языковую модель, не затронутую современными предубеждениями. Если дообучать что-то вроде GPT-2, предобученная информация никуда не денется. Если обучать с нуля — модель не будет "притворяться" старой, она будет такой на самом деле. Цель сейчас — сделать модель, способную рассуждать исключительно на основе знаний из лондонских книг 1800–1850 годов.</p><h2>Какие данные использовались для обучения?</h2></p><p>Я использую книги, правовые документы, газеты и другие тексты Лондона 1800–1850 годов. В ссылке — около 200 наименований, но для первой тренировки было взято только 50 файлов, около 187 МБ. Список документов можно посмотреть тут:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Какой размер у модели версии 0?</h2></p><p>Сейчас эта модель очень мала, я делаю это для удовольствия и строго следую правилу — никаких современных источников. В ней около 16 миллионов параметров, но я собираюсь подобрать больше старых текстов для следующей тренировки. Буду делиться обновлениями.</p><h2>Характеристики обучения?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F 
ОЗУ: 16 ГБ DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>