<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - LLM, обученная только на данных из определённых временных периодов для снижения современного уклона</title>
    <meta name="description" content="LLM, обученная только на данных из определённых временных периодов для снижения современного уклона">
    <meta name="keywords" content="TimeCapsuleLLM, Russian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "LLM, обученная только на данных из определённых временных периодов для снижения современного уклона",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-ru.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">Russian</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Язык</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (скоро)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (скоро)</a> |
        | <a href="#" title="Coming soon">ไทย (скоро)</a> |
        | <a href="#" title="Coming soon">Français (скоро)</a>
        | <a href="#" title="Coming soon">Deutsch (скоро)</a>
        | <a href="#" title="Coming soon">Español (скоро)</a>
        | <a href="#" title="Coming soon">Italiano (скоро)</a>
        | <a href="#" title="Coming soon">Русский (скоро)</a>
        | <a href="#" title="Coming soon">Português (скоро)</a>
        | <a href="#" title="Coming soon">Nederlands (скоро)</a>
        | <a href="#" title="Coming soon">Polski (скоро)</a>
        | <a href="#" title="Coming soon">العربية (скоро)</a>
        | <a href="#" title="Coming soon">فارسی (скоро)</a>
        | <a href="#" title="Coming soon">Türkçe (скоро)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (скоро)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (скоро)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
LLM, обученная только на данных из определённых исторических периодов для снижения современной предвзятости.</p><p>Представьте себе ИИ-модель, которая не просто изображает историческую личность, но действительно ею является.</p><p>Построено на <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT Андрея Карпаты</a> Основные обучающие скрипты и архитектура модели — его работа. </p><h1>Цели проекта</h1></p><p>TimeCapsule LLM — это экспериментальный проект, который будет обучаться только на текстах, написанных в определённые исторические периоды. Цель — смоделировать мировоззрение и язык конкретных исторических эпох.</p><h1>Почему дообучения недостаточно</h1></p><p>Если просто дообучить предварительно обученную модель, ваша LLM всё равно будет знать современные концепции. Конечно, достичь нулевого современного уклона сложно, но я хочу максимально приблизиться к этому. Для отсутствия современного уклона требуется обучение модели с нуля.</p><h1>Ожидаемые результаты</h1></p><p>Надеюсь, когда модель будет готова, она не будет знать современных концепций и не сможет рассуждать за пределами обучающих данных. Она не должна распознавать современную лексику/понятия и не должна генерировать современные знания.</p><h1>Обновления прогресса</h1></p><h2>9 июля 2025</h2></p><p>Я выбрал временной период 1800-1850 и регион: Лондон</p><p>Я собрал список текстов, книг, документов</p><p>На данный момент у меня есть 50 файлов в формате txt, и я скоро начну обучение NanoGPT</p><p>Буду обновлять это по мере продвижения</p><h2>13 июля 2025</h2></p><p>Обучил nanoGPT на 187 МБ исторических текстовых данных.</p><h2>15 июля 2025</h2></p><p>Я начал скачивать тексты для второго запуска обучения. Я беру всё с Internet Archive и расширил временной период до 1800-1875. Для получения разнообразных текстов можно использовать фильтры по теме, месту публикации, периоду и тематике на Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>16 июля 2025</h2></p><p>Я скачал около 500 txt файлов с Internet Archive, и после очистки (просто удаление пробелов, заголовков Гутенберга и т.д.) у меня получилось около 500 МБ данных. Это крошечный датасет, но в прошлый раз я обучал на 187 МБ, так что должен быть хотя бы какой-то заметный прирост в результате после обучения второй модели. Надеюсь, эта модель хотя бы сможет генерировать более связные предложения, которые имеют смысл. Конечно, это не гарантия, ведь датасет очень маленький, но всё же это больше, чем в прошлый раз.</p><p>Это должно быть выполнимо на моём оборудовании, и это хорошо, потому что я, возможно, увижу какие-то улучшения, прежде чем перейду к большему датасету, для которого мне придётся арендовать GPU. Но не волнуйтесь, я всё ещё планирую арендовать GPU скоро, но прежде хочу удостовериться, что мой датасет максимально чистый и хорошо подобран. Одна из проблем — очистка, многие txt-файлы содержат бессмыслицу. Скрипты, которые я использовал для очистки, работают, но они не на 100% эффективны.</p><p>Я буду обучать этот датасет сегодня, это должно занять около 4-5 часов. Как только закончу и протестирую, дам обновления. Спасибо всем, кто интересуется моим проектом, даже были люди, которые присылали мне ссылки на OCR-ресурсы, так что спасибо! Надеюсь, больше людей попробует это и поэкспериментирует со своими датасетами.</p><h2>12 августа 2025</h2></p><p>Версия 1 завершена, эта версия использует Phi 1.5 (700M параметров) и обучена на датасете почти из 7000 текстов. Основное улучшение этой модели — её способность иногда ссылаться на реальные имена и исторические события. Версии 0 и 0.5 не могли этого делать, когда я задавал этим моделям что-то вроде "В 1821 году", результатом были только галлюцинации, то есть вымышленные события. Пример с Версией 1:</p><p>Мой запрос: Был год Господень 1834,</p><p>Ответ: "Был год Господень 1834, и улицы Лондона были наполнены протестами и петициями. Причина, как многие вспоминали, не была связана с частным порядком, но, заняв тот же день во времена лорда Пальмерстона, публика получит краткое сообщение о трудностях, с которыми столкнулся закон. Глубокое сожаление вызывает то, что нынешние события в истории мира ясны и, следовательно, будут известны. Не правда, что те самые люди, которые первыми поселились в Евангелии в Иерусалиме, должны иметь столь обширную и интересную летопись процветания и успеха".</p><p>Сначала я подумал, что это совпадение, но посмотрите на это: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>Обновление по обучению</h3></p><p>Я начал обучение на корпусе размером 435 МБ (108 миллионов токенов), сейчас всё идёт довольно гладко. Потеря на обучении упала с 10.9 до 4.9 за первые 2800 итераций. Ожидаю, что завершение займет около 8–9 часов. После окончания выложу еще одно обновление.</p><h2>17 июля 2025</h2></p><p>Обучение второй модели завершено, моя 4060 справилась примерно за 8 часов 40 минут (3 900 итераций/час) для 33 000 итераций (5 эпох). Финальная потеря на обучении — 3.73. Результаты удивительно хорошие — теперь она действительно генерирует связные предложения в стиле XIX века.</p><h2>28 июля 2025</h2></p><p>Я выложил v0.5 на Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Посмотрите</a> если интересно. Теперь вы можете скачать мой репозиторий и запустить его локально. К сожалению, nanoGPT не работает нативно с HuggingFace, поэтому придётся скачивать и запускать модель локально.</p><p>Также я начну собирать данные для следующего обучающего запуска, думаю мне понадобится в 5–10 раз больше данных для достижения возможности рассуждения.</p><h2>2 августа 2025</h2></p><p>Скоро начну работу над Версией 1. Придётся перейти с архитектуры nanoGPT на что-то более современное. У меня на примете несколько open-source архитектур LLM, включая: OpenLLaMA v3, Phi-2 и Qwen 1.5B. А чтобы перейти к V1, потребуется тщательно собрать гораздо больший и разнообразный датасет. Мне понадобится как минимум 5 ГБ чистых обучающих данных.</p><h1>Поведение и ограничения модели V0</h1></p><p>Ранние запросы показывают, что модель отвечает языком и манерой XIX века. Например, я задал "Who art Henry?" и она ответила "I know that man, I have did not a black, the storm." — да, это предложение бессмысленно, но LLM распознала, что я спрашиваю о человеке.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>Нет упоминаний современных концепций, вывод содержит в основном слова и обороты XIX века.</p><p>Модели всё ещё требуется много доработки, обучение на 187 МБ не даст модель, способную генерировать текст со сложным рассуждением.</p><p>
Сейчас модель генерирует предложения, которые не имеют полноценной структуры и вообще не имеют смысла, но это нормально для такого размера обучающей выборки.</p><h1>Поведение и ограничения модели V0.5</h1></p><p>Это заметное улучшение по сравнению с предыдущей моделью. Стиль написания и словарный запас викторианские, почти каждое предложение грамматически правильное с правильной пунктуацией. И снова — модель обучалась с нуля, поэтому она придерживается тематики XIX века.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Пример вывода TimeLockLLM"></p><p>Присутствует множество фактических галлюцинаций. Очень много (около 100%) деталей (даты, события, исторические личности) вымышлены. Также предложения практически не связаны друг с другом, иногда только два предложения могут быть связаны, но не более. Еще одна проблема — иногда появляется случайный футер “Digitized by Google”, поэтому в следующий раз нужно обязательно тщательно очищать тексты. В целом я доволен результатами, пока это далеко не LLM, но определенно генератор предложений.</p><p>Я многому учусь и в ближайшие недели начну выяснять, что нужно улучшить. Скоро загружу файлы!</p><h1>Поведение и ограничения модели V1</h1></p><p>Скоро я загружу примеры вывода и проведу сравнение между тремя моделями с одинаковым запросом. Также я загружу V1 на huggingface, как делал с предыдущей версией, мой аккаунт здесь: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>Предстоящие планы</h1></p><p>(Выполнено) Я начинаю работу над версией 0.5, вместо обучения на 50 книгах буду использовать 500-600. Сейчас я обучаю nanoGPT на книгах 1800-1850 годов, преимущественно из Лондона. Есть определённые сложности, например, нужно убедиться, что найденные книги не обновлены или не имеют современных интерпретаций, а являются нетронутыми из выбранного периода.</p><p>Я хочу обучить новую модель (v1) на гораздо большем корпусе, возможно в 5-10 раз больше, чем тот, что был для v0.5. Моя цель — посмотреть, смогут ли появиться способности к рассуждению только за счет выборочного временного обучения, это более сложная задача, и я не уверен, что это возможно из-за ограничений исторических данных. В ближайшие недели я постараюсь собрать данных для корпуса размером 5-10 ГБ. Я верю, что если удастся получить чистые и качественные данные и арендовать GPU, прогресс будет.</p><h1>Как использовать этот проект</h1></p><p>Проект в основном посвящён сбору исторических данных, их подготовке к обучению и созданию токенизатора. Я не буду описывать весь процесс обучения LLM, для этого обратитесь к nanoGPT Андрея Карпаты.</p><h1>Шаг 1: Сбор и подготовка исторических текстов</h1></p><p>Соберите .txt-файлы книг, документов и др. из вашего выбранного периода (например, Лондон 1800-1850).</p><p>Вы можете использовать download_texts_improved.py для автоматической загрузки книг при необходимости.</p><p>Очистите текстовые файлы скриптом или вручную — удалите заголовки/футеры с Project Gutenberg, современные аннотации и ошибки OCR.</p><p>prepare_dataset.py должен работать корректно.</p><h1>Шаг 2: Создание собственного токенизатора</h1></p><p>Запустите train_tokenizer.py или train_tokenizer_hf.py на очищенных данных.
В результате вы получите vocab.json и merges.txt
Эти файлы определяют словарь и правила слияния для вашей модели</p><h1>Шаг 3: Обучение вашей модели (nanoGPT)</h1></p><p>Обратитесь к <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT от Andrej Karpathy</a> для ознакомления с процессом обучения.</p><p>Вы можете обучать другую LLM, если хотите, но я использовал nanoGPT</p><h1>FAQ</h1></p><h2>Что такое селективное временное обучение?</h2></p><p>Selective Temporal Training (STT) — это методология машинного обучения, при которой все обучающие данные специально подбираются так, чтобы соответствовать определённому историческому периоду. Это делается для моделирования языка и знаний той эпохи без влияния современных понятий. Например, текущая модель (v0.5) обучена исключительно на данных 1800–1875 годов, она не дообучалась, а обучалась с нуля, что приводит к результату, отражающему языковой стиль и исторический контекст того времени.</p><h2>Почему бы просто не использовать дообучение или LoRA?</h2></p><p>Для этого проекта я пытаюсь создать языковую модель, свободную от современных предвзятостей. Если я буду дообучать что-то вроде GPT-2, она уже предварительно обучена, и эта информация не исчезнет. Если я обучу модель с нуля, она не будет притворяться старой — она действительно будет такой. Цель проекта сейчас — создать что-то, что способно рассуждать исключительно, используя знания из лондонских книг, опубликованных между 1800 и 1850 годами.</p><h2>Какие данные вы использовали для обучения?</h2></p><p>Я использую книги, юридические документы, газеты и другие тексты из Лондона 1800–1850 годов. В списке, на который я дал ссылку, около 200 документов, но для первого обучения я использовал только 50 файлов объёмом примерно ~187 МБ. Список документов можно посмотреть здесь:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Какого размера модели?</h2></p><p>V0: 16M параметров</p><p>V0.5: 123M параметров</p><p>V1: 700M параметров</p><h1>Технические характеристики обучения</h1></p><p>#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F
ОЗУ: 16GB DDR5.</p><p>#V1</p><p>GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>