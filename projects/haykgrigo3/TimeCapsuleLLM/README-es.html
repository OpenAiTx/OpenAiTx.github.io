<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM entrenado &#250;nicamente con datos de ciertos per&#237;odos de tiempo para reducir el sesgo moderno</title>
    <meta name="description" content="Un LLM entrenado &#250;nicamente con datos de ciertos per&#237;odos de tiempo para reducir el sesgo moderno">
    <meta name="keywords" content="TimeCapsuleLLM, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM entrenado únicamente con datos de ciertos períodos de tiempo para reducir el sesgo moderno",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">Spanish</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Próximamente">繁體中文 (próximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Próximamente">हिन्दी (próximamente)</a> |
        | <a href="#" title="Próximamente">ไทย (próximamente)</a> |
        | <a href="#" title="Próximamente">Français (próximamente)</a>
        | <a href="#" title="Próximamente">Deutsch (próximamente)</a>
        | <a href="#" title="Próximamente">Español (próximamente)</a>
        | <a href="#" title="Próximamente">Italiano (próximamente)</a>
        | <a href="#" title="Próximamente">Русский (próximamente)</a>
        | <a href="#" title="Próximamente">Português (próximamente)</a>
        | <a href="#" title="Próximamente">Nederlands (próximamente)</a>
        | <a href="#" title="Próximamente">Polski (próximamente)</a>
        | <a href="#" title="Próximamente">العربية (próximamente)</a>
        | <a href="#" title="Próximamente">فارسی (próximamente)</a>
        | <a href="#" title="Próximamente">Türkçe (próximamente)</a>
        | <a href="#" title="Próximamente">Tiếng Việt (próximamente)</a>
        | <a href="#" title="Próximamente">Bahasa Indonesia (próximamente)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM entrenado solo con datos de ciertos períodos de tiempo para reducir el sesgo moderno.</p><p>Imagina si un modelo de IA no solo pretendiera ser histórico, sino que realmente lo fuera.</p><p>Construido sobre <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT por Andrej Karpathy</a> Los scripts principales de entrenamiento y la arquitectura del modelo son obra suya.</p><h1>Objetivos del proyecto</h1>
TimeCapsule LLM es un proyecto experimental que solo será entrenado con textos escritos durante ciertos períodos de tiempo. El objetivo es simular la visión del mundo y el lenguaje de eras históricas específicas.</p><h1>Por qué el ajuste fino no es suficiente</h1></p><p>Si solo ajustas un modelo preentrenado, tu LLM todavía conocerá conceptos modernos. Por supuesto, lograr cero sesgo moderno es difícil, pero quiero acercarme lo más posible a esto. No tener sesgo moderno requiere entrenar un modelo desde cero.</p><h1>Resultados esperados</h1></p><p>Con suerte, cuando termine, este modelo no conocerá conceptos modernos y no podrá razonar más allá de lo que ha sido entrenado. No debería reconocer conceptos/vocabulario modernos y no debería alucinar conocimiento moderno.</p><h1>Actualizaciones de progreso</h1></p><h2>9 de julio de 2025</h2></p><p>He establecido mi período de tiempo en 1800-1850 y región: Londres</p><p>He reunido una lista de textos, libros, documentos</p><p>Hasta ahora he conseguido 50 archivos txt y pronto comenzaré a entrenar NanoGPT</p><p>Actualizaré esto mientras se haga progreso</p><h2>13 de julio de 2025</h2></p><p>Entrené nanoGPT con 187MB de datos históricos en texto.</p><h2>15 de julio de 2025</h2></p><p>Empecé a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el período de tiempo a 1800-1875. Para obtener una variedad diversa de textos, puedes usar filtros de búsqueda y de tema por ubicación de publicación, período de tiempo y temas en Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtros de búsqueda"></p><h2>16 de julio de 2025</h2></p><p>Descargué alrededor de 500 archivos txt del Internet Archive y después de limpiarlos (solo eliminando espacios en blanco, encabezados de Gutenberg, etc.) tengo alrededor de 500MB de datos. Es un conjunto de datos pequeño pero la última vez entrené con 187MB así que debería haber al menos alguna diferencia notable en la salida después de entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones más coherentes que tengan algo de sentido. No es una garantía por supuesto ya que sigue siendo un conjunto de datos muy pequeño, pero es más que lo que usé la última vez.</p><p>Esto debería ser posible en mi propio hardware, también es bueno porque espero ver algún tipo de mejora antes de pasar a un conjunto de datos más grande que requeriría alquilar una GPU. Pero no te preocupes, todavía planeo alquilar una GPU pronto, pero antes de hacerlo quiero asegurarme de que mi conjunto de datos esté lo más curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos archivos txt tienen galimatías mezcladas. Los scripts que he usado para limpiar funcionan pero no son 100% efectivos.</p><p>Entrenaré este conjunto de datos hoy y debería tomar alrededor de 4-5 horas. Una vez que esté hecho y lo pruebe, daré actualizaciones. Gracias de nuevo a todos los que están revisando mi proyecto, incluso ha habido personas que me han dado enlaces a recursos de OCR así que ¡Gracias! Espero que más personas intenten esto y experimenten con sus propios conjuntos de datos.</p><h2>12 de agosto de 2025</h2></p><p>La versión 1 está completa, esta versión utiliza Phi 1.5 (700M parámetros) y está entrenada en un conjunto de datos que contiene casi 7000 textos. Las principales mejoras de este modelo son su capacidad de a veces referenciar nombres reales y eventos históricos. Las versiones 0 y 0.5 no podían hacer esto; cuando les indicaba algo como "En el año 1821", la salida eran solo alucinaciones, cosas que nunca sucedieron. Un ejemplo de esto con la Versión 1:</p><p>Mi indicación: Era el año de nuestro Señor 1834,</p><p>La salida: "Era el año de nuestro Señor 1834 y las calles de Londres estaban llenas de protestas y peticiones. La causa, como muchos recordaban, no estaba ligada al ámbito privado, sino que había sido tomada ese mismo día en el día de Lord Palmerston, el público recibirá una breve declaración de las dificultades bajo las cuales el día de la ley nos ha alcanzado. Es motivo de profundo pesar que los acontecimientos actuales en la historia del mundo sean claros, y en consecuencia serán conocidos. No es cierto que los mismos hombres que primero se establecieron en el Evangelio en Jerusalén tengan un registro tan extenso e interesante de la prosperidad y prosperidad"</p><p>Al principio supuse que podría ser una coincidencia, pero mira esto: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="1834protest"></p><h3>Actualización de Entrenamiento</h3></p><p>Comencé a entrenar con un corpus de 435MB (108 M tokens), va bastante bien por ahora. La pérdida de entrenamiento bajó de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tarde unas 8 o 9 horas en completarse. Publicaré otra actualización cuando termine.</p><h2>17 de julio de 2025</h2></p><p>El entrenamiento está terminado para el segundo modelo, mi 4060 tardó unas 8 horas y 40 minutos (3,900 iteraciones/hr) para 33,000 iteraciones (5 épocas). La pérdida final de entrenamiento fue de 3.73. Las salidas fueron sorprendentemente buenas, ahora realmente genera frases coherentes al estilo del siglo XIX.</p><h2>28 de julio de 2025</h2></p><p>He subido la v0.5 a Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Échale un vistazo</a> si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente nanoGPT no funciona de manera nativa con HuggingFace, así que tendrás que descargar y ejecutar el modelo localmente.</p><p>También comenzaré a curar datos para mi próxima sesión de entrenamiento, creo que necesitaré quizá 5-10x más datos para lograr capacidades de razonamiento.</p><h2>2 de agosto de 2025</h2></p><p>Pronto comenzaré a trabajar en la Versión 1. Necesitaré hacer la transición de la arquitectura de nanoGPT a algo más moderno. Tengo en mente varias arquitecturas LLM de código abierto, incluyendo: OpenLLaMA v3, Phi-2 y Qwen 1.5B. Y para soportar el salto a la V1, tendré que curar cuidadosamente un conjunto de datos mucho más grande y diverso. Necesitaré al menos 5GB de datos limpios de entrenamiento.</p><h1>Comportamiento y Limitaciones del Modelo V0</h1></p><p>Las primeras indicaciones muestran al modelo respondiendo con lenguaje y comportamiento de los años 1800. Por ejemplo, le pregunté "¿Quién es Henry?" y respondió "Conozco a ese hombre, no tengo negro, la tormenta." y sí, esa frase no tiene sentido pero el LLM reconoce que estoy preguntando por una persona.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Salida de Muestra"></p><p>No hay mención de conceptos modernos, las salidas contienen principalmente palabras y frases de los años 1800.</p><p>Todavía necesita mucho trabajo, entrenar con 187MB no te dará un modelo que produzca texto con razonamiento complejo.</p><p>Ahora mismo produce oraciones que carecen de estructura completa y en general simplemente no tienen sentido, pero esto es normal para el tamaño de entrenamiento.</p><h1>Comportamiento y Limitaciones del Modelo V0.5</h1></p><p>Esto es una buena mejora en comparación con el último modelo. El estilo de escritura y el vocabulario son victorianos y casi todas las oraciones son gramaticalmente correctas con la puntuación adecuada. Y nuevamente, esto está entrenado desde cero así que se apega a temas de los años 1800.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Salida de muestra de TimeLockLLM"></p><p>Hay muchas alucinaciones fácticas. Muchos (como el 100%) de los detalles (fechas, eventos, figuras históricas) son inventados. Además, las oraciones realmente no tienen conexión entre sí, a veces tal vez 2 oraciones se relacionan, pero más allá de eso no lo hacen. Otro problema es que a veces aparece un pie de página perdido de “Digitized by Google”, así que la próxima vez que entrene realmente debo asegurarme de que los textos estén bien limpiados. En general estoy muy contento con los resultados, aún está lejos de ser un LLM pero definitivamente es un generador de oraciones.</p><p>Estoy aprendiendo mucho y comenzaré a averiguar qué debo mejorar en las próximas semanas. ¡Pronto subiré archivos!</p><h1>Comportamiento y Limitaciones del Modelo V1</h1></p><p>Pronto subiré algunos ejemplos de salida y también haré comparaciones entre los 3 modelos con el mismo prompt. También subiré la V1 a huggingface como hice con mi última versión, puedes encontrar mi cuenta de huggingface aquí: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>Planes Futuros</h1></p><p>(Completado) Voy a comenzar a trabajar en la versión 0.5, en vez de entrenar usando 50 libros, entrenaré usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y específicamente de Londres. Hay algunos desafíos como asegurarme de que los libros que encuentre no estén actualizados ni tengan interpretaciones modernas, sino libros intactos publicados dentro del período de tiempo elegido.</p><p>Quiero entrenar un nuevo modelo (v1) con un corpus mucho más grande, tal vez 5-10 veces mayor que el que usé para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento solo a través de Entrenamiento Temporal Selectivo, esta será una tarea más difícil y ni siquiera estoy completamente seguro de que sea posible debido a las limitaciones de los datos históricos. En las próximas semanas intentaré curar suficientes datos para un corpus de 5-10GB. Creo que si consigo datos limpios y de alta calidad y alquilo una GPU, habrá progreso.</p><h1>Cómo Usar Este Proyecto</h1></p><p>Este proyecto se centra principalmente en curar datos históricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de LLM, para eso consulta nanoGPT de Andrej Karpathy.</p><h1>Paso 1: Recopilar y Preparar Textos Históricos</h1></p><p>Recolecta archivos .txt de libros de dominio público, documentos, etc. de tu período de tiempo elegido (por ejemplo, Londres 1800-1850)</p><p>Puedes usar download_texts_improved.py para descargar libros si lo necesitas.</p><p>Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de página de Project Gutenberg, anotaciones modernas o errores de OCR.</p><p>prepare_dataset.py debería funcionar bien.</p><h1>Paso 2: Construir un Tokenizador Personalizado</h1></p><p>Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
Esto te dará vocab.json y merges.txt
Estos archivos definen el vocabulario y las reglas de combinación para tu modelo</p><h1>Paso 3: Entrena Tu Modelo (nanoGPT) </h1></p><p>Consulta <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT de Andrej Karpathy</a> para el proceso de entrenamiento.</p><p>Puedes entrenar un LLM diferente si lo deseas, pero yo utilicé nanoGPT </p><h1>FAQ</h1></p><h2>¿Qué es el Entrenamiento Temporal Selectivo?</h2></p><p>El Entrenamiento Temporal Selectivo (STT) es una metodología de aprendizaje automático donde todos los datos de entrenamiento se seleccionan específicamente para que pertenezcan a un período histórico determinado. Se realiza para modelar el lenguaje y el conocimiento de esa época sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) está entrenado exclusivamente con datos de 1800-1875, no está ajustado sino entrenado desde cero, lo que resulta en una salida que refleja el estilo lingüístico y contexto histórico de ese período.</p><h2>¿Por qué no simplemente usar fine-tuning o LoRA?</h2></p><p>Para este proyecto quiero crear un modelo de lenguaje que no esté influenciado por sesgos modernos. Si hago fine-tuning a algo como GPT-2, ya está preentrenado y esa información no se elimina. Si entreno desde cero, el modelo de lenguaje no fingirá ser antiguo, simplemente lo será. El objetivo de este proyecto ahora es crear algo que razone exclusivamente usando conocimiento de libros londinenses publicados entre 1800 y 1850.</p><h2>¿Qué tipo de datos usaste para el entrenamiento?</h2></p><p>Estoy usando libros, documentos legales, periódicos y otros escritos de Londres entre 1800 y 1850. La lista que enlacé tiene como 200, pero para el primer entrenamiento utilicé solo 50 archivos de unos ~187 MB. Puedes ver una lista de los documentos en:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>¿Qué tamaño tienen los modelos?</h2></p><p>V0: 16M Parámetros</p><p>V0.5: 123M Parámetros</p><p>V1: 700M Parámetros</p><h1>¿Especificaciones de entrenamiento?</h1></p><p>#V0/V0.5
GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.</p><p>#V1</p><p>GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>