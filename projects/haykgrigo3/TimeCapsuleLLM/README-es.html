<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM entrenado &#250;nicamente con datos de ciertos per&#237;odos de tiempo para reducir el sesgo moderno.</title>
    <meta name="description" content="Un LLM entrenado &#250;nicamente con datos de ciertos per&#237;odos de tiempo para reducir el sesgo moderno.">
    <meta name="keywords" content="TimeCapsuleLLM, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM entrenado únicamente con datos de ciertos períodos de tiempo para reducir el sesgo moderno.",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Spanish</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (próximamente)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (próximamente)</a> |
        | <a href="#" title="Coming soon">ไทย (próximamente)</a> |
        | <a href="#" title="Coming soon">Français (próximamente)</a>
        | <a href="#" title="Coming soon">Deutsch (próximamente)</a>
        | <a href="#" title="Coming soon">Español (próximamente)</a>
        | <a href="#" title="Coming soon">Italiano (próximamente)</a>
        | <a href="#" title="Coming soon">Русский (próximamente)</a>
        | <a href="#" title="Coming soon">Português (próximamente)</a>
        | <a href="#" title="Coming soon">Nederlands (próximamente)</a>
        | <a href="#" title="Coming soon">Polski (próximamente)</a>
        | <a href="#" title="Coming soon">العربية (próximamente)</a>
        | <a href="#" title="Coming soon">فارسی (próximamente)</a>
        | <a href="#" title="Coming soon">Türkçe (próximamente)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (próximamente)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (próximamente)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM entrenado solo con datos de ciertos periodos de tiempo para reducir el sesgo moderno.</p><p>Imagina que un modelo de IA no solo pretende ser histórico, sino que realmente lo es.</p><p>Basado en <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT de Andrej Karpathy</a>. Los scripts principales de entrenamiento y la arquitectura del modelo son obra suya.</p><h1>Objetivos del Proyecto</h1></p><p>TimeCapsule LLM es un proyecto experimental que solo será entrenado con textos escritos durante ciertos periodos históricos. El objetivo es simular la visión del mundo y el lenguaje de épocas históricas específicas.</p><h1>Por qué el fine tuning no es suficiente</h1></p><p>Si solo haces fine tuning a un modelo pre-entrenado, tu LLM seguirá conociendo conceptos modernos. Por supuesto, lograr cero sesgo moderno es difícil, pero quiero acercarme lo máximo posible a esto. Obtener ningún sesgo moderno requiere entrenar un modelo desde cero.</p><h1>Resultados esperados</h1></p><p>Espero que, una vez terminado, este modelo no conozca conceptos modernos y no pueda razonar más allá de lo que ha sido entrenado. No debería reconocer conceptos/vocabulario modernos y espero que no alucine conocimientos actuales.</p><h1>Actualizaciones de Progreso</h1></p><h2>9 de julio de 2025</h2></p><p>He establecido mi periodo de tiempo entre 1800-1850 y región: Londres</p><p>He reunido una lista de textos, libros y documentos</p><p>Hasta ahora he conseguido 50 archivos txt y pronto comenzaré a entrenar NanoGPT</p><p>Actualizaré esto mientras siga avanzando</p><h2>13 de julio de 2025</h2></p><p>Entrené nanoGPT con 187MB de datos de texto históricos.</p><h2>15 de julio de 2025</h2></p><p>Empecé a descargar textos para la segunda ronda de entrenamiento. Estoy obteniendo todo del Internet Archive y he ampliado el periodo a 1800-1875. Para conseguir una gama diversa de textos, puedes usar los filtros de búsqueda por ubicación de publicación, periodo y temas en Internet Archive.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtros de Búsqueda"></p><h2>16 de julio de 2025</h2></p><p>Descargué unos 500 archivos txt de Internet Archive y tras limpiarlos (solo borrando espacios en blanco, cabeceras de Gutenberg, etc.) tengo unos 500MB de datos. Es un conjunto de datos pequeño pero la última vez entrené con 187MB así que debería haber al menos alguna diferencia notable en la salida tras entrenar el segundo modelo. Espero que este modelo pueda al menos producir oraciones más coherentes que tengan sentido. No es una garantía, por supuesto, ya que sigue siendo un conjunto de datos muy pequeño, pero es más de lo que usé la vez pasada.</p><p>Esto debería poder hacerse con mi propio hardware, lo cual es bueno porque espero ver mejoras antes de pasar a un conjunto de datos más grande que requeriría alquilar una GPU. Pero no te preocupes, aún planeo alquilar una GPU pronto, pero antes quiero asegurarme de que mi conjunto de datos esté lo más curado y limpio posible. Uno de los problemas que tengo es la limpieza, muchos de estos txt tienen galimatías mezclados. Los scripts de limpieza que he usado funcionan pero no son 100% efectivos.</p><p>Voy a entrenar este conjunto de datos hoy y debería tomar unas 4-5 horas. Cuando termine y lo pruebe, daré actualizaciones. ¡Gracias de nuevo a quienes revisan mi proyecto, incluso me han pasado enlaces a recursos de OCR así que gracias! Espero que más personas prueben esto y experimenten con sus propios conjuntos de datos.</p><h2>28 de julio de 2025</h2></p><p>He subido la v0.5 a Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">échale un vistazo</a> si quieres. Ahora puedes descargar mi repositorio y ejecutarlo localmente. Desafortunadamente nanoGPT no funciona de forma nativa con HuggingFace, así que tendrás que descargar y ejecutar el modelo localmente.</p><p>También comenzaré a curar datos para mi siguiente ronda de entrenamiento, creo que necesitaré quizá 5-10 veces más datos para lograr capacidades de razonamiento.</p><h3>Actualización de Entrenamiento</h3></p><p>Comencé a entrenar con un corpus de 435MB (108 M tokens), va bastante bien por ahora. La pérdida de entrenamiento bajó de 10.9 a 4.9 en las primeras 2800 iteraciones. Espero que tome unas 8 o 9 horas completarlo. Publicaré otra actualización cuando termine.</p><h2>17 de julio de 2025 2:13AM</h2></p><p>El entrenamiento ha terminado para el segundo modelo, mi 4060 tardó unas 8 horas y 40 minutos (3,900 iter/h) para 33,000 iteraciones (5 épocas). La pérdida final de entrenamiento fue 3.73. Los resultados fueron sorprendentemente buenos, ahora realmente genera frases coherentes al estilo del siglo XIX.</p><h1>Comportamiento y Limitaciones del Modelo V0</h1></p><p>Las primeras pruebas muestran que el modelo responde con lenguaje y comportamiento de los años 1800. Por ejemplo, lo estimulé con "Who art Henry?" y respondió "I know that man, I have did not a black, the storm." y sí, esa frase no tiene sentido pero el LLM reconoce que pregunto por una persona.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Salida de Ejemplo de TimeLockLLM"></p><p>No hay mención de conceptos modernos, las salidas contienen mayormente palabras y frases propias de los años 1800.</p><p>Todavía necesita mucho trabajo, entrenar con 187MB no te dará un modelo que produzca texto con razonamiento complejo.</p><p>Por ahora produce oraciones que carecen de una estructura completa y en general no tienen sentido, pero esto es normal para el tamaño de entrenamiento.</p><h1>Comportamiento y Limitaciones del Modelo V0.5</h1></p><p>Esto es una buena mejora comparado con el último modelo. El estilo de escritura y vocabulario es victoriano y casi cada frase es gramaticalmente correcta con la puntuación adecuada. Y nuevamente, esto se entrena desde cero, por lo que se apega a temas de los años 1800.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Salida de muestra de TimeLockLLM"></p><p>Hay muchas alucinaciones fácticas. Muchos (como el 100%) de los detalles (fechas, eventos, figuras históricas) son inventados. Además, las oraciones realmente no tienen conexión entre sí, a veces tal vez 2 frases se relacionan pero más allá de eso no. Otro problema es que a veces aparece un pie de página errante de “Digitized by Google”, así que la próxima vez que entrene tengo que asegurarme de limpiar bien los textos. En general, estoy muy feliz con los resultados, todavía está lejos de ser un LLM pero definitivamente es un generador de frases.</p><p>Estoy aprendiendo mucho y empezaré a averiguar qué debo mejorar en las próximas semanas. ¡Pronto subiré archivos!</p><h1>Planes Futuros</h1></p><p>(Completado) Voy a empezar a trabajar en la versión 0.5, en vez de entrenar usando 50 libros, entrenaré usando idealmente 500-600. Ahora mismo estoy entrenando nanoGPT usando libros de 1800-1850 y específicamente de Londres. Hay algunos desafíos como asegurarme de que los libros que encuentro no estén actualizados o tengan interpretaciones modernas, sino libros intactos publicados dentro de mi periodo elegido.</p><p>Quiero entrenar un nuevo modelo (v1) con un corpus mucho más grande, tal vez 5-10 veces mayor que el que usé para v0.5. Mi objetivo es ver si puedo lograr que emerjan habilidades de razonamiento solo con Entrenamiento Temporal Selectivo, será una tarea más difícil y ni siquiera estoy seguro de si es posible debido a las limitaciones de datos históricos. En las próximas semanas trataré de reunir suficientes datos para un corpus de 5-10GB. Creo que si consigo datos limpios y de alta calidad y alquilo una GPU, habrá progreso.</p><h1>Cómo Usar Este Proyecto</h1></p><p>Este proyecto se centra principalmente en recopilar datos históricos, prepararlos para el entrenamiento y construir un tokenizador. No voy a cubrir el proceso completo de entrenamiento de un LLM, para eso consulta nanoGPT por Andrej Karpathy.</p><h1>Paso 1: Recopilar y Preparar Textos Históricos</h1></p><p>Reúne archivos .txt de libros de dominio público, documentos, etc. del periodo que elijas (por ejemplo, Londres 1800-1850)</p><p>Puedes usar download_texts_improved.py para descargar libros si lo necesitas.</p><p>Limpia los archivos de texto usando un script o elimina manualmente encabezados/pies de página de Project Gutenberg, anotaciones modernas o errores de OCR.</p><p>prepare_dataset.py debería funcionar bien.</p><h1>Paso 2: Construir un Tokenizador Personalizado</h1></p><p>Ejecuta train_tokenizer.py o train_tokenizer_hf.py en los datos limpios.
Esto te dará vocab.json y merges.txt</p><p>Estos archivos definen el vocabulario y las reglas de combinación para tu modelo</p><h1>Paso 3: Entrena Tu Modelo (nanoGPT)</h1></p><p>Consulta <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT por Andrej Karpathy</a> para el proceso de entrenamiento.</p><p>Puedes entrenar otro LLM si lo deseas, pero yo usé nanoGPT</p><h1>FAQ</h1></p><h2>¿Qué es el Entrenamiento Temporal Selectivo?</h2></p><p>El Entrenamiento Temporal Selectivo (STT) es una metodología de aprendizaje automático donde todos los datos de entrenamiento se seleccionan específicamente para que pertenezcan a un periodo histórico concreto. Se hace para modelar el lenguaje y conocimiento de esa época sin influencia de conceptos modernos. Por ejemplo, el modelo actual que tengo (v0.5) está entrenado con datos exclusivamente de 1800-1875, no está afinado sino entrenado desde cero, resultando en salidas que reflejan el estilo lingüístico y contexto histórico de ese periodo.</p><h2>¿Por qué no usar simplemente fine-tuning o LoRA?</h2></p><p>Para este proyecto estoy intentando crear un modelo de lenguaje sin sesgo moderno. Si afino algo como GPT-2, ya está pre-entrenado y esa información no desaparecerá. Si entreno desde cero el modelo no fingirá ser antiguo, simplemente lo será. El objetivo de este proyecto por ahora es crear algo que razone exclusivamente usando conocimientos de libros de Londres publicados entre 1800 y 1850.</p><h2>¿Qué tipo de datos usaste para entrenar?</h2></p><p>Estoy usando libros, documentos legales, periódicos y otros escritos de Londres entre 1800 y 1850. La lista que enlacé tiene como 200 pero para el primer entrenamiento solo usé 50 archivos de unos ~187 MB. Puedes ver la lista de documentos:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>¿Qué tamaño tiene el modelo Versión 0?</h2></p><p>Este modelo es muy pequeño por ahora, solo lo hago por diversión y siguiendo una regla estricta de no usar fuentes modernas. Tiene casi 16 millones de parámetros pero voy a empezar a recopilar más textos antiguos para iniciar otro entrenamiento de modelo. Daré actualizaciones en el camino.</p><h2>¿Especificaciones de Entrenamiento?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
Ram: 16GB DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>