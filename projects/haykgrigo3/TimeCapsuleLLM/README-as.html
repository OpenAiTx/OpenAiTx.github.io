<!DOCTYPE html>
<html lang="as">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - আধুনিক পক্ষপাত হ্ৰাস কৰিবলৈ কেৱল নিৰ্দিষ্ট সময়ৰ তথ্যত প্ৰশিক্ষিত LLM</title>
    <meta name="description" content="আধুনিক পক্ষপাত হ্ৰাস কৰিবলৈ কেৱল নিৰ্দিষ্ট সময়ৰ তথ্যত প্ৰশিক্ষিত LLM">
    <meta name="keywords" content="TimeCapsuleLLM, Assamese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "আধুনিক পক্ষপাত হ্ৰাস কৰিবলৈ কেৱল নিৰ্দিষ্ট সময়ৰ তথ্যত প্ৰশিক্ষিত LLM",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-as.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">Assamese</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 ভাষা</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">ইংৰাজী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">সৰল চীনা</a>
        | <a href="#" title="Coming soon">পৰম্পৰাগত চীনা (চিগৈ অহা)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">জাপানী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">কোৰিয়ান</a>
        | <a href="#" title="Coming soon">হিন্দী (চিগৈ অহা)</a> |
        | <a href="#" title="Coming soon">থাই (চিগৈ অহা)</a> |
        | <a href="#" title="Coming soon">ফ্ৰেঞ্চ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">জাৰ্মান (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">স্পেনিছ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ইটালিয়ান (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ৰাছিয়ান (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">পৰ্তুগীজ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ডাচ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">প'লিছ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">আৰবী (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ফাৰ্ছী (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">টাৰ্কী (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ভিয়েটনামিছ (চিগৈ অহা)</a>
        | <a href="#" title="Coming soon">ইন্দোনেছিয়ান (চিগৈ অহা)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
এলএলএম এটা কেৱল নিৰ্দিষ্ট সময়ৰ ডাটাতকৈ প্ৰশিক্ষণ দিয়া হৈছে যাতে আধুনিক পক্ষপাত কমোৱা যায়।</p><p>ভাবক, যদি এটা AI মডেল কেৱল ভাও নকৰিল, বৰং সঁচাকৈয়ে ঐতিহাসিক হ’ব পাৰে।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT by Andrej Karpathy</a>ৰ ওপৰত নিৰ্মিত। মূল প্ৰশিক্ষণ স্ক্ৰিপ্ট আৰু মডেল স্থাপত্য তেওঁৰ কাম।</p><h1>প্ৰকল্পৰ লক্ষ্যসমূহ</h1></p><p>TimeCapsule LLM এটা প্ৰয়োগধৰ্মী প্ৰকল্প যিটো কেৱল নিৰ্দিষ্ট সময়ৰ লিখিত পাঠ্যত প্ৰশিক্ষণ দিয়া হ’ব। উদ্দেশ্য হৈছে নিৰ্দিষ্ট ঐতিহাসিক যুগৰ দৃষ্টিভংগী আৰু ভাষা অনুকৰণ কৰা।</p><h1>কেৱল ফাইন-টিউনিং যথেষ্ট নহয় কিয়</h1></p><p>আপুনি যদি কেৱল প্ৰি-ট্ৰেইনড মডেল ফাইন-টিউন কৰে, তেতিয়াও আপোনাৰ এলএলএম এ আধুনিক ধাৰণা জানিব। সম্পূৰ্ণ আধুনিক পক্ষপাত মুক্ত হোৱাটো কঠিন, কিন্তু মই ইয়াৰ ওচৰলৈ যাব খুজিছোঁ। আধুনিক পক্ষপাত একেবাৰে নাথাকিবলৈ মডেলটো আৰম্ভণিৰে প্ৰশিক্ষণ দিয়াটো প্ৰয়োজন।</p><h1>প্ৰত্যাশিত ফলাফলসমূহ</h1></p><p>আশা কৰোঁ, সম্পূৰ্ণ হ’লে এই মডেলটোৱে আধুনিক ধাৰণা নাজানিব আৰু কেৱল যি তথ্যত প্ৰশিক্ষিত হৈছে তাৰ বাহিৰত যুক্তি কৰিব নোৱাৰিব। ই আধুনিক শব্দ/ধাৰণা চিনাকি নকৰিব লাগিব আৰু মই আশা কৰোঁ ই আধুনিক জ্ঞান কল্পনা নকৰে।</p><h1>অগ্ৰগতিৰ আপডেটসমূহ</h1></p><h2>জুলাই ৯, ২০২৫</h2></p><p>মই সময়সীমা ১৮০০-১৮৫০ আৰু অঞ্চল: লণ্ডন নিৰ্ধাৰণ কৰিছোঁ</p><p>মই পাঠ্য, কিতাপ, নথিপত্ৰৰ এটা তালিকা সংগ্ৰহ কৰিছোঁ</p><p>এতিয়ালৈকে মই ৫০টা txt ফাইল পাইছোঁ আৰু NanoGPT প্ৰশিক্ষণ আৰম্ভ কৰিম</p><p>অগ্ৰগতি হ’লে ইয়াত আপডেট দিম</p><h2>জুলাই ১৩, ২০২৫</h2></p><p>nanoGPTক ১৮৭MB ঐতিহাসিক পাঠ্য ডাটাত প্ৰশিক্ষণ দিছোঁ।</p><h2>জুলাই ১৫, ২০২৫</h2></p><p>দ্বিতীয় প্ৰশিক্ষণৰ বাবে পাঠ্য ডাউনলোড কৰা আৰম্ভ কৰিছোঁ। সকলো Internet Archiveৰ পৰা লৈছোঁ আৰু সময়সীমা ১৮০০-১৮৭৫লৈ বৃদ্ধি কৰিছোঁ। বিৱিধ পাঠ্য পাবলৈ Internet Archiveত বিষয়বস্তু, প্ৰকাশ স্থান, সময় আৰু বিষয়ৰ ফিল্টাৰ ব্যৱহাৰ কৰিব পাৰি।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>জুলাই ১৬, ২০২৫</h2></p><p>মই Internet Archiveৰ পৰা ৫০০টাৰ ওচৰৰ txt ফাইল ডাউনলোড কৰিছোঁ আৰু পৰিষ্কাৰ কৰাৰ পিছত (কেৱল whitespaces, Gutenberg header আদি ডিলিট কৰিছোঁ) প্ৰায় ৫০০MB ডাটা পাইছোঁ। ই অতি সৰু dataset, কিন্তু আগতে ১৮৭MBত প্ৰশিক্ষণ দিছিলো, সেয়ে দ্বিতীয় মডেল প্ৰশিক্ষণৰ পিছত outputত কিছুটা পাৰ্থক্য দেখা যাব লাগিব। আশা কৰোঁ এই মডেলটোৱে অধিক সংলগ্ন বাক্য উত্পন্ন কৰিব পাৰে। নিশ্চয়তা নাই, কাৰণ dataset এতিয়াও সৰু, কিন্তু আগতেৰ তুলনাত বেছি।</p><p>এইটো নিজৰ হাৰ্ডৱেৰতেই কৰিব পাৰিম, ভাল লাগিছে কাৰণ ডাঙৰ dataset লৈ যাওঁতে কিছু উন্নতি দেখিবলৈ পাবলৈ আশা কৰিছোঁ, যিটোত GPU ভাড়া লাগিব। কিন্তু চিন্তা নকৰিব, মই GPU ভাড়া ল’বই, কিন্তু তাৰ আগতে dataset যথেষ্ট চয়ন আৰু পৰিষ্কাৰ কৰিবলৈ চেষ্টা কৰিছোঁ। এজন সমস্যা হৈছে পৰিষ্কাৰ কৰাটো, বহু txt ফাইলত অচিনাকি শব্দ আছে। মোৰ ব্যৱহৃত স্ক্ৰিপ্টবোৰে কাম কৰে, কিন্তু ১০০% কার্যকৰী নহয়।</p><p>আজিৰে দিনত এই dataset প্ৰশিক্ষণ কৰিম আৰু প্ৰায় ৪-৫ ঘণ্টা লাগিব। শেষ হ’লে আৰু পৰীক্ষা কৰোঁতে আপডেট দিম। মোৰ প্ৰকল্প চাই থকা সকলোলৈ ধন্যবাদ, কিছুমানে OCR resourceৰ লিংকো দিছে, ধন্যবাদ! আশা কৰোঁ, আৰু মানুহে নিজৰ datasetৰে চেষ্টা কৰক আৰু প্ৰয়োগ কৰক।</p><h3>প্ৰশিক্ষণ আপডেট</h3></p><p>৪৩৫MB (১০৮M টোকেন) corpusত প্ৰশিক্ষণ আৰম্ভ কৰিছোঁ, বৰ্তমানে ভালেই চলি আছে। Train loss ১০.৯ৰ পৰা ৪.৯লৈ আহিল ২৮০০ iterationত। আশা কৰোঁ ৮-৯ ঘণ্টা লাগিব। শেষ হ’লে আপডেট দিম।</p><h2>জুলাই ১৭, ২০২৫ ২:১৩AM</h2></p><p>দ্বিতীয় মডেলৰ প্ৰশিক্ষণ শেষ, মোৰ ৪০৬০ত প্ৰায় ৮ ঘণ্টা ৪০ মিনিট (৩,৯০০ iteration/ঘণ্টা)ত ৩৩,০০০ iteration (৫ epoch) সম্পূৰ্ণ হ’ল। শেষত train loss ৩.৭৩। Output অদ্ভুত ভাল, বৰ্তমান সঁচাকৈয়ে ১৯শ শতিকীৰ সংলগ্ন বাক্য উত্পন্ন কৰে।</p><h2>জুলাই ২৮, ২০২৫</h2></p><p>v0.5 Hugging Faceত আপলোড কৰিছোঁ, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">চাওক</a> ইচ্ছা থাকিলে। এতিয়া মোৰ repo ডাউনলোড কৰি স্থানীয়ভাৱে চলাব পাৰিব। দুঃখজনকভাৱে nanoGPT HuggingFaceত native চলেন, সেয়ে model locally চলাব লাগিব।</p><p>এইবাৰ মই পৰৱৰ্তী প্ৰশিক্ষণৰ বাবে ডাটা চয়ন আৰম্ভ কৰিম, বিশ্বাস কৰোঁ ৫-১০ গুণ বেছি ডাটা লাগিব reasoning ক্ষমতা পাবলৈ।</p><h1>V0 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>প্ৰাথমিক promptত মডেলটোৱে ১৮০০ দশকৰ ভাষা আৰু আচৰণৰে প্ৰতিক্ৰিয়া দিছে। উদাহৰণস্বৰূপ, মই “Who art Henry?” দিছিলোঁ আৰু ই উত্তৰ দিলে “I know that man, I have did not a black, the storm.” আৰু সেই বাক্যৰ অৰ্থ নাই, কিন্তু LLMয়ে বুজিছে মোৰ প্ৰশ্নজন ব্যক্তি সম্পৰ্কে।</p><p>
<img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM নমুনা উত্পাদন"></p><p>আধুনিক ধাৰণাৰ কোনো উল্লেখ নাই, উত্পাদনত প্ৰায় সকলো শব্দ আৰু বাক্য গঠন ১৮০০ দশকৰ।</p><p>এতিয়াও বৰ্ধনীৰ প্ৰয়োজন, কেৱল ১৮৭MB-তকৈ শিকিলে জটিল যুক্তি সম্পন্ন পাঠ্য উত্পাদন কৰা মডেল পাব পৰা নাযায়।</p><p>এতিয়া এইটো এনে বাক্য উত্পাদন কৰে যি সম্পূৰ্ণ বাক্য গঠনৰ অভাৱ থাকে আৰু সাধাৰণতে বুজি নোপোৱা হয়, কিন্তু এইটো এই পৰিমাণৰ ডেটাৰ বাবে স্বাভাৱিক।</p><h1>V0.5 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>এইটো আগৰ মডেলতকৈ যথেষ্ট উন্নতি। লিখন শৈলী আৰু শব্দভাণ্ডাৰ ভিক্টোৰিয়ান আৰু প্ৰায় প্ৰতিটো বাক্য ব্যাকৰণগতভাৱে শুদ্ধ আৰু সঠিক যতিচিহ্নেৰে। আৰু এইটো সম্পূৰ্ণকৈ নতুনকৈ শিকোৱা, সেইবাবে ১৮০০ দশকৰ বিষয়বোৰত কেন্দ্ৰিত।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM নমুনা উত্পাদন"></p><p>অসত্য তথ্যৰ কল্পনা বহুত আছে। বহুতো (প্ৰায় ১০০%) তথ্য (তাৰিখ, ঘটনা, ঐতিহাসিক ব্যক্তি) গঢ়ি কৰা। লগতে বাক্যবোৰৰ মাজত সত্যিকাৰ সংযোগ নাই, কেতিয়াবা হয়তো ২টা বাক্যৰ মাজত সম্পৰ্ক পোৱা যায়, তাৰ ওপৰত নাই। আন এটা সমস্যা, কেতিয়াবা “Digitized by Google” ফুটাৰ দেখা যায়, গতিকে অহা সময়ত শিকোৱাৰ আগতে পাঠ্যবোৰ ভালদৰে পৰিষ্কাৰ কৰিব লাগিব। মুঠতে মই ফলাফলত যথেষ্ট সন্তুষ্ট, LLM হবলৈ বহু বাকি কিন্তু বাক্য উত্পাদক হিচাপে নিশ্চিত।</p><p>মই বহুত শিকিছো আৰু অহা সপ্তাহত কি উন্নত কৰিব পাৰো সেইটো বিচাৰিম। মই চিগ্ৰেই ফাইল আপল'ড কৰিম!</p><h1>অহা পৰিকল্পনা</h1></p><p>(সম্পূৰ্ণ) মই এতিয়া সংস্কৰণ ০.৫-ৰ ওপৰত কাম আৰম্ভ কৰিম, ৫০খন বইৰ বিপৰীতে ৫০০-৬০০খন বই ব্যৱহাৰ কৰি শিকাম। এতিয়া মই nanoGPT-ত ১৮০০-১৮৫০ লণ্ডনৰ বই ব্যৱহাৰ কৰি শিকাইছো। সমস্যা আছিল পোৱা বইবোৰ আধুনিকীকৰণ বা ব্যাখ্যা নাথকা, কেৱল মোৰ নিৰ্বাচিত সময়ৰ প্ৰকৃত প্ৰকাশিত বই।</p><p>মই নতুন মডেল (v1) বহল কৰ্পাছৰ সৈতে শিকাব বিচাৰো, হয়তো v0.5-ৰ তুলনাত ৫-১০ গুণ ডাঙৰ। মোৰ লক্ষ্য হৈছে কেৱল Selective Temporal Training-এ যুক্তিৰ ক্ষমতা উন্মোচিত হয় নেকি চোৱা, এইটো কঠিন হ'ব আৰু সম্ভৱ নে নাই নিশ্চিত নহয় কাৰণ ঐতিহাসিক তথ্যৰ সীমাবদ্ধতা আছে। অহা সপ্তাহত মই ৫-১০GB ডেটা সংগ্ৰহ কৰিবলৈ চেষ্টা কৰিম। মই বিশ্বাস কৰো, যদি পৰিষ্কাৰ, উচ্চমানৰ তথ্য পাওঁ আৰু GPU ভাড়া লওঁ, অগ্ৰগতি হব।</p><h1>এই প্ৰজেক্ট কেনেকৈ ব্যৱহাৰ কৰিব</h1></p><p>এই প্ৰজেক্টত প্ৰধানকৈ ঐতিহাসিক ডেটা সংগ্ৰহ, প্ৰস্তুতকৰণ আৰু টোকেনাইজাৰ নিৰ্মাণত গুৰুত্ব দিয়া হৈছে। সম্পূৰ্ণ LLM শিকোৱাৰ প্ৰক্ৰিয়া ইয়াত দিয়া নাই, তাৰ বাবে Andrej Karpathy-ৰ nanoGPT চাওক।</p><h1>পদক্ষেপ ১: ঐতিহাসিক পাঠ্য সংগ্ৰহ আৰু প্ৰস্তুত কৰক</h1></p><p>আপোনাৰ নিৰ্বাচিত সময়ৰ (যেনে, লণ্ডন ১৮০০-১৮৫০) সাধাৰণ সম্পত্তি থকা বই, নথি আদি .txt ফাইল সংগ্ৰহ কৰক।</p><p>আপুনি চাইলে download_texts_improved.py ব্যৱহাৰ কৰি বই ডাউনলোড কৰিব পাৰে।</p><p>স্ক্ৰিপ্ট বা হাতেৰে পাঠ্যৰ Project Gutenberg-ৰ হেডাৰ/ফুটাৰ, আধুনিক টীকাভাষ্য বা OCR ত্ৰুটি আঁতৰাওক।</p><p>prepare_dataset.py-ও ভালকৈ কাম কৰিব।</p><h1>পদক্ষেপ ২: নিজস্ব টোকেনাইজাৰ নিৰ্মাণ কৰক</h1></p><p>পৰিষ্কৃত ডেটাত train_tokenizer.py বা train_tokenizer_hf.py চলাওক।
এইটোৱে vocab.json আৰু merges.txt দিব</p><p>এই ফাইলবোৰে আপোনাৰ মডেলৰ শব্দভাণ্ডাৰ আৰু মাৰ্জ নিয়ম সংজ্ঞায়িত কৰে</p><h1>পদক্ষেপ ৩: আপোনাৰ মডেল (nanoGPT) শিকাওক</h1></p><p>শিকোৱাৰ প্ৰক্ৰিয়াৰ বাবে <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy-ৰ nanoGPT</a> চাওক।</p><p>আপুনি চাইলে অন্য LLM-ও শিকাব পাৰে, কিন্তু মই nanoGPT ব্যৱহাৰ কৰিছিলো</p><h1>FAQ</h1></p><h2>Selective Temporal Training কি?</h2></p><p>Selective Temporal Training (STT) হৈছে এক যন্ত্ৰ শিক্ষণ পদ্ধতি য'ত সকলো শিকোৱাৰ ডেটা নিৰ্দিষ্ট ঐতিহাসিক সময়ৰ ভিতৰত সংগৃহীত হয়। এইটো সেই সময়ৰ ভাষা আৰু জ্ঞানক আধুনিক ধাৰণাৰ প্ৰভাৱৰ বাহিৰে মডেল কৰিবলৈ কৰা হয়। উদাহৰণস্বৰূপ, বৰ্তমান মোৰ (v0.5) মডেল ১৮০০-১৮৭৫ৰ ডেটাতহে শিকোৱা, এইটো সম্পূৰ্ণ নতুনকৈ শিকোৱা আৰু ফলত সেই সময়ৰ ভাষাশৈলী আৰু ঐতিহাসিক প্ৰসংগ প্ৰকাশ কৰে।</p><h2>Fine-tuning বা LoRA কিয় নুব্যৱহাৰ কৰা?</h2></p><p>এই প্ৰজেক্টত মই এখন ভাষা মডেল আধুনিক পক্ষপাতৰ পৰা মুক্ত ৰাখিব বিচাৰিছো। যদি মই GPT-2-ৰ দৰে কিবা fine-tune কৰো, সেইটো ইতিমধ্যে শিকোৱা আৰু সেই তথ্য আঁতৰিব নোৱাৰি। যদি মই নতুনকৈ শিকাওঁ, ভাষা মডেলটোৱে কেৱল পুৰণি হব, অভিনয় নকৰিব। এই প্ৰজেক্টৰ লক্ষ্য, ১৮০০-১৮৫০-ত লণ্ডনত প্ৰকাশিত বইৰ জ্ঞানতে কেৱল যুক্তি কৰিব পৰা এক মডেল নিৰ্মাণ।</p><h2>শিকোৱাৰ বাবে কিমান ডেটা ব্যৱহাৰ কৰিছিলা?</h2></p><p>মই ১৮০০-১৮৫০ লণ্ডনৰ বই, আইনী নথি, বাতৰি কাকত আৰু অন্যান্য লিখনি ব্যৱহাৰ কৰিছে। মই যি তালিকা দিছো তাত প্ৰায় ২০০টা আছে, কিন্তু প্ৰথম শিকাত মই কেৱল ৫০টা ফাইল, প্ৰায় ১৮৭MB ব্যৱহাৰ কৰিছিলো। আপুনি ডকুমেন্টৰ তালিকা চাব পাৰে:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Version 0 মডেলৰ আকাৰ কিমান?</h2></p><p>এই মডেল বৰ্তমানে বহুত সৰু, মই মজা আৰু কোনো আধুনিক উৎস নাথকা কঠোৰ নিয়ম অনুসৰণ কৰি কৰিছো। ইয়াত প্ৰায় ১.৬ কোটি পৰামিত্ৰ আছে, কিন্তু মই আৰু পুৰণি পাঠ্য সংগ্ৰহ কৰি নতুন শিকোৱা আৰম্ভ কৰিম। আগ্ৰগতি থাকিলে আপডেট দিম।</p><h2>শিকোৱাৰ স্পেচিফিকেশন?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F 
Ram: 16GB DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>