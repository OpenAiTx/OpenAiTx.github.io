<!DOCTYPE html>
<html lang="as">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - আধুনিক পক্ষপাত কমাবলৈ নিৰ্দিষ্ট সময়ছোৱাৰ তথ্যহে ব্যৱহাৰ কৰি প্ৰশিক্ষিত এলএলএম</title>
    <meta name="description" content="আধুনিক পক্ষপাত কমাবলৈ নিৰ্দিষ্ট সময়ছোৱাৰ তথ্যহে ব্যৱহাৰ কৰি প্ৰশিক্ষিত এলএলএম">
    <meta name="keywords" content="TimeCapsuleLLM, Assamese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "আধুনিক পক্ষপাত কমাবলৈ নিৰ্দিষ্ট সময়ছোৱাৰ তথ্যহে ব্যৱহাৰ কৰি প্ৰশিক্ষিত এলএলএম",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 267
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-as.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-07-29",
  "dateModified": "2025-07-29"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 267 stars</span>
                <span class="language">Assamese</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 ভাষা</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">ইংৰাজী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">সৰল চীনা</a>
        | <a href="#" title="Coming soon">প্ৰাচীন চীনা (চিগা আহি)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">জাপানী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">কোৰীয়</a>
        | <a href="#" title="Coming soon">হিন্দী (চিগা আহি)</a> |
        | <a href="#" title="Coming soon">থাই (চিগা আহি)</a> |
        | <a href="#" title="Coming soon">ফ্ৰেঞ্চ (চিগা আহি)</a>
        | <a href="#" title="Coming soon">জাৰ্মান (চিগা আহি)</a>
        | <a href="#" title="Coming soon">স্পেনিছ (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ইটালিয়ান (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ৰাছিয়ান (চিগা আহি)</a>
        | <a href="#" title="Coming soon">পৰ্তুগীজ (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ডাচ (চিগা আহি)</a>
        | <a href="#" title="Coming soon">পোলিচ (চিগা আহি)</a>
        | <a href="#" title="Coming soon">আৰবী (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ফাৰ্ছী (চিগা আহি)</a>
        | <a href="#" title="Coming soon">তুৰ্কী (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ভিয়েতনামী (চিগা আহি)</a>
        | <a href="#" title="Coming soon">ইন্দোনেছিয়ান (চিগা আহি)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
এখন LLM যি কেৱল নিৰ্দিষ্ট সময়ৰ তথ্যত শিকোৱা হৈছে যাতে আধুনিক পক্ষপাত কম হয়।</p><p>ভাবক, এটা AI মডেল কেৱল ঐতিহাসিকভাৱে আচৰণ নকৰে, সেয়া সঁচাকৈয়ে আছিলো।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy ৰ nanoGPT</a> ত নিৰ্মিত। মুখ্য শিকন স্ক্ৰিপ্ট আৰু মডেল স্থাপত্য তেওঁৰ কাম। </p><h1>প্ৰকল্পৰ লক্ষ্যসমূহ</h1></p><p>TimeCapsule LLM এটা পৰীক্ষামূলক প্ৰকল্প, যিটো কেৱল নিৰ্দিষ্ট সময়ত লিখা পাঠ্যতহে শিকোৱা হ'ব। লক্ষ্য হৈছে, নিৰ্দিষ্ট ঐতিহাসিক যুগৰ দৃষ্টিভংগি আৰু ভাষা অনুকৰণ কৰা।</p><h1>কিয় কেৱল fine tuning যথেষ্ট নহয়</h1></p><p>আপুনি কেৱল এটা pre-trained মডেলক fine tune কৰিলে, আপোনাৰ LLM-এ এতিয়াও আধুনিক ধাৰণা জানিব। সম্পূৰ্ণ আধুনিক পক্ষপাত নোহোৱা কঠিন, কিন্তু মই এইটোত সৰ্বাধিক ওচৰলৈ যাব বিচাৰোঁ। আধুনিক পক্ষপাত শূন্য কৰিবলৈ মডেলক গোড়াৰ পৰা শিকাব লাগিব।</p><h1>প্রত্যাশিত ফলাফলসমূহ</h1></p><p>আশা কৰোঁ, শেষ হোৱাৰ পিছত এই মডেলটোৱে আধুনিক ধাৰণা নাজানিব আৰু কেৱল শিকোৱা তথ্যৰ বাহিৰত যুক্তি কৰিব নোৱাৰিব। ইয়াই আধুনিক শব্দ/ধাৰণা চিনিব নোৱাৰে, আৰু মই আশা কৰোঁ আধুনিক জ্ঞান কল্পনা নকৰে।</p><h1>অগ্ৰগতিৰ আপডেটসমূহ</h1></p><h2>৯ জুলাই, ২০২৫</h2></p><p>মই মোৰ সময়সীমা ১৮০০-১৮৫০ আৰু অঞ্চল: লণ্ডন নিৰ্ধাৰণ কৰিছোঁ</p><p>মই পাঠ্য, কিতাপ, নথিপত্ৰৰ এটি তালিকা সংগ্ৰহ কৰিছোঁ</p><p>এতিয়ালৈকে ৫০টা txt ফাইল পাইছোঁ আৰু শীঘ্ৰে NanoGPT ত শিকন আৰম্ভ কৰিম</p><p>অগ্ৰগতি অব্যাহত থাকিলে ইয়াত আপডেট দিম</p><h2>১৩ জুলাই, ২০২৫</h2></p><p>nanoGPT ত ১৮৭MB ঐতিহাসিক তথ্যৰ সৈতে শিকোৱা হৈছে।</p><h2>১৫ জুলাই, ২০২৫</h2></p><p>মই দ্বিতীয়বাৰ শিকনৰ বাবে পাঠ্য ডাউনলোড আৰম্ভ কৰিছোঁ। সকলো Internet Archive ৰ পৰা সংগ্ৰহ কৰিছোঁ আৰু সময়সীমা ১৮০০-১৮৭৫ লৈ বৃদ্ধি কৰিছোঁ। বিভিন্ন ধৰণৰ পাঠ্য পোৱাৰ বাবে, Internet Archive ত বিষয় আৰু সন্ধান ফিল্টাৰ ব্যৱহাৰ কৰিব পাৰে প্ৰকাশস্থান, সময় আৰু বিষয় অনুসৰি।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>১৬ জুলাই, ২০২৫</h2></p><p>মই Internet Archive ৰ পৰা প্ৰায় ৫০০টা txt ফাইল ডাউনলোড কৰিছোঁ আৰু (কেৱল whitespaces, Gutenberg header ইত্যাদি আঁতৰাই) প্ৰায় ৫০০MB তথ্য পাইছোঁ। ই এটা সৰু dataset কিন্তু আগতে মই কেৱল ১৮৭MB ত শিকিছোঁ, সেয়ে দ্বিতীয় মডেলৰ পিছত output ত কোনো প্ৰভাৱ দেখা যাবই। আশা কৰিছোঁ এই মডেলটোৱে অধিক সুসংগত বাক্য উলিয়াব পাৰে। নিশ্চয়তা নাই কাৰণ এতিয়াও সৰু dataset, কিন্তু আগৰ তুলনাত বেছি।</p><p>এইটো মোৰ নিজা hardware ত কৰিব পাৰিম, ভাল হৈছে কাৰণ ডাঙৰ dataset লৈ যাবৰ আগত কিছু উন্নতি দেখা যাব বুলি আশা। কিন্তু চিন্তা নকৰিব, মই শীঘ্ৰে GPU ভাড়া ল’বই, কিন্তু তেতিয়ালৈকে মোৰ dataset যত্নসহকাৰে পৰিষ্কাৰ কৰিব বিচাৰোঁ। এটা সমস্যা হৈছে পৰিষ্কাৰ; বহু txt ফাইলত বেয়া ডাটা থাকে। মই যি স্ক্ৰিপ্ট ব্যৱহাৰ কৰিছোঁ, সেয়া কাম কৰে, কিন্তু ১০০% নহয়।</p><p>মই আজি এই dataset ত শিকাম আৰু প্ৰায় ৪-৫ ঘন্টা লাগিব। শেষ হ’লে পৰীক্ষা কৰি আপডেট দিম। মোৰ প্ৰকল্প চাই থকা সকলোকে ধন্যবাদ, কিছুমানে OCR resources ৰ লিংকো দিছে, ধন্যবাদ! আশা কৰোঁ, বহুতে নিজৰ dataset লৈ চেষ্টা কৰক।</p><h2>২৮ জুলাই, ২০২৫</h2></p><p>মই Hugging Face ত v0.5 আপলোড কৰিছোঁ, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">চাব পাৰে</a> যদি বিচাৰে। এতিয়া মোৰ repo ডাউনলোড কৰি স্থানীয়ভাৱে চলাব পাৰে। দু:খিত, nanoGPT HuggingFace ৰ সৈতে native কাম নকৰে, সেয়ে মডেল স্থানীয়ভাৱে চলাব লাগিব।</p><p>মই শীঘ্ৰে পৰৱৰ্তী শিকনৰ বাবে ডাটা সংগ্ৰহ আৰম্ভ কৰিম, বিশ্বাস কৰোঁ ৫-১০ গুণ বেছি ডাটা লাগিব যুক্তি ক্ষমতা পাবলৈ।</p><h3>শিকন আপডেট</h3></p><p>মই ৪৩৫MB (১০৮ M token) ৰ corpus ত শিকন আৰম্ভ কৰিছোঁ, বৰ বৰ ভালকৈ চলি আছে। Train loss ১০.৯ ৰ পৰা ৪.৯ লৈ নামিছে প্ৰথম ২৮০০ iteration ত। ৮-৯ ঘণ্টাৰ ভিতৰত সম্পূৰ্ণ হ’ব বুলি আশা। শেষ হ’লে আৰু আপডেট দিম।</p><h2>১৭ জুলাই, ২০২৫ ২:১৩AM</h2></p><p>দ্বিতীয় মডেলৰ শিকন শেষ হৈছে, মোৰ ৪০৬০ ত প্ৰায় ৮ ঘণ্টা ৪০ মিনিট (৩,৯০০ iteration/ঘণ্টা) লৈ ৩৩,০০০ iteration (৫ epoch) হ’ল। Final train loss আছিল ৩.৭৩। Output গৰাকী আচৰিতভাৱে ভাল, এতিয়া সঁচাকৈ ১৯শ শতিকীৰ ধৰণৰ বাক্য উলিয়াব পাৰে।</p><h1>V0 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>প্ৰাথমিক prompt ত মডেলটোৱে ১৮শ শতিকীৰ ভাষা আৰু আচৰণত সঁজুলিত হয়। উদাহৰণস্বৰূপে, মই "Who art Henry?" প্ৰশ্ন কৰোঁতে ই উত্তৰ দিলে "I know that man, I have did not a black, the storm." আৰু হঁয়, বাক্যটো অসংগতিপূৰ্ণ, কিন্তু LLM এ বুজিছে যে মই এজন ব্যক্তি বিষয়ে সুধিছোঁ।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>আধুনিক ধাৰণাৰ কোনো উল্লেখ নাই, আউটপুটত প্ৰধানকৈ ১৮০০-ৰ দশকৰ শব্দ আৰু বাক্য গঠন থাকে।</p><p>এতিয়াও বহু কাম কৰিব লাগিব, ১৮৭MB ডেটাতকৈ মডেলটোত জটিল যুক্তি সৃষ্টি নহ’ব।</p><p>এতিয়া এইটো এনেকুৱা বাক্য উৎপন্ন কৰে য’ত সম্পূৰ্ণ বাক্য গঠন নাই আৰু সাধাৰণতে কোনো অৰ্থ নাথাকে, কিন্তু এইটো প্ৰশিক্ষণ ডাটাৰ আকাৰৰ বাবে স্বাভাৱিক।</p><h1>V0.5 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>এইটো আগৰ মডেলৰ তুলনাত ভাল উন্নতি। লিখনীৰ ধৰণ আৰু শব্দভাণ্ডাৰ ভিক্টোৰিয়ান আৰু প্ৰায় প্ৰতিটো বাক্য ব্যাকৰণগতভাৱে শুদ্ধ আৰু সঠিক যতিচিহ্নযুক্ত। আৰু পুনৰ কওঁ, এইটো সুৰুঙাৰ পৰা প্ৰশিক্ষিত, গতিকে ১৮০০-ৰ বিষয়বস্তুতেই সীমাবদ্ধ।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>বহু তথ্যগত বিভ্ৰান্তি আছে। অধিকাংশ (প্ৰায় ১০০%) তথ্য (তাৰিখ, ঘটনা, ঐতিহাসিক ব্যক্তিত্ব) কল্পিত। লগতে বাক্যসমূহৰ মাজত সংযোগ নাই, কেতিয়াবা ২টা বাক্য সম্পৰ্কিত হ’ব পাৰে, কিন্তু সেয়া সীমিত। আন এটা সমস্যা হৈছে কেতিয়াবা “Digitized by Google” ফুটাৰ অহা, গতিকে পৰৱৰ্তী প্ৰশিক্ষণত পাঠবোৰ ভালকৈ পৰিষ্কাৰ কৰাটো নিশ্চিত কৰিব লাগিব। মুঠতে, ফলাফলত মই সুখী, ইমানবিধ LLM নাই, কিন্তু এটা বাক্য উৎপাদক হবলৈ সক্ষম।</p><p>মই বহু কথা শিকিছোঁ আৰু অহা কাইলৈ কি ভাল কৰিব পাৰিম সেয়া বিচাৰি আছোঁ। শীঘ্ৰে ফাইল আপল’ড কৰিম!</p><h1>আগন্তুক পৰিকল্পনা</h1></p><p>(সম্পূৰ্ণ) মই V0.5-ৰ কাম আৰম্ভ কৰিম, ৫০খন পুথিৰ বদলে ৫০০-৬০০খন ব্যৱহাৰ কৰিম। এতিয়া মই nanoGPT-ত ১৮০০-১৮৫০ সময়ৰ আৰু বিশেষকৈ লণ্ডনৰ পুথি প্ৰশিক্ষণ দিছোঁ। চেলেঞ্জ হৈছে, পুথিবোৰ আধুনিক সংশোধিত নহ’ব লাগে, নিৰ্বাচিত সময়ছোৱাৰ মূল পুথি হব লাগিব।</p><p>মই নতুন মডেল (v1) প্ৰশিক্ষণ দিব বিচাৰোঁ, আগৰ তুলনাত ৫-১০ গুণ ডাঙৰ ডেটা ব্যৱহাৰ কৰি। মোৰ লক্ষ্য হৈছে কেৱল Selective Temporal Training-ৰে যুক্তি ক্ষমতা উন্মোচিত হয় নে চোৱা, এইটো কঠিন হ’ব পাৰে কাৰণ ঐতিহাসিক ডেটাৰ সীমাবদ্ধতা আছে। অহা কাইলৈ ৫-১০GB ডেটা সংগ্ৰহ কৰিবলৈ চেষ্টা কৰিম। মই বিশ্বাস কৰোঁ, যদি পৰিষ্কাৰ, উচ্চ-গুণগত ডেটা আৰু GPU ভাড়া কৰিব পাৰোঁ, উন্নতি হব।</p><h1>এই প্ৰকল্প কেনেকৈ ব্যৱহাৰ কৰিব</h1></p><p>এই প্ৰকল্পটোত প্ৰধানকৈ ঐতিহাসিক ডেটা সংগ্ৰহ, প্ৰস্তুতকৰণ আৰু টোকেনাইজাৰ নিৰ্মাণৰ ওপৰত গুৰুত্ব দিয়া হৈছে। সম্পূৰ্ণ LLM প্ৰশিক্ষণ প্ৰক্ৰিয়া আমি আলোচনা নকৰোঁ, তাৰ বাবে Andrej Karpathy-ৰ nanoGPT চাওক।</p><h1>ধাপ ১: ঐতিহাসিক পাঠ সংগ্ৰহ আৰু প্ৰস্তুতি</h1></p><p>আপোনাৰ নিৰ্বাচিত সময়ছোৱা (উদাঃ, লণ্ডন ১৮০০-১৮৫০)-ৰ পাব্লিক ড’মেইন পুথি, নথি আদি .txt ফাইল সংগ্ৰহ কৰক।</p><p>প্ৰয়োজন হলে download_texts_improved.py ব্যৱহাৰ কৰি পুথি ডাউনল’ড কৰিব পাৰিব।</p><p>স্ক্ৰিপ্ট বা মেনুৱেলভাৱে Project Gutenberg-ৰ হেডাৰ/ফুটাৰ, আধুনিক টীকা, বা OCR ত্ৰুটি আঁতৰাওক।</p><p>prepare_dataset.py-য়ে যথেষ্ট ভাল কাম কৰিব।</p><h1>ধাপ ২: কাষ্টম টোকেনাইজাৰ নিৰ্মাণ</h1></p><p>train_tokenizer.py বা train_tokenizer_hf.py পৰিষ্কৃত ডেটাত চলাওক।
এইটো vocab.json আৰু merges.txt দিব</p><p>এই ফাইলসমূহে আপোনাৰ মডেলৰ শব্দভাণ্ডাৰ আৰু সংযোগ নিয়ম নিৰ্ধাৰণ কৰে</p><h1>ধাপ ৩: আপোনাৰ মডেল (nanoGPT) প্ৰশিক্ষণ কৰক</h1></p><p>প্ৰশিক্ষণ প্ৰক্ৰিয়াৰ বাবে <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy-ৰ nanoGPT</a> চাওক।</p><p>আৰু কোনো LLM প্ৰশিক্ষণ দিব পাৰিব, কিন্তু মই nanoGPT ব্যৱহাৰ কৰিছিলোঁ</p><h1>FAQ</h1></p><h2>Selective Temporal Training কি?</h2></p><p>Selective Temporal Training (STT) হৈছে এনে মেচিন লাৰ্নিং পদ্ধতি য’ত সকলো প্ৰশিক্ষণ ডেটা নিৰ্দিষ্ট এটি ঐতিহাসিক সময়ছোৱাৰ ভিতৰত সংগ্ৰহ কৰা হয়। আধুনিক ধাৰণাৰ প্ৰভাৱ নোহোৱাকৈ সেই যুগৰ ভাষা আৰু জ্ঞান মডেল কৰিবৰ বাবে এইটো কৰা হয়। উদাহৰণস্বৰূপ, মোৰ বৰ্তমান মডেল (v0.5) ১৮০০-১৮৭৫ৰ ডেটাতকৈ সুৰুঙাৰ পৰা প্ৰশিক্ষিত, ফলত সেই সময়ৰ ভাষা আৰু ইতিহাস প্ৰতিফলিত হয়।</p><h2>কেৱল ফাইন-টিউনিং বা LoRA ব্যৱহাৰ নকৰা কিয়?</h2></p><p>এই প্ৰকল্পৰ বাবে মই আধুনিক পক্ষপাতৰ পৰা মুক্ত ভাষা মডেল সৃষ্টি কৰিবলৈ চেষ্টা কৰিছোঁ। GPT-2 যেনেকুৱা প্ৰি-ট্ৰেইনড মডেল ফাইন-টিউন কৰিলে আধুনিক তথ্য আঁতৰিব নোৱাৰে। সুৰুঙাৰ পৰা প্ৰশিক্ষণ দিলে মডেলটো “পুৰণি” হ’ব, অভিনয় নকৰিব। বৰ্তমান উদ্দেশ্য হৈছে ১৮০০-১৮৫০ৰ লণ্ডনৰ পুথিতকৈ কেৱল সেই জ্ঞানৰে যুক্তি কৰিব পৰা মডেল।</p><h2>প্ৰশিক্ষণৰ বাবে কিমান ধৰণৰ ডেটা ব্যৱহাৰ কৰা হৈছে?</h2></p><p>মই ১৮০০–১৮৫০-ৰ লণ্ডনৰ পুথি, আইনী নথি, বাতৰিকাকত, আৰু অন্যান্য লিখনি ব্যৱহাৰ কৰিছোঁ। সংলগ্ন তালিকাত প্ৰায় ২০০টা আছে, প্ৰথম প্ৰশিক্ষণত ৫০টা ফাইল, প্ৰায় ১৮৭MB ব্যৱহাৰ কৰা। ডকুমেন্টৰ তালিকা চাব পাৰিব:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Version 0 মডেলৰ আকাৰ কিমান?</h2></p><p>এই মডেল বৰ্তমান খুব সৰু, মই মাত্ৰ আনন্দৰ বাবে আৰু আধুনিক উৎস নোহোৱাকৈ প্ৰশিক্ষণ দিছোঁ। ইয়াত প্ৰায় ১৬ নিযুত পেৰামিটাৰ আছে, আৰু মই অধিক পুৰণি পাঠ সংগ্ৰহ কৰি নতুন প্ৰশিক্ষণ আৰম্ভ কৰিম। আগবঢ়া অনুযায়ী আপডেট দিম।</p><h2>প্ৰশিক্ষণৰ স্পেচিফিকেচন?</h2></p><p>GPU: Geforce rtx 4060
CPU: i5-13400F
RAM: 16GB DDR5.</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-29

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-29 
    </div>
    
</body>
</html>