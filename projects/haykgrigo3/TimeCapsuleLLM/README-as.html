<!DOCTYPE html>
<html lang="as">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - আধুনিক পক্ষপাত কমাবলৈ কেৱল নিৰ্দিষ্ট সময়কালৰ তথ্যত প্ৰশিক্ষিত LLM</title>
    <meta name="description" content="আধুনিক পক্ষপাত কমাবলৈ কেৱল নিৰ্দিষ্ট সময়কালৰ তথ্যত প্ৰশিক্ষিত LLM">
    <meta name="keywords" content="TimeCapsuleLLM, Assamese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "আধুনিক পক্ষপাত কমাবলৈ কেৱল নিৰ্দিষ্ট সময়কালৰ তথ্যত প্ৰশিক্ষিত LLM",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 317
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-as.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-12",
  "dateModified": "2025-08-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 317 stars</span>
                <span class="language">Assamese</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 ভাষা</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">ইংৰাজী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">সৰল চীনীয়া</a>
        | <a href="#" title="Coming soon">জটিল চীনীয়া (শীঘ্ৰেই আহিব)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">জাপানী</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">কোৰিয়ান</a>
        | <a href="#" title="Coming soon">হিন্দী (শীঘ্ৰেই আহিব)</a> |
        | <a href="#" title="Coming soon">থাই (শীঘ্ৰেই আহিব)</a> |
        | <a href="#" title="Coming soon">ফ্ৰেঞ্চ (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">জার্মান (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">স্পেনী (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">ইটালিয়ান (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">ৰাছিয়ান (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">পৰ্তুগীজ (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">ডাচ (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">পোলিশ (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">আৰবী (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">ফাৰ্ছি (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">তুৰ্কী (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">ভিয়েতনামী (শীঘ্ৰেই আহিব)</a>
        | <a href="#" title="Coming soon">বাহাসা ইণ্ডোনেছিয়া (শীঘ্ৰেই আহিব)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>টাইমকেপচুল LLM</h1>
এটা LLM যি কেৱল নিৰ্দিষ্ট সময়ৰ তথ্যতকৈ শিকোৱা হৈছে, আধুনিক পক্ষপাত কমাবলৈ।</p><p>ভাবক, যদি এটা AI মডেল কেৱল ঐতিহাসিক বুলি অভিনয় নকৰে, বাস্তৱতে আছিল।</p><p><a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy-ৰ nanoGPT</a> ভিত্তিত নিৰ্মিত। মূল প্ৰশিক্ষণ স্ক্ৰিপ্ট আৰু মডেল স্থাপত্য তেওঁৰ কাম।</p><h1>প্ৰকল্পৰ লক্ষ্যসমূহ</h1>
TimeCapsule LLM এটা পৰীক্ষামূলক প্ৰকল্প যি কেৱল নিৰ্দিষ্ট সময়ছোৱাৰ ভিতৰত লিখা পাঠ্যত প্ৰশিক্ষিত কৰা হ'ব। লক্ষ্যটো হৈছে বিশেষ ঐতিহাসিক যুগৰ দৃষ্টিভংগি আৰু ভাষা অনুকৰণ কৰা।</p><h1>কিয় কেৱল fine tuning যথেষ্ট নহয় </h1></p><p>যদি আপুনি কেৱল এটা pre-trained মডেলক fine tune কৰে, তেন্তে আপোনাৰ LLM-এ আধুনিক ধাৰণা সদায় জানিব। নিশ্চয়েই শূন্য আধুনিক পক্ষপাত লাভ কৰা কঠিন, কিন্তু মই ইয়াৰ ওচৰলৈ যাব বিচাৰো। আধুনিক পক্ষপাত একেবাৰে নাথাকিবলৈ মডেলটো আৰম্ভণিৰ পৰা প্ৰশিক্ষণ দিয়াটো প্ৰয়োজন।</p><h1>প্রত্যাশিত ফলাফলসমূহ </h1></p><p>আশা কৰোঁ, যেতিয়া এই মডেলটো সম্পূর্ণ হ'ব, তেতিয়া ই আধুনিক ধাৰণা নাজানিব আৰু ইয়াক যি প্ৰশিক্ষণ দিয়া হৈছে, তাৰ বাহিৰে চিন্তা কৰিব নোৱাৰিব। ই আধুনিক শব্দ/ধাৰণা চিনিব নোৱাৰিব আৰু আধুনিক জ্ঞান hallucinate কৰিব নোৱাৰিব।</p><h1>অগ্ৰগতিৰ আপডেটসমূহ</h1></p><h2>July 9th, 2025</h2></p><p>মই মোৰ সময়ছোৱা 1800-1850 আৰু অঞ্চল: লণ্ডন স্থিৰ কৰিছোঁ</p><p>মই পাঠ্য, কিতাপ, নথিপত্ৰৰ তালিকা সংগ্ৰহ কৰিছোঁ</p><p>এতিয়ালৈকে ৫০টা txt ফাইল পাইছোঁ আৰু শীঘ্ৰে NanoGPT প্ৰশিক্ষণ আৰম্ভ কৰিম</p><p>অগ্ৰগতি হোৱা পৰ্যন্ত ইয়াত আপডেট দিম</p><h2>July 13th, 2025</h2></p><p>nanoGPT-ত ১৮৭MB ঐতিহাসিক পাঠ্য তথ্য প্ৰশিক্ষণ দিছোঁ।</p><h2>July 15th, 2025</h2></p><p>দ্বিতীয় প্ৰশিক্ষণৰ বাবে পাঠ্য ডাউনলোড কৰা আৰম্ভ কৰিছোঁ। মই সকলো Internet Archive-ৰ পৰা ল'ব আৰু সময়ছোৱা 1800-1875লৈ বৃদ্ধি কৰিছোঁ। বিভিন্ন ধৰণৰ পাঠ্য পোৱাৰ বাবে, আপুনি বিষয় আৰু সন্ধান ফিল্টাৰ ব্যৱহাৰ কৰিব পাৰে প্ৰকাশ স্থান, সময়ছোৱা আৰু বিষয়সমূহ Internet Archive-ত।</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Search Filters"></p><h2>July 16th, 2025</h2></p><p>Internet Archive-ৰ পৰা প্ৰায় ৫০০টা txt ফাইল ডাউনলোড কৰিছোঁ আৰু ইয়াক পৰিষ্কাৰ কৰাৰ পিছত (কেৱল whitespace, Gutenberg header ইত্যাদি আঁতৰাই) প্ৰায় ৫০০MB তথ্য আছে। ই এটা ক্ষুদ্ৰ dataset, কিন্তু আগতে ১৮৭MB-ৰ পৰা প্ৰশিক্ষণ দিছিলোঁ, গতিকে দ্বিতীয় মডেল প্ৰশিক্ষণ দিয়াৰ পিছত output-ত কমেও কিছু পৰিলক্ষিত পাৰ্থক্য থাকিব লাগিব। আশা কৰোঁ এই মডেলটোৱে কমেও অধিক সুসংগত বাক্য উত্পন্ন কৰিব পাৰিব যিটো কিছু পৰিমাণে অৰ্থবোধক। নিশ্চয়েই, নিশ্চয়তা নাই কাৰণ এইটো এতিয়াও খুব ক্ষুদ্ৰ dataset, কিন্তু আগতকৈ অধিক। </p><p>এইটো মোৰ নিজ hardware-ত কৰিব পাৰি, ভাল হৈছে কাৰণ মই আশা কৰোঁ কিছু উন্নতি দেখিবলৈ পাব আগতে যেতিয়া বৃহত্তৰ dataset-লৈ যাব, যাৰ বাবে GPU ভাড়া ল'ব লাগিব। কিন্তু চিন্তা নকৰিব, মই শীঘ্ৰে GPU ভাড়া ল'বই, কিন্তু তাৰ আগতে datasetটো যথাসম্ভৱ নিৰ্বাচিত আৰু পৰিষ্কাৰ কৰিবলৈ বিচাৰো। এটা সমস্যাৰ নাম হৈছে পৰিষ্কাৰকৰণ, বহু txt ফাইলত নিৰর্থক শব্দ মিশ্ৰিত হৈ থাকে। মই যি script ব্যৱহাৰ কৰিছোঁ পৰিষ্কাৰ কৰিবলৈ, সেইবোৰ কাম কৰে কিন্তু ১০০% কাৰ্যকৰী নহয়। </p><p>মই আজি এই dataset-টো প্ৰশিক্ষণ দিম আৰু প্ৰায় ৪-৫ ঘণ্টা সময় লাগিব। যেতিয়া শেষ হ'ব আৰু পৰীক্ষা কৰিম, তেতিয়া আপডেট দিম। মোৰ প্ৰকল্পটো চাবলৈ যিসকলে আগ্ৰহী হৈছে, সকলোৱে ধন্যবাদ, কিছুমানে OCR resources-ৰ link দিছে, ধন্যবাদ! আশা কৰোঁ অধিক মানুহে এইটো চেষ্টা কৰক আৰু নিজৰ dataset-ত পৰীক্ষা কৰক।</p><h2>১২ আগষ্ট, ২০২৫</h2></p><p>সংস্কৰণ ১ সম্পূৰ্ণ হৈছে, এই সংস্কৰণত Phi 1.5 (৭০০M পৰামিটাৰ) ব্যৱহাৰ কৰা হৈছে আৰু প্ৰায় ৭০০০ টেক্সট থকা ডেটাসেটত প্ৰশিক্ষণ দিয়া হৈছে। এই মডেলটোৰ মূল উন্নতি হৈছে, এইটো কিছু সময়ত বাস্তৱ নাম আৰু ঐতিহাসিক ঘটনা উল্লেখ কৰিবলৈ সক্ষম। সংস্কৰণ ০ আৰু ০.৫ এইটো কৰিব পৰা নাছিল, যেতিয়া মই সেইবোৰ মডেলক "১৮২১ বৰ্ষত" বুলি প্ৰশ্ন দিছিলোঁ, উত্তৰটো মাত্ৰ কল্পনা আছিল, কেতিয়াও হোৱা নাছিল। সংস্কৰণ ১-ৰ সৈতে ইয়াৰ এটা উদাহৰণ:</p><p>মোৰ প্ৰম্পট: আমাৰ প্ৰভুৰ বছৰ ১৮৩৪ আছিল,</p><p>উত্তৰ: "আমাৰ প্ৰভুৰ বছৰ ১৮৩৪ আছিল আৰু লণ্ডনৰ পথসমূহ প্ৰতিবাদ আৰু আবেদনৰে ভৰা আছিল। কাৰণ, বহুতে কৈছিল, ব্যক্তিগত পথত বান্ধি থকা নহয়, কিন্তু সেই দিনটো লৰ্ড পালমাৰষ্টনৰ দিনত লৈছিল, জনতাই আইনৰ দিনটোত আমাৰ আগত অহা অসুবিধাসমূহৰ সৰু বিবৃতি পাব। গভীৰ দু:খৰ কথা, বিশ্বৰ ইতিহাসৰ বৰ্তমান ঘটনাবোৰ স্পষ্ট, আৰু ফলত জনা যাব। সত্য নহয় যে যিসকল মানুহে প্ৰথমে যিৰুছালেমত গস্পেল স্থাপন কৰিছিল, তেওঁলোকৰ সমৃদ্ধি আৰু উন্নতিৰ এতিয়া বিস্তৃত আৰু ৰোমাঞ্চকৰ অভিলেখ আছে।"</p><p>প্ৰথমতে মই ভাবিছিলোঁ এইটো কাকতালিক, কিন্তু এইটো চাওঁক: <img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/1834protest.png" alt="১৮৩৪প্ৰতিবাদ"></p><h3>প্ৰশিক্ষণৰ আপডেট</h3></p><p>মই ৪৩৫MB (১০৮ M টোকেন) ক'ৰ্পাছত প্ৰশিক্ষণ আৰম্ভ কৰিছোঁ, বৰ্তমান বৰ ভালকৈ চলি আছে। প্ৰশিক্ষণ ক্ষতি ১০.৯ৰ পৰা ৪.৯লৈ প্ৰথম ২৮০০ ইটাৰেচনত সোমাইছে। মই আশা কৰোঁ সম্পূৰ্ণ কৰিবলৈ প্ৰায় ৮-৯ ঘণ্টা লাগিব। সম্পূৰ্ণ হ'লে মই পুনৰ আপডেট দিম।</p><h2>১৭ জুলাই, ২০২৫</h2></p><p>দ্বিতীয় মডেলৰ প্ৰশিক্ষণ সম্পূৰ্ণ হৈছে, মোৰ ৪০৬০-এ প্ৰায় ৮ ঘণ্টা ৪০ মিনিট (৩,৯০০ ইটাৰ/ঘণ্টা) লৈ ৩৩,০০০ ইটাৰ (৫ ইপক) চলিছে। চূড়ান্ত প্ৰশিক্ষণ ক্ষতি ৩.৭৩ আছিল। আউটপুটবোৰ আচৰিতভাৱে ভাল, এতিয়া সত্যই সমন্বিত ১৯শ শতিকাৰ শৈলীৰ বাক্য সৃষ্টি কৰে।</p><h2>২৮ জুলাই, ২০২৫</h2></p><p>মই v0.5 Hugging Face-ত আপলোড কৰি পেলাইছোঁ, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">চাওক</a> যদি আপুনি চাব বিচাৰে। এতিয়া আপুনি মোৰ ৰেপ' ডাউনলোড কৰি স্থানীয়ভাৱে চলাব পাৰে। দুঃখজনকভাৱে nanoGPT HuggingFace-ৰ সৈতে স্বাভাৱিকভাৱে কাম নকৰে, সেয়ে আপুনি ডাউনলোড কৰি স্থানীয়ভাৱে মডেল চলাব লাগিব।</p><p>আৰু মই মোৰ পৰৱৰ্তী প্ৰশিক্ষণৰ বাবে ডেটা চয়ন আৰম্ভ কৰিম, মই বিশ্বাস কৰোঁ মই যুক্তি ক্ষমতা পাবলৈ ৫-১০ গুণ অধিক ডেটা লাগিব।</p><h2>২ আগষ্ট, ২০২৫</h2></p><p>মই শীঘ্ৰেই সংস্কৰণ ১-ত কাম আৰম্ভ কৰিম। মই nanoGPT-ৰ স্থাপত্যৰ পৰা আধুনিক কিছুমানলৈ সলনি কৰিব লাগিব। মোৰ মনত কিছুমান মুক্ত উৎস LLM স্থাপত্য আছে, যেনে: OpenLLaMA v3, Phi-2 আৰু Qwen 1.5B। আৰু V1-লৈ যোৱাৰ বাবে, মই বহুত ডাঙৰ আৰু বৈচিত্ৰ্যপূৰ্ণ ডেটাসেট চয়ন কৰিব লাগিব। কমেও ৫GB নিৰ্ভুল প্ৰশিক্ষণ ডেটা লাগিব।</p><h1>V0 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>প্ৰাথমিক প্ৰম্প্টসমূহত দেখা গৈছে মডেলটোৱে ১৮শ শতিকাৰ ভাষা আৰু আচৰণৰে উত্তৰ দিছে। উদাহৰণস্বৰূপ, মই "Who art Henry?" প্ৰশ্ন কৰোঁতে উত্তৰ দিছিল "I know that man, I have did not a black, the storm." আৰু এই বাক্যটোৱে কোনো অৰ্থ নকৰে, কিন্তু LLM-এ বুজিছে মই এজন ব্যক্তিৰ বিষয়ে সুধিছোঁ।</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="TimeLockLLM নমুনা আউটপুট"></p><p>আধুনিক ধাৰণাৰ উল্লেখ নাই, আউটপুটবোৰত প্ৰায় ১৮শ শতিকাৰ শব্দ আৰু বাক্যৰ বিন্যাস থাকে।</p><p>এতিয়াও বহুত কামৰ প্ৰয়োজন, ১৮৭MB-ৰ প্ৰশিক্ষণৰে যুক্তি-পূৰ্ণ জটিল পাঠ্য উৎপন্ন কৰা মডেল পাব পৰা নাযায়।</p><p>বৰ্তমান ই সম্প্ৰস্তুত কৰা বাক্যসমূহত সম্পূৰ্ণ বাক্যৰ গঠন নোহোৱা দেখা যায় আৰু সাধাৰণতে একো বুজি নাপোৱা হয় কিন্তু এইটো প্ৰশিক্ষণৰ আকাৰৰ বাবে স্বাভাৱিক। </p><h1>V0.5 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা</h1></p><p>এইটো আগৰ মডেলটোৰ তুলনাত এটা ভাল উন্নতি। লিখন শৈলী আৰু শব্দভঁৰাল ভিক্টোৰিয়ান আৰু প্ৰায় প্ৰতিটো বাক্য ব্যাকৰণগতভাৱে সঠিক আৰু সঠিক যতি চিহ্ন সহ। আৰু আকৌ এইটো সম্পূৰ্ণ নতুনকৈ প্ৰশিক্ষণ দিয়া, সেইবাবে ১৮০০ দশকৰ বিষয়বস্তুত কেন্দ্ৰিত। </p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="TimeLockLLM Sample Output"></p><p>এতিয়া বহুতো তথ্যগত ভুল সৃষ্টি হয়। বহুতো (প্ৰায় ১০০%) তথ্য (তাৰিখ, ঘটনা, ঐতিহাসিক ব্যক্তি) কল্পিত। লগতে বাক্যসমূহ একে আনৰ সৈতে সংযোগ নাথাকে, কেতিয়াবা হয়তো ২টা বাক্য সংযুক্ত থাকে, কিন্তু তাৰ বাহিৰে নাথাকে। আন এটা সমস্যাত কেতিয়াবা “Digitized by Google” নামৰ ফুটাৰ দেখা যায়, সেয়ে পৰৱৰ্তী প্ৰশিক্ষণত পাঠ্যবোৰ ভালদৰে পৰিষ্কাৰ কৰাটো নিশ্চিত কৰিব লাগিব। সমগ্ৰতে ফলাফলত মই অতি সন্তুষ্ট, এতিয়াও LLM নহয়, কিন্তু নিশ্চিতভাৱে এটা বাক্য উৎপাদক। </p><p>মই বহুতো শিকিছোঁ আৰু অহা সপ্তাহসমূহত কিহবোৰ ভাল কৰিব পাৰিম চিন্তা কৰিম। মই শীঘ্ৰেই ফাইল আপলোড কৰিম! </p><h1>V1 মডেলৰ আচৰণ আৰু সীমাবদ্ধতা </h1></p><p>মই শীঘ্ৰেই কিছু উদাহৰণমূলক আউটপুট আপলোড কৰিম আৰু একে প্ৰম্পটৰে ৩টা মডেলৰ তুলনা কৰিম। মই V1কো আগৰ সংস্কৰণৰ দৰে huggingfaceত আপলোড কৰিম, মোৰ huggingface একাউন্ট ইয়াত পাব: https://huggingface.co/haykgrigorian/TimeCapsuleLLM</p><h1>আগন্তুক পৰিকল্পনা </h1></p><p>(সম্পন্ন) মই V0.5ৰ কাম আৰম্ভ কৰিম, ৫০খন কিতাপৰ বদলে আদর্শভাৱে ৫০০-৬০০খন কিতাপ ব্যৱহাৰ কৰি প্ৰশিক্ষণ দিম। বৰ্তমান মই nanoGPTক ১৮০০-১৮৫০ সময়ৰ আৰু বিশেষকৈ লণ্ডনৰ কিতাপৰে প্ৰশিক্ষণ দিছোঁ। কিতাপবোৰ আধুনিকীকৰণ নোহোৱা আৰু নিৰ্বাচিত সময়ত প্ৰকাশিত মূল কিতাপ বিচাৰি পাওঁত কিছু সমস্যা আছে।</p><p>মই নতুন মডেল (v1) বহু ডাঙৰ কোৰ্পাছৰে প্ৰশিক্ষণ দিব বিচাৰোঁ, হয়তো v0.5ৰ তুলনাত ৫-১০ গুণ ডাঙৰ। মোৰ লক্ষ্য Selective Temporal Trainingৰে যুক্তি ক্ষমতা উদ্ভৱ হয় নে চোৱা, এইটো অধিক কঠিন হব আৰু সম্পূৰ্ণৰূপে সম্ভৱ নে নাই নিশ্চিত নহয়, কাৰণ ঐতিহাসিক তথ্যৰ সীমাবদ্ধতা আছে। অহা সপ্তাহসমূহত ৫-১০GB কোৰ্পাছৰ বাবে যথেষ্ট তথ্য সংগ্ৰহ কৰাৰ চেষ্টা কৰিম। মই বিশ্বাস কৰোঁ যদি মই পৰিষ্কাৰ উচ্চ মানৰ তথ্য সংগ্ৰহ কৰোঁ আৰু GPU ভাড়া কৰোঁ, আগবঢ়া হব।</p><h1>এই প্ৰকল্প ব্যৱহাৰ কৰাৰ নিয়ম </h1></p><p>এই প্ৰকল্পটো প্ৰধানকৈ ঐতিহাসিক তথ্য সংগ্ৰহ, প্ৰশিক্ষণৰ বাবে প্ৰস্তুতকৰণ আৰু টোকেনাইজাৰ নিৰ্মাণত কেন্দ্ৰিত। মই সম্পূৰ্ণ LLM প্ৰশিক্ষণ প্ৰক্ৰিয়া আলোচনা নকৰোঁ, তাৰ বাবে Andrej Karpathyৰ nanoGPT চাওক।</p><h1>পদক্ষেপ ১: ঐতিহাসিক পাঠ্য সংগ্ৰহ আৰু প্ৰস্তুতকৰণ </h1></p><p>আপোনাৰ নিৰ্বাচিত সময়ৰ (যেনেঃ লণ্ডন ১৮০০-১৮৫০) পাব্লিক ডোমেইন কিতাপ, নথি আদি .txt ফাইল সংগ্ৰহ কৰক</p><p>যদি প্ৰয়োজন হয় download_texts_improved.py ব্যৱহাৰ কৰি কিতাপ ডাউনলোড কৰিব পাৰে।</p><p>Project Gutenbergৰ হেডাৰ/ফুটাৰ, আধুনিক টীকাতথ্য বা OCR ভুলসমূহ স্ক্ৰিপ্ট বা manually আঁতৰাই পাঠ্য পৰিষ্কাৰ কৰক।</p><p>prepare_dataset.py ভালদৰে কাম কৰিব।</p><h1>পদক্ষেপ ২: কাষ্টম টোকেনাইজাৰ নিৰ্মাণ</h1></p><p>পরিষ্কৃত তথ্যত train_tokenizer.py বা train_tokenizer_hf.py চলাও।
এইটোত vocab.json আৰু merges.txt পাব।
এই ফাইলসমূহে আপোনাৰ মডেলৰ বাবে শব্দভঁৰাল আৰু সংযুক্তি নিয়ম সংজ্ঞায়িত কৰা হৈছে</p><h1>পদক্ষেপ ৩: আপোনাৰ মডেল (nanoGPT) প্ৰশিক্ষণ কৰক</h1></p><p>প্ৰশিক্ষণ প্ৰক্ৰিয়াৰ বাবে <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">Andrej Karpathy ৰ nanoGPT</a> চাওক।</p><p>আপুনি ইচ্ছা কৰিলে ভিন্ন LLM প্ৰশিক্ষণ কৰিব পাৰে, কিন্তু মই nanoGPT ব্যৱহাৰ কৰিছিলোঁ</p><h1>FAQ</h1></p><h2>Selective Temporal Training (STT) কি?</h2></p><p>Selective Temporal Training (STT) হৈছে এক যান্ত্রিক অধ্যয়ন পদ্ধতি, য’ত সকলো প্ৰশিক্ষণ তথ্য বিশেষভাৱে নিৰ্বাচিত কৰা হয় যাতে এটি নিৰ্দিষ্ট ঐতিহাসিক সময়ছোৱাৰ ভিতৰতে থাকে। এইটো কৰা হয় সেই যুগৰ ভাষা আৰু জ্ঞানৰ মডেলিং কৰাৰ বাবে, আধুনিক ধাৰণাৰ প্ৰভাৱৰ বাহিৰে। উদাহৰণস্বৰূপে, বৰ্তমান যি মডেল আছে (v0.5) সেইটো ১৮০০-১৮৭৫ সময়ছোৱাৰ তথ্যতহঁততকৈ কেৱল, সুচিন্তিতভাৱে প্ৰশিক্ষিত হৈছে, fine-tune কৰা হোৱা নাই, একেবাৰে শূন্যৰ পৰা প্ৰশিক্ষণ কৰা হৈছে, যাতে সেই সময়ছোৱাৰ ভাষাশৈলী আৰু ঐতিহাসিক পৰিপ্ৰেক্ষিত স্পষ্ট হয়।</p><h2>Fine-tuning বা LoRA ব্যৱহাৰ নকৰাৰ কাৰণ কি?</h2></p><p>এই প্ৰকল্পত মই চেষ্টা কৰিছোঁ যেন আধুনিক পক্ষপাত মুক্ত ভাষা মডেল তৈয়াৰ কৰিব পাৰি। যদি মই GPT-2 যেন কোনো মডেল fine-tune কৰোঁ, সেইটো আগতে প্ৰশিক্ষিত হৈ থাকে আৰু সেই তথ্য আঁতৰি নাযায়। যদি মই একেবাৰে শূন্যৰ পৰা প্ৰশিক্ষণ কৰোঁ, ভাষা মডেলটোৱে পুৰণি বুলি অভিনয় নকৰে, সেইটো বাস্তৱতে পুৰণি হয়। এই প্ৰকল্পৰ লক্ষ্য হ’ল, কেৱল ১৮০০-১৮৫০ সময়ছোৱাৰ লণ্ডনৰ কিতাপৰ তথ্য ব্যৱহাৰ কৰি যুক্তি কৰিব পৰা মডেল তৈয়াৰ কৰা।</p><h2>প্ৰশিক্ষণৰ বাবে কোন ধৰণৰ তথ্য ব্যৱহাৰ কৰিছে?</h2></p><p>মই ১৮০০–১৮৫০ সময়ছোৱাৰ লণ্ডনৰ কিতাপ, আইনগত নথিপত্ৰ, বাতৰিকাকত, আৰু আন লেখাবলী ব্যৱহাৰ কৰিছোঁ। মই যি তালিকা সংযোগ দিছোঁ, তাত প্ৰায় ২০০টা আছে, কিন্তু প্ৰথম প্ৰশিক্ষণৰ বাবে মই কেৱল ৫০টা ফাইল ব্যৱহাৰ কৰিছিলোঁ, প্ৰায় ~১৮৭ এম.বি.। আপোনি নথিপত্ৰৰ তালিকা চাব পাৰে:
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>মডেলসমূহ কিমান ডাঙৰ?</h2></p><p>V0: ১৬M পাৰামিটাৰ</p><p>V0.5: ১২৩M পাৰামিটাৰ</p><p>V1: ৭০০M পাৰামিটাৰ</p><h1>প্ৰশিক্ষণৰ স্পেক্স?</h1></p><p>#V0/V0.5
GPU: Geforce RTX 4060
CPU: i5-13400F
RAM: ১৬GB DDR5.</p><p>#V1</p><p>GPU: A100 rented</p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-12 
    </div>
    
</body>
</html>