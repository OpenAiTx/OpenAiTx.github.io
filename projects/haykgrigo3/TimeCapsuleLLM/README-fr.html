<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TimeCapsuleLLM - Un LLM entra&#238;n&#233; uniquement sur des donn&#233;es de certaines p&#233;riodes afin de r&#233;duire les biais modernes</title>
    <meta name="description" content="Un LLM entra&#238;n&#233; uniquement sur des donn&#233;es de certaines p&#233;riodes afin de r&#233;duire les biais modernes">
    <meta name="keywords" content="TimeCapsuleLLM, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TimeCapsuleLLM",
  "description": "Un LLM entraîné uniquement sur des données de certaines périodes afin de réduire les biais modernes",
  "author": {
    "@type": "Person",
    "name": "haykgrigo3"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 269
  },
  "url": "https://OpenAiTx.github.io/projects/haykgrigo3/TimeCapsuleLLM/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md",
  "datePublished": "2025-08-02",
  "dateModified": "2025-08-02"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/haykgrigo3/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TimeCapsuleLLM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 269 stars</span>
                <span class="language">French</span>
                <span>by haykgrigo3</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=zh-CN">简体中文</a>
        | <a href="#" title="Coming soon">繁體中文 (bientôt disponible)</a> |
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=haykgrigo3&project=TimeCapsuleLLM&lang=ko">한국어</a>
        | <a href="#" title="Coming soon">हिन्दी (bientôt disponible)</a> |
        | <a href="#" title="Coming soon">ไทย (bientôt disponible)</a> |
        | <a href="#" title="Coming soon">Français (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Deutsch (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Español (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Italiano (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Русский (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Português (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Nederlands (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Polski (bientôt disponible)</a>
        | <a href="#" title="Coming soon">العربية (bientôt disponible)</a>
        | <a href="#" title="Coming soon">فارسی (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Türkçe (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Tiếng Việt (bientôt disponible)</a>
        | <a href="#" title="Coming soon">Bahasa Indonesia (bientôt disponible)</a></p><p>      </div>
    </div>
  </details>
</div></p><h1>TimeCapsule LLM</h1>
Un LLM entraîné uniquement sur des données provenant de certaines périodes pour réduire le biais moderne.</p><p>Imaginez si un modèle d'IA ne se contentait pas de faire semblant d'être historique, mais l'était réellement.</p><p>Basé sur <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT par Andrej Karpathy</a> Les scripts d'entraînement principaux et l'architecture du modèle sont de lui.</p><h1>Objectifs du projet</h1></p><p>TimeCapsule LLM est un projet expérimental qui ne sera entraîné que sur des textes écrits durant certaines périodes. L'objectif est de simuler la vision du monde et la langue d'époques historiques spécifiques.</p><h1>Pourquoi l'affinage n'est pas suffisant</h1></p><p>Si vous vous contentez d'affiner un modèle pré-entraîné, votre LLM connaîtra encore des concepts modernes. Bien sûr, atteindre un biais moderne nul est difficile, mais je veux m'en approcher le plus possible. Obtenir un modèle sans biais moderne nécessite un entraînement depuis zéro.</p><h1>Résultats attendus</h1></p><p>J'espère que, une fois terminé, ce modèle n'aura pas connaissance des concepts modernes et ne sera pas capable de raisonner au-delà de ce sur quoi il a été entraîné. Il ne devrait pas reconnaître les concepts/vocabulaire modernes et j'espère qu'il n'hallucinera pas de connaissances modernes.</p><h1>Mises à jour de l'avancement</h1></p><h2>9 juillet 2025</h2></p><p>J'ai défini ma période à 1800-1850 et la région : Londres</p><p>J'ai rassemblé une liste de textes, livres, documents</p><p>Jusqu'à présent, j'en ai obtenu 50 sous forme de fichiers txt et je vais bientôt commencer l'entraînement de NanoGPT</p><p>Je mettrai à jour ceci tant que des progrès seront réalisés</p><h2>13 juillet 2025</h2></p><p>NanoGPT entraîné avec 187 Mo de données textuelles historiques.</p><h2>15 juillet 2025</h2></p><p>J'ai commencé à télécharger des textes pour la seconde session d'entraînement. Je prends tout sur Internet Archive et j'ai élargi la période à 1800-1875. Pour obtenir une gamme diversifiée de textes, vous pouvez utiliser les filtres de sujet et de recherche sur Internet Archive selon le lieu de publication, la période et les sujets.</p><p><img src="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/searchfilter.jpg" alt="Filtres de recherche"></p><h2>16 juillet 2025</h2></p><p>J'ai téléchargé environ 500 fichiers txt depuis Internet Archive et après les avoir nettoyés (juste suppression des espaces, en-têtes Gutenberg, etc.) j'ai environ 500 Mo de données. C'est un petit jeu de données mais la dernière fois j'ai entraîné sur 187 Mo donc il devrait y avoir au moins une différence notable dans la sortie après l'entraînement du second modèle. J'espère que ce modèle pourra au moins produire des phrases plus cohérentes qui ont un certain sens. Ce n'est bien sûr pas garanti puisque c'est encore un tout petit jeu de données, mais c'est plus que ce que j'avais utilisé la dernière fois.</p><p>Cela devrait être faisable sur mon propre matériel, c'est aussi bien car je pourrai voir, je l'espère, des améliorations avant de passer à un plus grand jeu de données qui nécessiterait de louer un GPU. Mais ne vous inquiétez pas, je prévois toujours de louer un GPU bientôt, mais avant cela, je veux m'assurer que mon jeu de données est aussi bien sélectionné et propre que possible. L'un des problèmes que j'ai est le nettoyage, beaucoup de ces fichiers txt contiennent du charabia mélangé. Les scripts que j'ai utilisés pour le nettoyage fonctionnent mais ne sont pas efficaces à 100 %.</p><p>Je vais entraîner ce jeu de données aujourd'hui et cela devrait prendre environ 4 à 5 heures. Une fois terminé et testé, je donnerai des nouvelles. Merci encore à tous ceux qui consultent mon projet, certains m'ont même envoyé des liens vers des ressources OCR alors merci ! J'espère que plus de gens essaieront cela et expérimenteront avec leurs propres jeux de données.</p><h3>Mise à jour de l'entraînement</h3></p><p>J'ai commencé l'entraînement sur un corpus de 435 Mo (108 M de tokens), ça se passe plutôt bien pour l'instant. La perte d'entraînement est passée de 10,9 à 4,9 lors des 2800 premières itérations. Je pense que cela prendra environ 8 ou 9 heures pour se terminer. Je posterai une autre mise à jour une fois que ce sera fait.</p><h2>17 juillet 2025 2:13AM</h2></p><p>L'entraînement du second modèle est terminé, cela a pris environ 8 heures et 40 minutes à mon 4060 (3 900 iters/h) pour 33 000 itérations (5 époques). La perte d'entraînement finale était de 3,73. Les sorties étaient étonnamment bonnes, il génère maintenant vraiment des phrases de style XIXe siècle cohérentes.</p><h2>28 juillet 2025</h2></p><p>J'ai uploadé la v0.5 sur Hugging Face, <a href="https://huggingface.co/haykgrigorian/TimeCapsuleLLM" target="_blank" rel="noopener noreferrer">Regardez ici</a> si vous voulez. Vous pouvez maintenant télécharger mon repo et l'exécuter localement. Malheureusement nanoGPT ne fonctionne pas nativement avec HuggingFace, donc vous devrez télécharger et exécuter le modèle localement.</p><p>Je vais également commencer à sélectionner les données pour mon prochain entraînement, je pense qu'il me faudra peut-être 5 à 10 fois plus de données pour atteindre des capacités de raisonnement.</p><h1>Comportement et limitations du modèle V0</h1></p><p>Les premiers prompts montrent le modèle répondant avec la langue et le comportement des années 1800. Par exemple, je lui ai demandé "Who art Henry?" et il a répondu "I know that man, I have did not a black, the storm." et oui, cette phrase n'a aucun sens mais le LLM reconnaît que je demande à propos d'une personne.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1850_v0/timelockllm_sample_output.png?raw=true" alt="Exemple de sortie TimeLockLLM"></p><p>Aucune mention de concepts modernes, les sorties contiennent principalement des mots et des formulations du XIXe siècle.</p><p>Il reste encore beaucoup de travail, s’entraîner sur 187 Mo ne permet pas d’obtenir un modèle capable de produire un texte avec un raisonnement complexe.</p><p>Actuellement, il produit des phrases qui manquent de structure complète et globalement n’ont aucun sens, mais c’est normal compte tenu de la taille de l’entraînement.</p><h1>Comportement et limitations du modèle V0.5</h1></p><p>C’est une belle amélioration par rapport au dernier modèle. Le style d’écriture et le vocabulaire sont victoriens et presque chaque phrase est grammaticalement correcte avec une ponctuation appropriée. Et encore une fois, ce modèle est entraîné à partir de zéro donc il reste sur des sujets du XIXe siècle.</p><p><img src="https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/london_1800_1875_v0.5/fellowcitizens.png?raw=true" alt="Exemple de sortie TimeLockLLM"></p><p>Il y a beaucoup d’hallucinations factuelles. Beaucoup (pratiquement 100 %) des détails (dates, événements, personnages historiques) sont inventés. De plus, les phrases n’ont pas vraiment de liens entre elles, parfois 2 phrases peuvent être liées mais pas au-delà. Un autre problème est qu’il arrive qu’un pied de page « Digitized by Google » apparaisse, donc la prochaine fois que j’entraîne, je dois vraiment m’assurer que les textes sont bien nettoyés. Globalement, je suis très content des résultats, on est loin d’un LLM mais c’est définitivement un générateur de phrases.</p><p>J’apprends beaucoup et je vais commencer à identifier ce que je dois améliorer dans les semaines à venir. Je mettrai bientôt les fichiers en ligne !</p><h1>Projets à venir</h1></p><p>(Terminé) Je vais commencer à travailler sur la version 0.5, au lieu d’entraîner sur 50 livres, j’entraînerai idéalement sur 500-600. Actuellement, j’entraîne nanoGPT en utilisant des livres de 1800-1850, spécifiquement de Londres. Il y a quelques défis comme s’assurer que les livres trouvés ne sont pas modernisés ou interprétés, mais bien des livres intacts publiés dans la période choisie.</p><p>Je veux entraîner un nouveau modèle (v1) avec un corpus beaucoup plus large, peut-être 5 à 10 fois plus grand que celui utilisé pour v0.5. Mon objectif est de voir si des capacités de raisonnement peuvent émerger grâce au Selective Temporal Training seul, ce qui sera plus difficile et je ne suis même pas sûr que ce soit possible à cause des limitations de données historiques. Dans les semaines à venir, je vais essayer de réunir assez de données pour un corpus de 5 à 10 Go. Je crois qu’avec des données propres et de haute qualité et en louant un GPU, il y aura des progrès.</p><h1>Comment utiliser ce projet</h1></p><p>Ce projet se concentre surtout sur la curation de données historiques, leur préparation pour l’entraînement et la création d’un tokenizer. Je ne couvrirai pas tout le processus d’entraînement d’un LLM, pour cela référez-vous à nanoGPT d’Andrej Karpathy.</p><h1>Étape 1 : Rassembler et préparer les textes historiques</h1></p><p>Collectez des fichiers .txt de livres, documents, etc. du domaine public de la période choisie (par ex. Londres 1800-1850)</p><p>Vous pouvez utiliser download_texts_improved.py pour télécharger des livres si besoin.</p><p>Nettoyez les fichiers texte à l’aide d’un script ou manuellement pour retirer les en-têtes/pieds de page de Project Gutenberg, annotations modernes ou erreurs d’OCR.</p><p>prepare_dataset.py devrait fonctionner correctement.</p><h1>Étape 2 : Construire un tokenizer personnalisé</h1></p><p>Lancez train_tokenizer.py ou train_tokenizer_hf.py sur les données nettoyées.
Cela vous donnera vocab.json et merges.txt</p><p>Ces fichiers définissent le vocabulaire et les règles de fusion pour votre modèle</p><h1>Étape 3 : Entraînez votre modèle (nanoGPT)</h1></p><p>Référez-vous à <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noopener noreferrer">nanoGPT d’Andrej Karpathy</a> pour le processus d’entraînement.</p><p>Vous pouvez entraîner un autre LLM si vous le souhaitez, mais j’ai utilisé nanoGPT</p><h1>FAQ</h1></p><h2>Qu’est-ce que le Selective Temporal Training ?</h2></p><p>Le Selective Temporal Training (STT) est une méthodologie d’apprentissage automatique où toutes les données d’entraînement sont spécifiquement sélectionnées pour appartenir à une période historique précise. Cela permet de modéliser la langue et les connaissances de cette époque sans influence de concepts modernes. Par exemple, le modèle actuel (v0.5) est entraîné uniquement sur des données de 1800-1875, il n’est pas affiné mais entraîné à partir de zéro, ce qui donne une sortie reflétant le style linguistique et le contexte historique de cette période.</p><h2>Pourquoi ne pas simplement utiliser le fine-tuning ou LoRA ?</h2></p><p>Pour ce projet, j’essaie de créer un modèle de langue non biaisé par la modernité. Si je fais du fine-tuning sur quelque chose comme GPT-2, il est déjà pré-entraîné et cette information ne disparaîtra pas. Si j’entraîne à partir de zéro, le modèle ne fera pas semblant d’être ancien, il le sera vraiment. L’objectif de ce projet actuellement est de créer quelque chose qui puisse raisonner exclusivement avec les connaissances issues de livres londoniens publiés entre 1800 et 1850.</p><h2>Quel type de données as-tu utilisé pour l’entraînement ?</h2></p><p>J’utilise des livres, documents juridiques, journaux et autres écrits de Londres 1800–1850. La liste que j’ai partagée contient environ 200 documents, mais pour le premier entraînement je n’ai utilisé que 50 fichiers d’environ ~187 Mo. Vous pouvez consulter la liste des documents :
https://github.com/haykgrigo3/TimeCapsuleLLM/blob/main/Copy%20of%20London%20Documents%20for%20Time%20Capsule%20LLM.txt</p><h2>Quelle est la taille du modèle Version 0 ?</h2></p><p>Ce modèle est très petit pour l’instant, je fais ça pour le plaisir et je respecte la règle stricte de n’utiliser aucune source moderne. Il a près de 16 millions de paramètres mais je vais commencer à rassembler plus de vieux textes pour débuter un nouvel entraînement. Je donnerai des nouvelles au fur et à mesure.</p><h2>Spécifications d’entraînement ?</h2></p><p>GPU : Geforce RTX 4060
CPU : i5-13400F
Ram : 16 Go DDR5.</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-08-02

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/haykgrigo3/TimeCapsuleLLM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-08-02 
    </div>
    
</body>
</html>