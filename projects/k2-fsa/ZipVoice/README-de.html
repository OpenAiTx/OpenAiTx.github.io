<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Schnelle und hochwertige Zero-Shot-Text-zu-Sprache mit Flow Matching</title>
    <meta name="description" content="Schnelle und hochwertige Zero-Shot-Text-zu-Sprache mit Flow Matching">
    <meta name="keywords" content="ZipVoice, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Schnelle und hochwertige Zero-Shot-Text-zu-Sprache mit Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 748
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-12-30",
  "dateModified": "2025-12-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 748 stars</span>
                <span class="language">German</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Sprache</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Schnelle und hochwertige Zero-Shot-Text-zu-Sprache mit Flow Matching</h2>
</div></p><h2>Übersicht</h2></p><p>ZipVoice ist eine Serie von schnellen und hochwertigen Zero-Shot-TTS-Modellen, die auf Flow Matching basieren.</p><h3>1. Hauptmerkmale</h3></p><ul><li>Klein und schnell: nur 123M Parameter.</li></p><p><li>Hochwertiges Voice Cloning: branchenführende Leistung bei Sprecherähnlichkeit, Verständlichkeit und Natürlichkeit.</li></p><p><li>Mehrsprachig: unterstützt Chinesisch und Englisch.</li></p><p><li>Multi-Mode: unterstützt sowohl Einzelsprecher- als auch Dialog-Sprachgenerierung.</li></p><p></ul><h3>2. Modellvarianten</h3></p><p><table>
  <thead>
    <tr>
      <th>Modellname</th>
      <th>Beschreibung</th>
      <th>Paper</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Das Basismodell, das Zero-Shot-Einzelsprecher-TTS in Chinesisch und Englisch unterstützt.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Die destillierte Version von ZipVoice mit verbesserter Geschwindigkeit und minimalem Leistungsverlust.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Ein Dialoggenerierungsmodell, das auf ZipVoice basiert und einsprachige Zwei-Parteien-Gespräche erzeugen kann.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>Die Stereo-Variante von ZipVoice-Dialog, ermöglicht zweikanalige Dialoggenerierung mit jedem Sprecher auf einem eigenen Kanal.</td>
    </tr>
  </tbody>
</table></p><h2>Neuigkeiten</h2></p><p><strong>2025/07/14</strong>: <strong>ZipVoice-Dialog</strong> und <strong>ZipVoice-Dialog-Stereo</strong>, zwei Modelle zur gesprochenen Dialoggenerierung, sind veröffentlicht. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="Demo-Seite"></a></p><p><strong>2025/07/14</strong>: <strong>OpenDialog</strong> Datensatz, ein 6,8k-Stunden-Datensatz für gesprochene Dialoge, ist veröffentlicht. Download unter <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Details unter <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: <strong>ZipVoice</strong> und <strong>ZipVoice-Distill</strong> sind veröffentlicht. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="Demo-Seite"></a></p><h2>Installation</h2></p><h3>1. Klone das ZipVoice-Repository</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Optional) Erstellen Sie eine Python-virtuelle Umgebung</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Installieren Sie die erforderlichen Pakete</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Installieren Sie k2 für das Training oder effizientes Inferenzieren</h3></p><p><strong>k2 ist für das Training notwendig</strong> und kann die Inferenz beschleunigen. Dennoch können Sie den Inferenzmodus von ZipVoice auch ohne die Installation von k2 verwenden.</p><blockquote><strong>Hinweis:</strong> Stellen Sie sicher, dass Sie die k2-Version installieren, die zu Ihrer PyTorch- und CUDA-Version passt. Wenn Sie beispielsweise pytorch 2.5.1 und CUDA 12.1 verwenden, können Sie k2 wie folgt installieren:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Bitte beachten Sie https://k2-fsa.org/get-started/k2/ für weitere Details.
Nutzer in Festlandchina können https://k2-fsa.org/zh-CN/get-started/k2/ nutzen.</p><ul><li>Um die k2-Installation zu überprüfen:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Verwendung</h2></p><h3>1. Sprachgenerierung mit einem Sprecher</h3></p><p>Um Sprachaufnahmen mit nur einem Sprecher mithilfe unserer vortrainierten ZipVoice- oder ZipVoice-Distill-Modelle zu erzeugen, verwenden Sie die folgenden Befehle (Erforderliche Modelle werden von HuggingFace heruntergeladen):</p><p>#### 1.1 Inferenz eines einzelnen Satzes</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> kann <code>zipvoice</code> oder <code>zipvoice_distill</code> sein, was jeweils die Modelle vor und nach der Destillation bezeichnet.</li>
<li>Wenn <code><></code> oder <code>[]</code> im Text erscheinen, werden von ihnen eingeschlossene Zeichenfolgen als spezielle Tokens behandelt. <code><></code> steht für chinesische Pinyin und <code>[]</code> für andere spezielle Tags.</li></p><p></ul>#### 1.2 Inferenz einer Liste von Sätzen</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Jede Zeile von <code>test.tsv</code> hat das Format <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Dialog-Sprachgenerierung</h3></p><p>#### 2.1 Inferenzbefehl</p><p>Um Zwei-Parteien-Dialoge mit unseren vortrainierten ZipVoice-Dialogue oder ZipVoice-Dialogue-Stereo Modellen zu generieren, verwenden Sie die folgenden Befehle (Die benötigten Modelle werden von HuggingFace heruntergeladen):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> kann entweder <code>zipvoice_dialog</code> oder <code>zipvoice_dialog_stereo</code> sein,</li>
    </ul>wobei jeweils Mono- bzw. Stereo-Dialoge erzeugt werden.</p><p>#### 2.2 Eingabeformate</p><p>Jede Zeile in <code>test.tsv</code> hat eines der folgenden Formate:</p><p>(1) <strong>Zusammengeführtes Prompt-Format</strong>, bei dem die Audiodateien und Transkriptionen der Prompts beider Sprecher in einer Prompt-WAV-Datei zusammengeführt werden:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> ist der Name der Ausgabedatei im wav-Format.</li>
<li><code>prompt_transcription</code> ist die Transkription der Konversationsaufforderung (Prompt-wav), z.B. "[S1] Hallo. [S2] Wie geht es dir?"</li>
<li><code>prompt_wav</code> ist der Pfad zur Prompt-wav-Datei.</li>
<li><code>text</code> ist der zu synthetisierende Text, z.B. "[S1] Mir geht es gut. [S2] Wie heißt du? [S1] Ich bin Eric. [S2] Hallo Eric."</li></p><p></ul>(2) <strong>Geteiltes Prompt-Format</strong>, bei dem die Audiodateien und Transkriptionen der beiden Sprecher in separaten Dateien vorliegen:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> ist der Name der Ausgabedatei im WAV-Format.</li>
<li><code>spk1_prompt_transcription</code> ist die Transkription der Prompt-WAV-Datei des ersten Sprechers, z. B. „Hallo“.</li>
<li><code>spk2_prompt_transcription</code> ist die Transkription der Prompt-WAV-Datei des zweiten Sprechers, z. B. „Wie geht's?“</li>
<li><code>spk1_prompt_wav</code> ist der Pfad zur Prompt-WAV-Datei des ersten Sprechers.</li>
<li><code>spk2_prompt_wav</code> ist der Pfad zur Prompt-WAV-Datei des zweiten Sprechers.</li>
<li><code>text</code> ist der zu synthetisierende Text, z. B. „[S1] Mir geht's gut. [S2] Wie heißt du? [S1] Ich bin Eric. [S2] Hallo Eric.“</li></p><p></ul><h3>3 Anleitung für bessere Nutzung:</h3></p><p>#### 3.1 Länge des Prompts</p><p>Wir empfehlen eine kurze Prompt-WAV-Datei (z. B. weniger als 3 Sekunden für die Einzelsprecher-Spracherzeugung, weniger als 10 Sekunden für die Dialog-Spracherzeugung) für eine schnellere Inferenzgeschwindigkeit. Ein sehr langer Prompt verlangsamt die Inferenz und verschlechtert die Sprachqualität.</p><p>#### 3.2 Geschwindigkeitsoptimierung</p><p>Wenn die Inferenzgeschwindigkeit unzufriedenstellend ist, können Sie wie folgt beschleunigen:</p><ul><li><strong>Distill-Modell und weniger Schritte</strong>: Für das Einzelsprecher-Spracherzeugungsmodell verwenden wir standardmäßig das <code>zipvoice</code>-Modell für bessere Sprachqualität. Wenn schnellere Geschwindigkeit Priorität hat, können Sie auf <code>zipvoice_distill</code> wechseln und die <code>--num-steps</code> auf bis zu <code>4</code> reduzieren (Standard ist 8).</li></p><p><li><strong>CPU-Beschleunigung durch Multi-Threading</strong>: Beim Ausführen auf der CPU können Sie den Parameter <code>--num-thread</code> übergeben (z. B. <code>--num-thread 4</code>), um die Anzahl der Threads für höhere Geschwindigkeit zu erhöhen. Standardmäßig wird 1 Thread verwendet.</li></p><p><li><strong>CPU-Beschleunigung mit ONNX</strong>: Bei CPU-Ausführung können Sie ONNX-Modelle mit <code>zipvoice.bin.infer_zipvoice_onnx</code> für höhere Geschwindigkeit verwenden (ONNX wird für Dialog-Generierungsmodelle noch nicht unterstützt). Für noch höhere Geschwindigkeit können Sie zusätzlich <code>--onnx-int8 True</code> setzen, um ein INT8-quantisiertes ONNX-Modell zu nutzen. Beachten Sie, dass das quantisierte Modell zu einer gewissen Verschlechterung der Sprachqualität führt. <strong>Verwenden Sie ONNX nicht auf der GPU</strong>, da es dort langsamer als PyTorch ist.</li></p><p><li><strong>GPU-Beschleunigung mit NVIDIA TensorRT</strong>: Für einen deutlichen Leistungsschub auf NVIDIA-GPUs exportieren Sie zunächst das Modell mit zipvoice.bin.tensorrt_export als TensorRT-Engine. Führen Sie dann die Inferenz auf Ihrem Datensatz (z. B. Hugging Face-Datensatz) mit zipvoice.bin.infer_zipvoice aus. Dies kann etwa die doppelte Durchsatzrate im Vergleich zur Standard-PyTorch-Implementierung auf einer GPU erreichen.</li></p><p></ul>#### 3.3 Speichersteuerung</p><p>Der angegebene Text wird anhand von Satzzeichen (bei Einzelsprecher-Spracherzeugung) oder Sprecherwechsel-Symbolen (bei Dialog-Spracherzeugung) in Abschnitte unterteilt. Anschließend werden die Abschnitte stapelweise verarbeitet. Dadurch kann das Modell beliebig lange Texte mit nahezu konstantem Speicherbedarf verarbeiten. Sie können den Speicherverbrauch durch Anpassung des Parameters <code>--max-duration</code> steuern.</p><p>#### 3.4 „Rohe“ Auswertung</p><p>Standardmäßig werden Eingaben (Prompt-WAV, Prompt-Transkription und Text) für effiziente Inferenz und bessere Leistung vorverarbeitet. Wenn Sie die „rohe“ Leistung des Modells mit den exakt angegebenen Eingaben bewerten möchten (z. B. zur Reproduktion der Ergebnisse unserer Publikation), können Sie <code>--raw-evaluation True</code> übergeben.</p><p>#### 3.5 Kurzer Text</p><p>Bei der Generierung von Sprache für sehr kurze Texte (z. B. ein oder zwei Wörter) kann es vorkommen, dass bestimmte Aussprachen im erzeugten Sprachsignal fehlen. Um dieses Problem zu beheben, können Sie <code>--speed 0.3</code> übergeben (wobei 0.3 ein anpassbarer Wert ist), um die Dauer der erzeugten Sprache zu verlängern.</p><p>#### 3.6 Korrektur von falsch ausgesprochenen chinesischen Polyphonen-Zeichen</p><p>Wir verwenden <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a>, um chinesische Schriftzeichen in Pinyin umzuwandeln. Allerdings kann es gelegentlich <strong>polyphone Zeichen</strong> (多音字) falsch aussprechen.</p><p>Um diese Fehl-Aussprache manuell zu korrigieren, schließen Sie das <strong>korrigierte Pinyin</strong> in spitze Klammern <code>< ></code> ein und fügen Sie das <strong>Tonzeichen</strong> hinzu.</p><p><strong>Beispiel:</strong></p><ul><li>Originaltext: <code>这把剑长三十公分</code></li>
<li>Pinyin von <code>长</code> korrigieren:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Hinweis:</strong> Wenn Sie mehreren Zeichen manuell Pinyin zuweisen möchten, schließen Sie jedes Pinyin in <code><></code> ein, z.B. <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Entfernen von langen Pausen aus der generierten Sprache</p><p>Das Modell bestimmt automatisch die Positionen und Längen der Pausen in der generierten Sprache. Gelegentlich gibt es lange Pausen mitten in der Sprache. Wenn Sie dies nicht wünschen, können Sie <code>--remove-long-sil</code> verwenden, um lange Pausen in der Mitte der generierten Sprache zu entfernen (Randpausen werden standardmäßig entfernt).</p><p>#### 3.8 Modell-Download</p><p>Wenn Sie beim Herunterladen der vortrainierten Modelle Schwierigkeiten haben, eine Verbindung zu HuggingFace herzustellen, versuchen Sie, den Endpunkt auf die Spiegelseite zu wechseln: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Eigenes Modell trainieren</h2></p><p>Siehe das <a href="egs" target="_blank" rel="noopener noreferrer">egs</a>-Verzeichnis für Beispiele zum Training, Fine-Tuning und zur Bewertung.</p><h2>Produktiv-Einsatz</h2></p><h3>NVIDIA Triton GPU-Laufzeit</h3></p><p>Für produktionsbereiten Einsatz mit hoher Leistung und Skalierbarkeit sehen Sie sich die <a href="runtime/nvidia_triton/" target="_blank" rel="noopener noreferrer">Triton Inference Server-Integration</a> an, die optimierte TensorRT-Engines, gleichzeitige Anfragebearbeitung und sowohl gRPC/HTTP-APIs für den Unternehmenseinsatz bietet.</p><h3>CPU-Bereitstellung</h3></p><p>Siehe <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> für die C++-Bereitstellungslösung auf der CPU.</p><h2>Diskussion & Kommunikation</h2></p><p>Sie können direkt auf <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a> diskutieren.</p><p>Sie können auch den QR-Code scannen, um unserer WeChat-Gruppe beizutreten oder unserem offiziellen WeChat-Account zu folgen.</p><p>| WeChat-Gruppe | Offizieller WeChat-Account |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Zitation</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-30 
    </div>
    
</body>
</html>