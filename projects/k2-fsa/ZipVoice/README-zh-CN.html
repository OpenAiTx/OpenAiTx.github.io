<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - 快速高质量零样本文本转语音，采用流匹配技术</title>
    <meta name="description" content="快速高质量零样本文本转语音，采用流匹配技术">
    <meta name="keywords" content="ZipVoice, Simplified Chinese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "快速高质量零样本文本转语音，采用流匹配技术",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 661
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-zh-CN.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-10-06",
  "dateModified": "2025-10-06"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 661 stars</span>
                <span class="language">Simplified Chinese</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 语言</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>基于流匹配的快速高质量零样本文本转语音</h2>
</div></p><h2>概述</h2></p><p>ZipVoice 是一系列基于流匹配的快速高质量零样本语音合成（TTS）模型。</p><h3>1. 主要特性</h3></p><ul><li>小巧且快速：仅有 123M 参数。</li></p><p><li>高质量语音克隆：在说话人相似性、可懂度和自然度方面达到业界领先性能。</li></p><p><li>多语言支持：支持中文和英文。</li></p><p><li>多模式支持：支持单说话人和对话语音生成。</li></p><p></ul><h3>2. 模型变体</h3></p><p><table>
  <thead>
    <tr>
      <th>模型名称</th>
      <th>描述</th>
      <th>论文</th>
      <th>演示</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>基础模型，支持中英文零样本单说话人语音合成。</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>ZipVoice 的蒸馏版本，速度更快且性能损失极小。</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>基于 ZipVoice 构建的对话生成模型，可生成单声道双方对话语音。</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>ZipVoice-Dialog 的立体声版本，实现双声道对话生成，每位说话人分配到一个独立声道。</td>
    </tr>
  </tbody>
</table></p><h2>新闻</h2></p><p><strong>2025/07/14</strong>：<strong>ZipVoice-Dialog</strong> 和 <strong>ZipVoice-Dialog-Stereo</strong> 两个语音对话生成模型已发布。<a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>：<strong>OpenDialog</strong> 数据集（6.8k 小时语音对话数据集）已发布。下载地址：<a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>。详情请查阅 <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>。</p><p><strong>2025/06/16</strong>：<strong>ZipVoice</strong> 和 <strong>ZipVoice-Distill</strong> 已发布。<a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>安装</h2></p><h3>1. 克隆 ZipVoice 仓库</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2.（可选）创建一个Python虚拟环境</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. 安装所需的软件包</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. 安装 k2 以进行训练或高效推理</h3></p><p><strong>k2 是训练所必需的</strong>，并且可以加快推理速度。不过，即使不安装 k2，你仍然可以使用 ZipVoice 的推理模式。</p><blockquote><strong>注意：</strong> 请确保安装与你的 PyTorch 和 CUDA 版本相匹配的 k2 版本。例如，如果你使用的是 pytorch 2.5.1 和 CUDA 12.1，可以按如下方式安装 k2：</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
请参阅 https://k2-fsa.org/get-started/k2/ 获取详细信息。
中国大陆用户可参阅 https://k2-fsa.org/zh-CN/get-started/k2/。</p><ul><li>检查 k2 是否已安装：</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>用法</h2></p><h3>1. 单说话人语音生成</h3></p><p>要使用我们预训练的 ZipVoice 或 ZipVoice-Distill 模型生成单说话人语音，请使用以下命令（所需模型将从 HuggingFace 下载）：</p><p>#### 1.1 单句推理</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> 可以是 <code>zipvoice</code> 或 <code>zipvoice_distill</code>，分别表示蒸馏前和蒸馏后的模型。</li>
<li>如果文本中出现 <code><></code> 或 <code>[]</code>，被其包围的字符串将被视为特殊标记。<code><></code> 表示中文拼音，<code>[]</code> 表示其他特殊标签。</li></p><p></ul>#### 1.2 一组句子的推理</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>test.tsv</code> 的每一行格式为 <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>。</li></p><p></ul><h3>2. 对话语音生成</h3></p><p>#### 2.1 推理命令</p><p>要使用我们预训练的 ZipVoice-Dialogue 或 ZipVoice-Dialogue-Stereo 模型生成双人对话语音，请使用以下命令（所需模型将从 HuggingFace 下载）：</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> 可以是 <code>zipvoice_dialog</code> 或 <code>zipvoice_dialog_stereo</code>，  </li>
    </ul>分别生成单声道和立体声对话。  </p><p>#### 2.2 输入格式  </p><p><code>test.tsv</code> 的每一行格式为以下之一：  </p><p>(1) <strong>合并提示格式</strong>，其中两个说话人的音频和转录内容合并为一个提示 wav 文件：</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> 是输出 wav 文件的名称。</li>
<li><code>prompt_transcription</code> 是对话提示 wav 的转录文本，例如，“[S1] 你好。[S2] 你好吗？”</li>
<li><code>prompt_wav</code> 是提示 wav 的路径。</li>
<li><code>text</code> 是要合成的文本，例如，“[S1] 我很好。[S2] 你叫什么名字？[S1] 我叫 Eric。[S2] 嗨，Eric。”</li></p><p></ul>(2) <strong>分割提示格式</strong>，即两位说话人的音频和转录分别存在于不同文件中：</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> 是输出 wav 文件的名称。</li>
<li><code>spk1_prompt_transcription</code> 是第一位说话者提示音频的转录内容，例如，“Hello”。</li>
<li><code>spk2_prompt_transcription</code> 是第二位说话者提示音频的转录内容，例如，“How are you?”。</li>
<li><code>spk1_prompt_wav</code> 是第一位说话者提示音频文件的路径。</li>
<li><code>spk2_prompt_wav</code> 是第二位说话者提示音频文件的路径。</li>
<li><code>text</code> 是需要合成的文本，例如，“[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric.”</li></p><p></ul><h3>3 更好使用的指导：</h3></p><p>#### 3.1 提示音频长度</p><p>我们推荐使用较短的提示音频文件（如单说话人语音生成时小于 3 秒，对话语音生成时小于 10 秒），以加快推理速度。提示过长会降低推理速度并影响语音质量。</p><p>#### 3.2 速度优化</p><p>如果推理速度不理想，可以按如下方式提升：</p><ul><li><strong>模型蒸馏与减少步数</strong>：单说话人语音生成模型默认使用 <code>zipvoice</code> 模型以获得更好的语音质量。如果优先考虑速度，可以切换到 <code>zipvoice_distill</code> 并将 <code>--num-steps</code> 降低至最低 <code>4</code>（默认 8）。</li>
  
<li><strong>CPU 多线程加速</strong>：在 CPU 上运行时，可以通过 <code>--num-thread</code> 参数（如 <code>--num-thread 4</code>）增加线程数提升速度。默认使用 1 个线程。</li></p><p><li><strong>CPU 使用 ONNX 加速</strong>：在 CPU 上运行时，可以用 ONNX 模型通过 <code>zipvoice.bin.infer_zipvoice_onnx</code> 加快速度（暂未支持对话生成模型的 ONNX）。如需更快速度，还可设置 <code>--onnx-int8 True</code> 使用 INT8 量化 ONNX 模型。注意，量化模型会有一定语音质量下降。<strong>不要在 GPU 上使用 ONNX</strong>，因其在 GPU 上比 PyTorch 慢。</li></p><p></ul>#### 3.3 内存控制</p><p>输入文本会根据标点（单说话人语音生成）或说话人切换符号（对话语音生成）拆分为多个片段。然后分批处理这些文本块。因此，模型能以几乎恒定的内存处理任意长度文本。可通过调整 <code>--max-duration</code> 参数控制内存使用。</p><p>#### 3.4 “原始”评估</p><p>默认情况下，我们会对输入（提示音频、提示转录与文本）进行预处理，以提升推理效率和表现。如果希望用精确的原始输入评估模型（如复现论文结果），可传递 <code>--raw-evaluation True</code>。</p><p>#### 3.5 短文本</p><p>对于极短文本（如一两个词）生成语音时，生成语音可能偶尔会遗漏某些发音。为解决此问题，可传递 <code>--speed 0.3</code>（0.3 为可调值）以延长生成语音的持续时间。</p><p>#### 3.6 修正中文多音字发音错误</p><p>我们使用 <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> 将中文字符转换为拼音。但偶尔会错误发音 <strong>多音字</strong>。</p><p>
要手动纠正这些发音错误，请将<strong>纠正后的拼音</strong>用尖括号 <code>< ></code> 括起来，并包含<strong>声调标记</strong>。</p><p><strong>示例：</strong></p><ul><li>原文：<code>这把剑长三十公分</code></li>
<li>纠正<code>长</code>的拼音：<code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>注意：</strong> 如果需要手动指定多个拼音，请用<code><></code>分别括起来，例如：<code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 从生成的语音中移除长静音</p><p>模型会自动检测生成语音中的静音位置和时长。语音中间有时会出现较长的静音。如果你不需要，可以传递<code>--remove-long-sil</code>参数来移除语音中间的长静音（边缘静音默认会被移除）。</p><p>#### 3.8 模型下载</p><p>如果在下载预训练模型时连接 HuggingFace 有困难，请尝试切换端点到镜像站点：<code>export HF_ENDPOINT=https://hf-mirror.com</code>。</p><h2>训练你自己的模型</h2></p><p>请参阅 <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> 目录，获取训练、微调和评估的示例。</p><h2>C++ 部署</h2></p><p>请查看 <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a>，了解在 CPU 上进行 C++ 部署的解决方案。</p><h2>讨论与交流</h2></p><p>你可以在 <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a> 上直接讨论。</p><p>你也可以扫码加入微信群或关注我们的微信公众号。</p><p>| 微信群 | 微信公众号 |
| ------ | ---------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>引用</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-10-06

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-10-06 
    </div>
    
</body>
</html>