<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Conversi&#243;n de texto a voz r&#225;pida y de alta calidad sin entrenamiento previo con Flow Matching</title>
    <meta name="description" content="Conversi&#243;n de texto a voz r&#225;pida y de alta calidad sin entrenamiento previo con Flow Matching">
    <meta name="keywords" content="ZipVoice, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Conversión de texto a voz rápida y de alta calidad sin entrenamiento previo con Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 748
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-12-30",
  "dateModified": "2025-12-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 748 stars</span>
                <span class="language">Spanish</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Texto a Voz Rápido y de Alta Calidad Zero-Shot con Flow Matching</h2>
</div></p><h2>Descripción General</h2></p><p>ZipVoice es una serie de modelos TTS de cero-shot rápidos y de alta calidad basados en flow matching.</p><h3>1. Características clave</h3></p><ul><li>Pequeño y rápido: solo 123M de parámetros.</li></p><p><li>Clonación de voz de alta calidad: rendimiento de vanguardia en similitud de hablante, inteligibilidad y naturalidad.</li></p><p><li>Multilingüe: soporta chino e inglés.</li></p><p><li>Multimodo: soporta generación de habla tanto de un solo hablante como de diálogos.</li></p><p></ul><h3>2. Variantes del modelo</h3></p><p><table>
  <thead>
    <tr>
      <th>Nombre del Modelo</th>
      <th>Descripción</th>
      <th>Artículo</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>El modelo básico que soporta TTS de un solo hablante en chino e inglés con cero-shot.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Página_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>La versión destilada de ZipVoice, con velocidad mejorada y mínima degradación de rendimiento.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Un modelo de generación de diálogos basado en ZipVoice, capaz de generar diálogos hablados de dos partes en un solo canal.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Página_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>La variante estéreo de ZipVoice-Dialog, que permite la generación de diálogos en dos canales con cada hablante asignado a un canal distinto.</td>
    </tr>
  </tbody>
</table></p><h2>Noticias</h2></p><p><strong>2025/07/14</strong>: Se lanzan <strong>ZipVoice-Dialog</strong> y <strong>ZipVoice-Dialog-Stereo</strong>, dos modelos para generación de diálogos hablados. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: Se publica el conjunto de datos <strong>OpenDialog</strong>, un dataset de diálogos hablados de 6.8k horas. Descárgalo en <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Consulta detalles en <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: Se lanzan <strong>ZipVoice</strong> y <strong>ZipVoice-Distill</strong>. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Instalación</h2></p><h3>1. Clona el repositorio de ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Opcional) Crear un entorno virtual de Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Instale los paquetes requeridos</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Instala k2 para entrenamiento o inferencia eficiente</h3></p><p><strong>k2 es necesario para el entrenamiento</strong> y puede acelerar la inferencia. Sin embargo, aún puedes usar el modo de inferencia de ZipVoice sin instalar k2.</p><blockquote><strong>Nota:</strong> Asegúrate de instalar la versión de k2 que coincida con tu versión de PyTorch y CUDA. Por ejemplo, si estás usando pytorch 2.5.1 y CUDA 12.1, puedes instalar k2 de la siguiente manera:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Por favor, consulte https://k2-fsa.org/get-started/k2/ para más detalles.
Los usuarios en China continental pueden consultar https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Para comprobar la instalación de k2:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Uso</h2></p><h3>1. Generación de voz de un solo hablante</h3></p><p>Para generar voz de un solo hablante con nuestros modelos preentrenados ZipVoice o ZipVoice-Distill, utilice los siguientes comandos (los modelos necesarios se descargarán desde HuggingFace):</p><p>#### 1.1 Inferencia de una sola oración</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> puede ser <code>zipvoice</code> o <code>zipvoice_distill</code>, que son modelos antes y después de la destilación, respectivamente.</li>
<li>Si aparecen <code><></code> o <code>[]</code> en el texto, las cadenas encerradas por ellos se tratarán como tokens especiales. <code><></code> denota pinyin chino y <code>[]</code> denota otras etiquetas especiales.</li></p><p></ul>#### 1.2 Inferencia de una lista de oraciones</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Cada línea de <code>test.tsv</code> tiene el formato <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Generación de habla en diálogo</h3></p><p>#### 2.1 Comando de inferencia</p><p>Para generar diálogos hablados de dos partes con nuestros modelos preentrenados ZipVoice-Dialogue o ZipVoice-Dialogue-Stereo, utilice los siguientes comandos (los modelos requeridos se descargarán desde HuggingFace):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> puede ser <code>zipvoice_dialog</code> o <code>zipvoice_dialog_stereo</code>,</li>
    </ul>que generan diálogos mono y estéreo, respectivamente.</p><p>#### 2.2 Formatos de entrada</p><p>Cada línea de <code>test.tsv</code> está en uno de los siguientes formatos:</p><p>(1) <strong>Formato de prompt combinado</strong> donde los audios y transcripciones de los prompts de dos hablantes se combinan en un solo archivo wav de prompt:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> es el nombre del archivo wav de salida.</li>
<li><code>prompt_transcription</code> es la transcripción del archivo wav del prompt conversacional, por ejemplo, "[S1] Hola. [S2] ¿Cómo estás?"</li>
<li><code>prompt_wav</code> es la ruta al archivo wav del prompt.</li>
<li><code>text</code> es el texto a sintetizar, por ejemplo, "[S1] Estoy bien. [S2] ¿Cómo te llamas? [S1] Soy Eric. [S2] Hola Eric."</li></p><p></ul>(2) <strong>Formato de prompt dividido</strong> donde los audios y transcripciones de dos hablantes existen en archivos separados:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> es el nombre del archivo wav de salida.</li>
<li><code>spk1_prompt_transcription</code> es la transcripción del archivo wav de indicación del primer hablante, por ejemplo, "Hola".</li>
<li><code>spk2_prompt_transcription</code> es la transcripción del archivo wav de indicación del segundo hablante, por ejemplo, "¿Cómo estás?"</li>
<li><code>spk1_prompt_wav</code> es la ruta al archivo wav de indicación del primer hablante.</li>
<li><code>spk2_prompt_wav</code> es la ruta al archivo wav de indicación del segundo hablante.</li>
<li><code>text</code> es el texto que se va a sintetizar, por ejemplo, "[S1] Estoy bien. [S2] ¿Cuál es tu nombre? [S1] Soy Eric. [S2] Hola Eric."</li></p><p></ul><h3>3 Guía para un mejor uso:</h3></p><p>#### 3.1 Longitud de la indicación</p><p>Recomendamos un archivo wav de indicación corto (por ejemplo, menos de 3 segundos para generación de habla de un solo hablante, menos de 10 segundos para generación de diálogos) para una velocidad de inferencia más rápida. Una indicación muy larga ralentizará la inferencia y degradará la calidad del habla.</p><p>#### 3.2 Optimización de velocidad</p><p>Si la velocidad de inferencia no es satisfactoria, puede acelerarla de la siguiente manera:</p><ul><li><strong>Modelo destilado y menos pasos</strong>: Para el modelo de generación de habla de un solo hablante, usamos el modelo <code>zipvoice</code> por defecto para mejor calidad de habla. Si la velocidad es una prioridad, puede cambiar a <code>zipvoice_distill</code> y reducir el parámetro <code>--num-steps</code> hasta <code>4</code> (8 por defecto).</li></p><p><li><strong>Aceleración de CPU con multiproceso</strong>: Al ejecutar en CPU, puede pasar el parámetro <code>--num-thread</code> (por ejemplo, <code>--num-thread 4</code>) para aumentar el número de hilos y acelerar la velocidad. Usamos 1 hilo por defecto.</li></p><p><li><strong>Aceleración de CPU con ONNX</strong>: Al ejecutar en CPU, puede usar modelos ONNX con <code>zipvoice.bin.infer_zipvoice_onnx</code> para mayor velocidad (aún no soporta ONNX para modelos de generación de diálogo). Para mayor velocidad, puede establecer <code>--onnx-int8 True</code> para usar un modelo ONNX cuantizado en INT8. Tenga en cuenta que el modelo cuantizado puede degradar la calidad del habla. <strong>No use ONNX en GPU</strong>, ya que es más lento que PyTorch en GPU.</li></p><p><li><strong>Aceleración de GPU con NVIDIA TensorRT</strong>: Para obtener un gran aumento de rendimiento en GPUs NVIDIA, primero exporte el modelo a un motor TensorRT usando zipvoice.bin.tensorrt_export. Luego, ejecute la inferencia en su conjunto de datos (por ejemplo, un conjunto de datos de Hugging Face) con zipvoice.bin.infer_zipvoice. Esto puede lograr aproximadamente el doble de rendimiento en comparación con la implementación estándar de PyTorch en GPU.</li></p><p></ul>#### 3.3 Control de memoria</p><p>El texto dado se dividirá en fragmentos según la puntuación (para generación de habla de un solo hablante) o el símbolo de cambio de hablante (para generación de diálogo). Luego, los textos fragmentados se procesarán en lotes. Por lo tanto, el modelo puede procesar textos arbitrariamente largos con uso de memoria casi constante. Puede controlar el uso de memoria ajustando el parámetro <code>--max-duration</code>.</p><p>#### 3.4 Evaluación "cruda"</p><p>Por defecto, preprocesamos las entradas (archivo wav de indicación, transcripción de indicación y texto) para una inferencia eficiente y mejor rendimiento. Si desea evaluar el rendimiento "crudo" del modelo usando exactamente las entradas proporcionadas (por ejemplo, para reproducir los resultados de nuestro artículo), puede pasar <code>--raw-evaluation True</code>.</p><p>#### 3.5 Texto corto</p><p>Al generar habla para textos muy cortos (por ejemplo, una o dos palabras), el habla generada puede omitir ciertas pronunciaciones. Para resolver este problema, puede pasar <code>--speed 0.3</code> (donde 0.3 es un valor ajustable) para extender la duración del habla generada.</p><p>#### 3.6 Corrección de caracteres polifónicos chinos mal pronunciados</p><p>
Utilizamos <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> para convertir caracteres chinos a pinyin. Sin embargo, ocasionalmente puede pronunciar incorrectamente <strong>caracteres polifónicos</strong> (多音字).</p><p>Para corregir manualmente estas malas pronunciaciones, encierre el <strong>pinyin corregido</strong> entre signos de menor y mayor <code>< ></code> e incluya la <strong>marca de tono</strong>.</p><p><strong>Ejemplo:</strong></p><ul><li>Texto original: <code>这把剑长三十公分</code></li>
<li>Corrija el pinyin de <code>长</code>:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Nota:</strong> Si desea asignar manualmente varios pinyins, encierre cada pinyin con <code><></code>, por ejemplo, <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Eliminar silencios largos del habla generada</p><p>El modelo determinará automáticamente las posiciones y longitudes de los silencios en el habla generada. Ocasionalmente tiene un silencio largo en medio del discurso. Si no desea esto, puede pasar <code>--remove-long-sil</code> para eliminar silencios largos en medio de la voz generada (los silencios en los extremos se eliminarán por defecto).</p><p>#### 3.8 Descarga del modelo</p><p>Si tiene problemas para conectarse a HuggingFace al descargar los modelos preentrenados, intente cambiar el endpoint al sitio espejo: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Entrene su propio modelo</h2></p><p>Consulte el directorio <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> para ejemplos de entrenamiento, ajuste fino y evaluación.</p><h2>Despliegue en producción</h2></p><h3>Tiempo de ejecución NVIDIA Triton GPU</h3></p><p>Para un despliegue listo para producción con alto rendimiento y escalabilidad, consulte la <a href="runtime/nvidia_triton/" target="_blank" rel="noopener noreferrer">integración con Triton Inference Server</a> que ofrece motores TensorRT optimizados, manejo de solicitudes concurrentes y APIs gRPC/HTTP para uso empresarial.</p><h3>Despliegue en CPU</h3></p><p>Consulte <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> para la solución de despliegue en C++ sobre CPU.</p><h2>Discusión y comunicación</h2></p><p>Puede discutir directamente en <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>También puede escanear el código QR para unirse a nuestro grupo de wechat o seguir nuestra cuenta oficial de wechat.</p><p>| Grupo de Wechat | Cuenta Oficial de Wechat |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Citación</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-30 
    </div>
    
</body>
</html>