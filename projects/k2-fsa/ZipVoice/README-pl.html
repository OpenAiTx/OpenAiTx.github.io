<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Szybkie i wysokiej jakości syntezowanie mowy z tekstu bez uczenia (Zero-Shot) z wykorzystaniem Flow Matching</title>
    <meta name="description" content="Szybkie i wysokiej jakości syntezowanie mowy z tekstu bez uczenia (Zero-Shot) z wykorzystaniem Flow Matching">
    <meta name="keywords" content="ZipVoice, Polish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Szybkie i wysokiej jakości syntezowanie mowy z tekstu bez uczenia (Zero-Shot) z wykorzystaniem Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 661
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-pl.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-10-06",
  "dateModified": "2025-10-06"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 661 stars</span>
                <span class="language">Polish</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Język</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Szybka i wysokiej jakości synteza mowy z tekstu bez uczenia głosu z dopasowaniem przepływu</h2>
</div></p><h2>Przegląd</h2></p><p>ZipVoice to seria szybkich i wysokiej jakości modeli TTS typu zero-shot opartych na flow matching.</p><h3>1. Kluczowe cechy</h3></p><ul><li>Mały i szybki: tylko 123M parametrów.</li></p><p><li>Wysokiej jakości klonowanie głosu: najnowocześniejsza wydajność w zakresie podobieństwa mówcy, zrozumiałości i naturalności.</li></p><p><li>Wielojęzyczny: obsługuje język chiński i angielski.</li></p><p><li>Wielomodowy: obsługuje generowanie mowy pojedynczego mówcy oraz dialogów.</li></p><p></ul><h3>2. Warianty modeli</h3></p><p><table>
  <thead>
    <tr>
      <th>Nazwa modelu</th>
      <th>Opis</th>
      <th>Publikacja</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Podstawowy model obsługujący zero-shot TTS dla pojedynczego mówcy w języku chińskim i angielskim.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Wersja destylowana ZipVoice, charakteryzująca się zwiększoną szybkością przy minimalnej utracie wydajności.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Model generowania dialogów oparty na ZipVoice, zdolny do generowania jednopasmowych, dwustronnych dialogów mówionych.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>Wariant stereo ZipVoice-Dialog, umożliwiający generowanie dwukanałowych dialogów z przypisaniem każdego rozmówcy do oddzielnego kanału.</td>
    </tr>
  </tbody>
</table></p><h2>Aktualności</h2></p><p><strong>2025/07/14</strong>: Modele generowania mowy dialogowej <strong>ZipVoice-Dialog</strong> oraz <strong>ZipVoice-Dialog-Stereo</strong> zostały wydane. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: Zbiór danych <strong>OpenDialog</strong>, obejmujący 6,8 tysiąca godzin dialogów mówionych, został wydany. Pobierz z <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Szczegóły na <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: <strong>ZipVoice</strong> oraz <strong>ZipVoice-Distill</strong> zostały wydane. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Instalacja</h2></p><h3>1. Sklonuj repozytorium ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Opcjonalnie) Utwórz wirtualne środowisko Pythona</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Zainstaluj wymagane pakiety</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Zainstaluj k2 do treningu lub wydajnego wnioskowania</h3></p><p><strong>k2 jest niezbędne do treningu</strong> i może przyspieszyć wnioskowanie. Niemniej jednak, możesz korzystać z trybu wnioskowania ZipVoice bez instalowania k2.</p><blockquote><strong>Uwaga:</strong> Upewnij się, że instalujesz wersję k2 pasującą do Twojej wersji PyTorch i CUDA. Na przykład, jeśli używasz pytorch 2.5.1 i CUDA 12.1, możesz zainstalować k2 w następujący sposób:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Proszę zapoznać się ze stroną https://k2-fsa.org/get-started/k2/ po szczegóły.
Użytkownicy z Chin kontynentalnych mogą zapoznać się ze stroną https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Aby sprawdzić instalację k2:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Użytkowanie</h2></p><h3>1. Generowanie mowy jednego mówcy</h3></p><p>Aby wygenerować mowę jednego mówcy przy użyciu naszych wytrenowanych modeli ZipVoice lub ZipVoice-Distill, użyj następujących poleceń (wymagane modele zostaną pobrane z HuggingFace):</p><p>#### 1.1 Wnioskowanie dla pojedynczego zdania</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> może być <code>zipvoice</code> lub <code>zipvoice_distill</code>, które oznaczają modele przed i po destylacji, odpowiednio.</li>
<li>Jeśli w tekście pojawią się <code><></code> lub <code>[]</code>, ciągi znaków otoczone nimi będą traktowane jako specjalne tokeny. <code><></code> oznacza chińską transkrypcję pinyin, a <code>[]</code> oznacza inne specjalne tagi.</li></p><p></ul>#### 1.2 Wnioskowanie listy zdań</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Każda linia pliku <code>test.tsv</code> ma format <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Generowanie mowy dialogowej</h3></p><p>#### 2.1 Polecenie inferencji</p><p>Aby wygenerować dwuosobowe dialogi mówione za pomocą naszych wytrenowanych modeli ZipVoice-Dialogue lub ZipVoice-Dialogue-Stereo, użyj następujących poleceń (Wymagane modele zostaną pobrane z HuggingFace):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> może przyjmować wartości <code>zipvoice_dialog</code> lub <code>zipvoice_dialog_stereo</code>,</li>
    </ul>które generują odpowiednio dialogi mono i stereo.</p><p>#### 2.2 Format wejściowy</p><p>Każda linia w pliku <code>test.tsv</code> ma jeden z poniższych formatów:</p><p>(1) <strong>Format scalonej podpowiedzi</strong>, gdzie nagrania audio i transkrypcje dwóch mówców są scalone w jeden plik wav z podpowiedzią:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> to nazwa wyjściowego pliku wav.</li>
<li><code>prompt_transcription</code> to transkrypcja konwersacyjnego pliku wav, np. "[S1] Cześć. [S2] Jak się masz?"</li>
<li><code>prompt_wav</code> to ścieżka do pliku wav z promptem.</li>
<li><code>text</code> to tekst do syntezy, np. "[S1] Wszystko w porządku. [S2] Jak masz na imię? [S1] Jestem Eric. [S2] Cześć Eric."</li></p><p></ul>(2) <strong>Format podzielonego promptu</strong>, gdzie nagrania i transkrypcje dwóch rozmówców znajdują się w osobnych plikach:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> to nazwa wyjściowego pliku wav.</li>
<li><code>spk1_prompt_transcription</code> to transkrypcja pliku wav z podpowiedzią pierwszego mówcy, np. "Hello"</li>
<li><code>spk2_prompt_transcription</code> to transkrypcja pliku wav z podpowiedzią drugiego mówcy, np. "How are you?"</li>
<li><code>spk1_prompt_wav</code> to ścieżka do pliku wav z podpowiedzią pierwszego mówcy.</li>
<li><code>spk2_prompt_wav</code> to ścieżka do pliku wav z podpowiedzią drugiego mówcy.</li>
<li><code>text</code> to tekst do syntezy, np. "[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric."</li></p><p></ul><h3>3 Wskazówki dla lepszego użytkowania:</h3></p><p>#### 3.1 Długość podpowiedzi</p><p>Zalecamy krótkie pliki wav z podpowiedzią (np. krótsze niż 3 sekundy dla generowania mowy jednego mówcy, krótsze niż 10 sekund dla generowania mowy w dialogu) dla szybszego działania. Bardzo długi plik z podpowiedzią spowolni działanie i pogorszy jakość mowy.</p><p>#### 3.2 Optymalizacja szybkości</p><p>Jeśli szybkość działania jest niezadowalająca, można ją zwiększyć następująco:</p><ul><li><strong>Model destylowany i mniej kroków</strong>: Dla modelu generowania mowy jednego mówcy domyślnie używamy modelu <code>zipvoice</code> dla lepszej jakości mowy. Jeśli priorytetem jest szybkość, można przełączyć na <code>zipvoice_distill</code> i zmniejszyć <code>--num-steps</code> nawet do <code>4</code> (domyślnie 8).</li></p><p><li><strong>Przyspieszenie CPU wielowątkowością</strong>: Przy uruchamianiu na CPU można podać parametr <code>--num-thread</code> (np. <code>--num-thread 4</code>) aby zwiększyć liczbę wątków dla szybszej pracy. Domyślnie używamy 1 wątku.</li></p><p><li><strong>Przyspieszenie CPU z ONNX</strong>: Przy uruchamianiu na CPU można użyć modeli ONNX z <code>zipvoice.bin.infer_zipvoice_onnx</code> dla szybszego działania (nieobsługiwane jeszcze dla modeli generowania dialogów). Aby uzyskać jeszcze większą szybkość, można ustawić <code>--onnx-int8 True</code> aby użyć modelu ONNX z kwantyzacją INT8. Należy pamiętać, że model kwantyzowany będzie miał pewien spadek jakości mowy. <strong>Nie używaj ONNX na GPU</strong>, ponieważ jest wolniejszy niż PyTorch na GPU.</li></p><p></ul>#### 3.3 Kontrola pamięci</p><p>Podany tekst zostanie podzielony na fragmenty na podstawie znaków interpunkcyjnych (dla generowania mowy jednego mówcy) lub symbolu zmiany mówcy (dla generowania mowy w dialogu). Następnie podzielone teksty będą przetwarzane w partiach. Dzięki temu model może przetwarzać dowolnie długi tekst przy niemal stałym zużyciu pamięci. Możesz kontrolować zużycie pamięci, regulując parametr <code>--max-duration</code>.</p><p>#### 3.4 Ocena "Raw"</p><p>Domyślnie wstępnie przetwarzamy wejścia (plik wav z podpowiedzią, transkrypcję podpowiedzi oraz tekst) dla efektywnego działania i lepszych wyników. Jeśli chcesz ocenić "surowe" działanie modelu na dokładnie podanych wejściach (np. aby odtworzyć wyniki z naszego artykułu), możesz podać <code>--raw-evaluation True</code>.</p><p>#### 3.5 Krótki tekst</p><p>Podczas generowania mowy dla bardzo krótkich tekstów (np. jedno lub dwa słowa), wygenerowana mowa może czasem pomijać pewne wymowy. Aby rozwiązać ten problem, można podać <code>--speed 0.3</code> (gdzie 0.3 to wartość do regulacji), aby wydłużyć czas trwania wygenerowanej mowy.</p><p>#### 3.6 Poprawianie błędnie wymawianych chińskich znaków wieloznacznych</p><p>Używamy <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> do konwersji chińskich znaków na pinyin. Jednak czasami może błędnie wymawiać <strong>znaki wieloznaczne</strong> (多音字).</p><p>
Aby ręcznie poprawić te błędne wymowy, umieść <strong>poprawiony pinyin</strong> w nawiasach kątowych <code>< ></code> i dodaj <strong>oznaczenie tonu</strong>.</p><p><strong>Przykład:</strong></p><ul><li>Oryginalny tekst: <code>这把剑长三十公分</code></li>
<li>Popraw pinyin <code>长</code>:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Uwaga:</strong> Jeśli chcesz ręcznie przypisać wiele pinyinów, umieść każdy pinyin w <code><></code>, np. <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Usuwanie długich pauz z wygenerowanej mowy</p><p>Model automatycznie określa pozycje i długości pauz w wygenerowanej mowie. Czasami pojawia się długa pauza w środku wypowiedzi. Jeśli nie chcesz tego, możesz przekazać <code>--remove-long-sil</code>, aby usunąć długie pauzy ze środka wygenerowanej mowy (pauzy na początku i końcu są usuwane domyślnie).</p><p>#### 3.8 Pobieranie modelu</p><p>Jeśli masz problem z połączeniem z HuggingFace podczas pobierania wstępnie wytrenowanych modeli, spróbuj przełączyć punkt końcowy na mirror: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Trenuj własny model</h2></p><p>Zobacz katalog <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> dla przykładów trenowania, dostrajania i ewaluacji.</p><h2>Wdrażanie C++ </h2></p><p>Sprawdź <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> jako rozwiązanie wdrożeniowe w C++ na CPU.</p><h2>Dyskusja i komunikacja</h2></p><p>Możesz bezpośrednio dyskutować na <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>Możesz także zeskanować kod QR, aby dołączyć do naszej grupy na WeChat lub śledzić nasz oficjalny profil WeChat.</p><p>| Grupa WeChat | Oficjalny profil WeChat |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Cytowanie</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-10-06

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-10-06 
    </div>
    
</body>
</html>