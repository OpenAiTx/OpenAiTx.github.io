<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Sintesi vocale zero-shot veloce e di alta qualit&#224; con Flow Matching</title>
    <meta name="description" content="Sintesi vocale zero-shot veloce e di alta qualit&#224; con Flow Matching">
    <meta name="keywords" content="ZipVoice, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Sintesi vocale zero-shot veloce e di alta qualità con Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 748
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-12-30",
  "dateModified": "2025-12-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 748 stars</span>
                <span class="language">Italian</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Lingua</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Sintesi Vocale Testo-a-Voce Rapida e di Alta Qualità Zero-Shot con Flow Matching</h2>
</div></p><h2>Panoramica</h2></p><p>ZipVoice è una serie di modelli TTS zero-shot rapidi e di alta qualità basati su flow matching.</p><h3>1. Caratteristiche principali</h3></p><ul><li>Piccolo e veloce: solo 123M di parametri.</li></p><p><li>Clonazione vocale di alta qualità: prestazioni all'avanguardia in somiglianza vocale, intelligibilità e naturalezza.</li></p><p><li>Multilingue: supporta cinese e inglese.</li></p><p><li>Multimodale: supporta sia la generazione vocale singola che dialoghi.</li></p><p></ul><h3>2. Varianti del modello</h3></p><p><table>
  <thead>
    <tr>
      <th>Nome Modello</th>
      <th>Descrizione</th>
      <th>Articolo</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Il modello base che supporta TTS zero-shot a singolo parlante sia in cinese che in inglese.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Articolo-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Pagina_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>La versione distillata di ZipVoice, con velocità migliorata e degrado minimo delle prestazioni.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Un modello di generazione di dialoghi basato su ZipVoice, capace di generare dialoghi parlati a due parti su singolo canale.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Articolo-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Pagina_Demo-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>La variante stereo di ZipVoice-Dialog, che consente la generazione di dialoghi a due canali con ogni speaker assegnato a un canale distinto.</td>
    </tr>
  </tbody>
</table></p><h2>Novità</h2></p><p><strong>14/07/2025</strong>: <strong>ZipVoice-Dialog</strong> e <strong>ZipVoice-Dialog-Stereo</strong>, due modelli di generazione di dialoghi vocali, sono stati rilasciati. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>14/07/2025</strong>: Il dataset <strong>OpenDialog</strong>, un dataset di dialoghi vocali di 6,8k ore, è stato rilasciato. Scarica da <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Consulta i dettagli su <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>16/06/2025</strong>: <strong>ZipVoice</strong> e <strong>ZipVoice-Distill</strong> sono stati rilasciati. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Installazione</h2></p><h3>1. Clona il repository ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Opzionale) Crea un ambiente virtuale Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Installa i pacchetti richiesti</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Installa k2 per l'addestramento o l'inferenza efficiente</h3></p><p><strong>k2 è necessario per l'addestramento</strong> e può velocizzare l'inferenza. Tuttavia, puoi comunque utilizzare la modalità di inferenza di ZipVoice senza installare k2.</p><blockquote><strong>Nota:</strong> Assicurati di installare la versione di k2 che corrisponde alla tua versione di PyTorch e CUDA. Ad esempio, se stai usando pytorch 2.5.1 e CUDA 12.1, puoi installare k2 come segue:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Si prega di fare riferimento a https://k2-fsa.org/get-started/k2/ per i dettagli.
Gli utenti nella Cina continentale possono fare riferimento a https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Per verificare l'installazione di k2:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Utilizzo</h2></p><h3>1. Generazione di parlato a singolo parlante</h3></p><p>Per generare parlato a singolo parlante con i nostri modelli pre-addestrati ZipVoice o ZipVoice-Distill, utilizzare i seguenti comandi (i modelli necessari verranno scaricati da HuggingFace):</p><p>#### 1.1 Inferenza di una singola frase</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> può essere <code>zipvoice</code> o <code>zipvoice_distill</code>, che sono rispettivamente i modelli prima e dopo la distillazione.</li>
<li>Se nel testo compaiono <code><></code> o <code>[]</code>, le stringhe racchiuse da questi saranno trattate come token speciali. <code><></code> indica il pinyin cinese e <code>[]</code> indica altri tag speciali.</li></p><p></ul>#### 1.2 Inferenza di una lista di frasi</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Ogni riga di <code>test.tsv</code> è nel formato <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Generazione di dialoghi vocali</h3></p><p>#### 2.1 Comando di inferenza</p><p>Per generare dialoghi vocali a due voci con i nostri modelli pre-addestrati ZipVoice-Dialogue o ZipVoice-Dialogue-Stereo, utilizzare i seguenti comandi (i modelli necessari saranno scaricati da HuggingFace):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> può essere <code>zipvoice_dialog</code> oppure <code>zipvoice_dialog_stereo</code>,</li>
    </ul>che generano rispettivamente dialoghi mono e stereo.</p><p>#### 2.2 Formati di input</p><p>Ogni riga di <code>test.tsv</code> è in uno dei seguenti formati:</p><p>(1) <strong>Formato prompt unificato</strong> in cui gli audio e le trascrizioni dei prompt di due speaker sono uniti in un unico file wav prompt:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> è il nome del file wav di output.</li>
<li><code>prompt_transcription</code> è la trascrizione del file wav del prompt conversazionale, ad esempio, "[S1] Ciao. [S2] Come stai?"</li>
<li><code>prompt_wav</code> è il percorso del file wav del prompt.</li>
<li><code>text</code> è il testo da sintetizzare, ad esempio, "[S1] Sto bene. [S2] Come ti chiami? [S1] Sono Eric. [S2] Ciao Eric."</li></p><p></ul>(2) <strong>Formato prompt separato</strong> dove gli audio e le trascrizioni dei due interlocutori esistono in file separati:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> è il nome del file wav di output.</li>
<li><code>spk1_prompt_transcription</code> è la trascrizione del prompt wav del primo speaker, ad esempio, "Ciao"</li>
<li><code>spk2_prompt_transcription</code> è la trascrizione del prompt wav del secondo speaker, ad esempio, "Come stai?"</li>
<li><code>spk1_prompt_wav</code> è il percorso del file wav del prompt del primo speaker.</li>
<li><code>spk2_prompt_wav</code> è il percorso del file wav del prompt del secondo speaker.</li>
<li><code>text</code> è il testo da sintetizzare, ad esempio, "[S1] Sto bene. [S2] Come ti chiami? [S1] Sono Eric. [S2] Ciao Eric."</li></p><p></ul><h3>3 Guida per un uso migliore:</h3></p><p>#### 3.1 Lunghezza del prompt</p><p>Si consiglia un file wav di prompt breve (ad esempio, meno di 3 secondi per la generazione di parlato a singolo speaker, meno di 10 secondi per la generazione di dialoghi) per una velocità di inferenza più rapida. Un prompt molto lungo rallenterà l'inferenza e peggiorerà la qualità del parlato.</p><p>#### 3.2 Ottimizzazione della velocità</p><p>Se la velocità di inferenza non è soddisfacente, puoi velocizzarla come segue:</p><ul><li><strong>Distillazione del modello e meno step</strong>: Per il modello di generazione di parlato a singolo speaker, utilizziamo di default il modello <code>zipvoice</code> per una migliore qualità del parlato. Se la velocità è prioritaria, puoi passare a <code>zipvoice_distill</code> e ridurre <code>--num-steps</code> fino a <code>4</code> (8 di default).</li></p><p><li><strong>Velocizzazione CPU con multi-threading</strong>: In esecuzione su CPU, puoi passare il parametro <code>--num-thread</code> (es., <code>--num-thread 4</code>) per aumentare il numero di thread e accelerare la velocità. Usiamo 1 thread di default.</li></p><p><li><strong>Velocizzazione CPU con ONNX</strong>: In esecuzione su CPU, puoi utilizzare modelli ONNX con <code>zipvoice.bin.infer_zipvoice_onnx</code> per una velocità maggiore (ONNX non è ancora supportato per i modelli di generazione dialogica). Per una velocità ancora superiore, puoi impostare <code>--onnx-int8 True</code> per usare un modello ONNX quantizzato INT8. Nota che il modello quantizzato comporterà un certo degrado della qualità del parlato. <strong>Non usare ONNX su GPU</strong>, poiché è più lento di PyTorch su GPU.</li></p><p><li><strong>Accelerazione GPU con NVIDIA TensorRT</strong>: Per un notevole aumento delle prestazioni su GPU NVIDIA, esporta prima il modello in un engine TensorRT usando zipvoice.bin.tensorrt_export. Poi, esegui l'inferenza sul tuo dataset (ad esempio, un dataset Hugging Face) con zipvoice.bin.infer_zipvoice. Questo può ottenere circa 2x il throughput rispetto all'implementazione standard PyTorch su GPU.</li></p><p></ul>#### 3.3 Controllo della memoria</p><p>Il testo fornito verrà suddiviso in blocchi in base alla punteggiatura (per la generazione di parlato a singolo speaker) o al simbolo di cambio speaker (per la generazione di dialoghi). Successivamente, i blocchi di testo verranno processati in batch. Pertanto, il modello può processare testi arbitrariamente lunghi con un uso di memoria quasi costante. Puoi controllare l'uso di memoria regolando il parametro <code>--max-duration</code>.</p><p>#### 3.4 Valutazione "Raw"</p><p>Per impostazione predefinita, pre-processiamo gli input (prompt wav, trascrizione prompt e testo) per un'inferenza efficiente e migliori prestazioni. Se vuoi valutare le prestazioni "raw" del modello usando gli input esatti forniti (ad esempio, per riprodurre i risultati del nostro articolo), puoi passare <code>--raw-evaluation True</code>.</p><p>#### 3.5 Testo breve</p><p>Quando si genera parlato per testi molto brevi (ad esempio, una o due parole), il parlato generato può talvolta omettere alcune pronunce. Per risolvere questo problema, puoi passare <code>--speed 0.3</code> (dove 0.3 è un valore regolabile) per estendere la durata del parlato generato.</p><p>#### 3.6 Correzione dei caratteri polifonici cinesi pronunciati in modo errato</p><p>
Usiamo <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> per convertire i caratteri cinesi in pinyin. Tuttavia, talvolta può pronunciare erroneamente i <strong>caratteri polifonici</strong> (多音字).</p><p>Per correggere manualmente queste pronunce errate, racchiudi il <strong>pinyin corretto</strong> tra parentesi angolari <code>< ></code> e includi il <strong>segno tonale</strong>.</p><p><strong>Esempio:</strong></p><ul><li>Testo originale: <code>这把剑长三十公分</code></li>
<li>Correggi il pinyin di <code>长</code>:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Nota:</strong> Se vuoi assegnare manualmente più pinyin, racchiudi ciascun pinyin con <code><></code>, ad esempio: <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Rimuovere lunghe pause dal parlato generato</p><p>Il modello determinerà automaticamente le posizioni e le lunghezze delle pause nel parlato generato. Talvolta ci sono lunghe pause nel mezzo del parlato. Se non desideri questo, puoi passare <code>--remove-long-sil</code> per rimuovere le lunghe pause nel mezzo del parlato generato (le pause ai bordi verranno rimosse automaticamente).</p><p>#### 3.8 Download del modello</p><p>Se hai problemi di connessione a HuggingFace durante il download dei modelli pre-addestrati, prova a cambiare endpoint al sito mirror: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Addestra il tuo modello</h2></p><p>Consulta la directory <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> per esempi di addestramento, fine-tuning e valutazione.</p><h2>Distribuzione in produzione</h2></p><h3>Runtime GPU NVIDIA Triton</h3></p><p>Per una distribuzione pronta per la produzione con alte prestazioni e scalabilità, consulta l'integrazione con <a href="runtime/nvidia_triton/" target="_blank" rel="noopener noreferrer">Triton Inference Server</a> che offre motori TensorRT ottimizzati, gestione concorrente delle richieste e API gRPC/HTTP per uso aziendale.</p><h3>Distribuzione su CPU</h3></p><p>Consulta <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> per la soluzione di distribuzione C++ su CPU.</p><h2>Discussione & Comunicazione</h2></p><p>Puoi discutere direttamente su <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>Puoi anche scansionare il codice QR per unirti al nostro gruppo wechat o seguire il nostro account ufficiale wechat.</p><p>| Gruppo Wechat | Account Ufficiale Wechat |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Citazione</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-30 
    </div>
    
</body>
</html>