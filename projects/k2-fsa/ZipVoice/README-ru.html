<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Быстрый и высококачественный Zero-Shot синтез речи по тексту с использованием Flow Matching</title>
    <meta name="description" content="Быстрый и высококачественный Zero-Shot синтез речи по тексту с использованием Flow Matching">
    <meta name="keywords" content="ZipVoice, Russian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Быстрый и высококачественный Zero-Shot синтез речи по тексту с использованием Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 748
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-ru.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-12-30",
  "dateModified": "2025-12-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 748 stars</span>
                <span class="language">Russian</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Язык</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Быстрый и высококачественный текст-в-голос без обучения на примерах с Flow Matching</h2>
</div></p><h2>Обзор</h2></p><p>ZipVoice — это серия быстрых и высококачественных моделей TTS с нулевым обучением, основанных на flow matching.</p><h3>1. Ключевые особенности</h3></p><ul><li>Маленькая и быстрая: всего 123M параметров.</li></p><p><li>Высококачественное клонирование голоса: передовые показатели по похожести голоса, разборчивости и естественности.</li></p><p><li>Многоязычность: поддержка китайского и английского языков.</li></p><p><li>Мультирежимность: поддержка генерации речи как для одного говорящего, так и для диалога.</li></p><p></ul><h3>2. Варианты моделей</h3></p><p><table>
  <thead>
    <tr>
      <th>Название модели</th>
      <th>Описание</th>
      <th>Статья</th>
      <th>Демо</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Базовая модель, поддерживающая нулевое обучение TTS для одного говорящего на китайском и английском языках.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Дистиллированная версия ZipVoice, отличающаяся повышенной скоростью при минимальных потерях качества.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Модель для генерации диалогов на основе ZipVoice, способная генерировать одноканальные диалоги между двумя собеседниками.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>Стерео-версия ZipVoice-Dialog, позволяющая создавать диалоги с двумя каналами, при этом каждому собеседнику назначается отдельный канал.</td>
    </tr>
  </tbody>
</table></p><h2>Новости</h2></p><p><strong>2025/07/14</strong>: Вышли две модели генерации устных диалогов: <strong>ZipVoice-Dialog</strong> и <strong>ZipVoice-Dialog-Stereo</strong>. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: Выпущен датасет <strong>OpenDialog</strong> — 6,8 тысяч часов устных диалогов. Скачать можно на <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Подробнее на <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: Выпущены <strong>ZipVoice</strong> и <strong>ZipVoice-Distill</strong>. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Установка</h2></p><h3>1. Клонируйте репозиторий ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Необязательно) Создайте виртуальное окружение Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Установите необходимые пакеты</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Установите k2 для обучения или эффективного вывода</h3></p><p><strong>k2 необходим для обучения</strong> и может ускорить вывод. Тем не менее, вы всё равно можете использовать режим вывода ZipVoice без установки k2.</p><blockquote><strong>Примечание:</strong> Убедитесь, что устанавливаете версию k2, соответствующую вашей версии PyTorch и CUDA. Например, если вы используете pytorch 2.5.1 и CUDA 12.1, вы можете установить k2 следующим образом:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Пожалуйста, ознакомьтесь с https://k2-fsa.org/get-started/k2/ для подробностей.
Пользователи из материкового Китая могут обратиться к https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Чтобы проверить установку k2:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Использование</h2></p><h3>1. Генерация речи одним говорящим</h3></p><p>Чтобы сгенерировать речь от одного говорящего с помощью наших предобученных моделей ZipVoice или ZipVoice-Distill, используйте следующие команды (необходимые модели будут загружены с HuggingFace):</p><p>#### 1.1 Инференс одного предложения</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> может быть <code>zipvoice</code> или <code>zipvoice_distill</code>, что соответствует моделям до и после дистилляции соответственно.</li>
<li>Если в тексте встречаются <code><></code> или <code>[]</code>, строки, заключённые в них, будут рассматриваться как специальные токены. <code><></code> обозначает китайскую пиньинь, а <code>[]</code> — другие специальные теги.</li></p><p></ul>#### 1.2 Инференс списка предложений</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Каждая строка файла <code>test.tsv</code> имеет формат <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Диалоговая генерация речи</h3></p><p>#### 2.1 Команда для инференса</p><p>Чтобы сгенерировать двухсторонние устные диалоги с помощью наших предобученных моделей ZipVoice-Dialogue или ZipVoice-Dialogue-Stereo, используйте следующие команды (необходимые модели будут загружены с HuggingFace):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> может быть <code>zipvoice_dialog</code> или <code>zipvoice_dialog_stereo</code>,</li>
    </ul>которые генерируют соответственно моно- и стерео-диалоги.</p><p>#### 2.2 Форматы входных данных</p><p>Каждая строка файла <code>test.tsv</code> соответствует одному из следующих форматов:</p><p>(1) <strong>Формат объединённого запроса</strong>, где аудиозаписи и транскрипции двух реплик объединены в один wav-файл запроса:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> — это имя выходного wav-файла.</li>
<li><code>prompt_transcription</code> — это транскрипция звукового файла с разговорным запросом, например, "[S1] Привет. [S2] Как дела?"</li>
<li><code>prompt_wav</code> — это путь к звуковому файлу запроса.</li>
<li><code>text</code> — это текст для синтеза, например: "[S1] У меня всё хорошо. [S2] Как тебя зовут? [S1] Я Эрик. [S2] Привет, Эрик."</li></p><p></ul>(2) <strong>Формат разделённого запроса</strong>, где аудиофайлы и транскрипции двух говорящих существуют в отдельных файлах:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> — имя выходного wav-файла.</li>
<li><code>spk1_prompt_transcription</code> — транскрипция примера речи первого говорящего, например, "Привет".</li>
<li><code>spk2_prompt_transcription</code> — транскрипция примера речи второго говорящего, например, "Как дела?"</li>
<li><code>spk1_prompt_wav</code> — путь к wav-файлу примера речи первого говорящего.</li>
<li><code>spk2_prompt_wav</code> — путь к wav-файлу примера речи второго говорящего.</li>
<li><code>text</code> — текст для синтеза, например: "[S1] У меня всё хорошо. [S2] Как тебя зовут? [S1] Я Эрик. [S2] Привет, Эрик."</li></p><p></ul><h3>3 Рекомендации по использованию:</h3></p><p>#### 3.1 Длина примера</p><p>Рекомендуется использовать короткий wav-файл примера (например, менее 3 секунд для генерации речи одного говорящего, менее 10 секунд для генерации диалога) для ускорения вывода. Слишком длинный пример замедлит вывод и ухудшит качество речи.</p><p>#### 3.2 Оптимизация скорости</p><p>Если скорость вывода неудовлетворительная, ускорить её можно следующим образом:</p><ul><li><strong>Distill-модель и меньше шагов</strong>: Для модели генерации речи одного говорящего мы по умолчанию используем модель <code>zipvoice</code> для лучшего качества речи. Если приоритет — скорость, переключитесь на <code>zipvoice_distill</code> и уменьшите параметр <code>--num-steps</code> до значения не менее <code>4</code> (по умолчанию 8).</li></p><p><li><strong>Ускорение на CPU с помощью многопоточности</strong>: При запуске на CPU можно добавить параметр <code>--num-thread</code> (например, <code>--num-thread 4</code>), чтобы увеличить количество потоков для ускорения. По умолчанию используется 1 поток.</li></p><p><li><strong>Ускорение на CPU с помощью ONNX</strong>: При запуске на CPU используйте ONNX-модели с <code>zipvoice.bin.infer_zipvoice_onnx</code> для ускорения (ONNX для моделей генерации диалогов пока не поддерживается). Для ещё большего ускорения укажите <code>--onnx-int8 True</code> для использования INT8-квантованной ONNX-модели. Обратите внимание, что качество речи при квантовании может немного снизиться. <strong>Не используйте ONNX на GPU</strong>, так как это медленнее, чем PyTorch на GPU.</li></p><p><li><strong>Ускорение на GPU с NVIDIA TensorRT</strong>: Для значительного прироста производительности на NVIDIA GPU сначала экспортируйте модель в движок TensorRT с помощью zipvoice.bin.tensorrt_export. Затем запускайте вывод на вашем датасете (например, Hugging Face) с помощью zipvoice.bin.infer_zipvoice. Это даст примерно 2-кратный прирост производительности по сравнению со стандартной реализацией PyTorch на GPU.</li></p><p></ul>#### 3.3 Контроль памяти</p><p>Введённый текст будет разбит на части по знакам препинания (для генерации речи одного говорящего) или символам смены говорящего (для генерации диалога). Затем части текста будут обработаны пакетно. Таким образом, модель может обрабатывать текст любой длины при почти постоянном потреблении памяти. Управлять расходом памяти можно с помощью параметра <code>--max-duration</code>.</p><p>#### 3.4 "Raw"-оценка</p><p>По умолчанию мы предварительно обрабатываем входные данные (пример wav, транскрипция примера и текст) для ускорения вывода и повышения качества. Если вы хотите оценить "сырое" качество модели по точным входным данным (например, для воспроизведения результатов из нашей статьи), укажите <code>--raw-evaluation True</code>.</p><p>#### 3.5 Короткий текст</p><p>При генерации речи для очень коротких текстов (например, одно-два слова) сгенерированная речь иногда может пропускать отдельные звуки. Для решения этой проблемы укажите <code>--speed 0.3</code> (где 0.3 — настраиваемое значение), чтобы увеличить длительность сгенерированной речи.</p><p>#### 3.6 Коррекция ошибочного произношения китайских полифонических иероглифов</p><p>Мы используем <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> для преобразования китайских иероглифов в пиньинь. Однако иногда он может неправильно произносить <strong>многозначные иероглифы</strong> (多音字).</p><p>Чтобы вручную исправить такие ошибки, заключайте <strong>исправленный пиньинь</strong> в угловые скобки <code>< ></code> и указывайте <strong>тон</strong>.</p><p><strong>Пример:</strong></p><ul><li>Оригинальный текст: <code>这把剑长三十公分</code></li>
<li>Исправьте пиньинь у <code>长</code>:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Примечание:</strong> Если вы хотите вручную указать несколько пиньиней, заключайте каждый пиньинь в <code><></code>, например, <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Удаление длинных пауз из сгенерированной речи</p><p>Модель автоматически определяет позиции и длительность пауз в сгенерированной речи. Иногда в середине речи встречаются длительные паузы. Если вы не хотите этого, вы можете передать <code>--remove-long-sil</code>, чтобы удалить длинные паузы в середине сгенерированной речи (краевые паузы удаляются по умолчанию).</p><p>#### 3.8 Загрузка модели</p><p>Если у вас возникли трудности с подключением к HuggingFace при загрузке предобученных моделей, попробуйте переключить endpoint на зеркальный сайт: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Обучение собственной модели</h2></p><p>Смотрите каталог <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> для примеров обучения, дообучения и оценки.</p><h2>Промышленное развертывание</h2></p><h3>NVIDIA Triton GPU Runtime</h3></p><p>Для промышленного развертывания с высокой производительностью и масштабируемостью ознакомьтесь с <a href="runtime/nvidia_triton/" target="_blank" rel="noopener noreferrer">интеграцией Triton Inference Server</a>, которая предоставляет оптимизированные движки TensorRT, обработку параллельных запросов и API gRPC/HTTP для корпоративного использования.</p><h3>Развертывание на CPU</h3></p><p>Посмотрите <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> для решения по развертыванию на CPU на C++.</p><h2>Обсуждение и коммуникация</h2></p><p>Вы можете напрямую обсуждать на <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>Также вы можете отсканировать QR-код, чтобы присоединиться к нашей группе в WeChat или подписаться на наш официальный аккаунт WeChat.</p><p>| Wechat Group | Wechat Official Account |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Цитирование</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-30 
    </div>
    
</body>
</html>