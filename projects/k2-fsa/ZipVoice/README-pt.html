<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Read ZipVoice documentation in Portuguese. This project has 308 stars on GitHub.</title>
    <meta name="description" content="Read ZipVoice documentation in Portuguese. This project has 308 stars on GitHub.">
    <meta name="keywords" content="ZipVoice, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Read ZipVoice documentation in Portuguese. This project has 308 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 308
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 308 stars</span>
                <span class="language">Portuguese</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Texto-para-Fala Zero-Shot Rápido e de Alta Qualidade com Flow Matching</h2>
</div></p><h2>Visão Geral</h2></p><p>ZipVoice é uma série de modelos TTS zero-shot rápidos e de alta qualidade baseados em flow matching.</p><h3>1. Principais características</h3></p><ul><li>Pequeno e rápido: apenas 123M de parâmetros.</li></p><p><li>Clonagem de voz de alta qualidade: desempenho de ponta em similaridade de locutor, inteligibilidade e naturalidade.</li></p><p><li>Multi-idiomas: suporta Chinês e Inglês.</li></p><p><li>Multi-modo: suporta geração de fala de locutor único e de diálogo.</li></p><p></ul><h3>2. Variantes do modelo</h3></p><p><table>
  <thead>
    <tr>
      <th>Nome do Modelo</th>
      <th>Descrição</th>
      <th>Artigo</th>
      <th>Demonstração</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>O modelo básico que suporta TTS zero-shot de locutor único em Chinês e Inglês.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>A versão destilada do ZipVoice, com velocidade aprimorada e mínima degradação de desempenho.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Um modelo de geração de diálogo baseado no ZipVoice, capaz de gerar diálogos falados de duas partes em um canal único.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>A variante estéreo do ZipVoice-Dialog, permitindo geração de diálogos em dois canais com cada locutor em um canal distinto.</td>
    </tr>
  </tbody>
</table></p><h2>Novidades</h2></p><p><strong>2025/07/14</strong>: <strong>ZipVoice-Dialog</strong> e <strong>ZipVoice-Dialog-Stereo</strong>, dois modelos de geração de diálogo falado, foram lançados. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: O dataset <strong>OpenDialog</strong>, um conjunto de dados de diálogo falado com 6,8k horas, foi lançado. Baixe em <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Confira detalhes em <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: <strong>ZipVoice</strong> e <strong>ZipVoice-Distill</strong> foram lançados. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Instalação</h2></p><h3>1. Clone o repositório ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Opcional) Crie um ambiente virtual Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Instale os pacotes necessários</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Instale o k2 para treinamento ou inferência eficiente</h3></p><p><strong>k2 é necessário para o treinamento</strong> e pode acelerar a inferência. No entanto, você ainda pode usar o modo de inferência do ZipVoice sem instalar o k2.</p><blockquote><strong>Nota:</strong> Certifique-se de instalar a versão do k2 que corresponde à sua versão do PyTorch e do CUDA. Por exemplo, se você estiver usando pytorch 2.5.1 e CUDA 12.1, você pode instalar o k2 da seguinte forma:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Consulte https://k2-fsa.org/get-started/k2/ para mais detalhes.
Usuários na China continental podem consultar https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Para verificar a instalação do k2:</li></p><p>
</ul><pre><code class="language-">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Uso</h2></p><h3>1. Geração de fala de um único locutor</h3></p><p>Para gerar fala de um único locutor com nossos modelos pré-treinados ZipVoice ou ZipVoice-Distill, use os seguintes comandos (Os modelos necessários serão baixados do HuggingFace):</p><p>#### 1.1 Inferência de uma única sentença</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> pode ser <code>zipvoice</code> ou <code>zipvoice_distill</code>, que são modelos antes e depois da destilação, respectivamente.</li>
<li>Se <code><></code> ou <code>[]</code> aparecerem no texto, as strings entre eles serão tratadas como tokens especiais. <code><></code> denota pinyin chinês e <code>[]</code> denota outras tags especiais.</li>
<li>Pode executar modelos ONNX na CPU mais rápido com <code>zipvoice.bin.infer_zipvoice_onnx</code>.</li></p><p></ul>> <strong>Nota:</strong> Se você tiver problemas para se conectar ao HuggingFace, tente:
<blockquote><pre><code class="language-bash">> export HF_ENDPOINT=https://hf-mirror.com</blockquote>
<blockquote>``<code></blockquote></p><p>#### 1.2 Inferência de uma lista de sentenças</p><p></code></pre>bash
python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results
<pre><code class="language-">- Cada linha de </code>test.tsv<code> está no formato </code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}<code>.</p><h3>2. Geração de fala em diálogo</h3></p><p>#### 2.1 Comando de inferência</p><p>Para gerar diálogos falados entre duas pessoas com nossos modelos pré-treinados ZipVoice-Dialogue ou ZipVoice-Dialogue-Stereo, use os seguintes comandos (Os modelos necessários serão baixados do HuggingFace):</p><p></code></pre>bash
python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results
<pre><code class="language-">- </code>--model-name<code> pode ser </code>zipvoice_dialog<code> ou </code>zipvoice_dialog_stereo<code>,
    que geram diálogos mono e estéreo, respectivamente.</p><p>#### 2.2 Formatos de entrada</p><p>Cada linha do </code>test.tsv<code> está em um dos seguintes formatos:</p><p>(1) <strong>Formato de prompt mesclado</strong> onde os áudios e transcrições dos prompts de dois falantes são mesclados em um único arquivo wav de prompt:
</code></pre>
{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}
<pre><code class="language-">- </code>wav_name<code> é o nome do arquivo wav de saída.
<ul><li></code>prompt_transcription<code> é a transcrição do arquivo wav do prompt de conversação, por exemplo, "[S1] Olá. [S2] Como você está?"</li>
<li></code>prompt_wav<code> é o caminho para o arquivo wav do prompt.</li>
<li></code>text<code> é o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual é o seu nome?"</li></p><p></ul>(2) <strong>Formato de prompt dividido</strong> onde os áudios e transcrições de dois falantes existem em arquivos separados:</p><p></code></pre>
{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}'
<pre><code class="language-">- </code>wav_name<code> é o nome do arquivo wav de saída.
<ul><li></code>spk1_prompt_transcription<code> é a transcrição do prompt wav do primeiro falante, por exemplo, "Olá"</li>
<li></code>spk2_prompt_transcription<code> é a transcrição do prompt wav do segundo falante, por exemplo, "Como vai você?"</li>
<li></code>spk1_prompt_wav<code> é o caminho para o arquivo wav do prompt do primeiro falante.</li>
<li></code>spk2_prompt_wav<code> é o caminho para o arquivo wav do prompt do segundo falante.</li>
<li></code>text<code> é o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual é o seu nome?"</li></p><p></ul><h3>3. Outras funcionalidades</h3></p><p>#### 3.1 Corrigindo caracteres polifônicos chineses pronunciados incorretamente</p><p>Usamos <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> para converter caracteres chineses em pinyin. No entanto, ocasionalmente pode pronunciar incorretamente <strong>caracteres polifônicos</strong> (多音字).</p><p>Para corrigir manualmente essas pronúncias, coloque o <strong>pinyin corrigido</strong> entre sinais de menor e maior </code>< ><code> e inclua o <strong>marcador de tom</strong>.</p><p><strong>Exemplo:</strong></p><ul><li>Texto original: </code>这把剑长三十公分<code></li>
<li>Corrija o pinyin de </code>长<code>:  </code>这把剑<chang2>三十公分<code></li></p><p></ul>> <strong>Nota:</strong> Se quiser atribuir manualmente vários pinyins, coloque cada pinyin entre </code><><code>, por exemplo, </code>这把<jian4><chang2><san1>十公分<code></p><h2>Treine Seu Próprio Modelo</h2></p><p>Veja o diretório <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> para exemplos de treinamento, ajuste fino e avaliação.</p><h2>Discussão & Comunicação</h2></p><p>Você pode discutir diretamente em <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>Você também pode escanear o QR code para entrar no nosso grupo do WeChat ou seguir nossa conta oficial no WeChat.</p><p>| Grupo WeChat | Conta Oficial WeChat |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Citação</h2></p><p></code></pre>bibtex
@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}
</code>``</p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-22

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>