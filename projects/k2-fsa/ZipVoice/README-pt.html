<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Texto para fala r&#225;pido e de alta qualidade sem treinamento pr&#233;vio com Flow Matching</title>
    <meta name="description" content="Texto para fala r&#225;pido e de alta qualidade sem treinamento pr&#233;vio com Flow Matching">
    <meta name="keywords" content="ZipVoice, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Texto para fala rápido e de alta qualidade sem treinamento prévio com Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 661
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-10-06",
  "dateModified": "2025-10-06"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 661 stars</span>
                <span class="language">Portuguese</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Idioma</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Texto para Fala Zero-Shot Rápido e de Alta Qualidade com Flow Matching</h2>
</div></p><h2>Visão Geral</h2></p><p>ZipVoice é uma série de modelos TTS de zero-shot rápidos e de alta qualidade baseados em flow matching.</p><h3>1. Principais características</h3></p><ul><li>Pequeno e rápido: apenas 123M de parâmetros.</li></p><p><li>Clonagem de voz de alta qualidade: desempenho de ponta em similaridade de locutor, inteligibilidade e naturalidade.</li></p><p><li>Multi-idiomas: suporta chinês e inglês.</li></p><p><li>Multi-modo: suporta geração de fala de locutor único e de diálogos.</li></p><p></ul><h3>2. Variantes do modelo</h3></p><p><table>
  <thead>
    <tr>
      <th>Nome do Modelo</th>
      <th>Descrição</th>
      <th>Artigo</th>
      <th>Demo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>O modelo básico que suporta TTS zero-shot de locutor único em chinês e inglês.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>Versão destilada do ZipVoice, com velocidade aprimorada e mínima degradação de desempenho.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Modelo de geração de diálogos baseado no ZipVoice, capaz de gerar diálogos falados de dois participantes em canal único.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>A variante estéreo do ZipVoice-Dialog, permitindo geração de diálogos em dois canais com cada falante atribuído a um canal distinto.</td>
    </tr>
  </tbody>
</table></p><h2>Novidades</h2></p><p><strong>2025/07/14</strong>: <strong>ZipVoice-Dialog</strong> e <strong>ZipVoice-Dialog-Stereo</strong>, dois modelos de geração de diálogos falados, foram lançados. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: O conjunto de dados <strong>OpenDialog</strong>, um dataset de diálogos falados de 6,8 mil horas, foi lançado. Baixe em <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Confira os detalhes em <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: <strong>ZipVoice</strong> e <strong>ZipVoice-Distill</strong> foram lançados. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Instalação</h2></p><h3>1. Clone o repositório ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Opcional) Crie um ambiente virtual Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Instale os pacotes necessários</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Instale o k2 para treinamento ou inferência eficiente</h3></p><p><strong>k2 é necessário para o treinamento</strong> e pode acelerar a inferência. No entanto, você ainda pode usar o modo de inferência do ZipVoice sem instalar o k2.</p><blockquote><strong>Nota:</strong> Certifique-se de instalar a versão do k2 que corresponde à sua versão do PyTorch e do CUDA. Por exemplo, se você estiver usando pytorch 2.5.1 e CUDA 12.1, você pode instalar o k2 da seguinte forma:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Consulte https://k2-fsa.org/get-started/k2/ para mais detalhes.
Usuários na China continental podem consultar https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Para verificar a instalação do k2:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Uso</h2></p><h3>1. Geração de fala de um único locutor</h3></p><p>Para gerar fala de um único locutor com nossos modelos pré-treinados ZipVoice ou ZipVoice-Distill, use os seguintes comandos (Os modelos necessários serão baixados do HuggingFace):</p><p>#### 1.1 Inferência de uma única sentença</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> pode ser <code>zipvoice</code> ou <code>zipvoice_distill</code>, que são modelos antes e depois da destilação, respectivamente.</li>
<li>Se <code><></code> ou <code>[]</code> aparecerem no texto, cadeias de caracteres entre eles serão tratadas como tokens especiais. <code><></code> denota pinyin chinês e <code>[]</code> denota outras tags especiais.</li></p><p></ul>#### 1.2 Inferência de uma lista de sentenças</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Cada linha de <code>test.tsv</code> está no formato <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Geração de fala em diálogo</h3></p><p>#### 2.1 Comando de inferência</p><p>Para gerar diálogos falados entre duas pessoas com nossos modelos pré-treinados ZipVoice-Dialogue ou ZipVoice-Dialogue-Stereo, use os seguintes comandos (Os modelos necessários serão baixados do HuggingFace):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> pode ser <code>zipvoice_dialog</code> ou <code>zipvoice_dialog_stereo</code>,</li>
    </ul>que geram diálogos mono e estéreo, respectivamente.</p><p>#### 2.2 Formatos de entrada</p><p>Cada linha do <code>test.tsv</code> está em um dos seguintes formatos:</p><p>(1) <strong>Formato de prompt mesclado</strong> onde os áudios e transcrições dos prompts de dois falantes são mesclados em um único arquivo wav de prompt:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> é o nome do arquivo wav de saída.</li>
<li><code>prompt_transcription</code> é a transcrição do arquivo wav de prompt de conversação, por exemplo, "[S1] Olá. [S2] Como vai você?"</li>
<li><code>prompt_wav</code> é o caminho para o arquivo wav de prompt.</li>
<li><code>text</code> é o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual é o seu nome? [S1] Sou Eric. [S2] Oi Eric."</li></p><p></ul>(2) <strong>Formato de prompt dividido</strong> onde os áudios e transcrições de dois falantes existem em arquivos separados:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> é o nome do arquivo wav de saída.</li>
<li><code>spk1_prompt_transcription</code> é a transcrição do arquivo wav de prompt do primeiro falante, por exemplo, "Olá"</li>
<li><code>spk2_prompt_transcription</code> é a transcrição do arquivo wav de prompt do segundo falante, por exemplo, "Como vai você?"</li>
<li><code>spk1_prompt_wav</code> é o caminho para o arquivo wav de prompt do primeiro falante.</li>
<li><code>spk2_prompt_wav</code> é o caminho para o arquivo wav de prompt do segundo falante.</li>
<li><code>text</code> é o texto a ser sintetizado, por exemplo, "[S1] Estou bem. [S2] Qual é o seu nome? [S1] Sou Eric. [S2] Olá Eric."</li></p><p></ul><h3>3 Orientações para melhor uso:</h3></p><p>#### 3.1 Comprimento do prompt</p><p>Recomendamos um arquivo wav de prompt curto (por exemplo, menos de 3 segundos para geração de fala de um único falante, menos de 10 segundos para geração de diálogos) para maior velocidade de inferência. Um prompt muito longo irá desacelerar a inferência e prejudicar a qualidade da fala.</p><p>#### 3.2 Otimização de velocidade</p><p>Se a velocidade de inferência não for satisfatória, você pode acelerá-la da seguinte forma:</p><ul><li><strong>Modelo distilado e menos etapas</strong>: Para o modelo de geração de fala de um único falante, usamos o modelo <code>zipvoice</code> por padrão para melhor qualidade de fala. Se a velocidade for prioridade, você pode alternar para <code>zipvoice_distill</code> e reduzir o parâmetro <code>--num-steps</code> para até <code>4</code> (8 por padrão).</li></p><p><li><strong>Aceleração da CPU com multithreading</strong>: Ao executar na CPU, você pode passar o parâmetro <code>--num-thread</code> (por exemplo, <code>--num-thread 4</code>) para aumentar o número de threads e obter maior velocidade. Usamos 1 thread por padrão.</li></p><p><li><strong>Aceleração da CPU com ONNX</strong>: Ao executar na CPU, você pode usar modelos ONNX com <code>zipvoice.bin.infer_zipvoice_onnx</code> para maior velocidade (ainda não há suporte para modelos de geração de diálogos em ONNX). Para velocidade ainda maior, você pode definir <code>--onnx-int8 True</code> para usar um modelo ONNX quantizado em INT8. Observe que o modelo quantizado pode resultar em alguma degradação na qualidade da fala. <strong>Não use ONNX na GPU</strong>, pois é mais lento do que PyTorch na GPU.</li></p><p></ul>#### 3.3 Controle de memória</p><p>O texto fornecido será dividido em partes com base em pontuação (para geração de fala de um único falante) ou símbolo de troca de falante (para geração de diálogos). Em seguida, os textos divididos serão processados em lotes. Portanto, o modelo pode processar textos arbitrariamente longos com uso de memória quase constante. Você pode controlar o uso de memória ajustando o parâmetro <code>--max-duration</code>.</p><p>#### 3.4 Avaliação "Bruta"</p><p>Por padrão, pré-processamos as entradas (wav de prompt, transcrição do prompt e texto) para inferência eficiente e melhor desempenho. Se você quiser avaliar o desempenho "bruto" do modelo usando exatamente as entradas fornecidas (por exemplo, para reproduzir os resultados do nosso artigo), pode passar <code>--raw-evaluation True</code>.</p><p>#### 3.5 Texto curto</p><p>Ao gerar fala para textos muito curtos (por exemplo, uma ou duas palavras), a fala gerada pode, às vezes, omitir certas pronúncias. Para resolver esse problema, você pode passar <code>--speed 0.3</code> (onde 0.3 é um valor ajustável) para estender a duração da fala gerada.</p><p>#### 3.6 Corrigindo caracteres polifônicos chineses pronunciados incorretamente</p><p>Usamos <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> para converter caracteres chineses em pinyin. No entanto, ocasionalmente pode pronunciar incorretamente <strong>caracteres polifônicos</strong> (多音字).</p><p>
Para corrigir manualmente essas pronúncias incorretas, coloque o <strong>pinyin corrigido</strong> entre sinais de menor <code>< ></code> e inclua o <strong>marcador de tom</strong>.</p><p><strong>Exemplo:</strong></p><ul><li>Texto original: <code>这把剑长三十公分</code></li>
<li>Corrija o pinyin de <code>长</code>:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Nota:</strong> Se quiser atribuir manualmente múltiplos pinyins, coloque cada pinyin entre <code>< ></code>, por exemplo: <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Remover longos períodos de silêncio da fala gerada</p><p>O modelo determinará automaticamente as posições e durações dos silêncios na fala gerada. Ocasionalmente, há longos períodos de silêncio no meio da fala. Se você não quiser isso, pode passar <code>--remove-long-sil</code> para remover longos silêncios no meio da fala gerada (os silêncios nas bordas serão removidos por padrão).</p><p>#### 3.8 Download do modelo</p><p>Se você tiver problemas para conectar ao HuggingFace ao baixar os modelos pré-treinados, tente mudar o endpoint para o site espelho: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Treine Seu Próprio Modelo</h2></p><p>Veja o diretório <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> para exemplos de treinamento, ajuste fino e avaliação.</p><h2>Implantação em C++</h2></p><p>Confira <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> para a solução de implantação em C++ na CPU.</p><h2>Discussão & Comunicação</h2></p><p>Você pode discutir diretamente nas <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Issues do Github</a>.</p><p>Você também pode escanear o código QR para entrar em nosso grupo no WeChat ou seguir nossa conta oficial do WeChat.</p><p>| Grupo WeChat | Conta Oficial WeChat |
| ------------ | ------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Citação</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-10-06

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-10-06 
    </div>
    
</body>
</html>