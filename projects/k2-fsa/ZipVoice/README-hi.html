<!DOCTYPE html>
<html lang="hi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - फास्ट और उच्च-गुणवत्ता वाली ज़ीरो-शॉट टेक्स्ट-टू-स्पीच फ्लो मैचिंग के साथ</title>
    <meta name="description" content="फास्ट और उच्च-गुणवत्ता वाली ज़ीरो-शॉट टेक्स्ट-टू-स्पीच फ्लो मैचिंग के साथ">
    <meta name="keywords" content="ZipVoice, Hindi, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "फास्ट और उच्च-गुणवत्ता वाली ज़ीरो-शॉट टेक्स्ट-टू-स्पीच फ्लो मैचिंग के साथ",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 748
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-hi.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-12-30",
  "dateModified": "2025-12-30"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 748 stars</span>
                <span class="language">Hindi</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 भाषा</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>फ्लो मैचिंग के साथ तेज़ और उच्च गुणवत्ता वाली ज़ीरो-शॉट टेक्स्ट-टू-स्पीच</h2>
</div></p><h2>अवलोकन</h2></p><p>ZipVoice एक तेज़ और उच्च-गुणवत्ता वाली ज़ीरो-शॉट TTS मॉडल श्रृंखला है, जो फ्लो मैचिंग पर आधारित है।</p><h3>1. मुख्य विशेषताएँ</h3></p><ul><li>छोटा और तेज़: केवल 123M पैरामीटर।</li></p><p><li>उच्च गुणवत्ता वाली वॉयस क्लोनिंग: वक्ता समानता, स्पष्टता और प्राकृतिकता में अत्याधुनिक प्रदर्शन।</li></p><p><li>बहुभाषी: चीनी और अंग्रेज़ी का समर्थन करता है।</li></p><p><li>बहु-मोड: एकल वक्ता और संवाद भाषण जनरेशन दोनों का समर्थन करता है।</li></p><p></ul><h3>2. मॉडल वेरिएंट्स</h3></p><p><table>
  <thead>
    <tr>
      <th>मॉडल नाम</th>
      <th>विवरण</th>
      <th>पेपर</th>
      <th>डेमो</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>मूल मॉडल जो चीनी और अंग्रेज़ी में ज़ीरो-शॉट एकल-वक्ता TTS का समर्थन करता है।</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>ZipVoice का डिस्टिल्ड संस्करण, जिसमें न्यूनतम प्रदर्शन हानि के साथ बेहतर गति है।</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>ZipVoice पर आधारित एक संवाद जनरेशन मॉडल, जो एकल-चैनल दो-पक्षीय बोले गए संवाद उत्पन्न कर सकता है।</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>ZipVoice-Dialog का स्टीरियो संस्करण, जो दो-चैनल संवाद निर्माण सक्षम करता है, जिसमें प्रत्येक वक्ता को एक अलग चैनल सौंपा जाता है।</td>
    </tr>
  </tbody>
</table></p><h2>समाचार</h2></p><p><strong>2025/07/14</strong>: <strong>ZipVoice-Dialog</strong> और <strong>ZipVoice-Dialog-Stereo</strong>, दो बोले गए संवाद निर्माण मॉडल जारी किए गए हैं। <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong>: <strong>OpenDialog</strong> डेटासेट, एक 6.8k-घंटे की बोले गए संवाद डेटासेट, जारी की गई है। डाउनलोड करें <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>। विवरण देखें <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong>: <strong>ZipVoice</strong> और <strong>ZipVoice-Distill</strong> जारी किए गए हैं। <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>स्थापना</h2></p><h3>1. ZipVoice रिपॉजिटरी क्लोन करें</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (वैकल्पिक) एक पायथन वर्चुअल एनवायरनमेंट बनाएं</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. आवश्यक पैकेज इंस्टॉल करें</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. प्रशिक्षण या कुशल अनुकरण के लिए k2 स्थापित करें</h3></p><p><strong>प्रशिक्षण के लिए k2 आवश्यक है</strong> और यह अनुकरण को तेज कर सकता है। फिर भी, आप k2 स्थापित किए बिना भी ZipVoice के अनुकरण मोड का उपयोग कर सकते हैं।</p><blockquote><strong>नोट:</strong>  सुनिश्चित करें कि आप अपने PyTorch और CUDA संस्करण के अनुसार k2 का संस्करण स्थापित कर रहे हैं। उदाहरण के लिए, यदि आप pytorch 2.5.1 और CUDA 12.1 उपयोग कर रहे हैं, तो आप इस प्रकार k2 स्थापित कर सकते हैं:</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
कृपया विवरण के लिए https://k2-fsa.org/get-started/k2/ देखें।
चीन मुख्यभूमि के उपयोगकर्ता https://k2-fsa.org/zh-CN/get-started/k2/ देख सकते हैं।</p><ul><li>k2 इंस्टॉलेशन जांचने के लिए:</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>उपयोग</h2></p><h3>1. एकल-वक्ता भाषण जनरेशन</h3></p><p>हमारे प्री-ट्रेंड ZipVoice या ZipVoice-Distill मॉडल के साथ एकल-वक्ता भाषण जनरेट करने के लिए, निम्नलिखित कमांड्स का उपयोग करें (आवश्यक मॉडल HuggingFace से डाउनलोड किए जाएंगे):</p><p>#### 1.1 एकल वाक्य का इनफरेंस</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> को <code>zipvoice</code> या <code>zipvoice_distill</code> सेट किया जा सकता है, जो क्रमशः आसवन से पहले और बाद के मॉडल हैं।</li>
<li>यदि पाठ में <code><></code> या <code>[]</code> आते हैं, तो उनके भीतर बंद स्ट्रिंग्स को विशेष टोकन के रूप में माना जाएगा। <code><></code> चीनी पिनयिन को दर्शाता है और <code>[]</code> अन्य विशेष टैग को दर्शाता है।</li></p><p></ul>#### 1.2 वाक्यों की सूची का अनुमान</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>test.tsv</code> की प्रत्येक पंक्ति इस प्रारूप में होती है: <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>।</li></p><p></ul><h3>2. संवाद भाषण निर्माण</h3></p><p>#### 2.1 पूर्वानुमान कमांड</p><p>हमारे प्री-ट्रेंड ZipVoice-Dialogue या ZipVoice-Dialogue-Stereo मॉडल के साथ दो-पक्षीय बोले गए संवाद उत्पन्न करने के लिए, निम्नलिखित कमांड का उपयोग करें (आवश्यक मॉडल HuggingFace से डाउनलोड किए जाएंगे):</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> या तो <code>zipvoice_dialog</code> या <code>zipvoice_dialog_stereo</code> हो सकता है,</li>
    </ul>जो क्रमशः मोनो और स्टीरियो संवाद उत्पन्न करते हैं।</p><p>#### 2.2 इनपुट प्रारूप</p><p><code>test.tsv</code> की प्रत्येक पंक्ति निम्नलिखित प्रारूपों में से एक में होती है:</p><p>(1) <strong>मर्ज्ड प्रॉम्प्ट प्रारूप</strong> जिसमें दो वक्ताओं के प्रॉम्प्ट ऑडियो और ट्रांसक्रिप्शन को एक प्रॉम्प्ट वेव फ़ाइल में मर्ज कर दिया जाता है:</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> आउटपुट वेव फाइल का नाम है।</li>
<li><code>prompt_transcription</code> वार्तालाप संकेत वेव की ट्रांसक्रिप्शन है, उदाहरण के लिए, "[S1] हैलो। [S2] आप कैसे हैं?"</li>
<li><code>prompt_wav</code> संकेत वेव का पथ है।</li>
<li><code>text</code> वह टेक्स्ट है जिसे सिंथेसाइज़ किया जाना है, जैसे कि "[S1] मैं ठीक हूँ। [S2] आपका नाम क्या है? [S1] मेरा नाम एरिक है। [S2] हाय एरिक।"</li></p><p></ul>(2) <strong>विभाजित संकेत प्रारूप</strong> जिसमें दो वक्ताओं की ऑडियो और ट्रांसक्रिप्शन अलग-अलग फाइलों में होती हैं:</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre>
<ul><li><code>wav_name</code> आउटपुट wav फ़ाइल का नाम है।</li>
<li><code>spk1_prompt_transcription</code> पहले वक्ता के प्रॉम्प्ट wav की ट्रांसक्रिप्शन है, जैसे, "Hello"</li>
<li><code>spk2_prompt_transcription</code> दूसरे वक्ता के प्रॉम्प्ट wav की ट्रांसक्रिप्शन है, जैसे, "How are you?"</li>
<li><code>spk1_prompt_wav</code> पहले वक्ता के प्रॉम्प्ट wav फ़ाइल का पथ है।</li>
<li><code>spk2_prompt_wav</code> दूसरे वक्ता के प्रॉम्प्ट wav फ़ाइल का पथ है।</li>
<li><code>text</code> वह पाठ है जिसे सिंथेसाइज़ किया जाना है, जैसे, "[S1] I'm fine. [S2] What's your name? [S1] I'm Eric. [S2] Hi Eric."</li></p><p></ul><h3>3 बेहतर उपयोग के लिए मार्गदर्शन:</h3></p><p>#### 3.1 प्रॉम्प्ट की लंबाई</p><p>हम एक छोटा प्रॉम्प्ट wav फ़ाइल (जैसे, एकल-वक्ता भाषण निर्माण के लिए 3 सेकंड से कम, संवाद भाषण निर्माण के लिए 10 सेकंड से कम) की सलाह देते हैं ताकि इनफेरेंस गति तेज हो। बहुत लंबा प्रॉम्प्ट इनफेरेंस को धीमा कर देगा और भाषण की गुणवत्ता को घटा सकता है।</p><p>#### 3.2 गति अनुकूलन</p><p>यदि इनफेरेंस गति असंतोषजनक है, तो आप इसे निम्नलिखित तरीकों से तेज कर सकते हैं:</p><ul><li><strong>डिस्टिल मॉडल और कम स्टेप्स</strong>: एकल-वक्ता भाषण निर्माण मॉडल के लिए, हम बेहतर भाषण गुणवत्ता के लिए डिफ़ॉल्ट रूप से <code>zipvoice</code> मॉडल का उपयोग करते हैं। यदि तेज गति प्राथमिकता है, तो आप <code>zipvoice_distill</code> पर स्विच कर सकते हैं और <code>--num-steps</code> को न्यूनतम <code>4</code> (डिफ़ॉल्ट 8) तक कम कर सकते हैं।</li></p><p><li><strong>CPU मल्टी-थ्रेडिंग के साथ गति बढ़ाना</strong>: यदि आप CPU पर चला रहे हैं, तो आप तेज गति के लिए <code>--num-thread</code> पैरामीटर (जैसे, <code>--num-thread 4</code>) दे सकते हैं ताकि थ्रेड्स की संख्या बढ़ाई जा सके। डिफ़ॉल्ट रूप से हम 1 थ्रेड का उपयोग करते हैं।</li></p><p><li><strong>CPU पर ONNX के साथ गति बढ़ाना</strong>: CPU पर चलते समय, आप तेज गति के लिए ONNX मॉडल का उपयोग कर सकते हैं <code>zipvoice.bin.infer_zipvoice_onnx</code> के साथ (संवाद निर्माण मॉडल के लिए ONNX का समर्थन नहीं है)। और भी तेज गति के लिए, आप <code>--onnx-int8 True</code> सेट कर सकते हैं ताकि INT8-क्वांटाइज़्ड ONNX मॉडल का उपयोग किया जा सके। ध्यान दें कि क्वांटाइज़्ड मॉडल से भाषण गुणवत्ता में कुछ गिरावट आ सकती है। <strong>GPU पर ONNX का उपयोग न करें</strong>, क्योंकि GPU पर यह PyTorch से धीमा है।</li></p><p><li><strong>NVIDIA TensorRT के साथ GPU एक्सेलेरेशन</strong>: NVIDIA GPU पर महत्वपूर्ण प्रदर्शन बढ़ाने के लिए, पहले zipvoice.bin.tensorrt_export के साथ मॉडल को TensorRT इंजन में एक्सपोर्ट करें। फिर, अपने डेटासेट (जैसे, Hugging Face डेटासेट) पर zipvoice.bin.infer_zipvoice के साथ इनफेरेंस चलाएँ। यह GPU पर स्टैंडर्ड PyTorch इम्प्लीमेंटेशन की तुलना में लगभग 2x थ्रूपुट प्राप्त कर सकता है।</li></p><p></ul>#### 3.3 मेमोरी नियंत्रण</p><p>दिया गया टेक्स्ट विराम चिह्न (एकल-वक्ता भाषण निर्माण के लिए) या वक्ता-टर्न चिन्ह (संवाद भाषण निर्माण के लिए) के आधार पर भागों में विभाजित किया जाएगा। इसके बाद, इन भागों को बैच में प्रोसेस किया जाएगा। इस प्रकार, मॉडल लगभग स्थिर मेमोरी उपयोग के साथ किसी भी लंबाई का टेक्स्ट प्रोसेस कर सकता है। आप <code>--max-duration</code> पैरामीटर को समायोजित करके मेमोरी उपयोग को नियंत्रित कर सकते हैं।</p><p>#### 3.4 "रॉ" मूल्यांकन</p><p>डिफ़ॉल्ट रूप से, हम इनपुट्स (प्रॉम्प्ट wav, प्रॉम्प्ट ट्रांसक्रिप्शन, और टेक्स्ट) को कुशल इनफेरेंस और बेहतर प्रदर्शन के लिए पूर्व-प्रक्रिया करते हैं। यदि आप मॉडल के "रॉ" प्रदर्शन का मूल्यांकन करना चाहते हैं, जैसे कि हमारे पेपर में दिए गए परिणामों को पुनः उत्पन्न करने के लिए, तो आप <code>--raw-evaluation True</code> पास कर सकते हैं।</p><p>#### 3.5 छोटा टेक्स्ट</p><p>बहुत छोटे टेक्स्ट (जैसे, एक या दो शब्द) के लिए भाषण जेनरेट करते समय, कभी-कभी कुछ उच्चारण छूट सकते हैं। इस समस्या के समाधान के लिए, आप <code>--speed 0.3</code> (जहाँ 0.3 एक समायोज्य मान है) पास कर सकते हैं ताकि जेनरेटेड भाषण की अवधि बढ़ाई जा सके।</p><p>#### 3.6 गलत उच्चारित चीनी बहुवर्णी अक्षरों को सुधारना</p><p>
हम <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> का उपयोग चीनी अक्षरों को पिनयिन में बदलने के लिए करते हैं। हालांकि, यह कभी-कभी <strong>बहु-ध्वनि अक्षरों</strong> (多音字) का उच्चारण गलत कर सकता है।</p><p>इन उच्चारणों को मैन्युअली ठीक करने के लिए, <strong>सुधारा गया पिनयिन</strong> कोण ब्रैकेट <code>< ></code> में लिखें और <strong>स्वर चिन्ह</strong> शामिल करें।</p><p><strong>उदाहरण:</strong></p><ul><li>मूल पाठ: <code>这把剑长三十公分</code></li>
<li><code>长</code> के पिनयिन को ठीक करें:  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>नोट:</strong> यदि आप मैन्युअल रूप से कई पिनयिन निर्धारित करना चाहते हैं, तो प्रत्येक पिनयिन को <code><></code> में लिखें, जैसे: <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 उत्पन्न ध्वनि से लंबी खामोशी हटाएँ</p><p>मॉडल स्वतः ही उत्पन्न ध्वनि में खामोशी के स्थान और अवधि का निर्धारण करेगा। कभी-कभी यह बोलने के बीच में लंबी खामोशी रखता है। यदि आप ऐसा नहीं चाहते हैं, तो आप <code>--remove-long-sil</code> पास कर सकते हैं ताकि उत्पन्न ध्वनि के बीच की लंबी खामोशी हट जाए (किनारे की खामोशी स्वतः हट जाएगी)।</p><p>#### 3.8 मॉडल डाउनलोडिंग</p><p>यदि आपको प्री-ट्रेंड मॉडल डाउनलोड करते समय HuggingFace से कनेक्ट होने में समस्या आ रही है, तो एंडपॉइंट को मिरर साइट पर बदलने का प्रयास करें: <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>अपना खुद का मॉडल ट्रेन करें</h2></p><p>ट्रेनिंग, फाइन-ट्यूनिंग और मूल्यांकन उदाहरणों के लिए <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> डायरेक्टरी देखें।</p><h2>प्रोडक्शन डिप्लॉयमेंट</h2></p><h3>NVIDIA Triton GPU रनटाइम</h3></p><p>उच्च प्रदर्शन और स्केलेबिलिटी के साथ प्रोडक्शन-तैयार डिप्लॉयमेंट के लिए <a href="runtime/nvidia_triton/" target="_blank" rel="noopener noreferrer">Triton Inference Server integration</a> देखें, जो ऑप्टिमाइज़्ड TensorRT इंजन, एक साथ अनुरोध हैंडलिंग, और एंटरप्राइज के लिए gRPC/HTTP APIs प्रदान करता है।</p><h3>CPU डिप्लॉयमेंट</h3></p><p>CPU पर C++ डिप्लॉयमेंट समाधान के लिए <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> देखें।</p><h2>चर्चा एवं संवाद</h2></p><p>आप सीधे <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a> पर चर्चा कर सकते हैं।</p><p>आप हमारा Wechat ग्रुप जॉइन करने या हमारे Wechat आधिकारिक अकाउंट को फॉलो करने के लिए QR कोड भी स्कैन कर सकते हैं।</p><p>| Wechat ग्रुप | Wechat आधिकारिक अकाउंट |
| ------------ | ----------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>संदर्भ</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-30

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-30 
    </div>
    
</body>
</html>