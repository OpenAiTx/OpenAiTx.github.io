<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ZipVoice - Synth&#232;se vocale rapide et de haute qualit&#233; sans entra&#238;nement pr&#233;alable gr&#226;ce au Flow Matching</title>
    <meta name="description" content="Synth&#232;se vocale rapide et de haute qualit&#233; sans entra&#238;nement pr&#233;alable gr&#226;ce au Flow Matching">
    <meta name="keywords" content="ZipVoice, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "ZipVoice",
  "description": "Synthèse vocale rapide et de haute qualité sans entraînement préalable grâce au Flow Matching",
  "author": {
    "@type": "Person",
    "name": "k2-fsa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 661
  },
  "url": "https://OpenAiTx.github.io/projects/k2-fsa/ZipVoice/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md",
  "datePublished": "2025-10-06",
  "dateModified": "2025-10-06"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/k2-fsa/ZipVoice" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    ZipVoice
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 661 stars</span>
                <span class="language">French</span>
                <span>by k2-fsa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 Langue</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=it">Itapano</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=k2-fsa&project=ZipVoice&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>ZipVoice⚡</h1></p><h2>Synthèse vocale rapide et de haute qualité en zéro-shot grâce au Flow Matching</h2>
</div></p><h2>Vue d'ensemble</h2></p><p>ZipVoice est une série de modèles TTS zero-shot rapides et de haute qualité, basés sur le flow matching.</p><h3>1. Caractéristiques principales</h3></p><ul><li>Petit et rapide : seulement 123M de paramètres.</li></p><p><li>Clonage vocal de haute qualité : performance à l’état de l’art en similarité de locuteur, intelligibilité et naturel.</li></p><p><li>Multilingue : support du chinois et de l’anglais.</li></p><p><li>Multi-mode : support de la génération vocale mono-locuteur et du dialogue.</li></p><p></ul><h3>2. Variantes du modèle</h3></p><p><table>
  <thead>
    <tr>
      <th>Nom du modèle</th>
      <th>Description</th>
      <th>Article</th>
      <th>Démo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ZipVoice</td>
      <td>Le modèle de base supportant le TTS zero-shot mono-locuteur en chinois et anglais.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2506.13053"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Distill</td>
      <td>La version distillée de ZipVoice, offrant une vitesse améliorée avec une dégradation minimale des performances.</td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog</td>
      <td>Un modèle de génération de dialogue basé sur ZipVoice, capable de générer des dialogues parlés à deux voix sur un seul canal.</td>
      <td rowspan="2"><a href="https://arxiv.org/abs/2507.09318"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg"></a></td>
      <td rowspan="2"><a href="https://zipvoice-dialog.github.io"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square"></a></td>
    </tr>
    <tr>
      <td>ZipVoice-Dialog-Stereo</td>
      <td>La variante stéréo de ZipVoice-Dialog, permettant la génération de dialogues à deux canaux avec chaque interlocuteur assigné à un canal distinct.</td>
    </tr>
  </tbody>
</table></p><h2>Actualités</h2></p><p><strong>2025/07/14</strong> : <strong>ZipVoice-Dialog</strong> et <strong>ZipVoice-Dialog-Stereo</strong>, deux modèles de génération de dialogues parlés, sont publiés. <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice-dialog.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><p><strong>2025/07/14</strong> : Le jeu de données <strong>OpenDialog</strong>, un corpus de dialogues parlés de 6,8k heures, est publié. Téléchargez-le sur <a href="https://huggingface.co/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow" alt="hf"></a>, <a href="https://www.modelscope.cn/datasets/k2-fsa/OpenDialog" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/ModelScope-Dataset-blue?logo=data" alt="ms"></a>. Consultez les détails sur <a href="https://arxiv.org/abs/2507.09318" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a>.</p><p><strong>2025/06/16</strong> : <strong>ZipVoice</strong> et <strong>ZipVoice-Distill</strong> sont publiés. <a href="https://arxiv.org/abs/2506.13053" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-Paper-COLOR.svg" alt="arXiv"></a> <a href="https://zipvoice.github.io" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/GitHub.io-Demo_Page-blue?logo=Github&style=flat-square" alt="demo page"></a></p><h2>Installation</h2></p><h3>1. Cloner le dépôt ZipVoice</h3></p><pre><code class="language-bash">git clone https://github.com/k2-fsa/ZipVoice.git</code></pre>
<h3>2. (Optionnel) Créez un environnement virtuel Python</h3></p><pre><code class="language-bash">python3 -m venv zipvoice
source zipvoice/bin/activate</code></pre>
<h3>3. Installez les paquets requis</h3></p><pre><code class="language-bash">pip install -r requirements.txt</code></pre>
<h3>4. Installer k2 pour l'entraînement ou l'inférence efficace</h3></p><p><strong>k2 est nécessaire pour l'entraînement</strong> et peut accélérer l'inférence. Néanmoins, vous pouvez toujours utiliser le mode inférence de ZipVoice sans installer k2.</p><blockquote><strong>Remarque :</strong> Assurez-vous d'installer la version de k2 qui correspond à votre version de PyTorch et CUDA. Par exemple, si vous utilisez pytorch 2.5.1 et CUDA 12.1, vous pouvez installer k2 comme suit :</blockquote></p><pre><code class="language-bash">pip install k2==1.24.4.dev20250208+cuda12.1.torch2.5.1 -f https://k2-fsa.github.io/k2/cuda.html</code></pre>
Veuillez consulter https://k2-fsa.org/get-started/k2/ pour plus de détails.
Les utilisateurs en Chine continentale peuvent consulter https://k2-fsa.org/zh-CN/get-started/k2/.</p><ul><li>Pour vérifier l'installation de k2 :</li></p><p>
</ul><pre><code class="language-bash">python3 -c "import k2; print(k2.__file__)"</code></pre>
<h2>Utilisation</h2></p><h3>1. Génération de parole à un seul locuteur</h3></p><p>Pour générer de la parole à un seul locuteur avec nos modèles ZipVoice ou ZipVoice-Distill pré-entraînés, utilisez les commandes suivantes (Les modèles requis seront téléchargés depuis HuggingFace) :</p><p>#### 1.1 Inférence d'une seule phrase</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --prompt-wav prompt.wav \
    --prompt-text "I am the transcription of the prompt wav." \
    --text "I am the text to be synthesized." \
    --res-wav-path result.wav</code></pre>
<ul><li><code>--model-name</code> peut être <code>zipvoice</code> ou <code>zipvoice_distill</code>, qui sont respectivement les modèles avant et après distillation.</li>
<li>Si <code><></code> ou <code>[]</code> apparaissent dans le texte, les chaînes entourées par celles-ci seront traitées comme des jetons spéciaux. <code><></code> indique le pinyin chinois et <code>[]</code> indique d'autres balises spéciales.</li></p><p></ul>#### 1.2 Inférence d'une liste de phrases</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice \
    --model-name zipvoice \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li>Chaque ligne de <code>test.tsv</code> est au format <code>{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code>.</li></p><p></ul><h3>2. Génération de la parole en dialogue</h3></p><p>#### 2.1 Commande d'inférence</p><p>Pour générer des dialogues parlés à deux voix avec nos modèles pré-entraînés ZipVoice-Dialogue ou ZipVoice-Dialogue-Stereo, utilisez les commandes suivantes (les modèles requis seront téléchargés depuis HuggingFace) :</p><pre><code class="language-bash">python3 -m zipvoice.bin.infer_zipvoice_dialog \
    --model-name "zipvoice_dialog" \
    --test-list test.tsv \
    --res-dir results</code></pre>
<ul><li><code>--model-name</code> peut être <code>zipvoice_dialog</code> ou <code>zipvoice_dialog_stereo</code>,</li>
    </ul>qui génèrent respectivement des dialogues mono et stéréo.</p><p>#### 2.2 Formats d'entrée</p><p>Chaque ligne de <code>test.tsv</code> est dans l'un des formats suivants :</p><p>(1) <strong>Format de prompt fusionné</strong> où les audios et transcriptions des prompts des deux locuteurs sont fusionnés en un seul fichier wav de prompt :</p><pre><code class="language-">{wav_name}\t{prompt_transcription}\t{prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> est le nom du fichier wav de sortie.</li>
<li><code>prompt_transcription</code> est la transcription du fichier wav du prompt conversationnel, par exemple, "[S1] Bonjour. [S2] Comment ça va ?"</li>
<li><code>prompt_wav</code> est le chemin vers le fichier wav du prompt.</li>
<li><code>text</code> est le texte à synthétiser, par exemple "[S1] Je vais bien. [S2] Comment tu t'appelles ? [S1] Je m'appelle Eric. [S2] Salut Eric."</li></p><p></ul>(2) <strong>Format de prompt séparé</strong> où les audios et les transcriptions des deux interlocuteurs existent dans des fichiers distincts :</p><pre><code class="language-">{wav_name}\t{spk1_prompt_transcription}\t{spk2_prompt_transcription}\t{spk1_prompt_wav}\t{spk2_prompt_wav}\t{text}</code></pre></p><ul><li><code>wav_name</code> est le nom du fichier wav de sortie.</li>
<li><code>spk1_prompt_transcription</code> est la transcription du fichier wav de prompt du premier locuteur, par exemple "Bonjour"</li>
<li><code>spk2_prompt_transcription</code> est la transcription du fichier wav de prompt du second locuteur, par exemple "Comment ça va ?"</li>
<li><code>spk1_prompt_wav</code> est le chemin du fichier wav de prompt du premier locuteur.</li>
<li><code>spk2_prompt_wav</code> est le chemin du fichier wav de prompt du second locuteur.</li>
<li><code>text</code> est le texte à synthétiser, par exemple "[S1] Je vais bien. [S2] Comment tu t'appelles ? [S1] Je m'appelle Eric. [S2] Salut Eric."</li></p><p></ul><h3>3 Conseils pour une meilleure utilisation :</h3></p><p>#### 3.1 Longueur du prompt</p><p>Nous recommandons un fichier wav de prompt court (par exemple, moins de 3 secondes pour la génération de parole à un seul locuteur, moins de 10 secondes pour la génération de dialogue) pour une vitesse d'inférence plus rapide. Un prompt très long ralentira l'inférence et dégradera la qualité de la parole.</p><p>#### 3.2 Optimisation de la vitesse</p><p>Si la vitesse d'inférence n'est pas satisfaisante, vous pouvez l'accélérer comme suit :</p><ul><li><strong>Modèle distillé et moins d'étapes</strong> : Pour le modèle de génération de parole à un seul locuteur, nous utilisons le modèle <code>zipvoice</code> par défaut pour une meilleure qualité de parole. Si la rapidité est prioritaire, vous pouvez passer à <code>zipvoice_distill</code> et réduire le paramètre <code>--num-steps</code> jusqu'à <code>4</code> (8 par défaut).</li></p><p><li><strong>Accélération CPU avec multi-threading</strong> : Lors de l'exécution sur CPU, vous pouvez utiliser le paramètre <code>--num-thread</code> (par exemple, <code>--num-thread 4</code>) pour augmenter le nombre de threads et accélérer la vitesse. Nous utilisons 1 thread par défaut.</li></p><p><li><strong>Accélération CPU avec ONNX</strong> : Lors de l'exécution sur CPU, vous pouvez utiliser les modèles ONNX avec <code>zipvoice.bin.infer_zipvoice_onnx</code> pour une vitesse supérieure (ONNX n'est pas encore supporté pour les modèles de génération de dialogue). Pour encore plus de rapidité, vous pouvez définir <code>--onnx-int8 True</code> pour utiliser un modèle ONNX quantifié INT8. Notez que le modèle quantifié dégradera la qualité de la parole dans une certaine mesure. <strong>N'utilisez pas ONNX sur GPU</strong>, car il est plus lent que PyTorch sur GPU.</li></p><p></ul>#### 3.3 Contrôle de la mémoire</p><p>Le texte fourni sera découpé en morceaux selon la ponctuation (pour la génération de parole à un seul locuteur) ou le symbole de changement de locuteur (pour la génération de dialogue). Ensuite, les morceaux seront traités en lots. Ainsi, le modèle peut traiter des textes arbitrairement longs avec une utilisation mémoire quasiment constante. Vous pouvez contrôler l'utilisation mémoire en ajustant le paramètre <code>--max-duration</code>.</p><p>#### 3.4 Évaluation "brute"</p><p>Par défaut, nous prétraitons les entrées (prompt wav, transcription du prompt et texte) pour une inférence efficace et de meilleures performances. Si vous souhaitez évaluer la performance "brute" du modèle avec les entrées exactes fournies (par exemple, pour reproduire les résultats de notre article), vous pouvez utiliser <code>--raw-evaluation True</code>.</p><p>#### 3.5 Texte court</p><p>Lors de la génération de parole pour des textes très courts (par exemple, un ou deux mots), la parole générée peut parfois omettre certaines prononciations. Pour résoudre ce problème, vous pouvez utiliser <code>--speed 0.3</code> (où 0.3 est une valeur ajustable) pour prolonger la durée de la parole générée.</p><p>#### 3.6 Correction de la prononciation incorrecte des caractères chinois polyphoniques</p><p>Nous utilisons <a href="https://github.com/mozillazg/python-pinyin" target="_blank" rel="noopener noreferrer">pypinyin</a> pour convertir les caractères chinois en pinyin. Cependant, il peut parfois mal prononcer les <strong>caractères polyphoniques</strong> (多音字).</p><p>Pour corriger manuellement ces erreurs de prononciation, encadrez le <strong>pinyin corrigé</strong> entre chevrons <code>< ></code> et incluez l’<strong>accent tonique</strong>.</p><p><strong>Exemple :</strong></p><ul><li>Texte original : <code>这把剑长三十公分</code></li>
<li>Corrigez le pinyin de <code>长</code> :  <code>这把剑<chang2>三十公分</code></li></p><p></ul>> <strong>Remarque :</strong> Si vous souhaitez attribuer plusieurs pinyins manuellement, encadrez chaque pinyin avec <code><></code>, par exemple : <code>这把<jian4><chang2><san1>十公分</code></p><p>#### 3.7 Suppression des silences longs dans la parole générée</p><p>Le modèle détermine automatiquement les positions et la durée des silences dans la parole générée. Il arrive qu’il y ait de longs silences au milieu de la parole. Si vous ne souhaitez pas cela, vous pouvez passer l’option <code>--remove-long-sil</code> pour supprimer les longs silences au milieu de la parole générée (les silences en début et fin seront supprimés par défaut).</p><p>#### 3.8 Téléchargement du modèle</p><p>Si vous rencontrez des difficultés pour vous connecter à HuggingFace lors du téléchargement des modèles pré-entraînés, essayez de changer l’endpoint vers le site miroir : <code>export HF_ENDPOINT=https://hf-mirror.com</code>.</p><h2>Entraînez votre propre modèle</h2></p><p>Consultez le répertoire <a href="egs" target="_blank" rel="noopener noreferrer">egs</a> pour des exemples d’entraînement, de fine-tuning et d’évaluation.</p><h2>Déploiement C++</h2></p><p>Consultez <a href="https://github.com/k2-fsa/sherpa-onnx/pull/2487#issuecomment-3227884498" target="_blank" rel="noopener noreferrer">sherpa-onnx</a> pour la solution de déploiement C++ sur CPU.</p><h2>Discussion & Communication</h2></p><p>Vous pouvez discuter directement sur <a href="https://github.com/k2-fsa/ZipVoice/issues" target="_blank" rel="noopener noreferrer">Github Issues</a>.</p><p>Vous pouvez également scanner le code QR pour rejoindre notre groupe WeChat ou suivre notre compte officiel WeChat.</p><p>| Groupe WeChat | Compte Officiel WeChat |
| ------------- | ---------------------- |
|<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_group.jpg" alt="wechat"> |<img src="https://k2-fsa.org/zh-CN/assets/pic/wechat_account.jpg" alt="wechat"> |</p><h2>Citation</h2></p><pre><code class="language-bibtex">@article{zhu2025zipvoice,
      title={ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow Matching},
      author={Zhu, Han and Kang, Wei and Yao, Zengwei and Guo, Liyong and Kuang, Fangjun and Li, Zhaoqing and Zhuang, Weiji and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2506.13053},
      year={2025}
}</p><p>@article{zhu2025zipvoicedialog,
      title={ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching},
      author={Zhu, Han and Kang, Wei and Guo, Liyong and Yao, Zengwei and Kuang, Fangjun and Zhuang, Weiji and Li, Zhaoqing and Han, Zhifeng and Zhang, Dong and Zhang, Xin and Song, Xingchen and Lin, Long and Povey, Daniel},
      journal={arXiv preprint arXiv:2507.09318},
      year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-10-06

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/k2-fsa/ZipVoice/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-10-06 
    </div>
    
</body>
</html>