<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>no-time-to-train - کد رسمی برای &#171;زمانی برای آموزش نیست! تقسیم‌بندی نمونه مبتنی بر مرجع بدون نیاز به آموزش&#187;</title>
    <meta name="description" content="کد رسمی برای &#171;زمانی برای آموزش نیست! تقسیم‌بندی نمونه مبتنی بر مرجع بدون نیاز به آموزش&#187;">
    <meta name="keywords" content="no-time-to-train, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "no-time-to-train",
  "description": "کد رسمی برای «زمانی برای آموزش نیست! تقسیم‌بندی نمونه مبتنی بر مرجع بدون نیاز به آموزش»",
  "author": {
    "@type": "Person",
    "name": "miquel-espinosa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 145
  },
  "url": "https://OpenAiTx.github.io/projects/miquel-espinosa/no-time-to-train/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/miquel-espinosa/no-time-to-train/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/miquel-espinosa/no-time-to-train" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    no-time-to-train
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 145 stars</span>
                <span class="language">Persian</span>
                <span>by miquel-espinosa</span>
            </div>
        </div>
        
        <div class="content">
            <p>
<div align="right">
  <details>
    <summary >🌐 زبان</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">简体中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">繁體中文</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">日本語</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">한국어</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">हिन्दी</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">ไทย</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">Français</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">Español</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">Русский</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">Português</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">العربية</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">فارسی</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">Türkçe</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">Tiếng Việt</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">Bahasa Indonesia</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>🚀 وقتی برای آموزش نیست!  </h1>
<h3>تفکیک نمونه مبتنی بر مرجع بدون نیاز به آموزش  </h3>
<a href="https://github.com/miquel-espinosa/no-time-to-train" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github" alt="GitHub"></a>
<a href="https://miquel-espinosa.github.io/no-time-to-train/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/🌐-Project%20Page-grey" alt="Website"></a>
<a href="https://arxiv.org/abs/2507.02798" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2507.02798-b31b1b" alt="arXiv"></a></p><p><strong>به‌روزترین وضعیت (Papers with Code)</strong></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 1-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 10-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 30-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><!-- <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 1-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot" alt="PWC"></a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 10-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot" alt="PWC"></a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 30-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot" alt="PWC"></a> --></p><p></div></p><hr></p><blockquote>🚨 <strong>بروزرسانی (۲۲ ژوئیه ۲۰۲۵):</strong> دستورالعمل‌های مربوط به دیتاست‌های سفارشی اضافه شد!</blockquote>
<blockquote></blockquote>
<blockquote>🔔 <strong>بروزرسانی (۱۶ ژوئیه ۲۰۲۵):</strong> کد با دستورالعمل‌ها بروزرسانی شد!</blockquote></p><hr></p><h2>📋 فهرست مطالب</h2></p><ul><li><a href="#-highlights" target="_blank" rel="noopener noreferrer">🎯 نکات برجسته</a></li>
<li><a href="#-abstract" target="_blank" rel="noopener noreferrer">📜 چکیده</a></li>
<li><a href="#-architecture" target="_blank" rel="noopener noreferrer">🧠 معماری</a></li>
<li><a href="#️-installation-instructions" target="_blank" rel="noopener noreferrer">🛠️ دستورالعمل نصب</a></li>
  <li><a href="#1-clone-the-repository" target="_blank" rel="noopener noreferrer">1. کلون کردن مخزن</a></li>
  <li><a href="#2-create-conda-environment" target="_blank" rel="noopener noreferrer">2. ساخت محیط conda</a></li>
  <li><a href="#3-install-sam2-and-dinov2" target="_blank" rel="noopener noreferrer">3. نصب SAM2 و DinoV2</a></li>
  <li><a href="#4-download-datasets" target="_blank" rel="noopener noreferrer">4. دانلود دیتاست‌ها</a></li>
  <li><a href="#5-download-sam2-and-dinov2-checkpoints" target="_blank" rel="noopener noreferrer">5. دانلود چک‌پوینت‌های SAM2 و DinoV2</a></li>
<li><a href="#-inference-code" target="_blank" rel="noopener noreferrer">📊 کد استنتاج: بازتولید نتایج SOTA 30-shot در Few-shot COCO</a></li>
  <li><a href="#0-create-reference-set" target="_blank" rel="noopener noreferrer">0. ساخت مجموعه مرجع</a></li>
  <li><a href="#1-fill-memory-with-references" target="_blank" rel="noopener noreferrer">1. پر کردن حافظه با نمونه‌های مرجع</a></li>
  <li><a href="#2-post-process-memory-bank" target="_blank" rel="noopener noreferrer">2. پس‌پردازش بانک حافظه</a></li>
  <li><a href="#3-inference-on-target-images" target="_blank" rel="noopener noreferrer">3. استنتاج روی تصاویر هدف</a></li>
  <li><a href="#results" target="_blank" rel="noopener noreferrer">نتایج</a></li>
<li><a href="#-custom-dataset" target="_blank" rel="noopener noreferrer">🔍 دیتاست سفارشی</a></li>
  <li><a href="#0-prepare-a-custom-dataset" target="_blank" rel="noopener noreferrer">0. آماده‌سازی دیتاست سفارشی ⛵🐦</a></li>
  <li><a href="#01-if-only-bbox-annotations-are-available" target="_blank" rel="noopener noreferrer">0.1 اگر فقط نشانه‌گذاری bbox وجود دارد</a></li>
  <li><a href="#02-convert-coco-annotations-to-pickle-file" target="_blank" rel="noopener noreferrer">0.2 تبدیل نشانه‌گذاری‌های coco به فایل pickle</a></li>
  <li><a href="#1-fill-memory-with-references" target="_blank" rel="noopener noreferrer">1. پر کردن حافظه با نمونه‌های مرجع</a></li>
  <li><a href="#2-post-process-memory-bank" target="_blank" rel="noopener noreferrer">2. پس‌پردازش بانک حافظه</a></li>
<li><a href="#-citation" target="_blank" rel="noopener noreferrer">📚 استناد</a></li></p><p>
</ul><h2>🎯 نکات برجسته</h2>
<ul><li>💡 <strong>بدون نیاز به آموزش:</strong> بدون فاین‌تیون، بدون مهندسی prompt — تنها یک تصویر مرجع.  </li>
<li>🖼️ <strong>مبتنی بر مرجع:</strong> تفکیک اشیای جدید تنها با چند نمونه.  </li>
<li>🔥 <strong>عملکرد SOTA:</strong> عملکرد بهتر از سایر روش‌های بدون آموزش روی COCO، PASCAL VOC و Cross-Domain FSOD.</li></p><p></ul><strong>لینک‌ها:</strong>
<ul><li>🧾 <a href="https://arxiv.org/abs/2507.02798" target="_blank" rel="noopener noreferrer"><strong>مقاله arXiv</strong></a>  </li>
<li>🌐 <a href="https://miquel-espinosa.github.io/no-time-to-train/" target="_blank" rel="noopener noreferrer"><strong>وب‌سایت پروژه</strong></a>  </li>
<li>📈 <a href="https://paperswithcode.com/paper/no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>Papers with Code</strong></a></li></p><p></ul><h2>📜 چکیده</h2></p><blockquote>The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).</blockquote></p><p><img src="https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9" alt="cdfsod-results-final-comic-sans-min"></p><h2>🧠 Architecture</h2></p><p><img src="https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9" alt="training-free-architecture-comic-sans-min"></p><h2>🛠️ Installation instructions</h2></p><h3>1. Clone the repository</h3></p><pre><code class="language-bash">git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train</code></pre>
<h3>۲. ایجاد محیط conda</h3></p><p>ما یک محیط conda با بسته‌های مورد نیاز ایجاد خواهیم کرد.</p><pre><code class="language-bash">conda env create -f environment.yml
conda activate no-time-to-train</code></pre>
<h3>3. نصب SAM2 و DinoV2</h3></p><p>ما SAM2 و DinoV2 را از سورس نصب خواهیم کرد.</p><pre><code class="language-bash">pip install -e .
cd dinov2
pip install -e .
cd ..</code></pre>
<h3>۴. دانلود دیتاست‌ها</h3></p><p>لطفاً دیتاست COCO را دانلود کرده و در مسیر <code>data/coco</code> قرار دهید</p><h3>۵. دانلود چک‌پوینت‌های SAM2 و DinoV2</h3></p><p>ما دقیقاً همان چک‌پوینت‌های SAM2 که در مقاله استفاده شده را دانلود خواهیم کرد.
(با این حال توجه داشته باشید که چک‌پوینت‌های SAM2.1 هم‌اکنون در دسترس هستند و ممکن است عملکرد بهتری داشته باشند.)</p><pre><code class="language-bash">mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..</code></pre>
<h2>📊 کد استنتاج</h2></p><p>⚠️ سلب مسئولیت: این کد پژوهشی است — انتظار کمی بی‌نظمی داشته باشید!</p><h3>بازتولید نتایج 30-شات SOTA در Few-shot COCO</h3></p><p>متغیرهای مفید را تعریف کنید و یک پوشه برای نتایج ایجاد نمایید:</p><pre><code class="language-bash">CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4</p><p>mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl</code></pre>
#### ۰. ایجاد مجموعه مرجع</p><pre><code class="language-bash">python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT</code></pre>
#### 1. حافظه را با ارجاعات پر کنید</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS</code></pre>
#### 2. پردازش پس‌ازآن بانک حافظه</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1</code></pre>
#### 3. استنتاج بر روی تصاویر هدف</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS</code></pre>
اگر می‌خواهید نتایج استنتاج را به صورت آنلاین (در همان زمان که محاسبه می‌شوند) مشاهده کنید، خطوط 1746-1749 در فایل <code>no_time_to_train/models/Sam2MatchingBaseline_noAMG.py</code> را <a href="https://github.com/miquel-espinosa/no-time-to-train/blob/main/no_time_to_train/models/Sam2MatchingBaseline_noAMG.py#L1746" target="_blank" rel="noopener noreferrer">در اینجا</a> از حالت کامنت خارج کنید.
پارامتر آستانه امتیاز <code>score_thr</code> را بسته به نیاز تنظیم کنید تا نمونه‌های بخش‌بندی شده بیشتری یا کمتری را مشاهده کنید.
اکنون تصاویر در مسیر <code>results_analysis/few_shot_classes/</code> ذخیره خواهند شد. تصویر سمت چپ حقیقت زمینه را نشان می‌دهد و تصویر سمت راست نمونه‌های بخش‌بندی شده‌ای را که توسط روش بدون آموزش ما پیدا شده‌اند، نمایش می‌دهد.</p><p>توجه داشته باشید که در این مثال ما از تقسیم‌بندی <code>few_shot_classes</code> استفاده می‌کنیم؛ بنابراین باید فقط انتظار دیدن نمونه‌های بخش‌بندی شده از کلاس‌های این تقسیم را داشته باشیم (نه همه کلاس‌های COCO).</p><p>#### نتایج</p><p>پس از اجرای تمام تصاویر در مجموعه اعتبارسنجی، باید به این نتیجه برسید:</p><pre><code class="language-">BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368</p><p>SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342</code></pre>
<hr></p><h2>🔍 دیتاست سفارشی</h2></p><p>ما دستورالعمل‌هایی برای اجرای پایپلاین خود بر روی دیتاست سفارشی ارائه می‌دهیم. فرمت حاشیه‌نویسی همیشه به صورت COCO است.</p><blockquote><strong>خلاصه؛</strong> برای مشاهده مستقیم نحوه اجرای کامل پایپلاین بر روی <em>دیتاست‌های سفارشی</em>، فایل <code>scripts/matching_cdfsod_pipeline.sh</code> را به همراه اسکریپت‌های نمونه دیتاست‌های CD-FSOD (مانند <code>scripts/dior_fish.sh</code>) بیابید.</blockquote></p><h3>۰. آماده‌سازی دیتاست سفارشی ⛵🐦</h3></p><p>فرض کنیم می‌خواهیم <strong>قایق‌ها</strong>⛵ و <strong>پرندگان</strong>🐦 را در یک دیتاست سفارشی شناسایی کنیم. برای استفاده از روش ما به موارد زیر نیاز داریم:
<ul><li>حداقل ۱ تصویر مرجع <em>حاشیه‌نویسی‌شده</em> برای هر کلاس (یعنی ۱ تصویر مرجع برای قایق و ۱ تصویر مرجع برای پرنده)</li>
<li>چندین تصویر هدف برای یافتن نمونه‌های کلاس‌های مورد نظر ما.</li></p><p></ul>ما یک اسکریپت نمونه برای ایجاد یک دیتاست سفارشی با تصاویر coco برای تنظیمات <strong>۱-شات</strong> آماده کرده‌ایم.
<pre><code class="language-bash">python scripts/make_custom_dataset.py</code></pre>
این کار یک مجموعه داده سفارشی با ساختار پوشه زیر ایجاد خواهد کرد:
<pre><code class="language-">data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)</code></pre>
<strong>بصری‌سازی تصاویر مرجع (۱-نمونه‌ای):</strong></p><p>| تصویر مرجع ۱-نمونه‌ای برای پرنده 🐦 | تصویر مرجع ۱-نمونه‌ای برای قایق ⛵ |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |</p><h3>۰.۱ اگر تنها نشانه‌گذاری‌های bbox موجود باشد</h3></p><p>ما همچنین یک اسکریپت برای تولید ماسک‌های بخش‌بندی سطح نمونه با استفاده از SAM2 ارائه می‌دهیم. این ابزار زمانی مفید است که تنها نشانه‌گذاری‌های جعبه محاطی برای تصاویر مرجع در اختیار داشته باشید.</p><pre><code class="language-bash"># Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
<h1>Run automatic instance segmentation from ground truth bounding boxes.</h1>
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize</code></pre>
<strong>تصاویر مرجع با ماسک‌های سگمنتیشن در سطح نمونه (تولید شده توسط SAM2 از جعبه‌های مرجع واقعیت، 1-شات):</strong></p><p>تصاویر بصری از ماسک‌های سگمنتیشن تولید شده در مسیر <code>data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/</code> ذخیره شده‌اند.</p><p>
| تصویر مرجع 1-شات برای پرنده 🐦 (سگمنت شده به طور خودکار با SAM) | تصویر مرجع 1-شات برای قایق ⛵ (سگمنت شده به طور خودکار با SAM) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |</p><h3>۰.۲ تبدیل حاشیه‌نویسی‌های coco به فایل pickle</h3></p><pre><code class="language-bash">python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1</code></pre>
<h3>1. حافظه را با ارجاعات پر کنید</h3></p><p>ابتدا متغیرهای مفید را تعریف کرده و یک پوشه برای نتایج ایجاد کنید. برای نمایش صحیح برچسب‌ها، نام کلاس‌ها باید بر اساس شناسه دسته‌بندی به همان ترتیبی که در فایل json ظاهر می‌شود، مرتب شوند. به عنوان مثال، <code>bird</code> شناسه دسته‌بندی <code>16</code> را دارد، <code>boat</code> شناسه دسته‌بندی <code>9</code> را دارد. بنابراین، <code>CAT_NAMES=boat,bird</code>.</p><pre><code class="language-bash">DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS</code></pre>
مرحله 1 را اجرا کنید:</p><pre><code class="language-bash">python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1</code></pre>
<h3>2. بانک حافظه پس‌پردازش</h3></p><pre><code class="language-bash">python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1</code></pre>
<h3>3. استنتاج بر روی تصاویر هدف</h3></p><p>اگر مقدار <code>ONLINE_VIS</code> برابر با True باشد، نتایج پیش‌بینی در مسیر <code>results_analysis/my_custom_dataset/</code> ذخیره شده و همزمان با محاسبه نمایش داده می‌شوند. توجه داشته باشید که اجرای برنامه با نمایش آنلاین بسیار کندتر است.</p><p>می‌توانید آستانه امتیاز <code>VIS_THR</code> را تغییر دهید تا نمونه‌های قطعه‌بندی شده بیشتری یا کمتری را مشاهده کنید.</p><pre><code class="language-bash">ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1</code></pre>
<h3>نتایج</h3></p><p>شاخص‌های عملکرد (با دقیقاً همان پارامترهای دستورات بالا) باید به شرح زیر باشند:</p><pre><code class="language-">BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478</p><p>SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458</code></pre>
نتایج بصری در مسیر <code>results_analysis/my_custom_dataset/</code> ذخیره می‌شوند. توجه داشته باشید که روش ما برای تشخیص منفی‌های کاذب کار می‌کند، یعنی تصاویری که هیچ نمونه‌ای از کلاس‌های مورد نظر را ندارند.</p><p><em>برای بزرگ‌نمایی روی تصاویر کلیک کنید ⬇️</em></p><p>| تصویر هدف با قایق‌ها ⛵ (چپ GT، راست پیش‌بینی) | تصویر هدف با پرندگان 🐦 (چپ GT، راست پیش‌بینی) |
|:----------------------:|:----------------------:|
| <img src="https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b" alt="000000459673"> | <img src="https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1" alt="000000407180"> |</p><p>| تصویر هدف با قایق‌ها و پرندگان ⛵🐦 (چپ GT، راست پیش‌بینی) | تصویر هدف بدون قایق یا پرنده 🚫 (چپ GT، راست پیش‌بینی) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5" alt="000000517410"> | <img src="https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e" alt="000000460598"> |</p><h2>📚 ارجاع</h2></p><p>اگر از این کار استفاده می‌کنید، لطفاً ما را ارجاع دهید:</p><pre><code class="language-bibtex">@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-24

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/miquel-espinosa/no-time-to-train/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>