<!DOCTYPE html>
<html lang="as">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>no-time-to-train - &quot;নো টাইম টু ট্রেইন! ট্রেইনিং-ফ্ৰী ৰেফাৰেন্স-ভিত্তিক ইনষ্টেন্স ছেগমেণ্টেশ্যনৰ বাবে চৰকাৰী কোড&quot;</title>
    <meta name="description" content="&quot;নো টাইম টু ট্রেইন! ট্রেইনিং-ফ্ৰী ৰেফাৰেন্স-ভিত্তিক ইনষ্টেন্স ছেগমেণ্টেশ্যনৰ বাবে চৰকাৰী কোড&quot;">
    <meta name="keywords" content="no-time-to-train, Assamese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "no-time-to-train",
  "description": "\"নো টাইম টু ট্রেইন! ট্রেইনিং-ফ্ৰী ৰেফাৰেন্স-ভিত্তিক ইনষ্টেন্স ছেগমেণ্টেশ্যনৰ বাবে চৰকাৰী কোড\"",
  "author": {
    "@type": "Person",
    "name": "miquel-espinosa"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 228
  },
  "url": "https://OpenAiTx.github.io/projects/miquel-espinosa/no-time-to-train/README-as.html",
  "sameAs": "https://raw.githubusercontent.com/miquel-espinosa/no-time-to-train/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/miquel-espinosa/no-time-to-train" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    no-time-to-train
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 228 stars</span>
                <span class="language">Assamese</span>
                <span>by miquel-espinosa</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="right">
  <details>
    <summary >🌐 ভাষা</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=en">ইংৰাজী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-CN">সহজ চীন</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=zh-TW">পৰম্পৰাগত চীন</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ja">জাপানী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ko">কোৰিয়ান</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=hi">হিন্দী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=th">থাই</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fr">ফৰাছী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=de">জাৰ্মান</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=es">স্পেনিছ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=it">ইটালিয়ান</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ru">ৰাছিয়ান</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pt">পৰ্তুগীজ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=nl">ডাচ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=pl">পোলিচ</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=ar">আৰবী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=fa">ফাৰ্ছী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=tr">তুৰ্কী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=vi">ভিয়েটনামী</a>
        | <a href="https://openaitx.github.io/view.html?user=miquel-espinosa&project=no-time-to-train&lang=id">ইণ্ডোনেছিয়ান</a>
      </div>
    </div>
  </details>
</div></p><p><div align="center"></p><h1>🚀 প্ৰশিক্ষণৰ সময় নাই!  </h1>
<h3>প্ৰশিক্ষণ-মুক্ত ৰেফাৰেঞ্চ-ভিত্তিক ইনষ্টেন্স ছেগমেণ্টেচন  </h3>
<a href="https://github.com/miquel-espinosa/no-time-to-train" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/%E2%80%8B-No%20Time%20To%20Train-black?logo=github" alt="GitHub"></a>
<a href="https://miquel-espinosa.github.io/no-time-to-train/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/🌐-Project%20Page-grey" alt="Website"></a>
<a href="https://arxiv.org/abs/2507.02798" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/arXiv-2507.02798-b31b1b" alt="arXiv"></a></p><p><strong>অত্যাধুনিক (Papers with Code)</strong></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 1-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(1--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 10-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(10--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 30-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/State%20of%20the%20Art-Few--Shot%20Object%20Detection%20on%20MS--COCO%20(30--shot" alt="PWC">-21CBCE?style=flat&logo=paperswithcode)</a></p><p><!-- <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 1-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-1-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-1-shot" alt="PWC"></a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 10-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-10-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-10-shot" alt="PWC"></a></p><p><a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>_SOTA 30-shot_</strong></a> | <a href="https://paperswithcode.com/sota/few-shot-object-detection-on-ms-coco-30-shot?p=no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/no-time-to-train-training-free-reference/few-shot-object-detection-on-ms-coco-30-shot" alt="PWC"></a> --></p><p></div></p><hr></p><blockquote>🚨 <strong>আপডেট (২২ জুলাই ২০২৫):</strong> কাষ্টম ডেটাসেটৰ নিৰ্দেশনা যোগ কৰা হৈছে!</blockquote>
<blockquote></blockquote>
<blockquote>🔔 <strong>আপডেট (১৬ জুলাই ২০২৫):</strong> কোডটো নিৰ্দেশনাসহ আপডেট কৰা হৈছে!</blockquote></p><hr></p><h2>📋 সূচীপত্ৰ</h2></p><ul><li><a href="#-highlights" target="_blank" rel="noopener noreferrer">🎯 মূল বৈশিষ্ট্য</a></li>
<li><a href="#-abstract" target="_blank" rel="noopener noreferrer">📜 সংক্ষেপ</a></li>
<li><a href="#-architecture" target="_blank" rel="noopener noreferrer">🧠 স্থাপত্য</a></li>
<li><a href="#️-installation-instructions" target="_blank" rel="noopener noreferrer">🛠️ ইনষ্টলেশ্যন নিৰ্দেশনা</a></li>
  <li><a href="#1-clone-the-repository" target="_blank" rel="noopener noreferrer">1. ৰিপ'জিট'ৰী ক্লোন কৰক</a></li>
  <li><a href="#2-create-conda-environment" target="_blank" rel="noopener noreferrer">2. কণ্ডা পৰিৱেশ সৃষ্টি কৰক</a></li>
  <li><a href="#3-install-sam2-and-dinov2" target="_blank" rel="noopener noreferrer">3. SAM2 আৰু DinoV2 ইনষ্টল কৰক</a></li>
  <li><a href="#4-download-datasets" target="_blank" rel="noopener noreferrer">4. ডেটাসেটসমূহ ডাউনলোড কৰক</a></li>
  <li><a href="#5-download-sam2-and-dinov2-checkpoints" target="_blank" rel="noopener noreferrer">5. SAM2 আৰু DinoV2 চেকপইণ্ট ডাউনলোড কৰক</a></li>
<li><a href="#-inference-code" target="_blank" rel="noopener noreferrer">📊 ইনফাৰেন্স কোড: Few-shot COCO-ত ৩০-shot SOTA ফলাফল পুনৰুৎপাদন কৰক</a></li>
  <li><a href="#0-create-reference-set" target="_blank" rel="noopener noreferrer">0. ৰেফাৰেন্স ছেট সৃষ্টি কৰক</a></li>
  <li><a href="#1-fill-memory-with-references" target="_blank" rel="noopener noreferrer">1. ৰেফাৰেন্সেৰে মেম'ৰি পূৰ্ণ কৰক</a></li>
  <li><a href="#2-post-process-memory-bank" target="_blank" rel="noopener noreferrer">2. মেম'ৰি বেংক প'ষ্ট-প্ৰচেছ কৰক</a></li>
  <li><a href="#3-inference-on-target-images" target="_blank" rel="noopener noreferrer">3. লক্ষ্য চিত্ৰসমূহত ইনফাৰেন্স কৰক</a></li>
  <li><a href="#results" target="_blank" rel="noopener noreferrer">ফলাফল</a></li></p><p><li><a href="#-custom-dataset" target="_blank" rel="noopener noreferrer">🔍 কাষ্টম ডেটাসেট</a></li>
  <li><a href="#0-prepare-a-custom-dataset" target="_blank" rel="noopener noreferrer">0. কাষ্টম ডেটাসেট প্ৰস্তুত কৰক ⛵🐦</a></li>
  <li><a href="#01-if-only-bbox-annotations-are-available" target="_blank" rel="noopener noreferrer">0.1 কেৱল bbox এনোটেশ্বন উপলব্ধ থাকিলে</a></li>
  <li><a href="#02-convert-coco-annotations-to-pickle-file" target="_blank" rel="noopener noreferrer">0.2 coco এনোটেশ্বন pickle ফাইললৈ ৰূপান্তৰ কৰক</a></li>
  <li><a href="#1-fill-memory-with-references" target="_blank" rel="noopener noreferrer">1. মেম'ৰীত references ভৰ্তি কৰক</a></li>
  <li><a href="#2-post-process-memory-bank" target="_blank" rel="noopener noreferrer">2. মেম'ৰি বেংক post-process কৰক</a></li>
<li><a href="#-citation" target="_blank" rel="noopener noreferrer">📚 উদ্ধৃতি</a></li></p><p>
</ul><h2>🎯 মূল বিষয়বস্তু</h2>
<ul><li>💡 <strong>প্ৰশিক্ষণ-ৰহিত</strong>: পুনঃ-প্ৰশিক্ষণ নাই, prompt engineering নাই—কেৱল এটা reference image।  </li>
<li>🖼️ <strong>Reference-Based</strong>: কেইটামান উদাহৰণ ব্যৱহাৰ কৰি নতুন বস্তু চেগমেণ্ট কৰক।  </li>
<li>🔥 <strong>SOTA Performance</strong>: Training-free পদ্ধতি সমূহতকৈ COCO, PASCAL VOC, আৰু Cross-Domain FSOD-ত উৎকৃষ্ট ফলাফল।</li></p><p></ul><strong>লিংকসমূহ:</strong>
<ul><li>🧾 <a href="https://arxiv.org/abs/2507.02798" target="_blank" rel="noopener noreferrer"><strong>arXiv পেপাৰ</strong></a>  </li>
<li>🌐 <a href="https://miquel-espinosa.github.io/no-time-to-train/" target="_blank" rel="noopener noreferrer"><strong>Project Website</strong></a>  </li>
<li>📈 <a href="https://paperswithcode.com/paper/no-time-to-train-training-free-reference" target="_blank" rel="noopener noreferrer"><strong>Papers with Code</strong></a></li></p><p></ul><h2>📜 প্ৰবন্ধৰ সংক্ষিপ্তসাৰ</h2></p><blockquote>The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).</blockquote></p><p><img src="https://github.com/user-attachments/assets/ab302c02-c080-4042-99fc-0e181ba8abb9" alt="cdfsod-results-final-comic-sans-min"></p><h2>🧠 Architecture</h2></p><p><img src="https://github.com/user-attachments/assets/d84dd83a-505e-45a0-8ce3-98e1838017f9" alt="training-free-architecture-comic-sans-min"></p><h2>🛠️ Installation instructions</h2></p><h3>1. Clone the repository</h3></p><pre><code class="language-bash">git clone https://github.com/miquel-espinosa/no-time-to-train.git
cd no-time-to-train</code></pre></p><h3>2. কন্ডা পৰিবেশ সৃষ্টি কৰক</h3></p><p>আমিহঁতে প্ৰয়োজনীয় পেকেজসমূহৰ সৈতে এটা কন্ডা পৰিবেশ সৃষ্টি কৰিম।
<pre><code class="language-bash">conda env create -f environment.yml
conda activate no-time-to-train</code></pre></p><h3>৩. SAM2 আৰু DinoV2 ইনষ্টল কৰক</h3></p><p>আমি উৎসৰ পৰা SAM2 আৰু DinoV2 ইনষ্টল কৰিম।
<pre><code class="language-bash">pip install -e .
cd dinov2
pip install -e .
cd ..</code></pre></p><h3>৪. ডেটাছেটসমূহ ডাউনলোড কৰক</h3></p><p>অনুগ্ৰহ কৰি COCO ডেটাছেট ডাউনলোড কৰি <code>data/coco</code> ত ৰাখক</p><h3>৫. SAM2 আৰু DinoV2 চেকপইণ্টসমূহ ডাউনলোড কৰক</h3></p><p>আমি কাকতত ব্যৱহৃত ঠিক SAM2 চেকপইণ্টসমূহ ডাউনলোড কৰিম।
(তথাপিও মনত ৰাখিব, SAM2.1 চেকপইণ্টসমূহ ইতিমধ্যে উপলব্ধ আৰু সম্ভৱত বেছি ভালদৰে কাৰ্যকৰী হ'ব পাৰে।)</p><pre><code class="language-bash">mkdir -p checkpoints/dinov2
cd checkpoints
wget https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt
cd dinov2
wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth
cd ../..</code></pre></p><h2>📊 অনুমান কোড</h2></p><p>⚠️ সতর্কীকৰণ: এইটো গৱেষণা কোড — অলপ বিশৃঙ্খলা আশা কৰক!</p><h3>Few-shot COCO-ত ৩০-শ্বট SOTA ফলাফল পুনৰুত্পাদন কৰা</h3></p><p>প্ৰয়োজনীয় ভেৰিয়েবলসমূহ সংজ্ঞা কৰক আৰু ফলাফলৰ বাবে এটা ফোল্ডাৰ সৃষ্টি কৰক:</p><pre><code class="language-bash">CONFIG=./no_time_to_train/new_exps/coco_fewshot_10shot_Sam2L.yaml
CLASS_SPLIT="few_shot_classes"
RESULTS_DIR=work_dirs/few_shot_results
SHOTS=30
SEED=33
GPUS=4</p><p>mkdir -p $RESULTS_DIR
FILENAME=few_shot_${SHOTS}shot_seed${SEED}.pkl</code></pre></p><p>#### ০. উদ্বৃত্ত ছেট সৃষ্টি কৰক</p><pre><code class="language-bash">python no_time_to_train/dataset/few_shot_sampling.py \
        --n-shot $SHOTS \
        --out-path ${RESULTS_DIR}/${FILENAME} \
        --seed $SEED \
        --dataset $CLASS_SPLIT</code></pre></p><p>#### ১. স্মৃতি ৰেফাৰেঞ্চেৰে পূৰণ কৰক</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG \
                              --model.test_mode fill_memory \
                              --out_path ${RESULTS_DIR}/memory.ckpt \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.memory_pkl ${RESULTS_DIR}/${FILENAME} \
                              --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOTS \
                              --model.init_args.dataset_cfgs.fill_memory.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS</code></pre></p><p>#### 2. পোস্ট-প্ৰসেছ মেম'ৰি বেংক</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG \
                              --model.test_mode postprocess_memory \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --ckpt_path ${RESULTS_DIR}/memory.ckpt \
                              --out_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --trainer.devices 1</code></pre></p><p>#### ৩. লক্ষ্য চিত্ৰসমূহত অনুমান</p><pre><code class="language-bash">python run_lightening.py test --config $CONFIG  \
                              --ckpt_path ${RESULTS_DIR}/memory_postprocessed.ckpt \
                              --model.init_args.test_mode test \
                              --model.init_args.model_cfg.memory_bank_cfg.length $SHOTS \
                              --model.init_args.model_cfg.dataset_name $CLASS_SPLIT \
                              --model.init_args.dataset_cfgs.test.class_split $CLASS_SPLIT \
                              --trainer.logger.save_dir ${RESULTS_DIR}/ \
                              --trainer.devices $GPUS</code></pre>
যদি আপুনি অনলাইনত অনুমান ফলাফলসমূহ (তেওঁলোক গণনা হোৱা সময়ত) চাব বিচাৰে, তেন্তে যুক্তি যোগ কৰক:</p><pre><code class="language-bash">    --model.init_args.model_cfg.test.online_vis True</code></pre>
স্ক'ৰ থ্ৰেশহ'ল্ড <code>score_thr</code> পৰামিতি সমন্বয় কৰিবলৈ, যুক্তি যোগ কৰক (উদাহৰণস্বৰূপে, সকলো ইনষ্টেন্সৰ ভিজুৱেলাইজেশ্যন যাৰ স্ক'ৰ <code>0.4</code>তকৈ বেছি):
<pre><code class="language-bash">    --model.init_args.model_cfg.test.vis_thr 0.4</code></pre>
এতিয়া ছবিসমূহ <code>results_analysis/few_shot_classes/</code>-ত সংৰক্ষণ কৰা হ'ব। বাওঁফালে থকা ছবিখনে গ্ৰাউণ্ড ট্ৰুথ দেখুৱায়, সোঁফালে থকা ছবিখনে আমাৰ প্ৰশিক্ষণ-মুক্ত পদ্ধতিয়ে বিচ্ছিন্ন কৰা ইনষ্টেন্সসমূহ দেখুৱায়।</p><p>মনত ৰাখিব যে, এই উদাহৰণত আমি <code>few_shot_classes</code> বিভাজন ব্যৱহাৰ কৰি আছোঁ, সেইবাবে, আমি কেৱল এই বিভাজনৰ শ্ৰেণীসমূহৰ বিচ্ছিন্ন ইনষ্টেন্সসমূহহে আশা কৰিব লাগিব (COCO-ৰ সকলো শ্ৰেণী নহয়)।</p><p>#### ফলাফলসমূহ</p><p>ভেলিডেশ্যন ছেটৰ সকলো ছবি চলোৱাৰ পাছত, আপোনাৰ লাভ হ'ব:</p><pre><code class="language-">BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.368</p><p>SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342</code></pre>
<hr></p><h2>🔍 কাষ্টম ডেটাচেট</h2></p><p>আমাৰ পাইপলাইনটো কাষ্টম ডেটাচেটত কেনেদৰে চলাব পাৰি তাৰ নিৰ্দেশনা আমি দিয়া হৈছে। এনোটেশ্বন ফৰ্মেট সদায় COCO ফৰ্মেটত হ'ব লাগে।</p><blockquote><strong>TLDR;</strong> <em>কাষ্টম ডেটাচেট</em> ত সম্পূৰ্ণ পাইপলাইন কেনেদৰে চলাব পাৰি সেয়া সোজাকৈ চাবলৈ <code>scripts/matching_cdfsod_pipeline.sh</code> আৰু CD-FSOD ডেটাচেটৰ উদাহৰণ স্ক্ৰিপ্ট (যেনে <code>scripts/dior_fish.sh</code>) চাওক</blockquote></p><h3>0. এটা কাষ্টম ডেটাচেট প্ৰস্তুত কৰক ⛵🐦</h3></p><p>চাওঁ আহক আমি কল্পনা কৰোঁ যে আমি এটা কাষ্টম ডেটাচেটত <strong>নাও</strong>⛵ আৰু <strong>চৰাই</strong>🐦 চিনাক্ত কৰিব বিচাৰিছো। আমাৰ পদ্ধতি ব্যৱহাৰ কৰিবলৈ আপোনাৰ প্ৰয়োজন হ'ব:
<ul><li>প্ৰতিটো শ্ৰেণীৰ বাবে কমেও ১ টা <em>এনোটেটেড</em> ৰেফাৰেন্স চিত্ৰ (অৰ্থাৎ নাওৰ বাবে ১ টা আৰু চৰাইৰ বাবে ১ টা ৰেফাৰেন্স চিত্ৰ)</li>
<li>আমাৰ ইচ্ছা কৰা শ্ৰেণীৰ উদাহৰণ বিচাৰি উলিয়াবলৈ বহুতো লক্ষ্য চিত্ৰ।</li></p><p></ul>আমি coco images ব্যৱহাৰ কৰি কাষ্টম ডেটাচেট তৈয়াৰ কৰিবলৈ এটা সাধাৰণ স্ক্ৰিপ্ট প্ৰস্তুত কৰিছো, <strong>1-shot</strong> ছেটিংৰ বাবে।
<pre><code class="language-bash">mkdir -p data/my_custom_dataset
python scripts/make_custom_dataset.py</code></pre>
এইটো তলত দিয়া ফোল্ডাৰ গঠনটোসহ এটা কাষ্টম ডেটাচেট সৃষ্টি কৰিব:
<pre><code class="language-">data/my_custom_dataset/
    ├── annotations/
    │   ├── custom_references.json
    │   ├── custom_targets.json
    │   └── references_visualisations/
    │       ├── bird_1.jpg
    │       └── boat_1.jpg
    └── images/
        ├── 429819.jpg
        ├── 101435.jpg
        └── (all target and reference images)</code></pre>
<strong>ৰেফাৰেঞ্চ চিত্ৰৰ ভিজুৱেলাইজেচন (১-শ্বট):</strong></p><p>| চৰাইৰ বাবে ১-শ্বট ৰেফাৰেঞ্চ চিত্ৰ 🐦 | নাওৰ বাবে ১-শ্বট ৰেফাৰেঞ্চ চিত্ৰ ⛵ |
|:----------------------------------:|:-----------------------------------:|
| <img src="https://github.com/user-attachments/assets/e59e580d-a7db-42ac-b386-892af211fc85" alt="bird_1" width="500"/> | <img src="https://github.com/user-attachments/assets/f94ee025-ae37-4a45-9c3e-0cfe8f8cd2bc" alt="boat_1" width="500"/> |</p><h3>০.১ কেৱল bbox এন'টেশ্যন উপলব্ধ থাকিলে</h3></p><p>আমিয়ে SAM2 ব্যৱহাৰ কৰি ইনষ্টেন্স-লেভেল ছেগমেণ্টেশ্যন মাস্ক সৃষ্টি কৰাৰ বাবে এটা স্ক্ৰিপ্টো প্ৰদান কৰোঁ। এইটো উপযোগী, যদি আপুনি ৰেফাৰেঞ্চ চিত্ৰসমূহৰ বাবে কেৱল বাউণ্ডিং বক্স এন'টেশ্যনহে উপলব্ধ ৰাখে।</p><pre><code class="language-bash"># Download sam_h checkpoint. Feel free to use more recent checkpoints (note: code might need to be adapted)
wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -O checkpoints/sam_vit_h_4b8939.pth
<h1>Run automatic instance segmentation from ground truth bounding boxes.</h1>
python no_time_to_train/dataset/sam_bbox_to_segm_batch.py \
    --input_json data/my_custom_dataset/annotations/custom_references.json \
    --image_dir data/my_custom_dataset/images \
    --sam_checkpoint checkpoints/sam_vit_h_4b8939.pth \
    --model_type vit_h \
    --device cuda \
    --batch_size 8 \
    --visualize</code></pre>
<strong>ইনষ্টেন্স-স্তৰৰ ছেগমেন্টেশ্বন মাক্স থকা ৰেফাৰেঞ্চ চিত্ৰসমূহ (gt বাউণ্ডিং বক্সৰ পৰা SAM2 দ্বাৰা উৎপন্ন, ১-শ্বট):</strong></p><p>উৎপন্ন কৰা ছেগমেন্টেশ্বন মাক্সৰ ভিজুৱালাইজেচন <code>data/my_custom_dataset/annotations/custom_references_with_SAM_segm/references_visualisations/</code> ত সংৰক্ষণ কৰা হৈছে।</p><p>| চৰাইৰ বাবে ১-শ্বট ৰেফাৰেঞ্চ চিত্ৰ 🐦 (স্বয়ংক্ৰিয়ভাৱে SAM দ্বাৰা ছেগমেন্ট কৰা) | নাওৰ বাবে ১-শ্বট ৰেফাৰেঞ্চ চিত্ৰ ⛵ (স্বয়ংক্ৰিয়ভাৱে SAM দ্বাৰা ছেগমেন্ট কৰা) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/65d38dc4-1454-43cd-9600-e8efc67b3a82" alt="bird_1_with_SAM_segm" width="500"/> | <img src="https://github.com/user-attachments/assets/43a558ad-50ca-4715-8285-9aa3268843c6" alt="boat_1_with_SAM_segm" width="500"/> |</p><h3>০.২ কোকো এনোটেশ্বনসমূহ পিকল ফাইললৈ ৰূপান্তৰ কৰা</h3></p><pre><code class="language-bash">python no_time_to_train/dataset/coco_to_pkl.py \
    data/my_custom_dataset/annotations/custom_references_with_segm.json \
    data/my_custom_dataset/annotations/custom_references_with_segm.pkl \
    1</code></pre></p><h3>১. ৰেফাৰেন্সৰ সৈতে মেম'ৰি পূৰণ কৰক</h3></p><p>প্ৰথমে, উপযোগী ভেৰিয়েবল সংজ্ঞা কৰক আৰু ফলাফলসমূহৰ বাবে এটা ফ'ল্ডাৰ সৃষ্টি কৰক। লেবেলসমূহ সঠিকভাৱে দৃশ্যায়িত হোৱাৰ বাবে, শ্ৰেণী নামসমূহ কেটেগৰি আই.ডি. অনুসৰি json ফাইলত যি অনুসৰি থাকে সেই অনুসাৰে সজ্জিত হ'ব লাগিব। যেনে, <code>bird</code>-ৰ কেটেগৰি আই.ডি. হৈছে <code>16</code>, <code>boat</code>-ৰ কেটেগৰি আই.ডি. হৈছে <code>9</code>। সেইবাবে, <code>CAT_NAMES=boat,bird</code>।</p><pre><code class="language-bash">DATASET_NAME=my_custom_dataset
DATASET_PATH=data/my_custom_dataset
CAT_NAMES=boat,bird
CATEGORY_NUM=2
SHOT=1
YAML_PATH=no_time_to_train/pl_configs/matching_cdfsod_template.yaml
PATH_TO_SAVE_CKPTS=./tmp_ckpts/my_custom_dataset
mkdir -p $PATH_TO_SAVE_CKPTS</code></pre></p><p>ধাপ ১ চলাও:
<pre><code class="language-bash">python run_lightening.py test --config $YAML_PATH \
    --model.test_mode fill_memory \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --model.init_args.dataset_cfgs.fill_memory.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.fill_memory.json_file $DATASET_PATH/annotations/custom_references_with_segm.json \
    --model.init_args.dataset_cfgs.fill_memory.memory_pkl $DATASET_PATH/annotations/custom_references_with_segm.pkl \
    --model.init_args.dataset_cfgs.fill_memory.memory_length $SHOT \
    --model.init_args.dataset_cfgs.fill_memory.cat_names $CAT_NAMES \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1</code></pre></p><h3>2. পোষ্ট-প্ৰসেছ মেমৰি বেংক</h3></p><pre><code class="language-bash">python run_lightening.py test --config $YAML_PATH \
    --model.test_mode postprocess_memory \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory.pth \
    --out_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --trainer.devices 1</code></pre></p><h3>৩. লক্ষ্য চিত্ৰসমূহত অনুমান</h3></p><p>যদি <code>ONLINE_VIS</code> True হিচাপে ছেট কৰা হয়, তেন্তে পূৰ্বানুমান ফলাফলসমূহ <code>results_analysis/my_custom_dataset/</code> ত সংৰক্ষণ কৰা হ'ব আৰু গননা কৰাৰ লগে লগে প্ৰদৰ্শন কৰা হ'ব। লক্ষ্য কৰক যে অনলাইন ভিজুৱালাইজেশ্যনৰ সৈতে চলালে ইয়াৰ গতি বহু মন্থৰ হয়।</p><p>আপুনি ইচ্ছা কৰিলে স্ক'ৰ থ্ৰেশহ’ল্ড <code>VIS_THR</code> পৰিবৰ্তন কৰি অধিক বা কম ছেগমেণ্ট কৰা ইনষ্টেন্স দেখিব পাৰে।
<pre><code class="language-bash">ONLINE_VIS=True
VIS_THR=0.4
python run_lightening.py test --config $YAML_PATH \
    --model.test_mode test \
    --ckpt_path $PATH_TO_SAVE_CKPTS/$DATASET_NAME\_$SHOT\_refs_memory_postprocessed.pth \
    --model.init_args.model_cfg.dataset_name $DATASET_NAME \
    --model.init_args.model_cfg.memory_bank_cfg.length $SHOT \
    --model.init_args.model_cfg.memory_bank_cfg.category_num $CATEGORY_NUM \
    --model.init_args.model_cfg.test.imgs_path $DATASET_PATH/images \
    --model.init_args.model_cfg.test.online_vis $ONLINE_VIS \
    --model.init_args.model_cfg.test.vis_thr $VIS_THR \
    --model.init_args.dataset_cfgs.test.root $DATASET_PATH/images \
    --model.init_args.dataset_cfgs.test.json_file $DATASET_PATH/annotations/custom_targets.json \
    --model.init_args.dataset_cfgs.test.cat_names $CAT_NAMES \
    --trainer.devices 1</code></pre></p><h3>ফলাফল</h3></p><p>কাৰ্যসম্পাদন মেট্ৰিক্স (ওপৰৰ নিৰ্দেশনাসমূহৰ একে একে পৰামিত্ৰৰে) হ'ব লাগিব:</p><pre><code class="language-">BBOX RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.478</p><p>SEGM RESULTS:
  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458</code></pre></p><p>দৃশ্যমান ফলাফলসমূহ <code>results_analysis/my_custom_dataset/</code>-ত সংৰক্ষিত হয়। লক্ষ্য কৰক যে, আমাৰ পদ্ধতিটো মিছা ঋণাত্মকসমূহৰ বাবে কাম কৰে, অৰ্থাৎ, সেইসকল চিত্ৰ য’ত প্ৰয়োজনীয় শ্ৰেণীসমূহৰ কোনো উদাহৰণ নাথাকে।</p><p><em>চিত্ৰসমূহ ডাঙৰ কৰিবলৈ ক্লিক কৰক ⬇️</em></p><p>| নৌকা থকা লক্ষ্য চিত্ৰ ⛵ (বাওঁফালে GT, সোঁফালে অনুমান) | চৰাই থকা লক্ষ্য চিত্ৰ 🐦 (বাওঁফালে GT, সোঁফালে অনুমান) |
|:----------------------:|:----------------------:|
| <img src="https://github.com/user-attachments/assets/678dc15a-dd3b-49d5-9287-6290da16aa6b" alt="000000459673"> | <img src="https://github.com/user-attachments/assets/fe306e48-af49-4d83-ac82-76fac6c456d1" alt="000000407180"> |</p><p>| নৌকা আৰু চৰাই থকা লক্ষ্য চিত্ৰ ⛵🐦 (বাওঁফালে GT, সোঁফালে অনুমান) | নৌকা বা চৰাই নথকা লক্ষ্য চিত্ৰ 🚫 (বাওঁফালে GT, সোঁফালে অনুমান) |
|:---------------------------------:|:----------------------------------:|
| <img src="https://github.com/user-attachments/assets/9849b227-7f43-43d7-81ea-58010a623ad5" alt="000000517410"> | <img src="https://github.com/user-attachments/assets/7587700c-e09d-4cf6-8590-3df129c2568e" alt="000000460598"> |</p><h2>📚 উদ্ধৃতি</h2></p><p>আপুনি এই কাম ব্যৱহাৰ কৰিলে, অনুগ্ৰহ কৰি আমাক উদ্ধৃতি দিয়ক:</p><pre><code class="language-bibtex">@article{espinosa2025notimetotrain,
  title={No time to train! Training-Free Reference-Based Instance Segmentation},
  author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
  journal={arXiv preprint arXiv:2507.02798},
  year={2025},
  primaryclass={cs.CV}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-09-06

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/miquel-espinosa/no-time-to-train/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>