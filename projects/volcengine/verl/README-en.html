<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - Read verl documentation in English. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read verl documentation in English. This project has 0 stars on GitHub.">
    <meta name="keywords" content="verl, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "verl",
  "description": "Read verl documentation in English. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "volcengine"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/volcengine/verl/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/volcengine/verl/master/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/volcengine/verl" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    verl
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">English</span>
                <span>by volcengine</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center">
 👋 Hi, everyone! 
    verl is an RL training library initiated by the <b>ByteDance Seed team</b> and maintained by the verl community.
    <br>
    <br>
</div></p><p><div align="center"></p><p><a href="https://deepwiki.com/volcengine/verl" target="_blank" rel="noopener noreferrer"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars"></a>
<a href="https://twitter.com/verl_project" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter"></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation"></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/WeChat-green?logo=wechat&amp"></a></p><p></div></p><p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo"></p><p><h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning for LLMs</h1></p><p>verl is a flexible, efficient, and production-ready RL training library for large language models (LLMs).</p><p>verl is the open-source implementation of the <strong><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> paper.</p><p>verl is flexible and easy to use with:</p><ul><li><strong>Easy extension of diverse RL algorithms</strong>: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in just a few lines of code.</li></p><p><li><strong>Seamless integration with existing LLM infrastructure via modular APIs</strong>: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc.</li></p><p><li><strong>Flexible device mapping</strong>: Supports various model placements onto different sets of GPUs for efficient resource utilization and scalability across cluster sizes.</li></p><p><li>Ready integration with popular HuggingFace models</li></p><p></ul>verl is fast with:</p><ul><li><strong>State-of-the-art throughput</strong>: SOTA LLM training and inference engine integrations and SOTA RL throughput.</li></p><p><li><strong>Efficient actor model resharding with 3D-HybridEngine</strong>: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.</li></p><p></ul></p></p><h2>News</h2></p><ul><li>[2025/06] verl with Megatron backend enables large MoE models such as <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html" target="_blank" rel="noopener noreferrer">DeepSeek-671b and Qwen3-236b</a>.</li>
<li>[2025/06] The verl team will provide the latest project updates at <a href="https://www.lfasiallc.com/pytorch-day-china/" target="_blank" rel="noopener noreferrer">PyTorch Day China</a> on June 7th. Meet our dev team in Beijing!</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a>, accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.</li>
<li>[2025/04] We will give a tutorial about the latest post-training techniques and programming guide for verl at <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=" target="_blank" rel="noopener noreferrer">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/" target="_blank" rel="noopener noreferrer">SCI-FM workshop</a> and <a href="https://lu.ma/d23nyynm" target="_blank" rel="noopener noreferrer">LMSys afterparty</a>. Talk materials available <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>[2025/04] <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf" target="_blank" rel="noopener noreferrer">Seed-Thinking-v1.5</a> tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces, and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118" target="_blank" rel="noopener noreferrer">VAPO</a> (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.</li>
<li>[2025/03] verl v0.3.0.post1 is released! See <a href="https://github.com/volcengine/verl/releases/" target="_blank" rel="noopener noreferrer">release note</a> for details. It achieves <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms" target="_blank" rel="noopener noreferrer">~1.4x speedup</a> compared to previous versions.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/" target="_blank" rel="noopener noreferrer">DAPO</a> is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek's GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO's training is fully powered by verl and the reproduction code is now available in <code>recipe/dapo</code>.</li>
</ul><details><summary> more... </summary>
<ul>
  <ul><li>[2025/05] verl will be presented at <a href="https://a2m.msup.com.cn/home/?aid=4488&city=shanghai" target="_blank" rel="noopener noreferrer">A2M Shanghai</a> on 5/16 - 5/17.</li>
  <li>[2025/05] verl will be presented at <a href="https://paris2025.gosim.org/" target="_blank" rel="noopener noreferrer">GOSIM x PyTorch Day 2025</a>. See you in Paris! </li>
  <li>[2025/03] We introduced the programming model of verl at the <a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg" target="_blank" rel="noopener noreferrer">vLLM Beijing Meetup</a> and <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf" target="_blank" rel="noopener noreferrer">verl intro and updates</a> at the <a href="https://lu.ma/ntjrr7ig" target="_blank" rel="noopener noreferrer">SGLang-LMSYS Org Meetup</a> in Sunnyvale mid-March.</li>
  <li>[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!</li>
  <li>[2025/02] verl v0.2.0.post2 is released!</li>
  <li>[2025/02] We presented verl in the <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. See you in San Jose!</li>
  <li>[2025/01] <a href="https://team.doubao.com/zh/special/doubao_1_5_pro" target="_blank" rel="noopener noreferrer">Doubao-1.5-pro</a> is released with SOTA-level performance on LLM & VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).</li>
  <li>[2024/12] verl is presented at Ray Forward 2024. Slides available <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">here</a></li>
  <li>[2024/12] The team presented <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> at NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> and <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">video</a> available.</li>
  <li>[2024/10] verl is presented at Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube video</a> available.</li>
  <li>[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.</li>
</ul></ul>   
</details></p><h2>Key Features</h2></p><ul><li><strong>FSDP</strong>, <strong>FSDP2</strong> and <strong>Megatron-LM</strong> for training.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> and <strong>HF Transformers</strong> for rollout generation.</li>
<li>Compatible with Hugging Face Transformers and Modelscope Hub: <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh" target="_blank" rel="noopener noreferrer">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc.</li>
<li>Supervised fine-tuning.</li>
<li>Reinforcement learning with <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/" target="_blank" rel="noopener noreferrer">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/" target="_blank" rel="noopener noreferrer">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/" target="_blank" rel="noopener noreferrer">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm" target="_blank" rel="noopener noreferrer">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/" target="_blank" rel="noopener noreferrer">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/" target="_blank" rel="noopener noreferrer">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/" target="_blank" rel="noopener noreferrer">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo/" target="_blank" rel="noopener noreferrer">DrGRPO</a>, etc.</li>
  <li>Support for model-based and function-based rewards (verifiable reward) for math, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/" target="_blank" rel="noopener noreferrer">coding</a>, etc.</li>
  <li>Support for vision-language models (VLMs) and <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh" target="_blank" rel="noopener noreferrer">multi-modal RL</a> with Qwen2.5-vl, Kimi-VL.</li>
  <li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn/" target="_blank" rel="noopener noreferrer">Multi-turn with tool calling</a></li>
<li>LLM alignment recipes such as <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo/" target="_blank" rel="noopener noreferrer">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh" target="_blank" rel="noopener noreferrer">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh" target="_blank" rel="noopener noreferrer">sequence parallelism</a> support via DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh" target="_blank" rel="noopener noreferrer">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh" target="_blank" rel="noopener noreferrer">Liger-kernel</a>.</li>
<li>Scales up to 671B models and hundreds of GPUs with <a href="https://github.com/volcengine/verl/pull/1467" target="_blank" rel="noopener noreferrer">expert parallelism</a></li>
<li>Multi-GPU <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html" target="_blank" rel="noopener noreferrer">LoRA RL</a> support to save memory.</li>
<li>Experiment tracking with wandb, swanlab, mlflow, and tensorboard.</li></p><p></ul><h2>Upcoming Features and Changes</h2></p><ul><li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>DeepSeek 671b optimizations with Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout and tools using optimizations https://github.com/volcengine/verl/issues/1882</li>
<li>Environment interactions https://github.com/volcengine/verl/issues/1172</li>
<li>List of breaking changes since v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff defaults to 0</li>
<li>Lora for RL https://github.com/volcengine/verl/pull/1127 </li></p><p></ul><h2>Getting Started</h2></p><p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentation</b></a></p><p><strong>Quickstart:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/start/install.html" target="_blank" rel="noopener noreferrer">Installation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html" target="_blank" rel="noopener noreferrer">Quickstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html" target="_blank" rel="noopener noreferrer">Programming Guide</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html" target="_blank" rel="noopener noreferrer">PPO in verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html" target="_blank" rel="noopener noreferrer">GRPO in verl</a></li></p><p></ul><strong>Running a PPO example step-by-step:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html" target="_blank" rel="noopener noreferrer">Prepare Data for Post-Training</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html" target="_blank" rel="noopener noreferrer">Implement Reward Function for Dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html" target="_blank" rel="noopener noreferrer">PPO Example Architecture</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html" target="_blank" rel="noopener noreferrer">Config Explanation</a></li></p><p></ul><strong>Reproducible algorithm baselines:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html" target="_blank" rel="noopener noreferrer">RL performance on coding, math</a></li></p><p></ul><strong>For code explanation and advanced usage (extension):</strong></p><ul><li>PPO Trainer and Workers</li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html" target="_blank" rel="noopener noreferrer">PPO Ray Trainer</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html" target="_blank" rel="noopener noreferrer">PyTorch FSDP Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Megatron-LM Backend</a></li></p><p><li>Advanced Usage and Extension</li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html" target="_blank" rel="noopener noreferrer">Add Models with the FSDP Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html" target="_blank" rel="noopener noreferrer">Add Models with the Megatron-LM Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html" target="_blank" rel="noopener noreferrer">Multi-turn Rollout Support</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html" target="_blank" rel="noopener noreferrer">Search Tool Integration</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html" target="_blank" rel="noopener noreferrer">Sandbox Fusion Integration</a></li>
  <li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement/" target="_blank" rel="noopener noreferrer">Deployment using Separate GPU Resources</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html" target="_blank" rel="noopener noreferrer">Extend to Other RL(HF) algorithms</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html" target="_blank" rel="noopener noreferrer">Ray API design tutorial</a></li></p><p></ul><strong>Blogs from the community</strong></p><ul><li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md" target="_blank" rel="noopener noreferrer">SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html" target="_blank" rel="noopener noreferrer">Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA" target="_blank" rel="noopener noreferrer">veMLP x verl: Playing with RL Training</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942" target="_blank" rel="noopener noreferrer">Best Practices for GRPO Distributed RL Training with verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md" target="_blank" rel="noopener noreferrer">HybridFlow verl Paper Walkthrough</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90" target="_blank" rel="noopener noreferrer">Up to 20x Throughput Improvement! Doubao Model Team Releases Brand New RLHF Framework, Now Open Source!</a></li></p><p></ul><h2>Performance Tuning Guide</h2></p><p>Performance is essential for on-policy RL algorithms. We have written a detailed <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html" target="_blank" rel="noopener noreferrer">performance tuning guide</a> to help you optimize performance.</p><h2>Upgrade to vLLM >= v0.8.2</h2></p><p>verl now supports vLLM>=0.8.2 when using FSDP as the training backend. Please refer to <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md" target="_blank" rel="noopener noreferrer">this document</a> for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.</p><h2>Use Latest SGLang</h2></p><p>SGLang is fully supported with verl, and the SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html" target="_blank" rel="noopener noreferrer">this document</a> for the installation guide and more information.</p><h2>Upgrade to FSDP2</h2></p><p>verl is fully embracing FSDP2! FSDP2 is recommended by the torch distributed team, providing better throughput and memory usage, and is composable with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:
<pre><code class="language-">actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 </code></pre>
Furthermore, FSDP2 CPU offloading is compatible with gradient accumulation. You can turn it on to save memory with <code>actor_rollout_ref.actor.offload_policy=True</code>. For more details, see https://github.com/volcengine/verl/pull/1026</p><h2>AMD Support (ROCm Kernel)</h2></p><p>verl now supports FSDP as the training engine (Megatron support coming soon) and integrates with both vLLM and SGLang as inference engines. Please refer to <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst" target="_blank" rel="noopener noreferrer">this document</a> for the installation guide and more information, and <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst" target="_blank" rel="noopener noreferrer">this document</a> for vLLM performance tuning for ROCm.</p><h2>Citation and acknowledgement</h2></p><p>If you find the project helpful, please cite:</p><ul><li><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf" target="_blank" rel="noopener noreferrer">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li></p><p></ul><pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}</code></pre></p><p>verl is inspired by the design of Nemo-Aligner, Deepspeed-chat, and OpenRLHF. The project is adopted and contributed to by Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/" target="_blank" rel="noopener noreferrer">Alibaba Qwen team</a>, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, <a href="https://www.all-hands.dev/" target="_blank" rel="noopener noreferrer">All Hands AI</a>, <a href="http://modelbest.cn/" target="_blank" rel="noopener noreferrer">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/" target="_blank" rel="noopener noreferrer">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/" target="_blank" rel="noopener noreferrer">Camel-AI</a>, <a href="https://github.com/OpenManus" target="_blank" rel="noopener noreferrer">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home" target="_blank" rel="noopener noreferrer">Baichuan</a>, <a href="https://www.xiaohongshu.com/" target="_blank" rel="noopener noreferrer">RedNote</a>, <a href="https://www.swiss-ai.org/" target="_blank" rel="noopener noreferrer">SwissAI</a>, <a href="https://www.moonshot-ai.com/" target="_blank" rel="noopener noreferrer">Moonshot AI (Kimi)</a>, Baidu, Snowflake, and many more.</p><h2>Awesome work using verl</h2></p><ul><li><a href="https://github.com/Jiayi-Pan/TinyZero" target="_blank" rel="noopener noreferrer">TinyZero</a>: a reproduction of <strong>DeepSeek R1 Zero</strong> recipe for reasoning tasks <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought" target="_blank" rel="noopener noreferrer">SkyThought</a>: RL training for Sky-T1-7B by NovaSky AI team. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason" target="_blank" rel="noopener noreferrer">simpleRL-reason</a>: SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hiyouga/EasyR1" target="_blank" rel="noopener noreferrer">Easy-R1</a>: <strong>Multi-modal</strong> RL training framework <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL" target="_blank" rel="noopener noreferrer">OpenManus-RL</a>: LLM Agents RL tuning framework for multiple agent environments. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/agentica-project/rllm" target="_blank" rel="noopener noreferrer">rllm</a>: async RL training with <a href="https://github.com/agentica-project/verl-pipeline" target="_blank" rel="noopener noreferrer">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/PRIME" target="_blank" rel="noopener noreferrer">PRIME</a>: Process reinforcement through implicit rewards <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ZihanWang314/ragen" target="_blank" rel="noopener noreferrer">RAGEN</a>: a general-purpose reasoning <strong>agent</strong> training framework <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1" target="_blank" rel="noopener noreferrer">Search-R1</a>: RL with reasoning and <strong>searching (tool-call)</strong> interleaved LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval" target="_blank" rel="noopener noreferrer">DeepRetrieval</a>: RL Training of <strong>Search Agent</strong> with <strong>Search/Retrieval Outcome</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Agent-RL/ReSearch" target="_blank" rel="noopener noreferrer">ReSearch</a>: Learning to <strong>Re</strong>ason with <strong>Search</strong> for LLMs via Reinforcement Learning <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ganler/code-r1" target="_blank" rel="noopener noreferrer">Code-R1</a>: Reproducing R1 for <strong>Code</strong> with Reliable Rewards <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1" target="_blank" rel="noopener noreferrer">Skywork-OR1</a>: Skywork open reasoner series <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/ToRL" target="_blank" rel="noopener noreferrer">ToRL</a>: Scaling tool-integrated RL <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/langfengQ/verl-agent" target="_blank" rel="noopener noreferrer">verl-agent</a>: A scalable training framework for <strong>long-horizon LLM/VLM agents</strong>, along with a new algorithm <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a>: Policy Filtration for PPO based on the reliability of reward signals for more efficient and robust RLHF.</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1" target="_blank" rel="noopener noreferrer">GUI-R1</a>: <strong>GUI-R1</strong>: A Generalist R1-style Vision-Language Action Model For <strong>GUI Agents</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher" target="_blank" rel="noopener noreferrer">DeepResearcher</a>: Scaling deep research via reinforcement learning in real-world environments <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN" target="_blank" rel="noopener noreferrer">VAGEN</a>: Training VLM agents with multi-turn reinforcement learning <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars"></li>
<li><a href="https://retool-rl.github.io/" target="_blank" rel="noopener noreferrer">ReTool</a>: ReTool: reinforcement learning for strategic tool use in LLMs. Code release is in progress...</li>
<li><a href="https://arxiv.org/abs/2505.02387" target="_blank" rel="noopener noreferrer">RM-R1</a>: RL training of reasoning reward models <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2505.03335" target="_blank" rel="noopener noreferrer">Absolute Zero Reasoner</a>: A no human curated data self-play framework for reasoning<img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/pdf/2504.14945" target="_blank" rel="noopener noreferrer">LUFFY</a>: Learning to Reason under Off-Policy Guidance<img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool" target="_blank" rel="noopener noreferrer">verl-tool</a>: An unified and easy-to-extend tool-agent training framework based on verl<img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/zwhe99/DeepMath" target="_blank" rel="noopener noreferrer">DeepMath</a>: DeepMath-103K data and series models for math reasoning<img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars"></li></p><p></ul>and many more awesome works listed in <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md" target="_blank" rel="noopener noreferrer">recipe</a>.
<h2>Contribution Guide</h2></p><p>Contributions from the community are welcome! Please check out our <a href="https://github.com/volcengine/verl/issues/710" target="_blank" rel="noopener noreferrer">project roadmap</a> and <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22" target="_blank" rel="noopener noreferrer">good first issues</a> to see where you can contribute.</p><h3>Code Linting and Formatting</h3></p><p>We use pre-commit to help improve code quality. To initialize pre-commit, run:</p><pre><code class="language-bash">pip install pre-commit
pre-commit install</code></pre></p><p>To resolve CI errors locally, you can manually run pre-commit by:</p><pre><code class="language-bash">pre-commit run</code></pre></p><h3>Adding CI tests</h3></p><p>If possible, please add CI test(s) for your new feature:</p><ul><li>Find the most relevant workflow yml file, which usually corresponds to a <code>hydra</code> default config (e.g. <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code>, etc).</li>
<li>Add related path patterns to the <code>paths</code> section if not already included.</li>
<li>Minimize the workload of the test script(s) (see existing scripts for examples).</li></p><p></ul><h2>About <a href="https://team.doubao.com/" target="_blank" rel="noopener noreferrer">ByteDance Seed Team</a></h2></p><p>Founded in 2023, the ByteDance Seed Team is dedicated to crafting the industry's most advanced AI foundation models. The team aspires to become a world-class research team and make significant contributions to the advancement of science and society. You can get to know Bytedance Seed better through the following channels👇
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a></p><p></div>
<hr></p><p>We are HIRING! Send us an <a href="mailto:haibin.lin@bytedance.com" target="_blank" rel="noopener noreferrer">email</a> if you are interested in internship/FTE opportunities in RL for agents.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/volcengine/verl/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>