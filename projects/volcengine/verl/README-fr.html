<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - Read verl documentation in French. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read verl documentation in French. This project has 0 stars on GitHub.">
    <meta name="keywords" content="verl, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "verl",
  "description": "Read verl documentation in French. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "volcengine"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/volcengine/verl/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/volcengine/verl/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/volcengine/verl" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    verl
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">French</span>
                <span>by volcengine</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center">
 👋 Bonjour à tous ! 
    verl est une bibliothèque d'entraînement RL initiée par l'<b>équipe ByteDance Seed</b> et maintenue par la communauté verl.
    <br>
    <br>
</div></p><p><div align="center"></p><p><a href="https://deepwiki.com/volcengine/verl" target="_blank" rel="noopener noreferrer"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars"></a>
<a href="https://twitter.com/verl_project" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter"></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation"></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p><p></div></p><p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo"></p><p><h1 style="text-align: center;">verl : Volcano Engine Reinforcement Learning pour LLMs</h1></p><p>verl est une bibliothèque d'entraînement RL flexible, efficace et prête pour la production pour les grands modèles de langage (LLMs).</p><p>verl est la version open source de l'article <strong><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong>.</p><p>verl est flexible et facile à utiliser avec :</p><ul><li><strong>Extension facile de divers algorithmes RL</strong> : Le modèle de programmation hybrid-controller permet une représentation flexible et une exécution efficace de flux de données complexes de post-entraînement. Construisez des flux RL tels que GRPO, PPO en quelques lignes de code.</li></p><p><li><strong>Intégration transparente de l'infrastructure LLM existante avec des API modulaires</strong> : Découple les dépendances de calcul et de données, permettant une intégration transparente avec les frameworks LLM existants, tels que FSDP, Megatron-LM, vLLM, SGLang, etc.</li></p><p><li><strong>Mapping flexible des dispositifs</strong> : Prend en charge divers placements de modèles sur différents ensembles de GPU pour une utilisation efficace des ressources et une évolutivité sur différentes tailles de clusters.</li></p><p><li>Intégration prête à l'emploi avec les modèles populaires HuggingFace</li></p><p></ul>verl est rapide avec :</p><ul><li><strong>Débit à l'état de l'art</strong> : Intégration des moteurs d'entraînement et d'inférence SOTA pour LLM, ainsi qu'un débit RL SOTA.</li></p><p><li><strong>Resharding efficace des modèles acteurs avec 3D-HybridEngine</strong> : Élimine la redondance mémoire et réduit considérablement les surcoûts de communication lors des transitions entre les phases d'entraînement et de génération.</li></p><p></ul></p></p><h2>Actualités</h2></p><ul><li>[2025/06] verl avec backend Megatron permet de supporter de grands modèles MoE tels que <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html" target="_blank" rel="noopener noreferrer">DeepSeek-671b et Qwen3-236b</a>.</li>
<li>[2025/06] L'équipe verl présentera les dernières avancées du projet lors du <a href="https://www.lfasiallc.com/pytorch-day-china/" target="_blank" rel="noopener noreferrer">PyTorch Day China</a> le 7 juin. Rencontrez notre équipe de dev à Pékin !</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a>, accepté à l'ICML 2025, est désormais supporté dans verl ! PF-PPO améliore l'efficacité et la robustesse de l'apprentissage de la politique en filtrant les signaux de récompense potentiellement bruyants et en réutilisant les expériences de haute qualité via un buffer de relecture.</li>
<li>[2025/04] Nous donnerons un tutoriel sur les dernières techniques de post-entraînement et le guide de programmation pour verl lors de <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=" target="_blank" rel="noopener noreferrer">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/" target="_blank" rel="noopener noreferrer">atelier SCI-FM</a> et <a href="https://lu.ma/d23nyynm" target="_blank" rel="noopener noreferrer">LMSys afterparty</a>. Les supports de présentation sont disponibles <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25" target="_blank" rel="noopener noreferrer">ici</a>.</li>
<li>[2025/04] Le rapport technique <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf" target="_blank" rel="noopener noreferrer">Seed-Thinking-v1.5</a> est publié ! Entraîné avec verl, Seed-Thinking-v1.5 atteint 86.7 sur AIME 2024, 55.0 sur Codeforces et 77.3 sur GPQA, démontrant d'excellentes capacités de raisonnement en STEM et codage. Au-delà des tâches de raisonnement, la méthode démontre une généralisation notable sur divers domaines.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118" target="_blank" rel="noopener noreferrer">VAPO</a> (value-based augmented PPO) présente notre dernière méthode RL pour les modèles de raisonnement. Entraîné à partir du modèle Qwen-32B-base, VAPO atteint 60.4 sur AIME 2024, surpassant DAPO-32B.</li>
<li>[2025/03] verl v0.3.0.post1 est publié ! Voir la <a href="https://github.com/volcengine/verl/releases/" target="_blank" rel="noopener noreferrer">note de version</a> pour plus de détails. Il atteint <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms" target="_blank" rel="noopener noreferrer">~1.4x d'accélération</a> par rapport aux versions précédentes.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/" target="_blank" rel="noopener noreferrer">DAPO</a> est l'algorithme RL SOTA open source qui atteint 50 points sur AIME 2024 basé sur le modèle pré-entraîné Qwen2.5-32B, surpassant le précédent SOTA de DeepSeek GRPO (DeepSeek-R1-Zero-Qwen-32B). L'entraînement de DAPO est entièrement propulsé par verl et le code de reproduction est disponible dans <code>recipe/dapo</code> maintenant.</li>
</ul><details><summary> plus... </summary>
<ul>
  <ul><li>[2025/05] verl sera présenté à <a href="https://a2m.msup.com.cn/home/?aid=4488&city=shanghai" target="_blank" rel="noopener noreferrer">A2M Shanghai</a> du 16 au 17 mai.</li>
  <li>[2025/05] verl sera présenté à <a href="https://paris2025.gosim.org/" target="_blank" rel="noopener noreferrer">GOSIM x PyTorch Day 2025</a>. À bientôt à Paris !</li>
  <li>[2025/03] Nous avons présenté le modèle de programmation de verl lors du <a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg" target="_blank" rel="noopener noreferrer">vLLM Beijing Meetup</a> et <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf" target="_blank" rel="noopener noreferrer">présentation et mises à jour de verl</a> lors du <a href="https://lu.ma/ntjrr7ig" target="_blank" rel="noopener noreferrer">SGLang-LMSYS Org Meetup</a> à Sunnyvale mi-mars.</li>
  <li>[2025/03] Nous présenterons verl (HybridFlow) à EuroSys 2025. Rendez-vous à Rotterdam !</li>
  <li>[2025/02] verl v0.2.0.post2 est publié !</li>
  <li>[2025/02] Nous avons présenté verl lors du <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. Rendez-vous à San Jose !</li>
  <li>[2025/01] <a href="https://team.doubao.com/zh/special/doubao_1_5_pro" target="_blank" rel="noopener noreferrer">Doubao-1.5-pro</a> est publié avec des performances SOTA sur LLM & VLM. Le modèle d'aperçu RL scaling a été entraîné avec verl, atteignant des performances de niveau OpenAI O1 sur les benchmarks de mathématiques (70.0 pass@1 sur AIME).</li>
  <li>[2024/12] verl a été présenté à Ray Forward 2024. Slides disponibles <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">ici</a></li>
  <li>[2024/12] L'équipe a présenté <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> à NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> et <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">vidéo</a> disponibles.</li>
  <li>[2024/10] verl a été présenté au Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Vidéo Youtube</a> disponible.</li>
  <li>[2024/08] HybridFlow (verl) est accepté à EuroSys 2025.</li>
</ul></ul>   
</details></p><h2>Fonctionnalités clés</h2></p><ul><li><strong>FSDP</strong>, <strong>FSDP2</strong> et <strong>Megatron-LM</strong> pour l'entraînement.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> et <strong>HF Transformers</strong> pour la génération des rollouts.</li>
<li>Compatible avec Hugging Face Transformers et Modelscope Hub : <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh" target="_blank" rel="noopener noreferrer">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc.</li>
<li>Fine-tuning supervisé.</li>
<li>Apprentissage par renforcement avec <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/" target="_blank" rel="noopener noreferrer">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/" target="_blank" rel="noopener noreferrer">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/" target="_blank" rel="noopener noreferrer">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm" target="_blank" rel="noopener noreferrer">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/" target="_blank" rel="noopener noreferrer">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/" target="_blank" rel="noopener noreferrer">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/" target="_blank" rel="noopener noreferrer">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo" target="_blank" rel="noopener noreferrer">DrGRPO</a>, etc.</li>
  <li>Supporte la récompense basée sur un modèle et la récompense basée sur une fonction (récompense vérifiable) pour les maths, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo" target="_blank" rel="noopener noreferrer">le codage</a>, etc.</li>
  <li>Supporte les modèles vision-langage (VLMs) et <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh" target="_blank" rel="noopener noreferrer">RL multi-modal</a> avec Qwen2.5-vl, Kimi-VL</li>
  <li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn" target="_blank" rel="noopener noreferrer">Multi-turn avec appel d’outils</a></li>
<li>Recettes d'alignement LLM comme <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo" target="_blank" rel="noopener noreferrer">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh" target="_blank" rel="noopener noreferrer">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh" target="_blank" rel="noopener noreferrer">sequence parallelism</a> via DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh" target="_blank" rel="noopener noreferrer">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh" target="_blank" rel="noopener noreferrer">Liger-kernel</a>.</li>
<li>Évolue jusqu’à 671B de paramètres et des centaines de GPU avec <a href="https://github.com/volcengine/verl/pull/1467" target="_blank" rel="noopener noreferrer">expert parallelism</a></li>
<li>Support RL LoRA multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html" target="_blank" rel="noopener noreferrer">LoRA RL</a> pour économiser de la mémoire.</li>
<li>Suivi d'expérience avec wandb, swanlab, mlflow et tensorboard.</li></p><p></ul><h2>Fonctionnalités et changements à venir</h2></p><ul><li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>Optimisations DeepSeek 671b avec Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout et outils utilisant des optimisations https://github.com/volcengine/verl/issues/1882</li>
<li>Interactions avec l'environnement https://github.com/volcengine/verl/issues/1172</li>
<li>Liste des changements majeurs depuis v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff par défaut à 0</li>
<li>Lora pour RL https://github.com/volcengine/verl/pull/1127 </li></p><p></ul><h2>Démarrage rapide</h2></p><p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentation</b></a></p><p><strong>Démarrage rapide :</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/start/install.html" target="_blank" rel="noopener noreferrer">Installation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html" target="_blank" rel="noopener noreferrer">Quickstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html" target="_blank" rel="noopener noreferrer">Guide de programmation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html" target="_blank" rel="noopener noreferrer">PPO dans verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html" target="_blank" rel="noopener noreferrer">GRPO dans verl</a></li></p><p></ul><strong>Exécution pas à pas d'un exemple PPO :</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html" target="_blank" rel="noopener noreferrer">Préparer les données pour le post-entraînement</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html" target="_blank" rel="noopener noreferrer">Implémenter une fonction de récompense pour le dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html" target="_blank" rel="noopener noreferrer">Architecture d’un exemple PPO</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html" target="_blank" rel="noopener noreferrer">Explication des configs</a></li></p><p></ul><strong>Algorithmes de référence reproductibles :</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html" target="_blank" rel="noopener noreferrer">Performances RL sur le codage, les maths</a></li></p><p></ul><strong>Pour l'explication du code et l'utilisation avancée (extension) :</strong></p><ul><li>PPO Trainer et Workers</li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html" target="_blank" rel="noopener noreferrer">PPO Ray Trainer</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html" target="_blank" rel="noopener noreferrer">Backend PyTorch FSDP</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Backend Megatron-LM</a></li></p><p><li>Utilisation avancée et extension</li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html" target="_blank" rel="noopener noreferrer">Ajouter des modèles avec le backend FSDP</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html" target="_blank" rel="noopener noreferrer">Ajouter des modèles avec le backend Megatron-LM</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html" target="_blank" rel="noopener noreferrer">Support multi-turn rollout</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html" target="_blank" rel="noopener noreferrer">Intégration d’outils de recherche</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html" target="_blank" rel="noopener noreferrer">Intégration Sandbox Fusion</a></li>
  <li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/split_placement" target="_blank" rel="noopener noreferrer">Déploiement avec répartition GPU séparée</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html" target="_blank" rel="noopener noreferrer">Extension à d’autres algorithmes RL(HF)</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html" target="_blank" rel="noopener noreferrer">Tutoriel sur la conception d’API Ray</a></li></p><p></ul><strong>Blogs de la communauté</strong></p><ul><li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md" target="_blank" rel="noopener noreferrer">SGLang, verl, OpenBMB et Université Tsinghua : Pionniers du RLHF multi-turn de bout en bout</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html" target="_blank" rel="noopener noreferrer">Reinforcement Learning from Human Feedback sur GPU AMD avec intégration verl et ROCm</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA" target="_blank" rel="noopener noreferrer">veMLP x verl : Jouer avec l’entraînement RL</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942" target="_blank" rel="noopener noreferrer">Meilleures pratiques pour l'entraînement RL distribué GRPO avec verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md" target="_blank" rel="noopener noreferrer">Analyse du texte original HybridFlow verl</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90" target="_blank" rel="noopener noreferrer">Jusqu’à 20 fois plus de débit ! L’équipe Doubao publie un nouveau framework RLHF, désormais open source !</a></li></p><p></ul><h2>Guide d'optimisation des performances</h2></p><p>La performance est essentielle pour les algorithmes RL on-policy. Nous avons rédigé un <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html" target="_blank" rel="noopener noreferrer">guide détaillé d’optimisation des performances</a> pour vous aider à optimiser les performances.</p><h2>Mise à niveau vers vLLM >= v0.8.2</h2></p><p>verl supporte désormais vLLM>=0.8.2 lorsque FSDP est utilisé comme backend d'entraînement. Veuillez consulter <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/README_vllm0.8.md" target="_blank" rel="noopener noreferrer">ce document</a> pour le guide d'installation et plus d'informations. Évitez vllm 0.7.x, qui contient des bugs pouvant entraîner des OOM et des erreurs inattendues.</p><h2>Utilisez la dernière version de SGLang</h2></p><p>SGLang est entièrement pris en charge avec verl, et le groupe SGLang RL travaille activement sur le développement de fonctionnalités uniques, y compris RL agentique multi-turn, VLM RLHF, RL basé serveur et rollout partiel. Veuillez consulter <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html" target="_blank" rel="noopener noreferrer">ce document</a> pour le guide d'installation et plus d'informations.</p><h2>Mise à niveau vers FSDP2</h2></p><p>verl adopte pleinement FSDP2 ! FSDP2 est recommandé par l’équipe torch distributed, offrant un meilleur débit et une meilleure gestion de la mémoire, et il est composable avec d’autres fonctionnalités (par exemple torch.compile). Pour activer FSDP2, utilisez simplement verl main et définissez les options suivantes :
<pre><code class="language-">actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 </code></pre>
De plus, l’offloading CPU FSDP2 est compatible avec l’accumulation de gradients. Vous pouvez l’activer pour économiser de la mémoire avec <code>actor_rollout_ref.actor.offload_policy=True</code>. Pour plus de détails, voir https://github.com/volcengine/verl/pull/1026</p><h2>Support AMD (ROCm Kernel)</h2></p><p>verl prend maintenant en charge FSDP comme moteur d’entraînement (Megatron sera bientôt supporté) et s’intègre à la fois à vLLM et SGLang comme moteurs d’inférence. Veuillez consulter <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_build_dockerfile_page.rst" target="_blank" rel="noopener noreferrer">ce document</a> pour le guide d'installation et plus d'informations, et <a href="https://raw.githubusercontent.com/volcengine/verl/main/docs/amd_tutorial/amd_vllm_page.rst" target="_blank" rel="noopener noreferrer">ce document</a> pour l’optimisation des performances vLLM pour ROCm.</p><h2>Citation et remerciements</h2></p><p>Si ce projet vous a été utile, merci de citer :</p><ul><li><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf" target="_blank" rel="noopener noreferrer">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li></p><p></ul><pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}</code></pre></p><p>verl s’inspire du design de Nemo-Aligner, Deepspeed-chat et OpenRLHF. Le projet est adopté et contribué par Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/" target="_blank" rel="noopener noreferrer">équipe Alibaba Qwen</a>, Shanghai AI Lab, Université Tsinghua, UC Berkeley, UCLA, UIUC, Université de Hong Kong, ke.com, <a href="https://www.all-hands.dev/" target="_blank" rel="noopener noreferrer">All Hands AI</a>, <a href="http://modelbest.cn/" target="_blank" rel="noopener noreferrer">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/" target="_blank" rel="noopener noreferrer">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/" target="_blank" rel="noopener noreferrer">Camel-AI</a>, <a href="https://github.com/OpenManus" target="_blank" rel="noopener noreferrer">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home" target="_blank" rel="noopener noreferrer">Baichuan</a>, <a href="https://www.xiaohongshu.com/" target="_blank" rel="noopener noreferrer">RedNote</a>, <a href="https://www.swiss-ai.org/" target="_blank" rel="noopener noreferrer">SwissAI</a>, <a href="https://www.moonshot-ai.com/" target="_blank" rel="noopener noreferrer">Moonshot AI (Kimi)</a>, Baidu, Snowflake, et bien d’autres.</p><h2>Projets remarquables utilisant verl</h2></p><ul><li><a href="https://github.com/Jiayi-Pan/TinyZero" target="_blank" rel="noopener noreferrer">TinyZero</a> : une reproduction de la recette <strong>DeepSeek R1 Zero</strong> pour les tâches de raisonnement <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought" target="_blank" rel="noopener noreferrer">SkyThought</a> : entraînement RL pour Sky-T1-7B par l’équipe NovaSky AI. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason" target="_blank" rel="noopener noreferrer">simpleRL-reason</a> : SimpleRL-Zoo : étude et adaptation du Zero RL pour les modèles open base dans la nature <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hiyouga/EasyR1" target="_blank" rel="noopener noreferrer">Easy-R1</a> : framework d’entraînement RL <strong>multi-modal</strong> <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL" target="_blank" rel="noopener noreferrer">OpenManus-RL</a> : framework RL tuning d’agents LLM pour environnements multi-agents. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/agentica-project/rllm" target="_blank" rel="noopener noreferrer">rllm</a> : entraînement RL asynchrone avec <a href="https://github.com/agentica-project/verl-pipeline" target="_blank" rel="noopener noreferrer">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/PRIME" target="_blank" rel="noopener noreferrer">PRIME</a> : renforcement de processus via récompenses implicites <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ZihanWang314/ragen" target="_blank" rel="noopener noreferrer">RAGEN</a> : framework d’entraînement d’<strong>agent</strong> de raisonnement polyvalent <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1" target="_blank" rel="noopener noreferrer">Search-R1</a> : RL avec raisonnement et <strong>recherche (tool-call)</strong> intercalés dans les LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval" target="_blank" rel="noopener noreferrer">DeepRetrieval</a> : entraînement RL d’<strong>agents de recherche</strong> avec <strong>résultats de recherche/retrieval</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Agent-RL/ReSearch" target="_blank" rel="noopener noreferrer">ReSearch</a> : apprentissage du raisonnement avec recherche pour LLMs via RL <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ganler/code-r1" target="_blank" rel="noopener noreferrer">Code-R1</a> : reproduction de R1 pour le <strong>code</strong> avec récompenses fiables <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1" target="_blank" rel="noopener noreferrer">Skywork-OR1</a> : série open reasoner Skywork <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/ToRL" target="_blank" rel="noopener noreferrer">ToRL</a> : scaling RL intégré aux outils <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/langfengQ/verl-agent" target="_blank" rel="noopener noreferrer">verl-agent</a> : framework d’entraînement évolutif pour <strong>agents LLM/VLM longue-horizon</strong>, avec un nouvel algorithme <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a> : Policy Filtration pour PPO basée sur la fiabilité des signaux de récompense pour un RLHF plus efficace et robuste.</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1" target="_blank" rel="noopener noreferrer">GUI-R1</a> : <strong>GUI-R1</strong> : un modèle action Vision-Language R1 généraliste pour <strong>agents GUI</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher" target="_blank" rel="noopener noreferrer">DeepResearcher</a> : scaling deep research via RL dans des environnements réels <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN" target="_blank" rel="noopener noreferrer">VAGEN</a> : entraînement d’agents VLM avec RL multi-turn <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars"></li>
<li><a href="https://retool-rl.github.io/" target="_blank" rel="noopener noreferrer">ReTool</a> : ReTool : RL pour l’utilisation stratégique d’outils dans les LLMs. Publication du code en cours...</li>
<li><a href="https://arxiv.org/abs/2505.02387" target="_blank" rel="noopener noreferrer">RM-R1</a> : entraînement RL de modèles de récompense de raisonnement <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2505.03335" target="_blank" rel="noopener noreferrer">Absolute Zero Reasoner</a> : framework self-play sans données humaines pour le raisonnement <img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/pdf/2504.14945" target="_blank" rel="noopener noreferrer">LUFFY</a> : apprendre à raisonner sous guidance off-policy <img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool" target="_blank" rel="noopener noreferrer">verl-tool</a> : un framework unifié et facile à étendre pour l’entraînement d’agents-outils basé sur verl <img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/zwhe99/DeepMath" target="_blank" rel="noopener noreferrer">DeepMath</a> : données DeepMath-103K et modèles de la série pour le raisonnement mathématique <img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars"></li></p><p></ul>et bien d’autres projets remarquables listés dans <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md" target="_blank" rel="noopener noreferrer">recipe</a>.
<h2>Guide de contribution</h2></p><p>Les contributions de la communauté sont les bienvenues ! Consultez notre <a href="https://github.com/volcengine/verl/issues/710" target="_blank" rel="noopener noreferrer">roadmap du projet</a> et les <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22" target="_blank" rel="noopener noreferrer">good first issues</a> pour savoir où contribuer.</p><h3>Linting et formatage du code</h3></p><p>Nous utilisons pre-commit pour améliorer la qualité du code. Pour initialiser pre-commit, exécutez :</p><pre><code class="language-bash">pip install pre-commit
pre-commit install</code></pre></p><p>Pour résoudre les erreurs CI localement, vous pouvez lancer pre-commit manuellement :</p><pre><code class="language-bash">pre-commit run</code></pre></p><h3>Ajout de tests CI</h3></p><p>Si possible, merci d’ajouter un ou plusieurs tests CI pour votre nouvelle fonctionnalité :</p><ul><li>Trouvez le fichier workflow yml le plus pertinent, généralement associé à une config hydra par défaut (ex : <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code>, etc).</li>
<li>Ajoutez les patterns de chemin liés dans la section <code>paths</code> si ce n’est pas déjà fait.</li>
<li>Minimisez la charge de travail des scripts de test (voir les scripts existants pour des exemples).</li></p><p></ul><h2>À propos de l’<a href="https://team.doubao.com/" target="_blank" rel="noopener noreferrer">équipe ByteDance Seed</a></h2></p><p>Fondée en 2023, l’équipe ByteDance Seed s’engage à concevoir les modèles d’IA fondamentaux les plus avancés de l’industrie. L’équipe aspire à devenir une équipe de recherche de classe mondiale et à contribuer de manière significative à l’avancement de la science et de la société. Vous pouvez découvrir l’équipe ByteDance Seed via les canaux suivants👇
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a></p><p></div>
<hr></p><p>Nous RECRUTONS ! Envoyez-nous un <a href="mailto:haibin.lin@bytedance.com" target="_blank" rel="noopener noreferrer">email</a> si vous êtes intéressé(e) par un stage ou un poste FTE en RL pour agents.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/volcengine/verl/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>