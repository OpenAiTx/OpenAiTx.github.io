<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>verl - Read verl documentation in Dutch. This project has 0 stars on GitHub.</title>
    <meta name="description" content="Read verl documentation in Dutch. This project has 0 stars on GitHub.">
    <meta name="keywords" content="verl, Dutch, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "verl",
  "description": "Read verl documentation in Dutch. This project has 0 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "volcengine"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 0
  },
  "url": "https://OpenAiTx.github.io/projects/volcengine/verl/README-nl.html",
  "sameAs": "https://raw.githubusercontent.com/volcengine/verl/master/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/volcengine/verl" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    verl
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 0 stars</span>
                <span class="language">Dutch</span>
                <span>by volcengine</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center">
 👋 Hallo allemaal! 
    verl is een RL-trainingsbibliotheek geïnitieerd door het <b>ByteDance Seed team</b> en onderhouden door de verl-community.
    <br>
    <br>
</div></p><p><div align="center"></p><p><a href="https://deepwiki.com/volcengine/verl" target="_blank" rel="noopener noreferrer"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"/></a>
<a href="https://github.com/volcengine/verl/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars"></a>
<a href="https://twitter.com/verl_project" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter"></a>
<a href="https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/documentatie-blauw" alt="Documentation"></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/微信-green?logo=wechat&amp"></a></p><p></div></p><p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo"></p><p><h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning voor LLMs</h1></p><p>verl is een flexibele, efficiënte en productieklare RL-trainingsbibliotheek voor grote taalmodellen (LLMs).</p><p>verl is de open-sourceversie van het <strong><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> paper.</p><p>verl is flexibel en eenvoudig te gebruiken dankzij:</p><ul><li><strong>Eenvoudige uitbreiding van diverse RL-algoritmes</strong>: Het hybrid-controller programmeermodel maakt flexibele representatie en efficiënte uitvoering van complexe post-training dataflows mogelijk. Bouw RL-dataflows zoals GRPO, PPO in slechts enkele regels code.</li></p><p><li><strong>Naadloze integratie van bestaande LLM-infrastructuur met modulaire API's</strong>: Ontkoppelt rekenkundige en gegevensafhankelijkheden, waardoor naadloze integratie met bestaande LLM-frameworks zoals FSDP, Megatron-LM, vLLM, SGLang, etc. mogelijk is.</li></p><p><li><strong>Flexibele device mapping</strong>: Ondersteunt verschillende plaatsingen van modellen op diverse sets GPU's voor efficiënte resourcebenutting en schaalbaarheid over verschillende clusterformaten.</li></p><p><li>Directe integratie met populaire HuggingFace-modellen</li></p><p></ul>verl is snel dankzij:</p><ul><li><strong>State-of-the-art doorvoer</strong>: SOTA LLM training en inference engine-integraties en SOTA RL-doorvoer.</li></p><p><li><strong>Efficiënt herverdelen van actor-modellen met 3D-HybridEngine</strong>: Elimineert geheugenredundantie en vermindert de communicatielast aanzienlijk tijdens overgangen tussen training en generatiefase.</li></p><p></ul></p></p><h2>Nieuws</h2></p><ul><li>[2025/06] verl met Megatron-backend maakt grote MoE-modellen mogelijk zoals <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html" target="_blank" rel="noopener noreferrer">DeepSeek-671b en Qwen3-236b</a>.</li>
<li>[2025/06] verl-team zal de laatste projectupdates geven op <a href="https://www.lfasiallc.com/pytorch-day-china/" target="_blank" rel="noopener noreferrer">PyTorch Day China</a> op 7 juni. Ontmoet ons dev-team in Beijing!</li>
<li>[2025/05] <a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a>, geaccepteerd voor ICML 2025, wordt nu ondersteund in verl! PF-PPO verbetert de efficiëntie en robuustheid van policy learning door mogelijk ruisige reward-signalen te filteren en hoogwaardige ervaringen te hergebruiken via een replay buffer.</li>
<li>[2025/04] We geven een tutorial over de nieuwste post-training technieken en programmeergids voor verl op <a href="https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=" target="_blank" rel="noopener noreferrer">ICLR 2025 Expo</a>, <a href="https://open-foundation-model.github.io/" target="_blank" rel="noopener noreferrer">SCI-FM workshop</a> en <a href="https://lu.ma/d23nyynm" target="_blank" rel="noopener noreferrer">LMSys afterparty</a>. Presentatiematerialen beschikbaar <a href="https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25" target="_blank" rel="noopener noreferrer">hier</a>.</li>
<li>[2025/04] <a href="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf" target="_blank" rel="noopener noreferrer">Seed-Thinking-v1.5</a> technisch rapport is uitgebracht! Getraind met verl behaalt Seed-Thinking-v1.5 86.7 op AIME 2024, 55.0 op Codeforces en 77.3 op GPQA, wat uitstekende redeneervermogen in STEM en codering aantoont. Naast redeneertaken toont de methode opmerkelijke generalisatie over diverse domeinen.</li>
<li>[2025/04] <a href="https://arxiv.org/pdf/2504.05118" target="_blank" rel="noopener noreferrer">VAPO</a> (value-based augmented PPO) paper behandelt onze nieuwste RL-methode voor redeneermodellen. Getraind vanaf Qwen-32B-base model behaalt VAPO 60.4 op AIME 2024, beter dan DAPO-32B.</li>
<li>[2025/03] verl v0.3.0.post1 is uitgebracht! Zie <a href="https://github.com/volcengine/verl/releases/" target="_blank" rel="noopener noreferrer">releasenote</a> voor details. Het behaalt <a href="https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms" target="_blank" rel="noopener noreferrer">~1.4x versnelling</a> vergeleken met eerdere versies.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/" target="_blank" rel="noopener noreferrer">DAPO</a> is het open-source SOTA RL-algoritme dat 50 punten behaalt op AIME 2024 op basis van het Qwen2.5-32B pre-trained model, waarmee het vorige SOTA van DeepSeek's GRPO (DeepSeek-R1-Zero-Qwen-32B) wordt overtroffen. DAPO's training is volledig aangedreven door verl en de reproductiecode is nu beschikbaar in <code>recipe/dapo</code>.</li>
</ul><details><summary> meer... </summary>
<ul>
  <ul><li>[2025/05] verl zal worden gepresenteerd op <a href="https://a2m.msup.com.cn/home/?aid=4488&city=shanghai" target="_blank" rel="noopener noreferrer">A2M Shanghai</a> op 16/5 - 17/5.</li>
  <li>[2025/05] verl zal worden gepresenteerd op <a href="https://paris2025.gosim.org/" target="_blank" rel="noopener noreferrer">GOSIM x PyTorch Day 2025</a>. Tot ziens in Parijs! </li>
  <li>[2025/03] We introduceerden het programmeermodel van verl op de <a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg" target="_blank" rel="noopener noreferrer">vLLM Beijing Meetup</a> en <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf" target="_blank" rel="noopener noreferrer">verl intro en updates</a> op de <a href="https://lu.ma/ntjrr7ig" target="_blank" rel="noopener noreferrer">SGLang-LMSYS Org Meetup</a> in Sunnyvale medio maart.</li>
  <li>[2025/03] We presenteren verl(HybridFlow) op EuroSys 2025. Tot ziens in Rotterdam!</li>
  <li>[2025/02] verl v0.2.0.post2 is uitgebracht!</li>
  <li>[2025/02] We presenteerden verl in de <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. Tot ziens in San Jose!</li>
  <li>[2025/01] <a href="https://team.doubao.com/zh/special/doubao_1_5_pro" target="_blank" rel="noopener noreferrer">Doubao-1.5-pro</a> is uitgebracht met SOTA-niveau prestaties op LLM & VLM. Het RL scaling preview model is getraind met verl en bereikt OpenAI O1-niveau prestaties op wiskundige benchmarks (70.0 pass@1 op AIME).</li>
  <li>[2024/12] verl is gepresenteerd op Ray Forward 2024. Slides beschikbaar <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">hier</a></li>
  <li>[2024/12] Het team presenteerde <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> op NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> en <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">video</a> beschikbaar.</li>
  <li>[2024/10] verl is gepresenteerd op Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube-video</a> beschikbaar.</li>
  <li>[2024/08] HybridFlow (verl) is geaccepteerd voor EuroSys 2025.</li>
</ul></ul>   
</details></p><h2>Belangrijkste Kenmerken</h2></p><ul><li><strong>FSDP</strong>, <strong>FSDP2</strong> en <strong>Megatron-LM</strong> voor training.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> en <strong>HF Transformers</strong> voor rollout-generatie.</li>
<li>Compatibel met Hugging Face Transformers en Modelscope Hub: <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen3-8b.sh" target="_blank" rel="noopener noreferrer">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, enz.</li>
<li>Supervised fine-tuning.</li>
<li>Reinforcement learning met <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/" target="_blank" rel="noopener noreferrer">PPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/" target="_blank" rel="noopener noreferrer">GRPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/remax_trainer/" target="_blank" rel="noopener noreferrer">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm" target="_blank" rel="noopener noreferrer">REINFORCE++</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/rloo_trainer/" target="_blank" rel="noopener noreferrer">RLOO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/prime/" target="_blank" rel="noopener noreferrer">PRIME</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo/" target="_blank" rel="noopener noreferrer">DAPO</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/drgrpo" target="_blank" rel="noopener noreferrer">DrGRPO</a>, enz.</li>
  <li>Ondersteunt model-based reward en function-based reward (verifieerbare beloning) voor wiskunde, <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/dapo" target="_blank" rel="noopener noreferrer">coderen</a>, enz.</li>
  <li>Ondersteunt vision-language modellen (VLMs) en <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/grpo_trainer/run_qwen2_5_vl-7b.sh" target="_blank" rel="noopener noreferrer">multi-modale RL</a> met Qwen2.5-vl, Kimi-VL</li>
  <li><a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sglang_multiturn" target="_blank" rel="noopener noreferrer">Multi-turn met tool calling</a></li>
<li>LLM alignment recepten zoals <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/sppo" target="_blank" rel="noopener noreferrer">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_qwen2-7b_seq_balance.sh" target="_blank" rel="noopener noreferrer">sequence packing</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/ppo_trainer/run_deepseek7b_llm_sp2.sh" target="_blank" rel="noopener noreferrer">sequence parallelism</a> ondersteuning via DeepSpeed Ulysses, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_peft.sh" target="_blank" rel="noopener noreferrer">LoRA</a>, <a href="https://raw.githubusercontent.com/volcengine/verl/main/examples/sft/gsm8k/run_qwen_05_sp2_liger.sh" target="_blank" rel="noopener noreferrer">Liger-kernel</a>.</li>
<li>Schaalbaar tot 671B modellen en honderden GPU's met <a href="https://github.com/volcengine/verl/pull/1467" target="_blank" rel="noopener noreferrer">expert parallelism</a></li>
<li>Multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html" target="_blank" rel="noopener noreferrer">LoRA RL</a> ondersteuning om geheugen te besparen.</li>
<li>Experiment tracking met wandb, swanlab, mlflow en tensorboard.</li></p><p></ul><h2>Verwachte Functies en Wijzigingen</h2></p><ul><li>Roadmap https://github.com/volcengine/verl/issues/710</li>
<li>DeepSeek 671b optimalisaties met Megatron v0.11 https://github.com/volcengine/verl/issues/708</li>
<li>Multi-turn rollout en tools optimalisaties https://github.com/volcengine/verl/issues/1882</li>
<li>Omgevingsinteracties https://github.com/volcengine/verl/issues/1172</li>
<li>Lijst van breaking changes sinds v0.3 https://github.com/volcengine/verl/discussions/943, entropy_coeff standaard op 0</li>
<li>Lora voor RL https://github.com/volcengine/verl/pull/1127 </li></p><p></ul><h2>Aan de Slag</h2></p><p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentatie</b></a></p><p><strong>Snelstart:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/start/install.html" target="_blank" rel="noopener noreferrer">Installatie</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html" target="_blank" rel="noopener noreferrer">Snelstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html" target="_blank" rel="noopener noreferrer">Programmeerhandleiding</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html" target="_blank" rel="noopener noreferrer">PPO in verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html" target="_blank" rel="noopener noreferrer">GRPO in verl</a></li></p><p></ul><strong>Een PPO-voorbeeld stap-voor-stap uitvoeren:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html" target="_blank" rel="noopener noreferrer">Data voorbereiden voor post-training</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html" target="_blank" rel="noopener noreferrer">Beloningsfunctie implementeren voor dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html" target="_blank" rel="noopener noreferrer">PPO Voorbeeldarchitectuur</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html" target="_blank" rel="noopener noreferrer">Config Uitleg</a></li></p><p></ul><strong>Reproduceerbare algoritme-benchmarks:</strong></p><ul><li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html" target="_blank" rel="noopener noreferrer">RL-prestaties op coderen, wiskunde</a></li></p><p></ul><strong>Voor codeuitleg en gevorderd gebruik (uitbreiding):</strong></p><ul><li>PPO Trainer en Workers</li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html" target="_blank" rel="noopener noreferrer">PPO Ray Trainer</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html" target="_blank" rel="noopener noreferrer">PyTorch FSDP Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">Megatron-LM Backend</a></li></p><p><li>Geavanceerd gebruik en uitbreiding</li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html" target="_blank" rel="noopener noreferrer">Voeg Modellen toe met de FSDP Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html" target="_blank" rel="noopener noreferrer">Voeg Modellen toe met de Megatron-LM Backend</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html" target="_blank" rel="noopener noreferrer">Multi-turn Rollout Ondersteuning</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html" target="_blank" rel="noopener noreferrer">Zoektool Integratie</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html" target="_blank" rel="noopener noreferrer">Sandbox Fusion Integratie</a></li>
  <li><a href="https://github.com/volcengine/verl/tree/main/examples/split_placement" target="_blank" rel="noopener noreferrer">Deployen met gescheiden GPU-resources</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html" target="_blank" rel="noopener noreferrer">Uitbreiden naar andere RL(HF) algoritmes</a></li>
  <li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html" target="_blank" rel="noopener noreferrer">Ray API ontwerp tutorial</a></li></p><p></ul><strong>Blogs uit de community</strong></p><ul><li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md" target="_blank" rel="noopener noreferrer">SGLang, verl, OpenBMB en Tsinghua University: Pionieren met End-to-End Multi-Turn RLHF</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html" target="_blank" rel="noopener noreferrer">Reinforcement Learning van Menselijke Feedback op AMD GPU's met verl en ROCm-integratie</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA" target="_blank" rel="noopener noreferrer">veMLP x verl ：Spelen met reinforcement learning training</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942" target="_blank" rel="noopener noreferrer">Beste praktijken voor GRPO-distributieve reinforcement learning training met verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md" target="_blank" rel="noopener noreferrer">HybridFlow verl originele tekstanalyse</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90" target="_blank" rel="noopener noreferrer">Tot 20x hogere doorvoer! Doubao groot modelteam brengt volledig nieuw RLHF-framework uit, nu open-source!</a></li></p><p></ul><h2>Prestatie-tuning Gids</h2></p><p>De prestatie is essentieel voor on-policy RL-algoritmen. We hebben een gedetailleerde <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html" target="_blank" rel="noopener noreferrer">prestatie-tuning gids</a> geschreven om je te helpen de prestaties te optimaliseren.</p><h2>Upgrade naar vLLM >= v0.8.2</h2></p><p>verl ondersteunt nu vLLM>=0.8.2 bij gebruik van FSDP als trainingsbackend. Raadpleeg <a href="https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md" target="_blank" rel="noopener noreferrer">dit document</a> voor de installatiehandleiding en meer informatie. Vermijd vllm 0.7.x, dit bevat bugs die kunnen leiden tot OOMs en onverwachte fouten.</p><h2>Gebruik de nieuwste SGLang</h2></p><p>SGLang wordt volledig ondersteund door verl, en de SGLang RL Group werkt intensief aan unieke features, waaronder multi-turn agentische RL, VLM RLHF, servergebaseerde RL en gedeeltelijke rollout. Raadpleeg <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html" target="_blank" rel="noopener noreferrer">dit document</a> voor installatiehandleiding en meer informatie.</p><h2>Upgrade naar FSDP2</h2></p><p>verl omarmt FSDP2 volledig! FSDP2 wordt aanbevolen door het torch distributed team, biedt betere doorvoer en geheugengebruik, en is samenstelbaar met andere features (bijv. torch.compile). Om FSDP2 te activeren, gebruik verl main en stel de volgende opties in:
<pre><code class="language-">actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 </code></pre>
Bovendien is FSDP2 CPU offloading compatibel met graduele accumulatie. Je kunt dit inschakelen om geheugen te besparen met <code>actor_rollout_ref.actor.offload_policy=True</code>. Voor meer details, zie https://github.com/volcengine/verl/pull/1026</p><h2>AMD-ondersteuning (ROCm Kernel)</h2></p><p>verl ondersteunt nu FSDP als training engine (Megatron-ondersteuning komt binnenkort) en integreert zowel met vLLM als SGLang als inference engines. Raadpleeg <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst" target="_blank" rel="noopener noreferrer">dit document</a> voor de installatiehandleiding en meer informatie, en <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst" target="_blank" rel="noopener noreferrer">dit document</a> voor vLLM prestatie-tuning voor ROCm.</p><h2>Citation en erkenning</h2></p><p>Als je het project nuttig vindt, citeer dan:</p><ul><li><a href="https://arxiv.org/abs/2409.19256v2" target="_blank" rel="noopener noreferrer">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf" target="_blank" rel="noopener noreferrer">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li></p><p></ul><pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}</code></pre></p><p>verl is geïnspireerd op het ontwerp van Nemo-Aligner, Deepspeed-chat en OpenRLHF. Het project is geadopteerd en bijgedragen door Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/" target="_blank" rel="noopener noreferrer">Alibaba Qwen team</a>, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, <a href="https://www.all-hands.dev/" target="_blank" rel="noopener noreferrer">All Hands AI</a>, <a href="http://modelbest.cn/" target="_blank" rel="noopener noreferrer">ModelBest</a>, OpenPipe, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/" target="_blank" rel="noopener noreferrer">StepFun</a>, Amazon, Linkedin, Meituan, <a href="https://www.camel-ai.org/" target="_blank" rel="noopener noreferrer">Camel-AI</a>, <a href="https://github.com/OpenManus" target="_blank" rel="noopener noreferrer">OpenManus</a>, Xiaomi, Prime Intellect, NVIDIA research, <a href="https://www.baichuan-ai.com/home" target="_blank" rel="noopener noreferrer">Baichuan</a>, <a href="https://www.xiaohongshu.com/" target="_blank" rel="noopener noreferrer">RedNote</a>, <a href="https://www.swiss-ai.org/" target="_blank" rel="noopener noreferrer">SwissAI</a>, <a href="https://www.moonshot-ai.com/" target="_blank" rel="noopener noreferrer">Moonshot AI (Kimi)</a>, Baidu, Snowflake, en vele anderen.</p><h2>Geweldig werk met verl</h2></p><ul><li><a href="https://github.com/Jiayi-Pan/TinyZero" target="_blank" rel="noopener noreferrer">TinyZero</a>: een reproductie van het <strong>DeepSeek R1 Zero</strong> recept voor redeneertaken <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought" target="_blank" rel="noopener noreferrer">SkyThought</a>: RL-training voor Sky-T1-7B door het NovaSky AI team. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason" target="_blank" rel="noopener noreferrer">simpleRL-reason</a>: SimpleRL-Zoo: Onderzoek en temmen van Zero Reinforcement Learning voor Open Base Modellen in het wild <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hiyouga/EasyR1" target="_blank" rel="noopener noreferrer">Easy-R1</a>: <strong>Multi-modal</strong> RL-training framework <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL" target="_blank" rel="noopener noreferrer">OpenManus-RL</a>: LLM Agents RL-tuning framework voor meerdere agentomgevingen. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/agentica-project/rllm" target="_blank" rel="noopener noreferrer">rllm</a>: async RL-training met <a href="https://github.com/agentica-project/verl-pipeline" target="_blank" rel="noopener noreferrer">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/PRIME" target="_blank" rel="noopener noreferrer">PRIME</a>: Procesversterking door impliciete beloningen <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ZihanWang314/ragen" target="_blank" rel="noopener noreferrer">RAGEN</a>: een algemeen trainingsframework voor redeneer<strong>agenten</strong> <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1" target="_blank" rel="noopener noreferrer">Search-R1</a>: RL met redeneer- en <strong>zoek- (tool-call)</strong> interleaved LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval" target="_blank" rel="noopener noreferrer">DeepRetrieval</a>: RL-training van <strong>Search Agent</strong> met <strong>Search/Retrieval Outcome</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Agent-RL/ReSearch" target="_blank" rel="noopener noreferrer">ReSearch</a>: Leren <strong>re</strong>deneer met <strong>search</strong> voor LLMs via reinforcement learning <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ganler/code-r1" target="_blank" rel="noopener noreferrer">Code-R1</a>: Reproductie van R1 voor <strong>Code</strong> met betrouwbare beloningen <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1" target="_blank" rel="noopener noreferrer">Skywork-OR1</a>: Skywork open reasoner series <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/ToRL" target="_blank" rel="noopener noreferrer">ToRL</a>: Schalen van tool-geïntegreerde RL <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/langfengQ/verl-agent" target="_blank" rel="noopener noreferrer">verl-agent</a>: Een schaalbaar trainingsframework voor <strong>long-horizon LLM/VLM agents</strong>, samen met een nieuw algoritme <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2409.06957" target="_blank" rel="noopener noreferrer">PF-PPO</a>: Policy Filtration voor PPO op basis van de betrouwbaarheid van reward-signalen voor efficiëntere en robuustere RLHF.</li>
<li><a href="https://github.com/ritzz-ai/GUI-R1" target="_blank" rel="noopener noreferrer">GUI-R1</a>: <strong>GUI-R1</strong>: Een generalistisch R1-stijl Vision-Language Action Model voor <strong>GUI Agents</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher" target="_blank" rel="noopener noreferrer">DeepResearcher</a>: Schaal diepgaand onderzoek met reinforcement learning in echte omgevingen <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN" target="_blank" rel="noopener noreferrer">VAGEN</a>: Training van VLM-agenten met multi-turn reinforcement learning <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars"></li>
<li><a href="https://retool-rl.github.io/" target="_blank" rel="noopener noreferrer">ReTool</a>: ReTool: reinforcement learning voor strategisch toolgebruik in LLMs. Code release volgt...</li>
<li><a href="https://arxiv.org/abs/2505.02387" target="_blank" rel="noopener noreferrer">RM-R1</a>: RL training van redeneer rewardmodellen <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2505.03335" target="_blank" rel="noopener noreferrer">Absolute Zero Reasoner</a>: Een self-play framework zonder handmatig samengestelde data voor redeneren<img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/pdf/2504.14945" target="_blank" rel="noopener noreferrer">LUFFY</a>: Leren redeneren onder off-policy begeleiding<img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool" target="_blank" rel="noopener noreferrer">verl-tool</a>: Een verenigd en eenvoudig uit te breiden tool-agent trainingsframework gebaseerd op verl<img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/zwhe99/DeepMath" target="_blank" rel="noopener noreferrer">DeepMath</a>: DeepMath-103K data en serie modellen voor wiskundig redeneren<img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars"></li></p><p></ul>en nog veel meer geweldig werk, zie <a href="https://raw.githubusercontent.com/volcengine/verl/main/recipe/README.md" target="_blank" rel="noopener noreferrer">recipe</a>.
<h2>Bijdragen Gids</h2></p><p>Bijdragen vanuit de community zijn welkom! Bekijk onze <a href="https://github.com/volcengine/verl/issues/710" target="_blank" rel="noopener noreferrer">project roadmap</a> en <a href="https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22" target="_blank" rel="noopener noreferrer">good first issues</a> om te zien waar je kunt bijdragen.</p><h3>Code Linting en Formatteren</h3></p><p>We gebruiken pre-commit om de codekwaliteit te verbeteren. Om pre-commit te initialiseren, voer uit:</p><pre><code class="language-bash">pip install pre-commit
pre-commit install</code></pre></p><p>Om CI-fouten lokaal op te lossen, kun je pre-commit handmatig uitvoeren met:</p><pre><code class="language-bash">pre-commit run</code></pre></p><h3>CI-tests toevoegen</h3></p><p>Indien mogelijk, voeg graag CI-test(s) toe voor je nieuwe feature:</p><ul><li>Zoek het meest relevante workflow yml-bestand, wat meestal overeenkomt met een <code>hydra</code> default config (bijv. <code>ppo_trainer</code>, <code>ppo_megatron_trainer</code>, <code>sft_trainer</code>, enz).</li>
<li>Voeg gerelateerde padpatronen toe aan de <code>paths</code> sectie indien nog niet aanwezig.</li>
<li>Minimaliseer de workload van het testscript (zie bestaande scripts voor voorbeelden).</li></p><p></ul><h2>Over <a href="https://team.doubao.com/" target="_blank" rel="noopener noreferrer">ByteDance Seed Team</a></h2></p><p>Opgericht in 2023, is het ByteDance Seed Team toegewijd aan het creëren van de meest geavanceerde AI-basis modellen in de industrie. Het team streeft ernaar een wereldklasse onderzoeksteam te worden en een significante bijdrage te leveren aan de vooruitgang van wetenschap en samenleving. Je kunt Bytedance Seed beter leren kennen via de volgende kanalen👇
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a></p><p></div>
<hr></p><p>We ZOEKEN NIEUWE COLLEGA'S! Stuur ons een <a href="mailto:haibin.lin@bytedance.com" target="_blank" rel="noopener noreferrer">e-mail</a> als je geïnteresseerd bent in een stage/FTE-kans in RL voor agents.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-07

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/volcengine/verl/master/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>