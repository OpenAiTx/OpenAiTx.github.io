<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TAPIP3D - TAPIP3D : Suivi de n’importe quel point dans une g&#233;om&#233;trie 3D persistante</title>
    <meta name="description" content="TAPIP3D : Suivi de n’importe quel point dans une g&#233;om&#233;trie 3D persistante">
    <meta name="keywords" content="TAPIP3D, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "TAPIP3D",
  "description": "TAPIP3D : Suivi de n’importe quel point dans une géométrie 3D persistante",
  "author": {
    "@type": "Person",
    "name": "zbw001"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 369
  },
  "url": "https://OpenAiTx.github.io/projects/zbw001/TAPIP3D/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/zbw001/TAPIP3D/main/README.md",
  "datePublished": "2026-02-12",
  "dateModified": "2026-02-12"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/zbw001/TAPIP3D" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    TAPIP3D
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 369 stars</span>
                <span class="language">French</span>
                <span>by zbw001</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center"></p><h1>TAPIP3D : Suivi de Tout Point dans une Géométrie 3D Persistante</h1>
<a href="https://arxiv.org/abs/2504.14717"><img src='https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&logoColor=white' alt='arXiv'></a>
<a href='https://tapip3d.github.io'><img src='https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&logoColor=white' alt='Page du Projet'></a></p><p><a href="https://scholar.google.com/citations?user=tYH72AYAAAAJ" target="_blank" rel="noopener noreferrer">Bowei Zhang</a><sup>1,2</sup><em>, <a href="https://www.kelei.site/" target="_blank" rel="noopener noreferrer">Lei Ke</a><sup>1</sup>\</em>, <a href="https://adamharley.com/" target="_blank" rel="noopener noreferrer">Adam W. Harley</a><sup>3</sup>, <a href="https://www.cs.cmu.edu/~katef/" target="_blank" rel="noopener noreferrer">Katerina Fragkiadaki</a><sup>1</sup></p><p><sup>1</sup>Carnegie Mellon University   &nbsp;  <sup>2</sup>Peking University &nbsp;  <sup>3</sup>Stanford University</p><p><strong>NeurIPS 2025</strong></p><p>\* Contribution Égale</p><p><!-- <a href='https://huggingface.co/spaces/your-username/project'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Live_Demo-blue'></a> --></p><p></div></p><p><img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/teaser1.gif" width="100%" alt="Présentation de TAPIP3D"></p><hr></p><h3>🚀 Actualités</h3>
<ul><li><strong>(2025.12.28)</strong> 🔥 Nous avons mis à jour le code de <strong>Formation</strong> et d'<strong>Évaluation</strong> ! Consultez les nouvelles sections ci-dessous.</li></p><p></ul><h2>Vue d'ensemble</h2>
<strong>TAPIP3D</strong> est une méthode de suivi 3D <strong>feed-forward</strong> à long terme des points dans des séquences vidéo monoculaires RGB et RGB-D. Elle introduit une représentation en nuage de caractéristiques 3D qui élève les caractéristiques d’image dans un espace de coordonnées mondiales persistant, annulant le mouvement de la caméra et permettant une estimation précise des trajectoires entre les images.</p><p>Nous fournissons une <a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/117634#:~:text=Within%20this%20stabilized%203D%20representation,trained%20checkpoints%20will%20be%20public." target="_blank" rel="noopener noreferrer">illustration vidéo détaillée</a> de notre TAPIP3D.</p><h2>Installation</h2>
<h3>Installation des dépendances</h3></p><ul><li>Préparez l’environnement</li>
</ul><pre><code class="language-bash">conda create -n tapip3d python=3.10
conda activate tapip3d</p><p>pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 "xformers>=0.0.27" --index-url https://download.pytorch.org/whl/cu124
pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.1+cu124.html
pip install -r requirements.txt</code></pre></p><ul><li>Compiler pointops2</li></p><p></ul><pre><code class="language-bash">cd third_party/pointops2
LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH python setup.py install
cd ../..</code></pre></p><ul><li>Compiler megasam</li>
</ul><pre><code class="language-bash">cd third_party/megasam/base
LIBRARY_PATH=$CONDA_PREFIX/lib:$LIBRARY_PATH python setup.py install
cd ../../..</code></pre></p><h3>Téléchargement des points de contrôle</h3></p><p>Téléchargez notre point de contrôle du modèle TAPIP3D <a href="https://huggingface.co/zbww/tapip3d/resolve/main/tapip3d_final.pth" target="_blank" rel="noopener noreferrer">ici</a> vers <code>checkpoints/tapip3d_final.pth</code></p><p>Si vous souhaitez exécuter TAPIP3D sur des vidéos monoculaires, vous devez préparer manuellement les points de contrôle suivants pour exécuter MegaSAM :</p><ul><li>Téléchargez le point de contrôle DepthAnything V1 depuis <a href="https://huggingface.co/spaces/LiheYoung/Depth-Anything/resolve/main/checkpoints/depth_anything_vitl14.pth" target="_blank" rel="noopener noreferrer">ici</a> et placez-le dans <code>third_party/megasam/Depth-Anything/checkpoints/depth_anything_vitl14.pth</code></li></p><p><li>Téléchargez le point de contrôle RAFT depuis <a href="https://drive.google.com/drive/folders/1sWDsfuZ3Up38EUQt7-JDTT1HcGHuJgvT" target="_blank" rel="noopener noreferrer">ici</a> et placez-le dans <code>third_party/megasam/cvd_opt/raft-things.pth</code></li></p><p></ul>De plus, les points de contrôle de <a href="https://wangrc.site/MoGePage/" target="_blank" rel="noopener noreferrer">MoGe</a> et <a href="https://github.com/lpiccinelli-eth/UniDepth.git" target="_blank" rel="noopener noreferrer">UniDepth</a> seront téléchargés automatiquement lors de l'exécution de la démo. Veuillez vous assurer que votre connexion réseau est disponible.</p><h2>Utilisation de la démo</h2></p><p>Nous fournissons un script de démo simple <code>inference.py</code>, ainsi que des données d'entrée d'exemple situées dans le répertoire <code>demo_inputs/</code>.</p><p>Le script accepte en entrée soit un fichier vidéo <code>.mp4</code>, soit un fichier <code>.npz</code>. Si vous fournissez un fichier <code>.npz</code>, il doit suivre le format suivant :</p><ul><li><code>video</code> : tableau de forme (T, H, W, 3), dtype : uint8</li>
<li><code>depths</code> (optionnel) : tableau de forme (T, H, W), dtype : float32</li>
<li><code>intrinsics</code> (optionnel) : tableau de forme (T, 3, 3), dtype : float32</li>
<li><code>extrinsics</code> (optionnel) : tableau de forme (T, 4, 4), dtype : float32</li></p><p></ul>À des fins de démonstration, le script utilise une grille de points 32x32 sur la première image comme requêtes.</p><h3>Inférence avec vidéo monoculaire</h3></p><p>En fournissant une vidéo via <code>--input_path</code>, le script exécute d'abord <a href="https://github.com/mega-sam/mega-sam" target="_blank" rel="noopener noreferrer">MegaSAM</a> avec <a href="https://wangrc.site/MoGePage/" target="_blank" rel="noopener noreferrer">MoGe</a> pour estimer les cartes de profondeur et les paramètres de la caméra. Ensuite, le modèle traitera ces entrées dans le cadre global.</p><p><strong>Démo 1</strong></p><p><img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo1.gif" width="100%" alt="Démo 1"></p><p>Pour lancer l'inférence :</p><pre><code class="language-bash">python inference.py --input_path demo_inputs/sheep.mp4 --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2</code></pre></p><p>Un fichier npz sera enregistré dans <code>outputs/inference/</code>. Pour visualiser les résultats :</p><pre><code class="language-bash">python visualize.py <result_npz_path></code></pre></p><p><strong>Demo 2</strong></p><p><img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo2.gif" width="100%" alt="Demo 2"></p><pre><code class="language-bash">python inference.py --input_path demo_inputs/pstudio.mp4 --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2</code></pre></p><p><strong>Inférence avec Profondeurs et Paramètres de Caméra Connus</strong></p><p>Si un fichier <code>.npz</code> contenant les quatre clés (<code>rgb</code>, <code>depths</code>, <code>intrinsics</code>, <code>extrinsics</code>) est fourni, le modèle fonctionnera dans un cadre global aligné, générant des trajectoires de points en coordonnées mondiales.
Nous fournissons un exemple de fichier <code>.npz</code> <a href="https://huggingface.co/zbww/tapip3d/resolve/main/demo_inputs/dexycb.npz?download=true" target="_blank" rel="noopener noreferrer">ici</a> et veuillez le placer dans le répertoire <code>demo_inputs/</code>.</p><p><strong>Démo 3</strong></p><p><img src="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/./media/demo3.gif" width="100%" alt="Démo 3"></p><pre><code class="language-bash">python inference.py --input_path demo_inputs/dexycb.npz --checkpoint checkpoints/tapip3d_final.pth --resolution_factor 2</code></pre></p><h2>Entraînement et Évaluation</h2></p><h3>1. Préparation du Jeu de Données</h3>
Veuillez consulter <a href="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/DATASET.md" target="_blank" rel="noopener noreferrer">DATASET.md</a> pour les instructions sur la préparation des jeux de données pour l’entraînement et l’évaluation.</p><h3>2. Entraînement</h3>
Pour commencer l’entraînement, exécutez :
<pre><code class="language-bash">bash scripts/train.sh</code></pre>
<ul><li><code>experiment_name</code> : Le nom de l'exécution affiché sur <strong>WandB</strong>.  </li>
<li><code>experiment_id</code> : Un identifiant unique. Relancer avec le même <code>experiment_id</code> <strong>reprendra automatiquement</strong> l'entraînement à partir du dernier point de contrôle.</li></p><p></ul><h3>3. Évaluation  </h3>
Pour évaluer un point de contrôle, exécutez :
<pre><code class="language-bash">bash scripts/eval.sh</code></pre>
Vous pouvez spécifier le modèle à évaluer en modifiant la variable <code>checkpoint</code> dans <code>scripts/eval.sh</code>.</p><h2>Citation</h2>
Si vous trouvez ce projet utile, veuillez envisager de le citer :</p><pre><code class="language-">@article{tapip3d,
  title={TAPIP3D: Tracking Any Point in Persistent 3D Geometry},
  author={Zhang, Bowei and Ke, Lei and Harley, Adam W and Fragkiadaki, Katerina},
  journal={arXiv preprint arXiv:2504.14717},
  year={2025}
}</code></pre></p><p>
---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2026-02-12

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/zbw001/TAPIP3D/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2026-02-12 
    </div>
    
</body>
</html>