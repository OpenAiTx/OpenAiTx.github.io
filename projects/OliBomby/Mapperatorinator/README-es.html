<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mapperatorinator - Un marco de IA para generar y modificar mapas de osu! para todos los modos de juego a partir de entradas de espectrogramas.</title>
    <meta name="description" content="Un marco de IA para generar y modificar mapas de osu! para todos los modos de juego a partir de entradas de espectrogramas.">
    <meta name="keywords" content="Mapperatorinator, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "Mapperatorinator",
  "description": "Un marco de IA para generar y modificar mapas de osu! para todos los modos de juego a partir de entradas de espectrogramas.",
  "author": {
    "@type": "Person",
    "name": "OliBomby"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 381
  },
  "url": "https://OpenAiTx.github.io/projects/OliBomby/Mapperatorinator/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/README.md",
  "datePublished": "2025-12-28",
  "dateModified": "2025-12-28"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/OliBomby/Mapperatorinator" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    Mapperatorinator
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 381 stars</span>
                <span class="language">Spanish</span>
                <span>by OliBomby</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Mapperatorinator</h1></p><p>Prueba el modelo generativo <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb" target="_blank" rel="noopener noreferrer">aquí</a>, o MaiMod <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb" target="_blank" rel="noopener noreferrer">aquí</a>. Mira una demostración en video <a href="https://youtu.be/FEr7t1L2EoA" target="_blank" rel="noopener noreferrer">aquí</a>.</p><p>Mapperatorinator es un marco multimodelo que utiliza entradas de espectrogramas para generar mapas de osu! completamente detallados para todos los modos de juego y <a href="#maimod-the-ai-driven-modding-tool" target="_blank" rel="noopener noreferrer">modding asistido de mapas</a>.
El objetivo de este proyecto es generar automáticamente mapas de osu! de calidad clasificable a partir de cualquier canción con un alto grado de personalización.</p><p>Este proyecto está basado en <a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> y <a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a>. En el desarrollo de esto, dediqué alrededor de 2500 horas de cómputo en GPU a lo largo de 142 ejecuciones en mi 4060 Ti y en instancias alquiladas 4090 en vast.ai.</p><p>#### Usa esta herramienta responsablemente. Siempre revela el uso de IA en tus mapas.</p><h2>Instalación</h2></p><p>La instrucción a continuación te permite generar mapas en tu máquina local, alternativamente puedes ejecutarlo en la nube con el <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb" target="_blank" rel="noopener noreferrer">notebook de colab</a>.</p><h3>1. Clona el repositorio</h3></p><pre><code class="language-sh">git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator</code></pre></p><h3>2. (Opcional) Crear entorno virtual</h3></p><p>Utilice Python 3.10, versiones posteriores podrían no ser compatibles con las dependencias.</p><pre><code class="language-sh">python -m venv .venv</p><h1>In cmd.exe</h1>
.venv\Scripts\activate.bat
<h1>In PowerShell</h1>
.venv\Scripts\Activate.ps1
<h1>In Linux or MacOS</h1>
source .venv/bin/activate</code></pre>
<h3>3. Instalar dependencias</h3></p><ul><li>Python 3.10</li>
<li><a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Git</a></li>
<li><a href="http://www.ffmpeg.org/" target="_blank" rel="noopener noreferrer">ffmpeg</a></li>
<li><a href="https://developer.nvidia.com/cuda-zone" target="_blank" rel="noopener noreferrer">CUDA</a> (Para GPUs NVIDIA) o <a href="https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html" target="_blank" rel="noopener noreferrer">ROCm</a> (Para GPUs AMD en Linux)</li>
<li><a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">PyTorch</a>: Asegúrate de seguir la guía para comenzar para instalar <code>torch</code> y <code>torchaudio</code> con soporte para GPU. Selecciona la versión correcta de la Plataforma de Cómputo que instalaste en el paso anterior.</li></p><p><li>y las dependencias restantes de Python:</li></p><p>
</ul><pre><code class="language-sh">pip install -r requirements.txt</code></pre></p><h2>Interfaz Web (Recomendada)</h2></p><p>Para una experiencia más amigable, considere usar la Interfaz Web. Proporciona una interfaz gráfica para configurar los parámetros de generación, iniciar el proceso y monitorear la salida.</p><h3>Iniciar la Interfaz</h3></p><p>Navegue al directorio clonado <code>Mapperatorinator</code> en su terminal y ejecute:</p><pre><code class="language-sh">python web-ui.py</code></pre>
Esto iniciará un servidor web local y abrirá automáticamente la interfaz en una nueva ventana.</p><h3>Uso de la GUI</h3></p><ul><li><strong>Configurar:</strong> Establezca las rutas de entrada/salida usando los campos del formulario y los botones "Examinar". Ajuste los parámetros de generación como modo de juego, dificultad, estilo (año, ID del creador, descriptores), sincronización, características específicas (hitsounds, sincronización avanzada), y más, reflejando las opciones de línea de comandos. (Nota: Si proporciona una <code>beatmap_path</code>, la interfaz determinará automáticamente la <code>audio_path</code> y la <code>output_path</code> a partir de ella, por lo que puede dejar esos campos en blanco)</li>
<li><strong>Iniciar:</strong> Haga clic en el botón "Iniciar Inferencia" para comenzar la generación del beatmap.</li>
<li><strong>Cancelar:</strong> Puede detener el proceso en curso usando el botón "Cancelar Inferencia".</li>
<li><strong>Abrir Salida:</strong> Una vez finalizado, use el botón "Abrir Carpeta de Salida" para acceder rápidamente a los archivos generados.</li></p><p></ul>La interfaz web actúa como un envoltorio conveniente alrededor del script <code>inference.py</code>. Para opciones avanzadas o solución de problemas, consulte las instrucciones de línea de comandos.</p><p><img src="https://github.com/user-attachments/assets/5312a45f-d51c-4b37-9389-da3258ddd0a1" alt="python_u3zyW0S3Vs"></p><h2>Inferencia por Línea de Comandos</h2></p><p>Para usuarios que prefieren la línea de comandos o necesitan acceso a configuraciones avanzadas, siga los pasos a continuación. <strong>Nota:</strong> Para una interfaz gráfica más sencilla, consulte la sección <a href="#web-ui-recommended" target="_blank" rel="noopener noreferrer">Web UI (Recomendado)</a> arriba.</p><p>Ejecute <code>inference.py</code> y pase algunos argumentos para generar beatmaps. Para esto, use la <a href="https://hydra.cc/docs/advanced/override_grammar/basic/" target="_blank" rel="noopener noreferrer">sintaxis de sobrescritura Hydra</a>. Consulte <code>configs/inference_v29.yaml</code> para todos los parámetros disponibles.</p><pre><code class="language-">python inference.py \
  audio_path           [Path to input audio] \
  output_path          [Path to output directory] \
  beatmap_path         [Path to .osu file to autofill metadata, and output_path, or use as reference] \
  
  gamemode             [Game mode to generate 0=std, 1=taiko, 2=ctb, 3=mania] \
  difficulty           [Difficulty star rating to generate] \
  mapper_id            [Mapper user ID for style] \
  year                 [Upload year to simulate] \
  hitsounded           [Whether to add hitsounds] \
  slider_multiplier    [Slider velocity multiplier] \
  circle_size          [Circle size] \
  keycount             [Key count for mania] \
  hold_note_ratio      [Hold note ratio for mania 0-1] \
  scroll_speed_ratio   [Scroll speed ratio for mania and ctb 0-1] \
  descriptors          [List of beatmap user tags for style] \
  negative_descriptors [List of beatmap user tags for classifier-free guidance] \
  
  add_to_beatmap       [Whether to add generated content to the reference beatmap instead of making a new beatmap] \
  start_time           [Generation start time in milliseconds] \
  end_time             [Generation end time in milliseconds] \
  in_context           [List of additional context to provide to the model [NONE,TIMING,KIAI,MAP,GD,NO_HS]] \
  output_type          [List of content types to generate] \
  cfg_scale            [Scale of the classifier-free guidance] \
  super_timing         [Whether to use slow accurate variable BPM timing generator] \
  seed                 [Random seed for generation] \</code></pre>
Ejemplo:</p><pre><code class="language-">python inference.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'" gamemode=0 difficulty=5.5 year=2023 descriptors="['jump aim','clean']" in_context=[TIMING,KIAI]</code></pre></p><h2>CLI interactiva</h2>
Para quienes prefieren un flujo de trabajo basado en terminal pero desean una configuración guiada, el script CLI interactivo es una excelente alternativa a la interfaz web.</p><h3>Lanzar la CLI</h3>
Navega al directorio clonado. Es posible que necesites hacer el script ejecutable primero.</p><pre><code class="language-sh"># Make the script executable (only needs to be done once)
chmod +x cli_inference.sh</code></pre></p><pre><code class="language-sh"># Run the script
./cli_inference.sh</code></pre></p><h3>Uso de la CLI</h3>
El script te guiará a través de una serie de indicaciones para configurar todos los parámetros de generación, igual que la interfaz web.</p><p>Utiliza una interfaz codificada por colores para mayor claridad.
Ofrece un menú avanzado de selección múltiple para elegir descriptores de estilo usando las teclas de flecha y la barra espaciadora.
Después de responder todas las preguntas, mostrará el comando final para tu revisión.
Luego puedes confirmar para ejecutarlo directamente o cancelar y copiar el comando para uso manual.</p><h2>Consejos de Generación</h2></p><ul><li>Puedes editar <code>configs/inference_v29.yaml</code> y agregar tus argumentos allí en lugar de escribirlos en la terminal cada vez.</li>
<li>Todos los descriptores disponibles se pueden encontrar <a href="https://osu.ppy.sh/wiki/en/Beatmap/Beatmap_tags" target="_blank" rel="noopener noreferrer">aquí</a>.</li>
<li>Siempre proporciona un argumento de año entre 2007 y 2023. Si lo dejas desconocido, el modelo podría generar con un estilo inconsistente.</li>
<li>Siempre proporciona un argumento de dificultad. Si lo dejas desconocido, el modelo podría generar con una dificultad inconsistente.</li>
<li>Incrementa el parámetro <code>cfg_scale</code> para aumentar la efectividad de los argumentos <code>mapper_id</code> y <code>descriptors</code>.</li>
<li>Puedes usar el argumento <code>negative_descriptors</code> para guiar al modelo alejándolo de ciertos estilos. Esto solo funciona cuando <code>cfg_scale > 1</code>. Asegúrate que el número de descriptores negativos sea igual al número de descriptores.</li>
<li>Si el estilo de tu canción y el estilo deseado del mapa no coinciden bien, el modelo podría no seguir tus indicaciones. Por ejemplo, es difícil generar un mapa de SR alto y SV alto para una canción tranquila.</li>
<li>Si ya tienes el timing y los tiempos de kiai hechos para una canción, puedes dárselos al modelo para aumentar mucho la velocidad y precisión de la inferencia: Usa los argumentos <code>beatmap_path</code> e <code>in_context=[TIMING,KIAI]</code>.</li>
<li>Para remapear solo una parte de tu mapa, usa los argumentos <code>beatmap_path</code>, <code>start_time</code>, <code>end_time</code> y <code>add_to_beatmap=true</code>.</li>
<li>Para generar una dificultad guest para un mapa, usa los argumentos <code>beatmap_path</code> e <code>in_context=[GD,TIMING,KIAI]</code>.</li>
<li>Para generar hitsounds para un mapa, usa los argumentos <code>beatmap_path</code> e <code>in_context=[NO_HS,TIMING,KIAI]</code>.</li>
<li>Para generar solo timing para una canción, usa los argumentos <code>super_timing=true</code> y <code>output_type=[TIMING]</code>.</li></p><p></ul><h2>MaiMod: La Herramienta de Modding Impulsada por IA</h2></p><p>MaiMod es una herramienta de modding para mapas de osu! que usa predicciones de Mapperatorinator para encontrar posibles fallos e inconsistencias que no pueden ser detectadas por otras herramientas automáticas de modding como <a href="https://github.com/Naxesss/MapsetVerifier" target="_blank" rel="noopener noreferrer">Mapset Verifier</a>.
Puede detectar problemas como:
<ul><li>Snapping o patrones rítmicos incorrectos</li>
<li>Puntos de timing inexactos</li>
<li>Posiciones inconsistentes de objetos o colocaciones nuevas de combo</li>
<li>Formas extrañas de sliders</li>
<li>Hitsounds o volúmenes inconsistentes</li></p><p></ul>Puedes probar MaiMod <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb" target="_blank" rel="noopener noreferrer">aquí</a>, o ejecutarlo localmente:
Para ejecutar MaiMod localmente, necesitarás instalar Mapperatorinator. Luego, ejecuta el script <code>mai_mod.py</code>, especificando la ruta de tu mapa con el argumento <code>beatmap_path</code>.
<pre><code class="language-sh">python mai_mod.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'"</code></pre>
Esto imprimirá las sugerencias de modificación en la consola, que luego puedes aplicar manualmente a tu mapa de ritmo.  
Las sugerencias están ordenadas cronológicamente y agrupadas en categorías.  
El primer valor en el círculo indica la 'sorpresa', que es una medida de qué tan inesperado encontró el modelo el problema, para que puedas priorizar los problemas más importantes.  </p><p>El modelo puede cometer errores, especialmente en problemas de baja sorpresa, así que siempre verifica dos veces las sugerencias antes de aplicarlas a tu mapa de ritmo.  
El objetivo principal es ayudarte a reducir el espacio de búsqueda de posibles problemas, para que no tengas que revisar manualmente cada objeto de golpe en tu mapa de ritmo.  </p><h3>MaiMod GUI  </h3>
Para ejecutar la interfaz web de MaiMod, necesitarás instalar Mapperatorinator.  
Luego, ejecuta el script <code>mai_mod_ui.py</code>. Esto iniciará un servidor web local y abrirá automáticamente la interfaz en una nueva ventana:</p><pre><code class="language-sh">python mai_mod_ui.py</code></pre></p><p><img width="850" height="1019" alt="afbeelding" src="https://github.com/user-attachments/assets/67c03a43-a7bd-4265-a5b1-5e4d62aca1fa" /></p><h2>Visión general</h2></p><h3>Tokenización</h3></p><p>Mapperatorinator convierte los mapas de osu! en una representación intermedia de eventos que puede ser convertida directamente hacia y desde tokens.
Incluye objetos de golpe, sonidos de golpe, velocidades de sliders, nuevos combos, puntos de sincronización, tiempos kiai y velocidades de desplazamiento para taiko/mania.</p><p>Aquí hay un pequeño ejemplo del proceso de tokenización:</p><p><img src="https://github.com/user-attachments/assets/84efde76-4c27-48a1-b8ce-beceddd9e695" alt="mapperatorinator_parser"></p><p>Para ahorrar en el tamaño del vocabulario, los eventos de tiempo se cuantifican en intervalos de 10 ms y las coordenadas de posición se cuantifican en una cuadrícula de 32 píxeles.</p><h3>Arquitectura del modelo</h3>
El modelo es básicamente un envoltorio alrededor del modelo <a href="https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration" target="_blank" rel="noopener noreferrer">HF Transformers Whisper</a>, con embeddings de entrada personalizados y función de pérdida.
El tamaño del modelo es de 219 millones de parámetros.
Se encontró que este modelo es más rápido y preciso que T5 para esta tarea.</p><p>La visión general de alto nivel de la entrada y salida del modelo es la siguiente:</p><p><img src="https://user-images.githubusercontent.com/28675590/201044116-1384ad72-c540-44db-a285-7319dd01caad.svg" alt="Picture2"></p><p>El modelo usa marcos de espectrograma Mel como entrada del codificador, con un marco por posición de entrada. La salida del decodificador en cada paso es una distribución softmax sobre un vocabulario discreto y predefinido de eventos. Las salidas son escasas, los eventos solo se requieren cuando ocurre un objeto de golpe, en lugar de anotar cada marco de audio individual.</p><h3>Formato de entrenamiento multitarea</h3></p><p><img src="https://github.com/user-attachments/assets/62f490bc-a567-4671-a7ce-dbcc5f9cd6d9" alt="Multitask training format"></p><p>Antes del token SOS hay tokens adicionales que facilitan la generación condicional. Estos tokens incluyen el modo de juego, la dificultad, el ID del mapper, el año y otros metadatos.
Durante el entrenamiento, estos tokens no tienen etiquetas acompañantes, por lo que nunca son generados por el modelo. 
También durante el entrenamiento existe una probabilidad aleatoria de que un token de metadatos sea reemplazado por un token 'desconocido', de modo que durante la inferencia podemos usar estos tokens 'desconocidos' para reducir la cantidad de metadatos que debemos proporcionar al modelo.</p><h3>Generación continua sin interrupciones</h3></p><p>La longitud de contexto del modelo es de 8.192 segundos. Esto obviamente no es suficiente para generar un mapa completo, por lo que debemos dividir la canción en múltiples ventanas y generar el mapa en partes pequeñas.
Para asegurarnos de que el mapa generado no tenga uniones notorias entre ventanas, usamos una superposición del 90% y generamos las ventanas secuencialmente.
Cada ventana de generación, excepto la primera, comienza con el decodificador prellenado hasta un 50% de la ventana de generación con tokens de las ventanas anteriores.
Utilizamos un procesador logit para asegurarnos de que el modelo no pueda generar tokens de tiempo que estén en el primer 50% de la ventana de generación.  
Además, el último 40% de la ventana de generación está reservado para la siguiente ventana. Cualquier token de tiempo generado en ese rango se trata como tokens EOS.  
Esto asegura que cada token generado esté condicionado por al menos 4 segundos de tokens previos y 3.3 segundos de audio futuro para anticipar.  </p><p>Para evitar el desplazamiento del offset durante una generación larga, se han añadido offsets aleatorios a los eventos de tiempo en el decodificador durante el entrenamiento.  
Esto lo obliga a corregir errores de sincronización escuchando los inicios en el audio en su lugar, y resulta en un offset consistentemente preciso.  </p><h3>Coordenadas refinadas con difusión  </h3></p><p>Las coordenadas de posición generadas por el decodificador se cuantizan en una cuadrícula de 32 píxeles, por lo que luego usamos difusión para eliminar el ruido de las coordenadas hasta las posiciones finales.  
Para esto entrenamos una versión modificada de <a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a> que está especializada solo en el último 10% del programa de ruido, y acepta los tokens de metadatos más avanzados que Mapperatorinator usa para la generación condicional.  </p><p>Dado que el modelo Mapperatorinator genera el SV de los sliders, la longitud requerida del slider es fija independientemente de la forma del camino del punto de control.  
Por lo tanto, tratamos de guiar el proceso de difusión para crear coordenadas que se ajusten a las longitudes requeridas del slider.  
Hacemos esto recalculando las posiciones finales del slider después de cada paso del proceso de difusión basándonos en la longitud requerida y el camino actual del punto de control.  
Esto significa que el proceso de difusión no tiene control directo sobre las posiciones finales del slider, pero aún puede influenciarlas cambiando el camino del punto de control.  </p><h3>Post-procesamiento  </h3></p><p>Mapperatorinator realiza un post-procesamiento adicional para mejorar la calidad del beatmap generado:  </p><ul><li>Refinar coordenadas de posición con difusión.  </li>
<li>Reajustar eventos de tiempo al tick más cercano usando los divisores de snap generados por el modelo.  </li>
<li>Ajustar solapamientos posicionales casi perfectos.  </li>
<li>Convertir eventos de columnas mania a coordenadas X.  </li>
<li>Generar caminos de sliders para redobles de taiko.  </li>
<li>Corregir grandes discrepancias en la longitud requerida del slider y la longitud del camino del punto de control.  </li></p><p></ul><h3>Generador de super sincronización  </h3></p><p>El generador de super sincronización es un algoritmo que mejora la precisión y exactitud de la sincronización generada inferiendo la sincronización para toda la canción 20 veces y promediando los resultados.  
Esto es útil para canciones con BPM variable, o canciones con cambios de BPM. El resultado es casi perfecto con solo a veces una sección que necesita ajuste manual.  </p><h2>Entrenamiento  </h2></p><p>La instrucción a continuación crea un entorno de entrenamiento en tu máquina local.  </p><h3>1. Clona el repositorio  </h3></p><pre><code class="language-sh">git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator</code></pre></p><h3>2. Crear conjunto de datos</h3></p><p>Crea tu propio conjunto de datos usando la <a href="https://github.com/mappingtools/Mapperator/blob/master/README.md#create-a-high-quality-dataset" target="_blank" rel="noopener noreferrer">aplicación de consola Mapperator</a>. Requiere un <a href="https://osu.ppy.sh/home/account/edit" target="_blank" rel="noopener noreferrer">token de cliente OAuth de osu!</a> para verificar beatmaps y obtener metadatos adicionales. Coloca el conjunto de datos en un directorio <code>datasets</code> junto al directorio <code>Mapperatorinator</code>.</p><pre><code class="language-sh">Mapperator.ConsoleApp.exe dataset2 -t "/Mapperatorinator/datasets/beatmap_descriptors.csv" -i "path/to/osz/files" -o "/datasets/cool_dataset"</code></pre></p><h3>3. (Opcional) Configurar Weight & Biases para el registro</h3>
Crea una cuenta en <a href="https://wandb.ai/site" target="_blank" rel="noopener noreferrer">Weight & Biases</a> y obtén tu clave API desde la configuración de tu cuenta.
Luego, establece la variable de entorno <code>WANDB_API_KEY</code>, para que el proceso de entrenamiento sepa registrar con esta clave.</p><pre><code class="language-sh">export WANDB_API_KEY=<your_api_key></code></pre>
<h3>4. Crear contenedor docker</h3>
El entrenamiento en tu entorno virtual también es posible, pero recomendamos usar Docker en WSL para un mejor rendimiento.</p><pre><code class="language-sh">docker compose up -d --force-recreate
docker attach mapperatorinator_space
cd Mapperatorinator</code></pre></p><h3>5. Configurar parámetros y comenzar el entrenamiento</h3></p><p>Todas las configuraciones se encuentran en <code>./configs/train/default.yaml</code>. 
Asegúrese de establecer la ruta correcta <code>train_dataset_path</code> y <code>test_dataset_path</code> a su conjunto de datos, así como los índices de inicio y fin del conjunto de mapas para la división train/test.
La ruta es local al contenedor de docker, por lo que si colocó su conjunto de datos llamado <code>cool_dataset</code> en el directorio <code>datasets</code>, entonces debería ser <code>/workspace/datasets/cool_dataset</code>.</p><p>Recomiendo crear un archivo de configuración personalizado que sobrescriba la configuración predeterminada, para que tenga un registro de su configuración de entrenamiento para reproducibilidad.</p><pre><code class="language-yaml">data:
  train_dataset_path: "/workspace/datasets/cool_dataset"
  test_dataset_path: "/workspace/datasets/cool_dataset"
  train_dataset_start: 0
  train_dataset_end: 90
  test_dataset_start: 90
  test_dataset_end: 100</code></pre></p><p>Begin training by calling <code>python osuT5/train.py</code> or <code>torchrun --nproc_per_node=NUM_GPUS osuT5/train.py</code> for multi-GPU training.</p><pre><code class="language-sh">python osuT5/train.py -cn train_v29 train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100</code></pre></p><h3>6. Afinación LoRA</h3></p><p>También puedes afinar un modelo preentrenado con <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">LoRA</a> para adaptarlo a un estilo o modo de juego específico.
Para ello, adapta <code>configs/train/lora.yaml</code> a tus necesidades y ejecuta la configuración de entrenamiento <code>lora</code>:</p><pre><code class="language-sh">python osuT5/train.py -cn lora train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100</code></pre></p><p>Parámetros importantes de LoRA a considerar:
<ul><li><code>pretrained_path</code>: Ruta o repositorio HF del modelo base para ajustar.</li>
<li><code>r</code>: Rango de las matrices LoRA. Valores más altos aumentan la capacidad del modelo pero también el uso de memoria.</li>
<li><code>lora_alpha</code>: Factor de escalado para las actualizaciones LoRA.</li>
<li><code>total_steps</code>: Número total de pasos de entrenamiento. Ajústalo según el tamaño de tu conjunto de datos.</li>
<li><code>enable_lora</code>: Si usar LoRA o ajuste completo del modelo.</li></p><p></ul>Durante la inferencia, puedes especificar los pesos LoRA a usar con el argumento <code>lora_path</code>.
Esto puede ser una ruta local o un repositorio de Hugging Face.</p><h2>Ver también</h2>
<ul><li><a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./classifier/README.md" target="_blank" rel="noopener noreferrer">Mapper Classifier</a></li>
<li><a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./rcomplexion/README.md" target="_blank" rel="noopener noreferrer">RComplexion</a></li></p><p></ul><h2>Créditos</h2></p><p>Agradecimientos especiales a:
<ul><li>Los autores de <a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> por su código de entrenamiento.</li>
<li>El equipo de Hugging Face por sus <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener noreferrer">herramientas</a>.</li>
<li><a href="https://github.com/jaswon" target="_blank" rel="noopener noreferrer">Jason Won</a> y <a href="https://github.com/sedthh" target="_blank" rel="noopener noreferrer">Richard Nagyfi</a> por intercambiar ideas.</li>
<li><a href="https://github.com/minetoblend" target="_blank" rel="noopener noreferrer">Marvin</a> por donar créditos de entrenamiento.</li>
<li>La comunidad de osu! por los beatmaps.</li></p><p></ul><h2>Trabajos relacionados</h2></p><ul><li><a href="https://github.com/Syps/osu_beatmap_generator" target="_blank" rel="noopener noreferrer">osu! Beatmap Generator</a> por Syps (Nick Sypteras)</li>
<li><a href="https://github.com/kotritrona/osumapper" target="_blank" rel="noopener noreferrer">osumapper</a> por kotritrona, jyvden, Yoyolick (Ryan Zmuda)</li>
<li><a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a> por OliBomby (Olivier Schipper), NiceAesth (Andrei Baciu)</li>
<li><a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> por gyataro (Xiwen Teoh)</li>
<li><a href="https://github.com/sedthh/BeatLearning" target="_blank" rel="noopener noreferrer">Beat Learning</a> por sedthh (Richard Nagyfi)</li>
<li><a href="https://github.com/jaswon/osu-dreamer" target="_blank" rel="noopener noreferrer">osu!dreamer</a> por jaswon (Jason Won)</li></p><p>
</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-28

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-28 
    </div>
    
</body>
</html>