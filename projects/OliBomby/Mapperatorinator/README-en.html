<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mapperatorinator - An AI framework for generating and modifying osu! beatmaps for all game modes from spectrogram inputs.</title>
    <meta name="description" content="An AI framework for generating and modifying osu! beatmaps for all game modes from spectrogram inputs.">
    <meta name="keywords" content="Mapperatorinator, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "Mapperatorinator",
  "description": "An AI framework for generating and modifying osu! beatmaps for all game modes from spectrogram inputs.",
  "author": {
    "@type": "Person",
    "name": "OliBomby"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 381
  },
  "url": "https://OpenAiTx.github.io/projects/OliBomby/Mapperatorinator/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/README.md",
  "datePublished": "2025-12-28",
  "dateModified": "2025-12-28"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/OliBomby/Mapperatorinator" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    Mapperatorinator
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 381 stars</span>
                <span class="language">English</span>
                <span>by OliBomby</span>
            </div>
        </div>
        
        <div class="content">
            <h1>Mapperatorinator</h1></p><p>Try the generative model <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb" target="_blank" rel="noopener noreferrer">here</a>, or MaiMod <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb" target="_blank" rel="noopener noreferrer">here</a>. Check out a video showcase <a href="https://youtu.be/FEr7t1L2EoA" target="_blank" rel="noopener noreferrer">here</a>.</p><p>Mapperatorinator is a multi-model framework that uses spectrogram inputs to generate fully featured osu! beatmaps for all gamemodes and <a href="#maimod-the-ai-driven-modding-tool" target="_blank" rel="noopener noreferrer">assist modding beatmaps</a>.
The goal of this project is to automatically generate rankable quality osu! beatmaps from any song with a high degree of customizability.</p><p>This project is built upon <a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> and <a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a>. In developing this, I spent about 2500 hours of GPU compute across 142 runs on my 4060 Ti and rented 4090 instances on vast.ai.</p><p>#### Use this tool responsibly. Always disclose the use of AI in your beatmaps.</p><h2>Installation</h2></p><p>The instruction below allows you to generate beatmaps on your local machine, alternatively you can run it in the cloud with the <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mapperatorinator_inference.ipynb" target="_blank" rel="noopener noreferrer">colab notebook</a>.</p><h3>1. Clone the repository</h3></p><pre><code class="language-sh">git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator</code></pre></p><h3>2. (Optional) Create virtual environment</h3></p><p>Use Python 3.10, later versions might not be compatible with the dependencies.</p><pre><code class="language-sh">python -m venv .venv</p><h1>In cmd.exe</h1>
.venv\Scripts\activate.bat
<h1>In PowerShell</h1>
.venv\Scripts\Activate.ps1
<h1>In Linux or MacOS</h1>
source .venv/bin/activate</code></pre></p><h3>3. Install dependencies</h3></p><ul><li>Python 3.10</li>
<li><a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Git</a></li>
<li><a href="http://www.ffmpeg.org/" target="_blank" rel="noopener noreferrer">ffmpeg</a></li>
<li><a href="https://developer.nvidia.com/cuda-zone" target="_blank" rel="noopener noreferrer">CUDA</a> (For NVIDIA GPUs) or <a href="https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html" target="_blank" rel="noopener noreferrer">ROCm</a> (For AMD GPUs on linux)</li>
<li><a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">PyTorch</a>: Make sure to follow the Get Started guide so you install <code>torch</code> and <code>torchaudio</code> with GPU support. Select the correct Compute Platform version that you have installed in the previous step.</li></p><p><li>and the remaining Python dependencies:</li></p><p></ul><pre><code class="language-sh">pip install -r requirements.txt</code></pre></p><h2>Web GUI (Recommended)</h2></p><p>For a more user-friendly experience, consider using the Web UI. It provides a graphical interface to configure generation parameters, start the process, and monitor the output.</p><h3>Launch the GUI</h3></p><p>Navigate to the cloned <code>Mapperatorinator</code> directory in your terminal and run:</p><pre><code class="language-sh">python web-ui.py</code></pre>
This will start a local web server and automatically open the UI in a new window.</p><h3>Using the GUI</h3></p><ul><li><strong>Configure:</strong> Set input/output paths using the form fields and "Browse" buttons. Adjust generation parameters like gamemode, difficulty, style (year, mapper ID, descriptors), timing, specific features (hitsounds, super timing), and more, mirroring the command-line options. (Note: If you provide a <code>beatmap_path</code>, the UI will automatically determine the <code>audio_path</code> and <code>output_path</code> from it, so you can leave those fields blank)</li>
<li><strong>Start:</strong> Click the "Start Inference" button to begin the beatmap generation.</li>
<li><strong>Cancel:</strong> You can stop the ongoing process using the "Cancel Inference" button.</li>
<li><strong>Open Output:</strong> Once finished, use the "Open Output Folder" button for quick access to the generated files.</li></p><p></ul>The Web UI acts as a convenient wrapper around the <code>inference.py</code> script. For advanced options or troubleshooting, refer to the command-line instructions.</p><p><img src="https://github.com/user-attachments/assets/5312a45f-d51c-4b37-9389-da3258ddd0a1" alt="python_u3zyW0S3Vs"></p><h2>Command-Line Inference</h2></p><p>For users who prefer the command line or need access to advanced configurations, follow the steps below. <strong>Note:</strong> For a simpler graphical interface, please see the <a href="#web-ui-recommended" target="_blank" rel="noopener noreferrer">Web UI (Recommended)</a> section above.</p><p>Run <code>inference.py</code> and pass in some arguments to generate beatmaps. For this use <a href="https://hydra.cc/docs/advanced/override_grammar/basic/" target="_blank" rel="noopener noreferrer">Hydra override syntax</a>. See <code>configs/inference_v29.yaml</code> for all available parameters.</p><pre><code class="language-">python inference.py \
  audio_path           [Path to input audio] \
  output_path          [Path to output directory] \
  beatmap_path         [Path to .osu file to autofill metadata, and output_path, or use as reference] \
  
  gamemode             [Game mode to generate 0=std, 1=taiko, 2=ctb, 3=mania] \
  difficulty           [Difficulty star rating to generate] \
  mapper_id            [Mapper user ID for style] \
  year                 [Upload year to simulate] \
  hitsounded           [Whether to add hitsounds] \
  slider_multiplier    [Slider velocity multiplier] \
  circle_size          [Circle size] \
  keycount             [Key count for mania] \
  hold_note_ratio      [Hold note ratio for mania 0-1] \
  scroll_speed_ratio   [Scroll speed ratio for mania and ctb 0-1] \
  descriptors          [List of beatmap user tags for style] \
  negative_descriptors [List of beatmap user tags for classifier-free guidance] \
  
  add_to_beatmap       [Whether to add generated content to the reference beatmap instead of making a new beatmap] \
  start_time           [Generation start time in milliseconds] \
  end_time             [Generation end time in milliseconds] \
  in_context           [List of additional context to provide to the model [NONE,TIMING,KIAI,MAP,GD,NO_HS]] \
  output_type          [List of content types to generate] \
  cfg_scale            [Scale of the classifier-free guidance] \
  super_timing         [Whether to use slow accurate variable BPM timing generator] \
  seed                 [Random seed for generation] \</code></pre>
Example:</p><pre><code class="language-">python inference.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'" gamemode=0 difficulty=5.5 year=2023 descriptors="['jump aim','clean']" in_context=[TIMING,KIAI]</code></pre></p><h2>Interactive CLI</h2>
For those who prefer a terminal-based workflow but want a guided setup, the interactive CLI script is an excellent alternative to the Web UI.</p><h3>Launch the CLI</h3>
Navigate to the cloned directory. You may need to make the script executable first.</p><pre><code class="language-sh"># Make the script executable (only needs to be done once)
chmod +x cli_inference.sh</code></pre></p><pre><code class="language-sh"># Run the script
./cli_inference.sh</code></pre></p><h3>Using the CLI</h3>
The script will walk you through a series of prompts to configure all generation parameters, just like the Web UI.</p><p>It uses a color-coded interface for clarity.
It provides an advanced multi-select menu for choosing style descriptors using your arrow keys and spacebar.
After you've answered all the questions, it will display the final command for your review.
You can then confirm to execute it directly or cancel and copy the command for manual use.</p><h2>Generation Tips</h2></p><ul><li>You can edit <code>configs/inference_v29.yaml</code> and add your arguments there instead of typing them in the terminal every time.</li>
<li>All available descriptors can be found <a href="https://osu.ppy.sh/wiki/en/Beatmap/Beatmap_tags" target="_blank" rel="noopener noreferrer">here</a>.</li>
<li>Always provide a year argument between 2007 and 2023. If you leave it unknown, the model might generate with an inconsistent style.</li>
<li>Always provide a difficulty argument. If you leave it unknown, the model might generate with an inconsistent difficulty.</li>
<li>Increase the <code>cfg_scale</code> parameter to increase the effectiveness of the <code>mapper_id</code> and <code>descriptors</code> arguments.</li>
<li>You can use the <code>negative_descriptors</code> argument to guide the model away from certain styles. This only works when <code>cfg_scale > 1</code>. Make sure the number of negative descriptors is equal to the number of descriptors.</li>
<li>If your song style and desired beatmap style don't match well, the model might not follow your directions. For example, its hard to generate a high SR, high SV beatmap for a calm song. </li>
<li>If you already have timing and kiai times done for a song, then you can give this to the model to greatly increase inference speed and accuracy: Use the <code>beatmap_path</code> and <code>in_context=[TIMING,KIAI]</code> arguments.</li>
<li>To remap just a part of your beatmap, use the <code>beatmap_path</code>, <code>start_time</code>, <code>end_time</code>, and <code>add_to_beatmap=true</code> arguments.</li>
<li>To generate a guest difficulty for a beatmap, use the <code>beatmap_path</code> and <code>in_context=[GD,TIMING,KIAI]</code> arguments.</li>
<li>To generate hitsounds for a beatmap, use the <code>beatmap_path</code> and <code>in_context=[NO_HS,TIMING,KIAI]</code> arguments.</li>
<li>To generate only timing for a song, use the <code>super_timing=true</code> and <code>output_type=[TIMING]</code> arguments.</li></p><p></ul><h2>MaiMod: The AI-driven Modding Tool</h2></p><p>MaiMod is a modding tool for osu! beatmaps that uses Mapperatorinator predictions to find potential faults and inconsistencies which can't be detected by other automatic modding tools like <a href="https://github.com/Naxesss/MapsetVerifier" target="_blank" rel="noopener noreferrer">Mapset Verifier</a>.
It can detect issues like:
<ul><li>Incorrect snapping or rhythmic patterns</li>
<li>Inaccurate timing points</li>
<li>Inconsistent hit object positions or new combo placements</li>
<li>Weird slider shapes</li>
<li>Inconsistent hitsounds or volumes</li></p><p></ul>You can try MaiMod <a href="https://colab.research.google.com/github/OliBomby/Mapperatorinator/blob/main/colab/mai_mod_inference.ipynb" target="_blank" rel="noopener noreferrer">here</a>, or run it locally:
To run MaiMod locally, you'll need to install Mapperatorinator. Then, run the <code>mai_mod.py</code> script, specifying your beatmap's path with the <code>beatmap_path</code> argument.
<pre><code class="language-sh">python mai_mod.py beatmap_path="'C:\Users\USER\AppData\Local\osu!\Songs\1 Kenji Ninuma - DISCO PRINCE\Kenji Ninuma - DISCOPRINCE (peppy) [Normal].osu'"</code></pre>
This will print the modding suggestions to the console, which you can then apply to your beatmap manually.  
Suggestions are ordered chronologically and grouped into categories.  
The first value in the circle indicates the 'surprisal' which is a measure of how unexpected the model found the issue to be, so you can prioritize the most important issues.  </p><p>The model can make mistakes, especially on low surprisal issues, so always double-check the suggestions before applying them to your beatmap.  
The main goal is to help you narrow down the search space for potential issues, so you don't have to manually check every single hit object in your beatmap.  </p><h3>MaiMod GUI  </h3>
To run the MaiMod Web UI, you'll need to install Mapperatorinator.  
Then, run the <code>mai_mod_ui.py</code> script. This will start a local web server and automatically open the UI in a new window:</p><pre><code class="language-sh">python mai_mod_ui.py</code></pre></p><p><img width="850" height="1019" alt="image" src="https://github.com/user-attachments/assets/67c03a43-a7bd-4265-a5b1-5e4d62aca1fa" /></p><h2>Overview</h2></p><h3>Tokenization</h3></p><p>Mapperatorinator converts osu! beatmaps into an intermediate event representation that can be directly converted to and from tokens.
It includes hit objects, hitsounds, slider velocities, new combos, timing points, kiai times, and taiko/mania scroll speeds.</p><p>Here is a small example of the tokenization process:</p><p><img src="https://github.com/user-attachments/assets/84efde76-4c27-48a1-b8ce-beceddd9e695" alt="mapperatorinator_parser"></p><p>To save on vocabulary size, time events are quantized to 10ms intervals and position coordinates are quantized to 32 pixel grid points.</p><h3>Model architecture</h3>
The model is basically a wrapper around the <a href="https://huggingface.co/docs/transformers/en/model_doc/whisper#transformers.WhisperForConditionalGeneration" target="_blank" rel="noopener noreferrer">HF Transformers Whisper</a> model, with custom input embeddings and loss function.
Model size amounts to 219M parameters.
This model was found to be faster and more accurate than T5 for this task.</p><p>The high-level overview of the model's input-output is as follows:</p><p><img src="https://user-images.githubusercontent.com/28675590/201044116-1384ad72-c540-44db-a285-7319dd01caad.svg" alt="Picture2"></p><p>The model uses Mel spectrogram frames as encoder input, with one frame per input position. The model decoder output at each step is a softmax distribution over a discrete, predefined, vocabulary of events. Outputs are sparse, events are only needed when a hit-object occurs, instead of annotating every single audio frame.</p><h3>Multitask training format</h3></p><p><img src="https://github.com/user-attachments/assets/62f490bc-a567-4671-a7ce-dbcc5f9cd6d9" alt="Multitask training format"></p><p>Before the SOS token are additional tokens that facilitate conditional generation. These tokens include the gamemode, difficulty, mapper ID, year, and other metadata.
During training, these tokens do not have accompanying labels, so they are never output by the model. 
Also during training there is a random chance that a metadata token gets replaced by an 'unknown' token, so during inference we can use these 'unknown' tokens to reduce the amount of metadata we have to give to the model.</p><h3>Seamless long generation</h3></p><p>The context length of the model is 8.192 seconds long. This is obviously not enough to generate a full beatmap, so we have to split the song into multiple windows and generate the beatmap in small parts.
To make sure that the generated beatmap does not have noticeable seams in between windows, we use a 90% overlap and generate the windows sequentially.
Each generation window except the first starts with the decoder pre-filled up to 50% of the generation window with tokens from the previous windows.
We use a logit processor to make sure that the model can't generate time tokens that are in the first 50% of the generation window.  
Additionally, the last 40% of the generation window is reserved for the next window. Any generated time tokens in that range are treated as EOS tokens.  
This ensures that each generated token is conditioned on at least 4 seconds of previous tokens and 3.3 seconds of future audio to anticipate.  </p><p>To prevent offset drifting during long generation, random offsets have been added to time events in the decoder during training.  
This forces it to correct timing errors by listening to the onsets in the audio instead, and results in a consistently accurate offset.  </p><h3>Refined coordinates with diffusion  </h3></p><p>Position coordinates generated by the decoder are quantized to 32 pixel grid points, so afterward we use diffusion to denoise the coordinates to the final positions.  
For this we trained a modified version of <a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a> that is specialized to only the last 10% of the noise schedule, and accepts the more advanced metadata tokens that Mapperatorinator uses for conditional generation.  </p><p>Since the Mapperatorinator model outputs the SV of sliders, the required length of the slider is fixed regardless of the shape of the control point path.  
Therefore, we try to guide the diffusion process to create coordinates that fit the required slider lengths.  
We do this by recalculating the slider end positions after every step of the diffusion process based on the required length and the current control point path.  
This means that the diffusion process does not have direct control over the slider end positions, but it can still influence them by changing the control point path.  </p><h3>Post-processing  </h3></p><p>Mapperatorinator does some extra post-processing to improve the quality of the generated beatmap:  </p><ul><li>Refine position coordinates with diffusion.  </li>
<li>Resnap time events to the nearest tick using the snap divisors generated by the model.  </li>
<li>Snap near-perfect positional overlaps.  </li>
<li>Convert mania column events to X coordinates.  </li>
<li>Generate slider paths for taiko drumrolls.  </li>
<li>Fix big discrepancies in required slider length and control point path length.  </li></p><p></ul><h3>Super timing generator  </h3></p><p>Super timing generator is an algorithm that improves the precision and accuracy of generated timing by infering timing for the whole song 20 times and averaging the results.  
This is useful for songs with variable BPM, or songs with BPM changes. The result is almost perfect with only sometimes a section that needs manual adjustment.  </p><h2>Training  </h2></p><p>The instruction below creates a training environment on your local machine.  </p><h3>1. Clone the repository  </h3></p><pre><code class="language-sh">git clone https://github.com/OliBomby/Mapperatorinator.git
cd Mapperatorinator</code></pre></p><h3>2. Create dataset</h3></p><p>Create your own dataset using the <a href="https://github.com/mappingtools/Mapperator/blob/master/README.md#create-a-high-quality-dataset" target="_blank" rel="noopener noreferrer">Mapperator console app</a>. It requires an <a href="https://osu.ppy.sh/home/account/edit" target="_blank" rel="noopener noreferrer">osu! OAuth client token</a> to verify beatmaps and get additional metadata. Place the dataset in a <code>datasets</code> directory next to the <code>Mapperatorinator</code> directory.</p><pre><code class="language-sh">Mapperator.ConsoleApp.exe dataset2 -t "/Mapperatorinator/datasets/beatmap_descriptors.csv" -i "path/to/osz/files" -o "/datasets/cool_dataset"</code></pre></p><h3>3. (Optional) Set-up Weight & Biases for logging</h3>
Create an account on <a href="https://wandb.ai/site" target="_blank" rel="noopener noreferrer">Weight & Biases</a> and get your API key from your account settings.
Then set the <code>WANDB_API_KEY</code> environment variable, so the training process knows to log to this key.</p><pre><code class="language-sh">export WANDB_API_KEY=<your_api_key></code></pre></p><h3>4. Create docker container</h3>
Training in your venv is also possible, but we recommend using Docker on WSL for better performance.
<pre><code class="language-sh">docker compose up -d --force-recreate
docker attach mapperatorinator_space
cd Mapperatorinator</code></pre></p><h3>5. Configure parameters and begin training</h3></p><p>All configurations are located in <code>./configs/train/default.yaml</code>.  
Make sure to set the correct <code>train_dataset_path</code> and <code>test_dataset_path</code> to your dataset, as well as the start and end mapset indices for train/test split.  
The path is local to the docker container, so if you placed your dataset called <code>cool_dataset</code> into the <code>datasets</code> directory, then it should be <code>/workspace/datasets/cool_dataset</code>.  </p><p>I recommend making a custom config file that overrides the default config, so you have a record of your training config for reproducibility.</p><pre><code class="language-yaml">data:
  train_dataset_path: "/workspace/datasets/cool_dataset"
  test_dataset_path: "/workspace/datasets/cool_dataset"
  train_dataset_start: 0
  train_dataset_end: 90
  test_dataset_start: 90
  test_dataset_end: 100</code></pre></p><p>Begin training by calling <code>python osuT5/train.py</code> or <code>torchrun --nproc_per_node=NUM_GPUS osuT5/train.py</code> for multi-GPU training.</p><pre><code class="language-sh">python osuT5/train.py -cn train_v29 train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100</code></pre></p><h3>6. LoRA fine-tuning</h3></p><p>You can also fine-tune a pre-trained model with <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="noopener noreferrer">LoRA</a> to adapt it to a specific style or gamemode.
To do this, adapt <code>configs/train/lora.yaml</code> to your needs and run the <code>lora</code> training config:</p><pre><code class="language-sh">python osuT5/train.py -cn lora train_dataset_path="/workspace/datasets/cool_dataset" test_dataset_path="/workspace/datasets/cool_dataset" train_dataset_end=90 test_dataset_start=90 test_dataset_end=100</code></pre></p><p>Important LoRA parameters to consider:
<ul><li><code>pretrained_path</code>: Path or HF repo of the base model to fine-tune.</li>
<li><code>r</code>: Rank of the LoRA matrices. Higher values increase model capacity but also memory usage.</li>
<li><code>lora_alpha</code>: Scaling factor for the LoRA updates.</li>
<li><code>total_steps</code>: Total number of training steps. Balance this according to your dataset size.</li>
<li><code>enable_lora</code>: Whether to use LoRA or full model fine-tuning.</li></p><p></ul>During inference, you can specify the LoRA weights to use with the <code>lora_path</code> argument.
This can be a local path or a Hugging Face repo.</p><h2>See also</h2>
<ul><li><a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./classifier/README.md" target="_blank" rel="noopener noreferrer">Mapper Classifier</a></li>
<li><a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/./rcomplexion/README.md" target="_blank" rel="noopener noreferrer">RComplexion</a></li></p><p></ul><h2>Credits</h2></p><p>Special thanks to:
<ul><li>The authors of <a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> for their training code.</li>
<li>Hugging Face team for their <a href="https://huggingface.co/docs/transformers/index" target="_blank" rel="noopener noreferrer">tools</a>.</li>
<li><a href="https://github.com/jaswon" target="_blank" rel="noopener noreferrer">Jason Won</a> and <a href="https://github.com/sedthh" target="_blank" rel="noopener noreferrer">Richard Nagyfi</a> for bouncing ideas.</li>
<li><a href="https://github.com/minetoblend" target="_blank" rel="noopener noreferrer">Marvin</a> for donating training credits.</li>
<li>The osu! community for the beatmaps.</li></p><p></ul><h2>Related works</h2></p><ul><li><a href="https://github.com/Syps/osu_beatmap_generator" target="_blank" rel="noopener noreferrer">osu! Beatmap Generator</a> by Syps (Nick Sypteras)</li>
<li><a href="https://github.com/kotritrona/osumapper" target="_blank" rel="noopener noreferrer">osumapper</a> by kotritrona, jyvden, Yoyolick (Ryan Zmuda)</li>
<li><a href="https://github.com/OliBomby/osu-diffusion" target="_blank" rel="noopener noreferrer">osu-diffusion</a> by OliBomby (Olivier Schipper), NiceAesth (Andrei Baciu)</li>
<li><a href="https://github.com/gyataro/osuT5" target="_blank" rel="noopener noreferrer">osuT5</a> by gyataro (Xiwen Teoh)</li>
<li><a href="https://github.com/sedthh/BeatLearning" target="_blank" rel="noopener noreferrer">Beat Learning</a> by sedthh (Richard Nagyfi)</li>
<li><a href="https://github.com/jaswon/osu-dreamer" target="_blank" rel="noopener noreferrer">osu!dreamer</a> by jaswon (Jason Won)</li></p><p>
</ul>---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-12-28

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/OliBomby/Mapperatorinator/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-12-28 
    </div>
    
</body>
</html>