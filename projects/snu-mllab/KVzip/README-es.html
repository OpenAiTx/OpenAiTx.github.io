<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - Read KVzip documentation in Spanish. This project has 70 stars on GitHub.</title>
    <meta name="description" content="Read KVzip documentation in Spanish. This project has 70 stars on GitHub.">
    <meta name="keywords" content="KVzip, Spanish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "KVzip",
  "description": "Read KVzip documentation in Spanish. This project has 70 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "snu-mllab"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 70
  },
  "url": "https://OpenAiTx.github.io/projects/snu-mllab/KVzip/README-es.html",
  "sameAs": "https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/snu-mllab/KVzip" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    KVzip
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 70 stars</span>
                <span class="language">Spanish</span>
                <span>by snu-mllab</span>
            </div>
        </div>
        
        <div class="content">
            <h1>KVzip: Compresión Agnóstica de Caché KV con Reconstrucción de Contexto</h1></p><p><a href="https://arxiv.org/abs/2505.23416" target="_blank" rel="noopener noreferrer">[Artículo</a>] <a href="https://janghyun1230.github.io/kvzip/" target="_blank" rel="noopener noreferrer">[Blog</a>] </p><p><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800"></p><h2>¿Qué hay de nuevo?</h2>
<ul><li>KVzip comprime la caché KV para soportar <strong>diversas consultas futuras</strong>.</li>
<li>[Dependiente del contexto] Logra una <strong>reducción de 3 a 4 veces en el tamaño de la caché KV</strong> y una <strong>disminución de 2 veces en la latencia de decodificación</strong>, con una degradación mínima del rendimiento.</li>
<li>[Independiente del contexto] Mejora la compresión KV a nivel de cabeza estilo <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a>, utilizando sólo <strong>unas pocas pasadas hacia adelante en menos de un minuto</strong> para la optimización de la puntuación de importancia a nivel de cabeza (100 veces más rápido).</li>
<li>Ejecutar demo.py:</li>
</ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></p><h3>Benchmark en configuración agnóstica a la consulta</h3>
<ul><li>Tareas: <a href="https://huggingface.co/datasets/rajpurkar/squad" target="_blank" rel="noopener noreferrer">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench" target="_blank" rel="noopener noreferrer">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294" target="_blank" rel="noopener noreferrer">GSM8K</a>. </li>
<li>Modelo: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2.5-7B-Instruct-1M</a></li></p><p></ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800"></p><h2>Instalación</h2>
Usamos CUDA 12.1 y Python 3.10
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i</code></pre>
<ul><li>Para usar la cuantización de <a href="https://github.com/mit-han-lab/omniserve" target="_blank" rel="noopener noreferrer">QServe</a>, por favor siga <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model" target="_blank" rel="noopener noreferrer"><code>./model/quant_model</code></a>.</li>
</ul><h3>Dataset</h3>
<ul><li>Por favor descargue el dataset SCBench preprocesado desde <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link" target="_blank" rel="noopener noreferrer">Google Drive</a>.</li>
<li>Si descargó los archivos descomprimidos, simplemente mueva la carpeta scbench.</li>
</ul><pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  </code></pre></p><h2>Inicio rápido</h2>
<pre><code class="language-python">from model import ModelKVzip</p><p>model = ModelKVzip("Qwen/Qwen2.5-7B-Instruct-1M")
context = "Este es mi perfil básico. Mi nombre es Kim y vivo en Seúl. Mi especialidad es ciencias de la computación."
queries = ["¿Cuál es mi nombre?", "¿Vivo en Seúl?"]</p><p>kv = model.prefill(context, load_score=False)  # precarga la caché KV + puntuación de importancia
kv.prune(ratio=0.3)  # ratio de compresión, elimina el 70% del KV</p><p>for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inferencia eficiente
    print(q, output)</code></pre>
<ul><li>Los modelos soportados están listados en <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, incluyendo <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Configure <code>load_score=True</code> para eliminar la sobrecarga de compresión. Esto habilita la eliminación de KV independiente del contexto, con un compromiso en la relación de compresión de <code>ratio=0.6</code>.</li>
<li>Después de la generación, los pares KV correspondientes a las consultas y los tokens generados se eliminan selectivamente de la caché para un procesamiento posterior. Configure <code>update_cache=True</code> para habilitar inferencia multi-turno, reteniendo todo el historial de interacción durante la inferencia. </li></p><p></ul><h2>Perfilando memoria y tiempo de computación</h2>
<h3>Eliminación dependiente del contexto</h3>
<pre><code class="language-bash">python -B test.py -m [nombre_modelo] -d [nombre_datos] --kv_type evict --ratio 0.3</code></pre>
<ul><li>El código anterior también compara salidas generadas con cachés KV completas versus podadas.</li>
<li>Para una prueba rápida, use <code>-d squad</code>. Para pruebas de contexto largo, use <code>-d scbench_kv</code>.</li>
  <li>Nombres de datos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py" target="_blank" rel="noopener noreferrer"><code>data/load.py</code></a>.</li>
  <li>Nombres de modelos disponibles: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, por ejemplo, llama3.1-8b, qwen2.5-7b (o Qwen/Qwen2.5-7B-Instruct-1M).</li>
<li>Adaptamos el kernel CUDA de <a href="https://github.com/FFY0/AdaKV/tree/main" target="_blank" rel="noopener noreferrer">AdaKV</a>, soportando asignación de presupuesto no uniforme por cabeza.</li>
  <li>Actualmente, nuestro código carece de un kernel optimizado para Gemma3 que usa caché KV estática, por lo que el código no produce ganancias reales de eficiencia. Sin embargo, el rendimiento del modelo aún puede evaluarse usando atención reducida con submuestreo KV (<code>--kv_type retain</code>).</li></p><p>
</ul><h3>Eliminación independiente del contexto (sin sobrecarga de compresión en tiempo de ejecución)</h3>
<ul><li>Use el flag <code>--level head</code> con <code>--ratio 0.6</code> (recomendado).</li>
  <li>Eliminamos todos los pares KV de contexto asociados a una cabeza específica mientras retenemos los pares KV del prompt del sistema y la consulta.</li>
  <li>Las puntuaciones de cabeza precomputadas están disponibles para LLaMA3.1-8B y Qwen2.5-7/14B en <code>./utils/head_score</code>.</li>
<li>Para calcular las puntuaciones de cabeza para otros modelos:</li>
  </ul><pre><code class="language-bash">  python -B test.py -m [nombre_modelo] -d scbench_qa_eng --save_head_score
  ``<code>
  <ul><li>Los resultados se guardarán en </code>./utils/head_score<code>.</li>
  <li>Si el objetivo es una tarea de codificación, recomendamos ejecutar adicionalmente el comando con </code>-d scbench_repoqa<code>. Esto permite que el modelo use las puntuaciones máximas de cabeza tanto de lenguajes naturales como de codificación, lo que mejora el rendimiento.</li>
<li>Estas puntuaciones pueden integrarse perfectamente con el motor de inferencia optimizado de <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a> reemplazando sus datos de puntuación de cabeza con los nuestros.</li></p><p>
</ul><h2>Evaluación</h2>
<ul><li>Para generar respuestas del modelo con ratios de compresión KV desde 0.1 hasta 1.0:</li>
    </ul></code>`<code>bash
    python -B eval.py -m [nombre_modelo] -d [nombre_datos] --kv_type retain --num 100
    </code>`<code> 
  <ul><li>Los resultados se guardarán en </code>./results/[nombre_datos]<code>.</li>
  <li>Los datasets soportados están listados en </code>data/load.py<code>.</li>
<li>Para calcular métricas de evaluación a partir de los resultados generados:</li>
  </ul></code>`<code>bash
  python -B -m results.parse -m [nombre_modelo] -d [nombre_datos]
  </code>`<code></p><h2>Aplicación a nuevos modelos</h2>
Para integrar KVzip en un nuevo modelo, necesitará actualizar los siguientes archivos:
<ul><li></code>attention/attn.py<code>  </li>
  </ul>Modifique la lógica del pase hacia adelante de atención según sea necesario. En ciertos casos, también podrían requerirse actualizaciones en kvcache.py y score.py.
<ul><li></code>model/monkeypatch.py<code>  </li>
  </ul>Implemente el parcheo específico del modelo para la integración.
<ul><li></code>model/template.py<code>   </li>
  </ul>Defina el prompt del sistema y las plantillas de formato de chat del modelo.</p><h2>Citas</code></pre>bibtex</h2>
@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code>``</p><h2>Licencia</h2>
Licencia MIT

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>