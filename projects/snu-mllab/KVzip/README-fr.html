<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - Read KVzip documentation in French. This project has 70 stars on GitHub.</title>
    <meta name="description" content="Read KVzip documentation in French. This project has 70 stars on GitHub.">
    <meta name="keywords" content="KVzip, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "KVzip",
  "description": "Read KVzip documentation in French. This project has 70 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "snu-mllab"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 70
  },
  "url": "https://OpenAiTx.github.io/projects/snu-mllab/KVzip/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/snu-mllab/KVzip" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    KVzip
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 70 stars</span>
                <span class="language">French</span>
                <span>by snu-mllab</span>
            </div>
        </div>
        
        <div class="content">
            <h1>KVzip : Compression du cache KV indépendante des requêtes avec reconstruction de contexte</h1></p><p><a href="https://arxiv.org/abs/2505.23416" target="_blank" rel="noopener noreferrer">[Article</a>] <a href="https://janghyun1230.github.io/kvzip/" target="_blank" rel="noopener noreferrer">[Blog</a>] </p><p><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800"></p><h2>Quoi de neuf ?</h2>
<ul><li>KVzip compresse le cache KV pour supporter <strong>diverses requêtes futures</strong>.</li>
<li>[Dépendant du contexte] Obtention d'une <strong>réduction de 3 à 4× de la taille du cache KV</strong> et d'une <strong>diminution de 2× de la latence de décodage</strong>, avec une dégradation de performance minimale.</li>
<li>[Indépendant du contexte] Amélioration de la compression KV au niveau des têtes de type <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a>, en utilisant seulement <strong>quelques passages avant en moins d'une minute</strong> pour l'optimisation des scores d'importance au niveau des têtes (100x plus rapide).</li>
<li>Exécutez demo.py :</li>
</ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></p><h3>Benchmark dans un cadre indépendant des requêtes</h3>
<ul><li>Tâches : <a href="https://huggingface.co/datasets/rajpurkar/squad" target="_blank" rel="noopener noreferrer">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench" target="_blank" rel="noopener noreferrer">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294" target="_blank" rel="noopener noreferrer">GSM8K</a>. </li>
<li>Modèle : <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2.5-7B-Instruct-1M</a></li></p><p></ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800"></p><h2>Installation</h2>
Nous avons utilisé CUDA 12.1 et Python 3.10
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i</code></pre>
<ul><li>Pour utiliser la quantification <a href="https://github.com/mit-han-lab/omniserve" target="_blank" rel="noopener noreferrer">QServe</a>, veuillez suivre <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model" target="_blank" rel="noopener noreferrer"><code>./model/quant_model</code></a>.</li>
</ul><h3>Jeu de données</h3>
<ul><li>Veuillez télécharger le jeu de données SCBench pré-traité depuis <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link" target="_blank" rel="noopener noreferrer">Google Drive</a>.</li>
<li>Si vous avez téléchargé les fichiers décompressés, déplacez simplement le dossier scbench.</li>
</ul><pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  </code></pre></p><h2>Démarrage rapide</h2>
<pre><code class="language-python">from model import ModelKVzip</p><p>model = ModelKVzip("Qwen/Qwen2.5-7B-Instruct-1M")
context = "This is my basic profile. My name is Kim living in Seoul. My major is computer science."
queries = ["What is my name?", "Do I live in Seoul?"]</p><p>kv = model.prefill(context, load_score=False)  # préremplissage du cache KV + score d'importance
kv.prune(ratio=0.3)  # taux de compression, éviction de 70% du KV</p><p>for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # inférence efficace
    print(q, output)</code></pre>
<ul><li>Les modèles supportés sont listés dans <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, incluant <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Définissez <code>load_score=True</code> pour éliminer le surcoût de compression. Cela permet une éviction KV indépendante du contexte, avec un compromis sur le taux de compression à <code>ratio=0.6</code>.</li>
<li>Après la génération, les paires KV correspondant aux requêtes et aux tokens générés sont évincées sélectivement du cache pour un traitement ultérieur. Définissez <code>update_cache=True</code> pour permettre une inférence multi-tours, en conservant l'historique complet des interactions pendant toute l'inférence.</li></p><p></ul><h2>Profilage de la mémoire et du temps de calcul</h2>
<h3>Éviction dépendante du contexte</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3</code></pre>
<ul><li>Le code ci-dessus compare également les sorties générées avec le cache KV complet versus le cache KV élagué.</li>
<li>Pour un test rapide, utilisez <code>-d squad</code>. Pour un test en contexte long, utilisez <code>-d scbench_kv</code>.</li>
  <li>Noms des données disponibles : <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py" target="_blank" rel="noopener noreferrer"><code>data/load.py</code></a>.</li>
  <li>Noms des modèles disponibles : <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, ex. llama3.1-8b, qwen2.5-7b (ou Qwen/Qwen2.5-7B-Instruct-1M).</li>
<li>Nous adaptons le kernel CUDA de <a href="https://github.com/FFY0/AdaKV/tree/main" target="_blank" rel="noopener noreferrer">AdaKV</a>, supportant une allocation non uniforme du budget par tête.</li>
  <li>Actuellement, notre code ne comprend pas de kernel optimisé pour Gemma3 qui utilise un cache KV statique, donc le code n'apporte pas de gains d'efficacité réels. Cependant, la performance du modèle peut toujours être évaluée en utilisant une attention réduite avec sous-échantillonnage KV (<code>--kv_type retain</code>).</li></p><p>
</ul><h3>Éviction indépendante du contexte (sans surcoût de compression à l'exécution)</h3>
<ul><li>Utilisez le flag <code>--level head</code> avec <code>--ratio 0.6</code> (recommandé).</li>
  <li>Nous supprimons toutes les paires KV de contexte associées à une tête spécifique tout en conservant les paires KV du prompt système et des requêtes.</li>
  <li>Les scores de tête pré-calculés sont disponibles pour LLaMA3.1-8B et Qwen2.5-7/14B dans <code>./utils/head_score</code>.</li>
<li>Pour calculer les scores de tête pour d'autres modèles :</li>
  </ul><pre><code class="language-bash">  python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
  ``<code>
  <ul><li>Les résultats seront sauvegardés dans </code>./utils/head_score<code>.</li>
  <li>Si vous ciblez une tâche de codage, nous recommandons également d'exécuter la commande avec </code>-d scbench_repoqa<code>. Cela permet au modèle d'utiliser les scores de tête maximum issus des langues naturelles et du codage, ce qui améliore les performances.</li>
<li>Ces scores peuvent être intégrés sans difficulté avec le moteur d'inférence optimisé de <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a> en remplaçant leurs données de score de tête par les nôtres.</li></p><p>
</ul><h2>Évaluation</h2>
<ul><li>Pour générer des réponses de modèle avec des taux de compression KV allant de 0.1 à 1.0 :</li>
    </ul></code>`<code>bash
    python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
    </code>`<code> 
  <ul><li>Les résultats seront sauvegardés dans </code>./results/[data_name]<code>.</li>
  <li>Les jeux de données supportés sont listés dans </code>data/load.py<code>.</li>
<li>Pour calculer les métriques d'évaluation à partir des résultats générés :</li>
  </ul></code>`<code>bash
  python -B -m results.parse -m [model_name] -d [data_name]
  </code>`<code></p><h2>Application à de nouveaux modèles</h2>
Pour intégrer KVzip à un nouveau modèle, vous devrez mettre à jour les fichiers suivants :
<ul><li></code>attention/attn.py<code>  </li>
  </ul>Modifiez la logique du passage avant de l'attention selon les besoins. Dans certains cas, des mises à jour de kvcache.py et score.py peuvent aussi être nécessaires.
<ul><li></code>model/monkeypatch.py<code>  </li>
  </ul>Implémentez le monkey patch spécifique au modèle pour l'intégration.
<ul><li></code>model/template.py<code>   </li>
  </ul>Définissez le prompt système et les templates de formatage de chat du modèle.</p><h2>Citation</code></pre>bibtex</h2>
@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code>``</p><h2>Licence</h2>
Licence MIT

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>