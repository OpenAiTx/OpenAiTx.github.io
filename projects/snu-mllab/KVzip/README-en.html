<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>KVzip - Read KVzip documentation in English. This project has 70 stars on GitHub.</title>
    <meta name="description" content="Read KVzip documentation in English. This project has 70 stars on GitHub.">
    <meta name="keywords" content="KVzip, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "KVzip",
  "description": "Read KVzip documentation in English. This project has 70 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "snu-mllab"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 70
  },
  "url": "https://OpenAiTx.github.io/projects/snu-mllab/KVzip/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/snu-mllab/KVzip" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    KVzip
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 70 stars</span>
                <span class="language">English</span>
                <span>by snu-mllab</span>
            </div>
        </div>
        
        <div class="content">
            <h1>KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction</h1></p><p><a href="https://arxiv.org/abs/2505.23416" target="_blank" rel="noopener noreferrer">[Paper</a>] <a href="https://janghyun1230.github.io/kvzip/" target="_blank" rel="noopener noreferrer">[Blog</a>] </p><p><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/method.png" width="800"></p><h2>What's New?</h2>
<ul><li>KVzip compresses the KV cache to support <strong>diverse future queries</strong>.</li>
<li>[Context-dependent] Achieve a <strong>3–4× reduction in KV cache size</strong> and a <strong>2× decrease in decoding latency</strong>, with minimal performance degradation.</li>
<li>[Context-independent] Enhance <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a>-style head-level KV compression, using only <strong>a few forward passes within one minute</strong> for head-level importance-score optimization (100x faster).</li>
<li>Run demo.py:</li>
</ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/demo.png" width="800"></p><h3>Benchmarking on query-agnostic setting</h3>
<ul><li>Tasks: <a href="https://huggingface.co/datasets/rajpurkar/squad" target="_blank" rel="noopener noreferrer">SQuAD</a>, <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" target="_blank" rel="noopener noreferrer">NIAH</a>, <a href="https://github.com/microsoft/MInference/tree/main/scbench" target="_blank" rel="noopener noreferrer">SCBench</a>, <a href="https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294" target="_blank" rel="noopener noreferrer">GSM8K</a>. </li>
<li>Model: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct" target="_blank" rel="noopener noreferrer">Qwen2.5-7B-Instruct-1M</a></li></p><p></ul><img src="https://raw.githubusercontent.com/snu-mllab/KVzip/main/images/benchmark.png" width="800"></p><h2>Installation</h2>
We used CUDA 12.1 and Python 3.10
<pre><code class="language-bash">cd KVzip
pip install -r requirements.txt
pip install flash-attn==2.7.4.post1 --no-build-isolation
make i</code></pre>
<ul><li>To use <a href="https://github.com/mit-han-lab/omniserve" target="_blank" rel="noopener noreferrer">QServe</a> quantization, please follow <a href="https://github.com/snu-mllab/KVzip/tree/main/model/quant_model" target="_blank" rel="noopener noreferrer"><code>./model/quant_model</code></a>.</li>
</ul><h3>Dataset</h3>
<ul><li>Please download the preprocessed SCBench dataset from <a href="https://drive.google.com/file/d/1cqoR6pxxFcjFqvPZkuAmF-fBSAlAbjbN/view?usp=share_link" target="_blank" rel="noopener noreferrer">Google Drive</a>.</li>
<li>If you download the unzipped the files, simply move the scbench folder.</li>
</ul><pre><code class="language-bash">mv scbench.zip kvzip/data/
cd kvzip/data
unzip scbench.zip  </code></pre></p><h2>Quick Start</h2>
<pre><code class="language-python">from model import ModelKVzip</p><p>model = ModelKVzip("Qwen/Qwen2.5-7B-Instruct-1M")
context = "This is my basic profile. My name is Kim living in Seoul. My major is computer science."
queries = ["What is my name?", "Do I live in Seoul?"]</p><p>kv = model.prefill(context, load_score=False)  # prefill KV cache + importance scoring
kv.prune(ratio=0.3)  # compression ratio, evict 70% KV</p><p>for q in queries:
    query_ids = model.apply_template(q)
    output = model.generate(query_ids, kv=kv, update_cache=False)  # efficient inference
    print(q, output)</code></pre>
<ul><li>Supported models are listed in <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, including <strong>LLaMA3, Qwen2.5/3, Gemma3</strong>.</li>
<li>Set <code>load_score=True</code> to eliminate compression overhead. This enables context-independent KV eviction, with a trade-off in compression ratio of <code>ratio=0.6</code>.</li>
<li>After generation, KV pairs corresponding to the queries and generated tokens are selectively evicted from the cache for further processing. Set <code>update_cache=True</code> to enable multi-turn inference, retaining full interaction histories throughout the inference. </li></p><p></ul><h2>Profiling Memory and Computation Time</h2>
<h3>Context-dependent eviction</h3>
<pre><code class="language-bash">python -B test.py -m [model_name] -d [data_name] --kv_type evict --ratio 0.3</code></pre>
<ul><li>The code above also compares outputs generated with full versus pruned KV caches.</li>
<li>To quick test, use <code>-d squad</code>. For long-context testing, use <code>-d scbench_kv</code>.</li>
  <li>Available data names: <a href="https://github.com/snu-mllab/KVzip/blob/main/data/load.py" target="_blank" rel="noopener noreferrer"><code>data/load.py</code></a>.</li>
  <li>Available model names: <a href="https://github.com/snu-mllab/KVzip/blob/main/model/load.py" target="_blank" rel="noopener noreferrer"><code>model/load.py</code></a>, e.g., llama3.1-8b, qwen2.5-7b (or Qwen/Qwen2.5-7B-Instruct-1M).</li>
<li>We adapt CUDA kernel from <a href="https://github.com/FFY0/AdaKV/tree/main" target="_blank" rel="noopener noreferrer">AdaKV</a>, supporting non-uniform head budget allocation.</li>
  <li>Currently, our code lacks an optimized kernel for Gemma3 which uses static KV cache, so the code does not yield actual efficiency gains. However, model performance can still be evaluated using reduced attention with KV subsampling (<code>--kv_type retain</code>).</li></p><p>
</ul><h3>Context-independent eviction (no runtime compression overhead)</h3>
<ul><li>Use the <code>--level head</code> flag with <code>--ratio 0.6</code> (recommended).</li>
  <li>We remove all context KV pairs associated with a specific head while retaining system prompt and query KV pairs.</li>
  <li>Precomputed head scores are available for LLaMA3.1-8B and Qwen2.5-7/14B in <code>./utils/head_score</code>.</li>
<li>To compute head scores for other models:</li>
  </ul><pre><code class="language-bash">  python -B test.py -m [model_name] -d scbench_qa_eng --save_head_score
  ``<code>
  <ul><li>Results will be saved in </code>./utils/head_score<code>.</li>
  <li>If targeting a coding task, we recommend additionally running the command with </code>-d scbench_repoqa<code>. This allows the model to use the max head scores from both natural and coding languages, which improves performance.</li>
<li>These scores can be seamlessly integrated with <a href="https://github.com/mit-han-lab/duo-attention" target="_blank" rel="noopener noreferrer">DuoAttention</a>'s optimized inference engine by replacing their head score data with ours.</li></p><p>
</ul><h2>Evaluation</h2>
<ul><li>To generate model responses with KV compression ratios ranging from 0.1 to 1.0:</li>
    </ul></code>`<code>bash
    python -B eval.py -m [model_name] -d [data_name] --kv_type retain --num 100
    </code>`<code> 
  <ul><li>Results will be saved in </code>./results/[data_name]<code>.</li>
  <li>Supported datasets are listed in </code>data/load.py<code>.</li>
<li>To compute evaluation metrics from generated results:</li>
  </ul></code>`<code>bash
  python -B -m results.parse -m [model_name] -d [data_name]
  </code>`<code></p><h2>Applying to New Models</h2>
To integrate KVzip for a new model, you will need to update the following files:
<ul><li></code>attention/attn.py<code>  </li>
  </ul>Modify the attention forward pass logic as needed. In certain cases, updates to kvcache.py and score.py may also be required.
<ul><li></code>model/monkeypatch.py<code>  </li>
  </ul>Implement model-specific monkey patching for integration.
<ul><li></code>model/template.py<code>   </li>
  </ul>Define the model's system prompt and chat formatting templates.</p><h2>Citation</code></pre>bibtex</h2>
@article{kim2025kvzip,
        title={KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction},
        author={Kim, Jang-Hyun and Kim, Jinuk and Kwon, Sangwoo and Lee, Jae W and Yun, Sangdoo and Song, Hyun Oh},
        journal={arXiv preprint arXiv:2505.23416},
        year={2025}
}
</code>``</p><h2>License</h2>
MIT License

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-11

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/snu-mllab/KVzip/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>