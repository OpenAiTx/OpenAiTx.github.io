<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Polish. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Polish. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Polish, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Polish. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-pl.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Polish</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Prywatna, lokalna alternatywa dla Manus</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>W pełni <strong>lokalna alternatywa dla Manus AI</strong> – ten asystent AI z obsługą głosu autonomicznie przegląda internet, pisze kod i planuje zadania, zachowując wszystkie dane na Twoim urządzeniu. Dostosowany do lokalnych modeli rozumowania, działa całkowicie na Twoim sprzęcie, zapewniając pełną prywatność i brak zależności od chmury.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Odwiedź AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Dlaczego AgenticSeek?</h3></p><ul><li>🔒 W pełni lokalny i prywatny – Wszystko działa na Twoim komputerze — bez chmury, bez udostępniania danych. Twoje pliki, rozmowy i wyszukiwania pozostają prywatne.</li></p><p><li>🌐 Inteligentne przeglądanie sieci – AgenticSeek może samodzielnie przeglądać internet — wyszukiwać, czytać, wyodrębniać informacje, wypełniać formularze — wszystko bez użycia rąk.</li></p><p><li>💻 Autonomiczny asystent kodowania – Potrzebujesz kodu? Może pisać, debugować i uruchamiać programy w Pythonie, C, Go, Javie i innych — wszystko bez nadzoru.</li></p><p><li>🧠 Inteligentny wybór agenta – Zadajesz pytanie, a on sam wybiera najlepszego agenta do zadania. Jak zespół ekspertów gotowych do pomocy.</li></p><p><li>📋 Planuje i realizuje złożone zadania – Od planowania podróży po skomplikowane projekty — potrafi rozbić duże zadania na kroki i wykonać je, korzystając z wielu agentów AI.</li></p><p><li>🎙️ Obsługa głosu – Czysty, szybki, futurystyczny głos oraz zamiana mowy na tekst pozwala rozmawiać z nim jak z własnym AI z filmu science fiction. (W trakcie opracowania)</li></p><p></ul><h3><strong>Demo</strong></h3></p><blockquote><em>Czy możesz wyszukać projekt agenticSeek, dowiedzieć się, jakie są wymagane umiejętności, następnie otworzyć CV_candidates.zip i powiedzieć mi, które najlepiej pasują do projektu</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Zastrzeżenie: To demo, w tym wszystkie pojawiające się pliki (np.: CV_candidates.zip), są całkowicie fikcyjne. Nie jesteśmy korporacją, szukamy współtwórców open-source, a nie kandydatów.</p><blockquote>🛠⚠️️ <strong>Aktywnie rozwijane</strong></blockquote></p><blockquote>🙏 Ten projekt powstał jako poboczny i nie posiada żadnej mapy drogowej ani finansowania. Rozrósł się znacznie bardziej, niż się spodziewałem, kończąc na GitHub Trending. Wszelkie wkłady, opinie i cierpliwość są bardzo cenione.</blockquote></p><h2>Wymagania wstępne</h2></p><p>Przed rozpoczęciem upewnij się, że masz zainstalowane następujące oprogramowanie:</p><ul><li>  <strong>Git:</strong> Do klonowania repozytorium. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Pobierz Git</a></li>
<li>  <strong>Python 3.10.x:</strong> Zdecydowanie zalecamy użycie wersji Python 3.10.x. Używanie innych wersji może prowadzić do błędów zależności. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Pobierz Python 3.10</a> (wybierz wersję 3.10.x).</li>
<li>  <strong>Docker Engine i Docker Compose:</strong> Do uruchamiania usług takich jak SearxNG.</li>
    <li>  Zainstaluj Docker Desktop (zawiera Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Alternatywnie, zainstaluj Docker Engine i Docker Compose oddzielnie na Linuksie: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (upewnij się, że instalujesz Compose V2, np. <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Sklonuj repozytorium i skonfiguruj</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Zmień zawartość pliku .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>Zaktualizuj plik <code>.env</code> własnymi wartościami, jeśli to konieczne:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>REDIS_BASE_URL</strong>: Pozostaw bez zmian</li>
<li><strong>WORK_DIR</strong>: Ścieżka do Twojego katalogu roboczego na komputerze lokalnym. AgenticSeek będzie mógł czytać i pracować na tych plikach.</li>
<li><strong>OLLAMA_PORT</strong>: Numer portu dla usługi Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Numer portu dla LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Port dla dodatkowej, własnej usługi LLM.</li></p><p></ul><strong>Klucze API są całkowicie opcjonalne dla użytkowników, którzy zdecydują się uruchamiać LLM lokalnie. Jest to główny cel tego projektu. Pozostaw puste, jeśli masz wystarczająco wydajny sprzęt</strong></p><h3>3. <strong>Uruchom Docker</strong></h3></p><p>Upewnij się, że Docker jest zainstalowany i działa na Twoim systemie. Możesz uruchomić Dockera następującymi poleceniami:</p><ul><li><strong>Na Linux/macOS:</strong>  </li>
    </ul>Otwórz terminal i wpisz:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Lub uruchom Docker Desktop z menu aplikacji, jeśli jest zainstalowany.</p><ul><li><strong>Na Windows:</strong>  </li>
    </ul>Uruchom Docker Desktop z menu Start.</p><p>Możesz sprawdzić, czy Docker działa, wykonując:</code></pre>sh
docker info
<pre><code class="language-">Jeśli zobaczysz informacje o instalacji Dockera, wszystko działa poprawnie.</p><p>Zobacz tabelę <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Dostawcy lokalni</a> poniżej podsumowanie.</p><p>Następny krok: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Uruchom AgenticSeek lokalnie</a></p><p><em>Zobacz sekcję <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Rozwiązywanie problemów</a>, jeśli masz problemy.</em>
<em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>.</em>
<em>Szczegółowe wyjaśnienia pliku </code>config.ini<code> w sekcji <a href="#config" target="_blank" rel="noopener noreferrer">Konfiguracja</a>.</em></p><hr></p><h2>Konfiguracja do lokalnego uruchamiania LLM</h2></p><p><strong>Wymagania sprzętowe:</strong></p><p>Aby uruchamiać LLM lokalnie, potrzebujesz odpowiedniego sprzętu. Minimalnie wymagana jest karta GPU zdolna do obsługi Magistral, Qwen lub Deepseek 14B. Szczegółowe zalecenia dotyczące modeli/wydajności znajdziesz w FAQ.</p><p><strong>Uruchom lokalnego dostawcę</strong>  </p><p>Uruchom swojego lokalnego dostawcę, np. ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
Poniżej znajdziesz listę obsługiwanych lokalnych dostawców.</p><p><strong>Zaktualizuj config.ini</strong></p><p>Zmień plik config.ini, ustawiając provider_name na obsługiwanego dostawcę oraz provider_model na model LLM obsługiwany przez dostawcę. Zalecamy modele rozumowania takie jak <em>Magistral</em> lub <em>Deepseek</em>.</p><p>Szczegóły sprzętowe znajdziesz w <strong>FAQ</strong> na końcu README.
</code></pre>sh
[MAIN]
is_local = True # Czy uruchamiasz lokalnie lub zdalnie.
provider_name = ollama # lub lm-studio, openai, itd.
provider_model = deepseek-r1:14b # wybierz model odpowiedni do swojego sprzętu
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nazwa Twojej AI
recover_last_session = True # czy odzyskiwać ostatnią sesję
save_session = True # czy zapamiętywać bieżącą sesję
speak = False # zamiana tekstu na mowę
listen = False # zamiana mowy na tekst, tylko CLI, eksperymentalne
jarvis_personality = False # czy używać bardziej "Jarvisowej" osobowości (eksperymentalne)
languages = en zh # Lista języków, zamiana tekstu na mowę domyślnie w pierwszym języku z listy
[BROWSER]
headless_browser = True # pozostaw bez zmian, chyba że używasz CLI na hoście
stealth_mode = True # Użyj selenium nie do wykrycia, by zmniejszyć wykrywanie przeglądarki
<pre><code class="language-">
<strong>Uwaga</strong>:</p><ul><li>Plik </code>config.ini<code> nie obsługuje komentarzy.</li>
</ul>Nie kopiuj i nie wklejaj przykładowej konfiguracji bezpośrednio, ponieważ komentarze spowodują błędy. Zamiast tego ręcznie zmodyfikuj plik </code>config.ini<code>, wpisując wybrane ustawienia bez komentarzy.</p><ul><li><em>NIE</em> ustawiaj provider_name na </code>openai<code>, jeśli korzystasz z LM-studio do lokalnych LLM. Ustaw na </code>lm-studio<code>.</li></p><p><li>Niektórzy dostawcy (np. lm-studio) wymagają dodania </code>http://<code> przed adresem IP. Przykład: </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Lista lokalnych dostawców</strong></p><p>| Dostawca     | Lokalny? | Opis                                                                 |
|--------------|----------|----------------------------------------------------------------------|
| ollama       | Tak      | Uruchamiaj LLM lokalnie z łatwością używając ollama jako dostawcy     |
| lm-studio    | Tak      | Lokalny LLM przez LM studio (ustaw </code>provider_name<code> na </code>lm-studio<code>)    |
| openai       | Tak      | Użyj kompatybilnego API openai (np. serwer llama.cpp)                 |</p><p>Następny krok: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Uruchom usługi i AgenticSeek</a></p><p><em>Zobacz sekcję <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Rozwiązywanie problemów</a>, jeśli masz problemy.</em>
<em>Jeśli Twój sprzęt nie pozwala na lokalne uruchomienie LLM, zobacz <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>.</em>
<em>Szczegółowe wyjaśnienia pliku </code>config.ini<code> w sekcji <a href="#config" target="_blank" rel="noopener noreferrer">Konfiguracja</a>.</em></p><h2>Konfiguracja do uruchamiania przez API</h2></p><p>To ustawienie wykorzystuje zewnętrznych, chmurowych dostawców LLM. Potrzebujesz klucza API wybranego serwisu.</p><p><strong>1. Wybierz dostawcę API i uzyskaj klucz:</strong></p><p>Zobacz <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Listę dostawców API</a> poniżej. Odwiedź ich strony, zarejestruj się i pobierz klucz API.</p><p><strong>2. Ustaw swój klucz API jako zmienną środowiskową:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Otwórz terminal i użyj polecenia </code>export<code>. Najlepiej dodać to do pliku profilu swojej powłoki (np. </code>~/.bashrc<code>, </code>~/.zshrc<code>), aby było trwałe.
    </code>`<code>sh
    export PROVIDER_API_KEY="tutaj_twój_klucz_api"
    # Zamień PROVIDER_API_KEY na konkretną zmienną, np. OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Przykład dla TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Wiersz poleceń (Tymczasowo dla bieżącej sesji):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=twoj_klucz_api_tutaj
    </code>`<code>
<ul><li>  <strong>PowerShell (Tymczasowo dla bieżącej sesji):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="twoj_klucz_api_tutaj"
    </code>`<code>
<ul><li>  <strong>Na stałe:</strong> Wyszukaj "zmienne środowiskowe" w pasku wyszukiwania Windows, kliknij "Edytuj zmienne środowiskowe systemu", a następnie kliknij przycisk "Zmienne środowiskowe...". Dodaj nową zmienną użytkownika z odpowiednią nazwą (np. </code>OPENAI_API_KEY<code>) i Twoim kluczem jako wartością.</li></p><p></ul><em>(Zobacz FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Jak ustawić klucze API?</a> po więcej szczegółów).</em></p><p>
<strong>3. Zaktualizuj </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Lub google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Lub gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 itd.
provider_server_address = # Zazwyczaj ignorowane lub można pozostawić puste, gdy is_local = False dla większości API
<h1>... inne ustawienia ...</h1>
<pre><code class="language-"><em>Uwaga:</em> Upewnij się, że w wartościach </code>config.ini<code> nie ma spacji na końcu linii.</p><p><strong>Lista dostawców API</strong></p><p>| Dostawca      | </code>provider_name<code> | Lokalny? | Opis                                              | Link do klucza API (przykłady)              |
|---------------|-----------------|----------|---------------------------------------------------|---------------------------------------------|
| OpenAI        | </code>openai<code>        | Nie      | Użyj modeli ChatGPT przez API OpenAI.             | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini | </code>google<code>        | Nie      | Użyj modeli Google Gemini przez Google AI Studio. | <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek      | </code>deepseek<code>      | Nie      | Użyj modeli Deepseek przez ich API.               | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face  | </code>huggingface<code>   | Nie      | Użyj modeli z Hugging Face Inference API.         | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI    | </code>togetherAI<code>    | Nie      | Użyj różnych modeli open-source przez TogetherAI. | <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Uwaga:</em>
<ul><li>  Odradzamy używanie </code>gpt-4o<code> lub innych modeli OpenAI do złożonego przeglądania internetu i planowania zadań, ponieważ aktualne optymalizacje promptów są skierowane pod modele takie jak Deepseek.</li>
<li>  Zadania związane z kodowaniem/bash mogą napotkać problemy z Gemini, gdyż może nie przestrzegać ściśle formatowania promptów zoptymalizowanych pod Deepseek.</li>
<li>  </code>provider_server_address<code> w pliku </code>config.ini<code> jest zazwyczaj nieużywane, gdy </code>is_local = False<code>, ponieważ adres API jest zwykle zapisany na stałe w odpowiedniej bibliotece dostawcy.</li></p><p></ul>Następny krok: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Uruchom usługi i AgenticSeek</a></p><p><em>Zobacz sekcję <strong>Znane problemy</strong> jeśli napotkasz problemy</em></p><p><em>Zobacz sekcję <strong>Config</strong> po szczegółowe wyjaśnienie pliku konfiguracyjnego.</em></p><hr></p><h2>Uruchom usługi i AgenticSeek</h2></p><p>Domyślnie AgenticSeek uruchamiany jest w całości w dockerze.</p><p>Uruchom wymagane usługi. Rozpocznie to wszystkie usługi z pliku docker-compose.yml, w tym:
    <ul><li>searxng</li>
    <li>redis (wymagany przez searxng)</li>
    <li>frontend</li>
    <li>backend (jeśli używasz </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
<pre><code class="language-">
<strong>Uwaga:</strong> Ten krok pobierze i załaduje wszystkie obrazy Dockera, co może zająć do 30 minut. Po uruchomieniu usług poczekaj, aż usługa backend będzie w pełni uruchomiona (powinieneś zobaczyć <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> w logu), zanim wyślesz jakiekolwiek wiadomości. Usługi backend mogą potrzebować do 5 minut na pierwsze uruchomienie.</p><p>Przejdź do </code>http://localhost:3000/<code> i powinieneś zobaczyć interfejs webowy.</p><p><em>Rozwiązywanie problemów z uruchomieniem usług:</em> Jeśli te skrypty się nie powiodą, upewnij się, że Docker Engine jest uruchomiony oraz Docker Compose (V2, </code>docker compose<code>) jest poprawnie zainstalowany. Sprawdź komunikaty o błędach w terminalu. Zobacz <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Pomoc! Wystąpił błąd podczas uruchamiania AgenticSeek lub jego skryptów.</a></p><p><strong>Opcjonalnie:</strong> Uruchom na hoście (tryb CLI):</p><p>Aby uruchomić w interfejsie CLI musisz zainstalować pakiet na hoście:
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Uruchom usługi:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
<pre><code class="language-">
Użyj CLI: </code>python3 cli.py<code></p><hr></p><h2>Użytkowanie</h2></p><p>Upewnij się, że usługi są uruchomione poprzez </code>./start_services.sh full<code> i przejdź do </code>localhost:3000<code> aby uzyskać dostęp do interfejsu webowego.</p><p>Możesz także użyć funkcji rozpoznawania mowy ustawiając </code>listen = True<code> w konfiguracji. Tylko w trybie CLI.</p><p>Aby zakończyć, po prostu powiedz/napisz </code>goodbye<code>.</p><p>Oto przykłady użycia:</p><blockquote><em>Stwórz grę w węża w pythonie!</em></blockquote></p><blockquote><em>Wyszukaj w internecie najlepsze kawiarnie w Rennes, Francja, i zapisz listę trzech z ich adresami w pliku rennes_cafes.txt.</em></blockquote></p><blockquote><em>Napisz program w Go obliczający silnię liczby, zapisz go jako factorial.go w swoim katalogu roboczym</em></blockquote></p><blockquote><em>Wyszukaj w folderze summer_pictures wszystkie pliki JPG, zmień ich nazwy na dzisiejszą datę i zapisz listę zmienionych plików w photos_list.txt</em></blockquote></p><blockquote><em>Wyszukaj online popularne filmy sci-fi z 2024 i wybierz trzy do obejrzenia dziś wieczorem. Zapisz listę w movie_night.txt.</em></blockquote></p><blockquote><em>Wyszukaj w internecie najnowsze artykuły o AI z 2025, wybierz trzy, i napisz skrypt w Pythonie, który pobierze ich tytuły i podsumowania. Zapisz skrypt jako news_scraper.py, a podsumowania w ai_news.txt w /home/projects</em></blockquote></p><blockquote><em>W piątek wyszukaj w internecie darmowe API do notowań giełdowych, zarejestruj się jako supersuper7434567@gmail.com, potem napisz skrypt w Pythonie pobierający dzienne ceny Tesli i zapisz wyniki w stock_prices.csv</em></blockquote></p><p><em>Zwróć uwagę, że możliwości wypełniania formularzy są nadal eksperymentalne i mogą nie działać poprawnie.</em></p><p>Po wpisaniu zapytania AgenticSeek przydzieli najlepszego agenta do zadania.</p><p>Ponieważ to wczesny prototyp, system routingu agentów może nie zawsze przydzielić właściwego agenta na podstawie zapytania.</p><p>Dlatego powinieneś być bardzo precyzyjny w tym, czego oczekujesz i jak AI ma postępować — np. jeśli chcesz, by wyszukał informacje w internecie, nie pisz:</p><p></code>Czy znasz jakieś dobre kraje na podróż solo?<code></p><p>Zamiast tego, zapytaj:</p><p></code>Wyszukaj w internecie i dowiedz się, które kraje są najlepsze do podróżowania solo<code></p><hr></p><h2><strong>Konfiguracja uruchomienia LLM na własnym serwerze</strong></h2></p><p>Jeśli masz wydajny komputer lub serwer, z którego możesz korzystać, ale chcesz używać go z laptopa, masz możliwość uruchomienia LLM na zdalnym serwerze za pomocą naszego własnego serwera llm.</p><p>Na swoim "serwerze", który będzie uruchamiał model AI, pobierz adres IP
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokalny IP
curl https://ipinfo.io/ip # publiczny IP
<pre><code class="language-">
Uwaga: Dla Windows lub macOS użyj odpowiednio ipconfig lub ifconfig, aby znaleźć adres IP.</p><p>Sklonuj repozytorium i wejdź do folderu </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Zainstaluj wymagania serwera:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Uruchom skrypt serwera.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Masz wybór pomiędzy </code>ollama<code> i </code>llamacpp<code> jako usługą LLM.</p><p>
Teraz na swoim komputerze osobistym:</p><p>Zmień plik </code>config.ini<code>, ustawiając </code>provider_name<code> na </code>server<code> oraz </code>provider_model<code> na </code>deepseek-r1:xxb<code>.
Ustaw </code>provider_server_address<code> na adres IP maszyny, która będzie uruchamiała model.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>Następny krok: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Uruchom usługi i AgenticSeek</a>  </p><hr></p><h2>Rozpoznawanie mowy (Speech to Text)</h2></p><p>Uwaga: rozpoznawanie mowy działa obecnie tylko w trybie CLI.</p><p>Pamiętaj, że obecnie rozpoznawanie mowy działa tylko w języku angielskim.</p><p>Funkcjonalność rozpoznawania mowy jest domyślnie wyłączona. Aby ją włączyć, ustaw opcję listen na True w pliku config.ini:
</code></pre>
listen = True
<pre><code class="language-">
Po włączeniu, funkcja rozpoznawania mowy czeka na słowo kluczowe (nazwę agenta), zanim zacznie przetwarzać Twoje polecenie. Możesz dostosować nazwę agenta, aktualizując wartość </code>agent_name<code> w pliku <em>config.ini</em>:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Dla optymalnego rozpoznawania zalecamy użycie popularnego angielskiego imienia, takiego jak "John" lub "Emma" jako nazwy agenta</p><p>Gdy zobaczysz, że transkrypcja zaczyna się pojawiać, wypowiedz głośno imię agenta, aby go obudzić (np. "Friday").</p><p>Wypowiedz swoje zapytanie wyraźnie.</p><p>Zakończ swoją prośbę frazą potwierdzającą, aby zasygnalizować systemowi, że ma przejść dalej. Przykłady fraz potwierdzających to:</code></pre>
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
<pre><code class="language-">
<h2>Konfiguracja</h2></p><p>Przykładowa konfiguracja:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Przykład dla Ollama; użyj http://127.0.0.1:1234 dla LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Lista języków dla TTS i potencjalnie routingu.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Wyjaśnienie ustawień </code>config.ini<code></strong>:</p><ul><li>  <strong>Sekcja </code>[MAIN]<code>:</strong></li>
    <li>  </code>is_local<code>: </code>True<code> jeśli korzystasz z lokalnego dostawcy LLM (Ollama, LM-Studio, lokalny serwer zgodny z OpenAI) lub opcji serwera self-hosted. </code>False<code> jeśli używasz API w chmurze (OpenAI, Google, itd.).</li>
    <li>  </code>provider_name<code>: Określa dostawcę LLM.</li>
        <li>  Opcje lokalne: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (dla lokalnych serwerów zgodnych z OpenAI), </code>server<code> (dla własnego serwera self-hosted).</li>
        <li>  Opcje API: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: Konkretna nazwa lub ID modelu dla wybranego dostawcy (np. </code>deepseekcoder:6.7b<code> dla Ollama, </code>gpt-3.5-turbo<code> dla OpenAI API, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> dla TogetherAI).</li>
    <li>  </code>provider_server_address<code>: Adres twojego dostawcy LLM.</li>
        <li>  Dla dostawców lokalnych: np. </code>http://127.0.0.1:11434<code> dla Ollama, </code>http://127.0.0.1:1234<code> dla LM-Studio.</li>
        <li>  Dla typu </code>server<code>: Adres twojego własnego serwera LLM (np. </code>http://your_server_ip:3333<code>).</li>
        <li>  Dla API w chmurze (</code>is_local = False<code>): Zazwyczaj ignorowane lub można zostawić puste, ponieważ endpoint API jest zazwyczaj obsługiwany przez bibliotekę kliencką.</li>
    <li>  </code>agent_name<code>: Nazwa asystenta AI (np. Friday). Używana jako słowo wybudzające dla rozpoznawania mowy, jeśli jest włączone.</li>
    <li>  </code>recover_last_session<code>: </code>True<code> aby przywrócić stan poprzedniej sesji, </code>False<code> aby rozpocząć od nowa.</li>
    <li>  </code>save_session<code>: </code>True<code> aby zapisać stan bieżącej sesji do ewentualnego przywrócenia, </code>False<code> w przeciwnym wypadku.</li>
    <li>  </code>speak<code>: </code>True<code> aby włączyć syntezę mowy (TTS), </code>False<code> aby wyłączyć.</li>
    <li>  </code>listen<code>: </code>True<code> aby włączyć rozpoznawanie mowy (STT) w trybie CLI, </code>False<code> aby wyłączyć.</li>
    <li>  </code>work_dir<code>: <strong>Istotne:</strong> Katalog, w którym AgenticSeek będzie czytać/zapisywać pliki. <strong>Upewnij się, że ta ścieżka jest poprawna i dostępna na twoim systemie.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> aby użyć bardziej "Jarvisowego" prompta systemowego (eksperymentalne), </code>False<code> dla standardowego prompta.</li>
    <li>  </code>languages<code>: Lista języków rozdzielona przecinkami (np. </code>en, zh, fr<code>). Używana do wyboru głosu TTS (domyślnie pierwszy) i może pomagać routerowi LLM. Unikaj zbyt wielu lub bardzo podobnych języków dla efektywności routera.</li>
<li>  <strong>Sekcja </code>[BROWSER]<code>:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code> aby uruchomić zautomatyzowaną przeglądarkę bez widocznego okna (zalecane do interfejsu webowego lub użytku nieinteraktywnego). </code>False<code> aby pokazać okno przeglądarki (przydatne w trybie CLI lub debugowaniu).</li>
    <li>  </code>stealth_mode<code>: </code>True<code> aby włączyć środki utrudniające wykrycie automatyzacji przeglądarki. Może wymagać ręcznej instalacji rozszerzeń, takich jak anticaptcha.</li></p><p></ul>Ta sekcja podsumowuje obsługiwane typy dostawców LLM. Skonfiguruj je w </code>config.ini<code>.</p><p><strong>Dostawcy lokalni (uruchamiani na twoim sprzęcie):</strong></p><p>| Nazwa dostawcy w </code>config.ini<code> | </code>is_local<code> | Opis                                                                       | Sekcja konfiguracji                                                 |
|-------------------------------|------------|----------------------------------------------------------------------------|---------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Użyj Ollama do serwowania lokalnych LLM.                                   | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Konfiguracja lokalnego LLM</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Użyj LM-Studio do serwowania lokalnych LLM.                                | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Konfiguracja lokalnego LLM</a> |
| </code>openai<code> (dla lokalnego serwera) | </code>True<code> | Połącz z lokalnym serwerem udostępniającym API zgodne z OpenAI (np. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Konfiguracja lokalnego LLM</a> |
| </code>server<code>                      | </code>False<code>    | Połącz z własnym serwerem LLM AgenticSeek uruchomionym na innej maszynie.  | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Konfiguracja własnego serwera LLM</a> |</p><p><strong>Dostawcy API (chmurowi):</strong></p><p>| Nazwa dostawcy w </code>config.ini<code> | </code>is_local<code> | Opis                                            | Sekcja konfiguracji                                    |
|-------------------------------|------------|--------------------------------------------------|--------------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Użyj oficjalnego API OpenAI (np. GPT-3.5, GPT-4).| <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>         |
| </code>google<code>                      | </code>False<code>    | Użyj modeli Gemini Google przez API.             | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>         |
| </code>deepseek<code>                    | </code>False<code>    | Użyj oficjalnego API Deepseek.                   | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>         |
| </code>huggingface<code>                 | </code>False<code>    | Użyj Hugging Face Inference API.                 | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>         |
| </code>togetherAI<code>                  | </code>False<code>    | Użyj API TogetherAI dla różnych modeli open.     | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Konfiguracja z API</a>         |</p><hr>
<h2>Rozwiązywanie problemów</h2></p><p>Jeśli napotkasz problemy, ta sekcja zawiera wskazówki.</p><h1>Znane problemy</h1></p><h2>Problemy z ChromeDriver</h2></p><p><strong>Przykład błędu:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Przyczyna:</strong> Zainstalowana wersja ChromeDriver jest niezgodna z wersją przeglądarki Google Chrome.</li>
<li>  <strong>Rozwiązanie:</strong></li>
    <li> <strong>Sprawdź wersję Chrome:</strong> Otwórz Google Chrome, przejdź do </code>Ustawienia > O Google Chrome<code>, aby znaleźć wersję (np. "Wersja 120.0.6099.110").</li>
    <li> <strong>Pobierz pasującą wersję ChromeDriver:</strong></li>
        <li>  Dla Chrome w wersji 115 i nowszych: Przejdź do <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Znajdź kanał "stable" i pobierz ChromeDriver dla swojego systemu operacyjnego, zgodny z główną wersją przeglądarki Chrome.</li>
        <li>  Dla starszych wersji (rzadziej spotykane): Możesz je znaleźć na stronie <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  Poniższy obrazek przedstawia przykład ze strony CfT:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page">
    <ul><li> <strong>Zainstaluj ChromeDriver:</strong></li>
        <li>  Upewnij się, że pobrany plik </code>chromedriver<code> (lub </code>chromedriver.exe<code> w Windows) znajduje się w katalogu, który jest na liście PATH w systemie (np. </code>/usr/local/bin<code> w Linux/macOS lub własny folder skryptów dodany do PATH w Windows).</li>
        <li>  Alternatywnie umieść go w katalogu głównym projektu </code>agenticSeek<code>.</li>
        <li>  Upewnij się, że sterownik ma prawa wykonywania (np. </code>chmod +x chromedriver<code> w Linux/macOS).</li>
    <li> Zajrzyj do sekcji <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">ChromeDriver Installation</a> w głównym przewodniku instalacji po więcej szczegółów.</li></p><p></ul>Jeśli ta sekcja jest niepełna lub napotkasz inne problemy z ChromeDriver, rozważ sprawdzenie istniejących <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a> lub zgłoszenie nowego problemu.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>To dzieje się, jeśli wersje przeglądarki i chromedrivera się nie zgadzają.</p><p>Musisz przejść do pobrania najnowszej wersji:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Jeśli używasz Chrome w wersji 115 lub nowszej przejdź do:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>I pobierz wersję chromedrivera odpowiednią dla twojego systemu operacyjnego.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Jeśli ta sekcja jest niepełna, zgłoś problem.</p><h2>Problemy z connection adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Uwaga: port może być inny)
<pre><code class="language-">
<ul><li>  <strong>Przyczyna:</strong> <code>provider_server_address</code> w <code>config.ini</code> dla <code>lm-studio</code> (lub innych podobnych lokalnych serwerów zgodnych z OpenAI) nie zawiera prefiksu <code>http://</code> lub wskazuje zły port.</li>
<li>  <strong>Rozwiązanie:</strong></li>
    <li>  Upewnij się, że adres zawiera <code>http://</code>. LM-Studio domyślnie to <code>http://127.0.0.1:1234</code>.</li>
    <li>  Popraw <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (lub twój faktyczny port serwera LM-Studio).</li></p><p></ul><h2>Nie podano podstawowego adresu URL SearxNG</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>Q: Jakiego sprzętu potrzebuję?</strong>  </p><p>| Rozmiar modelu | GPU  | Komentarz                                                   |
|----------------|------|------------------------------------------------------------|
| 7B             | 8GB Vram | ⚠️ Niezalecane. Słaba wydajność, częste halucynacje, agenci planujący prawdopodobnie zawiodą. |
| 14B            | 12 GB VRAM (np. RTX 3060) | ✅ Używalny do prostych zadań. Może mieć trudności z przeglądaniem sieci i zadaniami planistycznymi. |
| 32B            | 24+ GB VRAM (np. RTX 4090) | 🚀 Sukces w większości zadań, nadal może mieć trudności z planowaniem zadań |
| 70B+           | 48+ GB Vram | 💪 Doskonały. Zalecany do zaawansowanych zastosowań.      |</p><p><strong>Q: Otrzymuję błąd, co mam zrobić?</strong>  </p><p>Upewnij się, że lokalny serwer działa (</code>ollama serve<code>), twoje </code>config.ini` pasuje do wybranego dostawcy, a zależności są zainstalowane. Jeśli żadne nie działa, śmiało zgłoś problem.</p><p><strong>Q: Czy to naprawdę może działać w 100% lokalnie?</strong>  </p><p>Tak, z dostawcami Ollama, lm-studio lub server wszystkie modele STT, LLM i TTS działają lokalnie. Opcje nielokalne (OpenAI lub inne API) są opcjonalne.</p><p><strong>Q: Dlaczego mam używać AgenticSeek, skoro mam Manus?</strong></p><p>W przeciwieństwie do Manus, AgenticSeek stawia na niezależność od zewnętrznych systemów, dając ci większą kontrolę, prywatność i brak kosztów API.</p><p><strong>Q: Kto stoi za projektem?</strong></p><p>Projekt został stworzony przeze mnie, wraz z dwoma przyjaciółmi, którzy są maintainerami i kontrybutorami z open-source na GitHubie. Jesteśmy po prostu grupą pasjonatów, a nie startupem ani organizacją.</p><p>Każde konto AgenticSeek na X poza moim osobistym kontem (https://x.com/Martin993886460) jest podszywaniem się.</p><h2>Współtwórz</h2></p><p>Szukamy developerów do rozwoju AgenticSeek! Zajrzyj do otwartych zgłoszeń lub dyskusji.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Przewodnik kontrybucji</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Maintainerzy:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | czas paryski </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | czas tajpejski </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | czas tajpejski </p><h2>Specjalne podziękowania:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> oraz <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> za pomoc w dockerowaniu backendu</p><h2>Sponsorzy:</h2></p><p>Sponsorzy z miesięczną wpłatą 5$ lub więcej pojawią się tutaj:
<ul><li><strong>tatra-labs</strong></li>
</ul>It seems you haven't provided the text of the technical document that needs to be translated. Please provide the content you'd like translated (Part 4 of 4), and I'll proceed with the translation while preserving the original Markdown format and updating the relative paths as requested.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>