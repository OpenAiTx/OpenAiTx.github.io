<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Russian. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Russian. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Russian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Russian. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-ru.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Russian</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Приватная локальная альтернатива Manus.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>Полностью локальная альтернатива Manus AI</em>: этот голосовой AI-ассистент автономно просматривает веб, пишет код и планирует задачи, сохраняя все данные только на вашем устройстве. Оптимизирован для локальных моделей рассуждения, полностью работает на вашем оборудовании, обеспечивая полную приватность и отсутствие облачной зависимости.</p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visit AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Почему AgenticSeek?</h3></p><ul><li>🔒 Полностью локальный и приватный — Всё работает на вашем компьютере: никаких облаков, никакой передачи данных. Ваши файлы, диалоги и поисковые запросы остаются приватными.</li></p><p><li>🌐 Умный веб-браузер — AgenticSeek может самостоятельно просматривать интернет: искать, читать, извлекать информацию, заполнять веб-формы — полностью без рук.</li></p><p><li>💻 Автономный помощник по программированию — Нужен код? Он может писать, отлаживать и запускать программы на Python, C, Go, Java и других языках — полностью автономно.</li></p><p><li>🧠 Умный выбор агента — Вы задаёте вопрос, и он автоматически выбирает лучшего агента для задачи. Как команда экспертов, готовых помочь.</li></p><p><li>📋 Планирование и выполнение сложных задач — От планирования путешествий до реализации сложных проектов — может разбивать большие задачи на этапы и выполнять их с помощью нескольких AI-агентов.</li></p><p><li>🎙️ Голосовое управление — Современное, быстрое, футуристичное распознавание речи и синтез позволяет общаться с ним голосом, как с личным AI из фантастики. (В разработке)</li></p><p></ul><h3><strong>Демонстрация</strong></h3></p><blockquote><em>Можешь найти проект agenticSeek, узнать какие навыки требуются, затем открыть CV_candidates.zip и сказать, кто из кандидатов лучше всего подходит для проекта?</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Дисклеймер: Эта демонстрация, включая все показанные файлы (например, CV_candidates.zip), полностью вымышлена. Мы не корпорация, мы ищем open-source контрибьюторов, а не кандидатов.</p><blockquote>🛠⚠️️ <strong>Активная разработка</strong></blockquote></p><blockquote>🙏 Этот проект начался как хобби и не имеет ни дорожной карты, ни финансирования. Он превзошёл все ожидания, попав в GitHub Trending. Вклад, обратная связь и терпение очень ценятся.</blockquote></p><h2>Необходимое ПО</h2></p><p>Перед началом убедитесь, что у вас установлено следующее ПО:</p><ul><li>  <strong>Git:</strong> Для клонирования репозитория. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Скачать Git</a></li>
<li>  <strong>Python 3.10.x:</strong> Настоятельно рекомендуется использовать Python версии 3.10.x. Использование других версий может привести к ошибкам зависимостей. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Скачать Python 3.10</a> (выберите версию 3.10.x).</li>
<li>  <strong>Docker Engine & Docker Compose:</strong> Для запуска встроенных сервисов, таких как SearxNG.</li>
    <li>  Установите Docker Desktop (включает Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Либо установите Docker Engine и Docker Compose отдельно для Linux: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (убедитесь, что установили Compose V2, например, <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Клонируйте репозиторий и настройте</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Измените содержимое файла .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>Обновите файл <code>.env</code> своими значениями по мере необходимости:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Оставьте без изменений </li>
<li><strong>REDIS_BASE_URL</strong>: Оставьте без изменений </li>
<li><strong>WORK_DIR</strong>: Путь к вашей рабочей директории на локальном устройстве. AgenticSeek сможет читать и взаимодействовать с этими файлами.</li>
<li><strong>OLLAMA_PORT</strong>: Порт для сервиса Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Порт для сервиса LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Порт для дополнительного собственного LLM-сервиса.</li></p><p></ul><strong>API-ключи полностью опциональны для пользователей, которые запускают LLM локально. Это основное назначение проекта. Оставьте пустыми, если у вас достаточно ресурсов.</strong></p><h3>3. <strong>Запустите Docker</strong></h3></p><p>Убедитесь, что Docker установлен и запущен на вашей системе. Запустить Docker можно следующими командами:</p><ul><li><strong>Linux/macOS:</strong>  </li>
    </ul>Откройте терминал и выполните:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Или запустите Docker Desktop из меню приложений, если он установлен.</p><ul><li><strong>Windows:</strong>  </li>
    </ul>Запустите Docker Desktop из меню "Пуск".</p><p>Проверьте, что Docker работает, выполнив:</code></pre>sh
docker info
<pre><code class="language-">Если вы видите информацию о вашей установке Docker, значит всё работает корректно.</p><p>См. таблицу <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Локальных провайдеров</a> ниже для сводки.</p><p>Следующий шаг: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Запуск AgenticSeek локально</a></p><p><em>Обратитесь к разделу <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Устранение неполадок</a>, если возникнут проблемы.</em>
<em>Если ваше оборудование не поддерживает локальный запуск LLM, смотрите <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка для работы с API</a>.</em>
<em>За подробным описанием </code>config.ini<code> смотрите <a href="#config" target="_blank" rel="noopener noreferrer">Раздел Конфигурации</a>.</em></p><hr></p><h2>Настройка для запуска LLM локально на вашем устройстве</h2></p><p><strong>Требования к оборудованию:</strong></p><p>Для локального запуска LLM необходимы соответствующие ресурсы. Минимум — GPU, способный запускать Magistral, Qwen или Deepseek 14B. См. раздел FAQ для рекомендаций по моделям и производительности.</p><p><strong>Настройте локального провайдера</strong>  </p><p>Запустите локального провайдера, например с помощью ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
См. ниже список поддерживаемых локальных провайдеров.</p><p><strong>Обновите config.ini</strong></p><p>Измените файл config.ini, установив provider_name на поддерживаемого провайдера и provider_model на LLM, поддерживаемую вашим провайдером. Рекомендуем использовать модели рассуждения, такие как <em>Magistral</em> или <em>Deepseek</em>.</p><p>См. <strong>FAQ</strong> в конце README для требований к оборудованию.
</code></pre>sh
[MAIN]
is_local = True # Локальный запуск или через удалённого провайдера.
provider_name = ollama # или lm-studio, openai и т.д.
provider_model = deepseek-r1:14b # выберите модель, подходящую вашему оборудованию
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # имя вашего AI
recover_last_session = True # восстанавливать предыдущую сессию
save_session = True # сохранять текущую сессию
speak = False # синтез речи
listen = False # распознавание речи, только для CLI, экспериментально
jarvis_personality = False # использовать стиль личности "Jarvis" (экспериментально)
languages = en zh # список языков, синтез речи по умолчанию на первом в списке
[BROWSER]
headless_browser = True # не меняйте, если не используете CLI на хосте
stealth_mode = True # использовать undetected selenium для уменьшения обнаружения браузера
<pre><code class="language-">
<strong>Внимание</strong>:</p><ul><li>Формат файла </code>config.ini<code> не поддерживает комментарии. </li>
</ul>Не копируйте и не вставляйте пример конфигурации напрямую: комментарии вызовут ошибки. Вместо этого вручную измените файл </code>config.ini<code>, исключив все комментарии.</p><ul><li>Не указывайте provider_name как </code>openai<code>, если используете LM-studio для запуска LLM. Установите </code>lm-studio<code>.</li></p><p><li>Некоторые провайдеры (например, lm-studio) требуют наличие </code>http://<code> перед IP. Например, </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Список локальных провайдеров</strong></p><p>| Провайдер   | Локальный? | Описание                                                 |
|-------------|------------|----------------------------------------------------------|
| ollama      | Да         | Запуск LLM локально с помощью ollama как LLM-провайдера  |
| lm-studio   | Да         | Запуск LLM локально с помощью LM studio (установите </code>provider_name<code> как </code>lm-studio<code>)|
| openai      | Да         | Использование совместимого с openai API (например, сервер llama.cpp)  |</p><p>Следующий шаг: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Запуск сервисов и AgenticSeek</a>  </p><p><em>Смотрите раздел <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Устранение неполадок</a>, если возникли проблемы.</em>
<em>Если ваше оборудование не поддерживает локальный запуск LLM, смотрите <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка для работы с API</a>.</em>
<em>Для подробных объяснений </code>config.ini<code> смотрите <a href="#config" target="_blank" rel="noopener noreferrer">Раздел Конфигурации</a>.</em></p><h2>Настройка для работы с API</h2></p><p>В этом режиме используются внешние облачные провайдеры LLM. Вам понадобится API-ключ от выбранного сервиса.</p><p><strong>1. Выберите API-провайдера и получите API-ключ:</strong></p><p>Смотрите <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Список API-провайдеров</a> ниже. Посетите их сайты, зарегистрируйтесь и получите API-ключ.</p><p><strong>2. Установите свой API-ключ как переменную окружения:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Откройте терминал и используйте команду </code>export<code>. Лучше добавить эту команду в профиль оболочки (например, </code>~/.bashrc<code>, </code>~/.zshrc<code>) для постоянства.
    </code>`<code>sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Замените PROVIDER_API_KEY на конкретное имя переменной, например, OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Пример для TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Командная строка (Временно для текущей сессии):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (Временно для текущей сессии):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>Постоянно:</strong> Найдите "переменные среды" в поисковой строке Windows, нажмите "Изменить системные переменные среды", затем нажмите кнопку "Переменные среды...". Добавьте новую пользовательскую переменную с соответствующим именем (например, </code>OPENAI_API_KEY<code>) и вашим ключом в качестве значения.</li></p><p></ul><em>(См. FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Как установить API ключи?</a> для получения подробностей).</em></p><p><strong>3. Обновите </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Или google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Или gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 и т.д.
provider_server_address = # Обычно игнорируется или может быть оставлен пустым, когда is_local = False для большинства API
<h1>... другие настройки ...</h1>
<pre><code class="language-"><em>Внимание:</em> Убедитесь, что в значениях </code>config.ini<code> нет пробелов в конце строк.</p><p><strong>Список API провайдеров</strong></p><p>| Провайдер    | </code>provider_name<code> | Локальный? | Описание                                            | Ссылка для получения API ключа                |
|--------------|-----------------|------------|-----------------------------------------------------|-----------------------------------------------|
| OpenAI       | </code>openai<code>        | Нет        | Использовать модели ChatGPT через API OpenAI.       | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini| </code>google<code>        | Нет        | Использовать модели Google Gemini через Google AI Studio. | <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek     | </code>deepseek<code>      | Нет        | Использовать модели Deepseek через их API.          | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face | </code>huggingface<code>   | Нет        | Использовать модели через Hugging Face Inference API.| <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI   | </code>togetherAI<code>    | Нет        | Использовать различные open-source модели через TogetherAI API.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Примечание:</em>
<ul><li>  Мы не рекомендуем использовать </code>gpt-4o<code> или другие модели OpenAI для сложного веб-сёрфинга и планирования задач, так как текущие оптимизации подсказок ориентированы на модели вроде Deepseek.</li>
<li>  Задания на программирование/bash могут сталкиваться с ошибками при использовании Gemini, поскольку он может не строго следовать форматированию подсказок, оптимизированных для Deepseek.</li>
<li>  Параметр </code>provider_server_address<code> в </code>config.ini<code> обычно не используется, когда </code>is_local = False<code>, так как конечная точка API обычно зашита в соответствующей библиотеке провайдера.</li></p><p></ul>Следующий шаг: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Запустите сервисы и AgenticSeek</a></p><p><em>См. раздел <strong>Известные проблемы</strong>, если у вас возникают сложности.</em></p><p><em>См. раздел <strong>Config</strong> для подробного объяснения файла конфигурации.</em></p><hr></p><h2>Запуск сервисов и работа</h2></p><p>По умолчанию AgenticSeek полностью запускается в docker.</p><p>Запустите необходимые сервисы. Это запустит все сервисы из docker-compose.yml, включая:
    <ul><li>searxng</li>
    <li>redis (требуется для searxng)</li>
    <li>frontend</li>
    <li>backend (если используется </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Window
<pre><code class="language-">
<strong>Внимание:</strong> На этом этапе будут загружены и распакованы все образы Docker, что может занять до 30 минут. После запуска сервисов дождитесь полной загрузки backend-сервиса (вы должны увидеть <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> в логе) перед отправкой каких-либо сообщений. При первом запуске backend-сервис может запускаться до 5 минут.</p><p>Перейдите на </code>http://localhost:3000/<code> — вы должны увидеть веб-интерфейс.</p><p><em>Устранение проблем с запуском сервисов:</em> Если эти скрипты не срабатывают, убедитесь, что Docker Engine запущен и Docker Compose (V2, </code>docker compose<code>) корректно установлен. Проверьте сообщения об ошибках в терминале. См. <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Помогите! Я получаю ошибку при запуске AgenticSeek или его скриптов.</a></p><p><strong>Необязательно:</strong> Запуск на хосте (CLI-режим):</p><p>Чтобы использовать CLI-интерфейс, необходимо установить пакет на хосте:
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Запустите сервисы:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Window
<pre><code class="language-">
Используйте CLI: </code>python3 cli.py<code></p><hr></p><h2>Использование</h2></p><p>Убедитесь, что сервисы запущены командой </code>./start_services.sh full<code>, и перейдите на </code>localhost:3000<code> для доступа к веб-интерфейсу.</p><p>Вы также можете использовать преобразование речи в текст, установив </code>listen = True<code> в конфиге. Только для CLI-режима.</p><p>Для выхода просто скажите/напишите </code>goodbye<code>.</p><p>Вот несколько примеров использования:</p><blockquote><em>Сделай игру "змейка" на python!</em></blockquote></p><blockquote><em>Найди лучшие кафе в Ренне (Франция) через интернет и сохрани список из трёх с их адресами в rennes_cafes.txt.</em></blockquote></p><blockquote><em>Напиши программу на Go для вычисления факториала числа, сохрани как factorial.go в свою рабочую папку</em></blockquote></p><blockquote><em>Найди в папке summer_pictures все файлы JPG, переименуй их с сегодняшней датой и сохрани список переименованных файлов в photos_list.txt</em></blockquote></p><blockquote><em>Ищи онлайн популярные научно-фантастические фильмы 2024 года и выбери три для просмотра сегодня вечером. Сохрани список в movie_night.txt.</em></blockquote></p><blockquote><em>Найди последние новости об ИИ за 2025 год, выбери три статьи и напиши скрипт на Python для парсинга их заголовков и кратких описаний. Сохрани скрипт как news_scraper.py, а описания в ai_news.txt в /home/projects</em></blockquote></p><blockquote><em>В пятницу найди бесплатный API для котировок акций, зарегистрируйся с supersuper7434567@gmail.com, затем напиши скрипт на Python для ежедневного получения цен Tesla по этому API и сохрани результаты в stock_prices.csv</em></blockquote></p><p><em>Обратите внимание, что заполнение форм пока экспериментальное и может не работать.</em></p><p>После ввода вашего запроса AgenticSeek выделит для задачи подходящего агента.</p><p>Поскольку это ранний прототип, система маршрутизации агентов может не всегда выбирать правильного агента по вашему запросу.</p><p>Поэтому старайтесь максимально чётко формулировать, что вы хотите и как AI должен действовать. Например, если вы хотите, чтобы он провёл веб-поиск, не говорите:</p><p></code>Ты знаешь хорошие страны для одиночных путешествий?<code></p><p>Вместо этого спросите:</p><p></code>Сделай веб-поиск и выясни, какие страны лучше всего подходят для одиночных путешествий<code></p><hr></p><h2><strong>Настройка для запуска LLM на вашем сервере</strong>  </h2></p><p>Если у вас есть мощный компьютер или сервер, который вы хотите использовать удалённо со своего ноутбука, вы можете запустить LLM на удалённом сервере с помощью нашего собственного llm-сервера.</p><p>На вашем "сервере", где будет запущена AI-модель, получите IP-адрес
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # локальный ip
curl https://ipinfo.io/ip # публичный ip
<pre><code class="language-">
Примечание: для Windows или macOS используйте соответственно ipconfig или ifconfig для определения IP-адреса.</p><p>Клонируйте репозиторий и перейдите в папку </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Установите специфические для сервера зависимости:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Запустите серверный скрипт.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Вы можете выбрать между использованием </code>ollama<code> и </code>llamacpp<code> как сервиса LLM.</p><p>
Теперь на вашем персональном компьютере:</p><p>Измените файл </code>config.ini<code>, чтобы установить </code>provider_name<code> в </code>server<code> и </code>provider_model<code> в </code>deepseek-r1:xxb<code>.
Установите </code>provider_server_address<code> на IP-адрес машины, где будет запускаться модель.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-">
Следующий шаг: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Запустите сервисы и AgenticSeek</a>  </p><hr></p><h2>Преобразование речи в текст</h2></p><p>Внимание: функция распознавания речи работает только в CLI-режиме на данный момент.</p><p>Обратите внимание, что сейчас речь в текст работает только на английском языке.</p><p>Функция распознавания речи по умолчанию отключена. Чтобы включить её, установите опцию listen в True в файле config.ini:
</code></pre>
listen = True
<pre><code class="language-">
Когда функция включена, перед началом обработки вашего ввода она будет слушать ключевое слово-триггер, которым является имя агента. Вы можете изменить имя агента, изменив значение </code>agent_name<code> в файле <em>config.ini</em>:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Для оптимального распознавания рекомендуется использовать распространённое английское имя, например "John" или "Emma", в качестве имени агента.</p><p>Как только вы увидите, что транскрипция начала появляться, произнесите имя агента вслух, чтобы разбудить его (например, "Friday").</p><p>Чётко озвучьте свой запрос.</p><p>Завершите свой запрос подтверждающей фразой, чтобы система приступила к выполнению. Примеры подтверждающих фраз:</code></pre>
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
<pre><code class="language-">
<h2>Config</h2></p><p>Пример конфигурации:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Пример для Ollama; используйте http://127.0.0.1:1234 для LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Список языков для TTS и, возможно, маршрутизации.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Пояснение к настройкам </code>config.ini<code></strong>:</p><ul><li>  <strong>Раздел </code>[MAIN]<code>:</strong></li>
    <li>  </code>is_local<code>: </code>True<code>, если используется локальный LLM-провайдер (Ollama, LM-Studio, локальный сервер совместимый с OpenAI) или опция самостоятельного размещения. </code>False<code>, если используется облачный API (OpenAI, Google и др.).</li>
    <li>  </code>provider_name<code>: Указывает провайдера LLM.</li>
        <li>  Локальные варианты: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (для локальных серверов, совместимых с OpenAI), </code>server<code> (для самостоятельного размещения).</li>
        <li>  Варианты API: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: Конкретное название или ID модели для выбранного провайдера (например, </code>deepseekcoder:6.7b<code> для Ollama, </code>gpt-3.5-turbo<code> для OpenAI API, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> для TogetherAI).</li>
    <li>  </code>provider_server_address<code>: Адрес вашего LLM-провайдера.</li>
        <li>  Для локальных провайдеров: например, </code>http://127.0.0.1:11434<code> для Ollama, </code>http://127.0.0.1:1234<code> для LM-Studio.</li>
        <li>  Для типа провайдера </code>server<code>: адрес вашего собственного сервера LLM (например, </code>http://your_server_ip:3333<code>).</li>
        <li>  Для облачных API (</code>is_local = False<code>): часто игнорируется или может быть пустым, так как конечная точка API обычно обрабатывается клиентской библиотекой.</li>
    <li>  </code>agent_name<code>: Имя AI-ассистента (например, Friday). Используется как ключевое слово для активации распознавания речи, если оно включено.</li>
    <li>  </code>recover_last_session<code>: </code>True<code> — попытка восстановить состояние предыдущей сессии, </code>False<code> — начать новую.</li>
    <li>  </code>save_session<code>: </code>True<code> — сохранить состояние текущей сессии для возможного восстановления, иначе </code>False<code>.</li>
    <li>  </code>speak<code>: </code>True<code> — включить голосовой вывод (TTS), </code>False<code> — отключить.</li>
    <li>  </code>listen<code>: </code>True<code> — включить голосовой ввод (распознавание речи, только CLI режим), </code>False<code> — отключить.</li>
    <li>  </code>work_dir<code>: <strong>Важное значение:</strong> Каталог, в котором AgenticSeek будет читать и записывать файлы. <strong>Убедитесь, что этот путь существует и доступен на вашей системе.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> — использовать более "Jarvis-подобный" системный prompt (экспериментально), </code>False<code> — стандартный prompt.</li>
    <li>  </code>languages<code>: Перечисление языков через запятую (например, </code>en, zh, fr<code>). Используется для выбора голоса TTS (по умолчанию — первый язык) и может помочь маршрутизатору LLM. Не указывайте слишком много или очень похожих языков для эффективности.</li>
<li>  <strong>Раздел </code>[BROWSER]<code>:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code> — запускать автоматизированный браузер без видимого окна (рекомендуется для веб-интерфейса или неинтерактивного использования). </code>False<code> — показать окно браузера (удобно для CLI или отладки).</li>
    <li>  </code>stealth_mode<code>: </code>True<code> — включить меры против обнаружения автоматизации браузера. Может потребовать ручной установки расширений, например, anticaptcha.</li></p><p>
</ul>В этом разделе приводится сводка поддерживаемых типов LLM-провайдеров. Настройте их в </code>config.ini<code>.</p><p><strong>Локальные провайдеры (на вашем оборудовании):</strong></p><p>| Имя провайдера в </code>config.ini<code> | </code>is_local<code> | Описание                                                                  | Раздел настройки                                                    |
|-------------------------------|------------|---------------------------------------------------------------------------|---------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Использовать Ollama для локального запуска LLM.                           | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Настройка локального LLM</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Использовать LM-Studio для локального запуска LLM.                        | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Настройка локального LLM</a> |
| </code>openai<code> (для локального сервера) | </code>True<code> | Подключение к локальному серверу с OpenAI-совместимым API (например, llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Настройка локального LLM</a> |
| </code>server<code>                      | </code>False<code>    | Подключение к самостоятельному серверу AgenticSeek LLM на другом устройстве. | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Настройка собственного сервера LLM</a> |</p><p><strong>API-провайдеры (облачные):</strong></p><p>| Имя провайдера в </code>config.ini<code> | </code>is_local<code> | Описание                                        | Раздел настройки                                       |
|-------------------------------|------------|--------------------------------------------------|--------------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Использовать официальный API OpenAI (например, GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка с API</a>           |
| </code>google<code>                      | </code>False<code>    | Использовать модели Gemini от Google через API.  | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка с API</a>           |
| </code>deepseek<code>                    | </code>False<code>    | Использовать официальный API Deepseek.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка с API</a>           |
| </code>huggingface<code>                 | </code>False<code>    | Использовать API Hugging Face Inference.         | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка с API</a>           |
| </code>togetherAI<code>                  | </code>False<code>    | Использовать API TogetherAI для различных open моделей. | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Настройка с API</a>           |</p><hr>
<h2>Поиск и устранение неисправностей</h2></p><p>Если вы сталкиваетесь с проблемами, этот раздел поможет вам.</p><h1>Известные проблемы</h1></p><h2>Проблемы с ChromeDriver</h2></p><p><strong>Пример ошибки:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Причина:</strong> Установленная версия ChromeDriver не совместима с вашей версией браузера Google Chrome.</li>
<li>  <strong>Решение:</strong></li>
    <li> <strong>Проверьте версию Chrome:</strong> Откройте Google Chrome, перейдите в </code>Настройки > О Chrome<code>, чтобы узнать версию (например, "Версия 120.0.6099.110").</li>
    <li> <strong>Скачайте подходящий ChromeDriver:</strong></li>
        <li>  Для версий Chrome 115 и новее: Перейдите на страницу <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Найдите канал "stable" и скачайте ChromeDriver для вашей ОС, соответствующий основной версии Chrome.</li>
        <li>  Для более старых версий (редко): Попробуйте найти их на странице <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  На изображении ниже показан пример CfT страницы:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page">
    <ul><li> <strong>Установите ChromeDriver:</strong></li>
        <li>  Убедитесь, что скачанный </code>chromedriver<code> (или </code>chromedriver.exe<code> на Windows) находится в каталоге, который прописан в переменной PATH вашей системы (например, </code>/usr/local/bin<code> на Linux/macOS или папка со скриптами, добавленная в PATH на Windows).</li>
        <li>  Либо поместите его в корневую папку проекта </code>agenticSeek<code>.</li>
        <li>  Проверьте, что драйвер исполняемый (например, выполните </code>chmod +x chromedriver<code> на Linux/macOS).</li>
    <li> Подробнее смотрите в разделе <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">ChromeDriver Installation</a> в основном руководстве по установке.</li></p><p></ul>Если этот раздел неполный или вы столкнулись с другими проблемами ChromeDriver, попробуйте поискать похожие <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a> или создайте новую заявку.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Это возникает, если версии браузера и chromedriver не совпадают.</p><p>Вам нужно скачать последнюю версию по адресу:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Если вы используете Chrome версии 115 или новее, перейдите по адресу:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>И скачайте chromedriver, соответствующий вашей ОС.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Если этот раздел неполный, пожалуйста, создайте issue.</p><h2>Проблемы connection adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Примечание: порт может отличаться)
<pre><code class="language-">
<ul><li>  <strong>Причина:</strong> В <code>provider_server_address</code> в <code>config.ini</code> для <code>lm-studio</code> (или других локальных серверов, совместимых с OpenAI) отсутствует префикс <code>http://</code> или указан неверный порт.</li>
<li>  <strong>Решение:</strong></li>
    <li>  Убедитесь, что адрес начинается с <code>http://</code>. По умолчанию для LM-Studio: <code>http://127.0.0.1:1234</code>.</li>
    <li>  Исправьте <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (или ваш реальный порт LM-Studio).</li></p><p></ul><h2>Не указан базовый URL SearxNG</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>В: Какое железо мне нужно?</strong>  </p><p>| Размер модели | GPU  | Комментарий                                               |
|--------------|------|-----------------------------------------------------------|
| 7B           | 8ГБ VRAM | ⚠️ Не рекомендуется. Низкая производительность, частые галлюцинации, агенты-планировщики, скорее всего, не будут работать. |
| 14B          | 12ГБ VRAM (например, RTX 3060) | ✅ Подходит для простых задач. Может испытывать трудности с веб-браузингом и планированием. |
| 32B          | 24+ ГБ VRAM (например, RTX 4090) | 🚀 Большинство задач успешно, могут быть сложности с планированием задач. |
| 70B+         | 48+ ГБ VRAM | 💪 Отлично. Рекомендуется для продвинутых сценариев. |</p><p><strong>В: Я получаю ошибку — что делать?</strong>  </p><p>Убедитесь, что локальный сервер запущен (</code>ollama serve<code>), ваш </code>config.ini` соответствует провайдеру, и все зависимости установлены. Если ничего не помогает — создайте issue.</p><p><strong>В: Может ли всё работать полностью локально?</strong>  </p><p>Да, с провайдерами Ollama, lm-studio или server, все компоненты — распознавание речи, LLM и синтез речи — работают локально. Не локальные варианты (OpenAI или другие API) — опциональны.</p><p><strong>В: Почему стоит использовать AgenticSeek, если есть Manus?</strong></p><p>В отличие от Manus, AgenticSeek ориентирован на независимость от внешних систем, что даёт больше контроля, приватности и позволяет избежать затрат на API.</p><p><strong>В: Кто стоит за проектом?</strong></p><p>Проект создан мной и двумя друзьями, которые также являются мейнтейнерами и контрибьюторами из open-source сообщества на GitHub. Мы — просто группа энтузиастов, не стартап и не связаны с какими-либо организациями.</p><p>Любой аккаунт AgenticSeek на X, кроме моего личного (https://x.com/Martin993886460), — это подделка.</p><h2>Вклад</h2></p><p>Мы ищем разработчиков для улучшения AgenticSeek! Смотрите открытые issue или обсуждения.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Руководство по вкладу</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Мейнтейнеры:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Парижское время </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Тайбэйское время </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Тайбэйское время </p><h2>Особая благодарность:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> и <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> за помощь с docker-изация backend</p><h2>Спонсоры:</h2></p><p>5$ и более — здесь отображаются ежемесячные спонсоры:
<ul><li><strong>tatra-labs</strong></li></p><p></ul>Certainly! Please provide the content for Part 4 of 4 that you would like translated.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>