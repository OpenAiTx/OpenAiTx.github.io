<!DOCTYPE html>
<html lang="pt">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Portuguese. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Portuguese. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Portuguese, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Portuguese. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-pt.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Portuguese</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Alternativa privada e local ao Manus.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>Uma <strong>alternativa 100% local ao Manus AI</strong>, este assistente de IA habilitado por voz navega autonomamente na web, escreve código e planeja tarefas enquanto mantém todos os dados em seu dispositivo. Projetado para modelos de raciocínio locais, ele roda inteiramente no seu hardware, garantindo total privacidade e nenhuma dependência da nuvem.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visite AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licença"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="Estrelas no GitHub"></a></p><h3>Por que AgenticSeek?</h3></p><ul><li>🔒 Totalmente Local & Privado - Tudo roda em sua máquina — sem nuvem, sem compartilhamento de dados. Seus arquivos, conversas e pesquisas permanecem privados.</li></p><p><li>🌐 Navegação Inteligente na Web - O AgenticSeek pode navegar na internet sozinho — pesquisar, ler, extrair informações, preencher formulários — tudo sem usar as mãos.</li></p><p><li>💻 Assistente Autônomo de Programação - Precisa de código? Ele pode escrever, depurar e executar programas em Python, C, Go, Java e mais — tudo sem supervisão.</li></p><p><li>🧠 Seleção Inteligente de Agentes - Você pede, ele descobre automaticamente o melhor agente para a tarefa. Como ter uma equipe de especialistas pronta para ajudar.</li></p><p><li>📋 Planeja & Executa Tarefas Complexas - De planejamento de viagens a projetos complexos — ele pode dividir grandes tarefas em etapas e realizar usando múltiplos agentes de IA.</li></p><p><li>🎙️ Habilitado por Voz - Voz limpa, rápida e futurista e conversão de fala para texto, permitindo que você converse como se fosse sua IA pessoal de um filme de ficção científica. (Em desenvolvimento)</li></p><p></ul><h3><strong>Demo</strong></h3></p><blockquote><em>Você pode pesquisar pelo projeto agenticSeek, aprender quais habilidades são necessárias, depois abrir o CV_candidates.zip e então me dizer quais candidatos combinam melhor com o projeto</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Aviso: Esta demonstração, incluindo todos os arquivos que aparecem (ex: CV_candidates.zip), é inteiramente fictícia. Não somos uma corporação, buscamos colaboradores open-source, não candidatos.</p><blockquote>🛠⚠️️ <strong>Trabalho Ativo em Andamento</strong></blockquote></p><blockquote>🙏 Este projeto começou como um projeto paralelo e não tem roteiro nem financiamento. Cresceu muito além do esperado, chegando ao GitHub Trending. Contribuições, feedback e paciência são profundamente apreciados.</blockquote></p><h2>Pré-requisitos</h2></p><p>Antes de começar, certifique-se de ter o seguinte software instalado:</p><ul><li>  <strong>Git:</strong> Para clonar o repositório. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Baixe o Git</a></li>
<li>  <strong>Python 3.10.x:</strong> Recomendamos fortemente o uso da versão Python 3.10.x. Usar outras versões pode causar erros de dependência. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Baixe o Python 3.10</a> (escolha uma versão 3.10.x).</li>
<li>  <strong>Docker Engine & Docker Compose:</strong> Para rodar serviços agrupados como o SearxNG.</li>
    <li>  Instale o Docker Desktop (que inclui Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Alternativamente, instale o Docker Engine e o Docker Compose separadamente no Linux: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (certifique-se de instalar o Compose V2, ex: <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Clone o repositório e configure</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Altere o conteúdo do arquivo .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='opcional'
DEEPSEEK_API_KEY='opcional'
OPENROUTER_API_KEY='opcional'
TOGETHER_API_KEY='opcional'
GOOGLE_API_KEY='opcional'
ANTHROPIC_API_KEY='opcional'</code></pre></p><p>Atualize o arquivo <code>.env</code> com seus próprios valores conforme necessário:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Deixe inalterado</li>
<li><strong>REDIS_BASE_URL</strong>: Deixe inalterado</li>
<li><strong>WORK_DIR</strong>: Caminho para seu diretório de trabalho em sua máquina local. O AgenticSeek poderá ler e interagir com estes arquivos.</li>
<li><strong>OLLAMA_PORT</strong>: Número da porta para o serviço Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Número da porta para o serviço LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Porta para qualquer serviço LLM personalizado adicional.</li></p><p></ul><strong>As chaves de API são totalmente opcionais para usuários que optam por rodar LLM localmente. Esse é o objetivo principal deste projeto. Deixe em branco se você possuir hardware suficiente</strong></p><h3>3. <strong>Inicie o Docker</strong></h3></p><p>Certifique-se de que o Docker está instalado e em execução em seu sistema. Você pode iniciar o Docker usando os seguintes comandos:</p><ul><li><strong>No Linux/macOS:</strong>  </li>
    </ul>Abra um terminal e execute:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Ou inicie o Docker Desktop a partir do menu de aplicativos, se instalado.</p><ul><li><strong>No Windows:</strong>  </li>
    </ul>Inicie o Docker Desktop pelo menu Iniciar.</p><p>Você pode verificar se o Docker está rodando executando:</code></pre>sh
docker info
<pre><code class="language-">Se você visualizar informações sobre sua instalação do Docker, ele está funcionando corretamente.</p><p>Veja a tabela de <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Provedores Locais</a> abaixo para um resumo.</p><p>Próximo passo: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Execute o AgenticSeek localmente</a></p><p><em>Veja a seção <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Solução de Problemas</a> se estiver tendo problemas.</em>
<em>Se seu hardware não conseguir rodar LLMs localmente, veja <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a>.</em>
<em>Para explicações detalhadas do </code>config.ini<code>, veja a <a href="#config" target="_blank" rel="noopener noreferrer">Seção de Configuração</a>.</em></p><hr></p><h2>Configuração para rodar LLM localmente em sua máquina</h2></p><p><strong>Requisitos de Hardware:</strong></p><p>Para rodar LLMs localmente, você precisará de hardware suficiente. No mínimo, é necessário uma GPU capaz de rodar Magistral, Qwen ou Deepseek 14B. Veja o FAQ para recomendações detalhadas de modelo/desempenho.</p><p><strong>Configure seu provedor local</strong></p><p>Inicie seu provedor local, por exemplo, com o ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
Veja abaixo uma lista de provedores locais suportados.</p><p><strong>Atualize o config.ini</strong></p><p>Altere o arquivo config.ini para definir o provider_name para um provedor suportado e provider_model para um LLM suportado por seu provedor. Recomendamos modelos de raciocínio como <em>Magistral</em> ou <em>Deepseek</em>.</p><p>Veja o <strong>FAQ</strong> no final do README para hardware necessário.
</code></pre>sh
[MAIN]
is_local = True # Sempre que estiver rodando localmente ou com provedor remoto.
provider_name = ollama # ou lm-studio, openai, etc..
provider_model = deepseek-r1:14b # escolha um modelo que se encaixe em seu hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nome da sua IA
recover_last_session = True # se deseja recuperar a sessão anterior
save_session = True # se deseja lembrar a sessão atual
speak = False # texto para fala
listen = False # fala para texto, apenas para CLI, experimental
jarvis_personality = False # Se deseja usar uma personalidade mais "Jarvis" (experimental)
languages = en zh # Lista de idiomas, Texto para fala usará o primeiro idioma da lista como padrão
[BROWSER]
headless_browser = True # deixe inalterado, a menos que use CLI no host.
stealth_mode = True # Use selenium não detectado para reduzir detecção do navegador
<pre><code class="language-">
<strong>Aviso</strong>:</p><ul><li>O formato do arquivo </code>config.ini<code> não suporta comentários. </li>
</ul>Não copie e cole a configuração de exemplo diretamente, pois os comentários causarão erros. Em vez disso, modifique manualmente o arquivo </code>config.ini<code> com suas configurações desejadas, excluindo quaisquer comentários.</p><ul><li><em>NÃO</em> defina provider_name como </code>openai<code> se estiver usando LM-studio para rodar LLMs. Defina como </code>lm-studio<code>.</li></p><p><li>Alguns provedores (ex: lm-studio) exigem que você tenha </code>http://<code> na frente do IP. Por exemplo, </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Lista de provedores locais</strong></p><p>| Provedor   | Local? | Descrição                                                        |
|------------|--------|------------------------------------------------------------------|
| ollama     | Sim    | Rode LLMs localmente com facilidade usando ollama como provedor   |
| lm-studio  | Sim    | Rode LLM localmente com o LM studio (defina </code>provider_name<code> como </code>lm-studio<code>)|
| openai     | Sim    |  Use API compatível com openai (ex: servidor llama.cpp)           |</p><p>Próximo passo: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Inicie os serviços e rode o AgenticSeek</a>  </p><p><em>Veja a seção <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Solução de Problemas</a> se estiver tendo problemas.</em>
<em>Se seu hardware não conseguir rodar LLMs localmente, veja <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a>.</em>
<em>Para explicações detalhadas do </code>config.ini<code>, veja a <a href="#config" target="_blank" rel="noopener noreferrer">Seção de Configuração</a>.</em></p><h2>Configuração para rodar com uma API</h2></p><p>Esta configuração usa provedores de LLM externos, baseados em nuvem. Você precisará de uma chave de API do serviço escolhido.</p><p><strong>1. Escolha um Provedor de API e obtenha uma chave de API:</strong></p><p>Consulte a <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Lista de Provedores de API</a> abaixo. Visite os sites deles para se cadastrar e obter uma chave de API.</p><p><strong>2. Defina sua chave de API como uma variável de ambiente:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Abra seu terminal e use o comando </code>export<code>. É melhor adicionar isso ao arquivo de perfil do seu shell (ex: </code>~/.bashrc<code>, </code>~/.zshrc<code>) para persistência.
    </code>`<code>sh
    export PROVIDER_API_KEY="sua_chave_api_aqui"
    # Substitua PROVIDER_API_KEY pelo nome da variável específica, ex: OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Exemplo para TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Prompt de Comando (Temporário para a sessão atual):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (Temporário para a sessão atual):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>Permanentemente:</strong> Pesquise por "variáveis de ambiente" na barra de pesquisa do Windows, clique em "Editar as variáveis de ambiente do sistema" e, em seguida, clique no botão "Variáveis de Ambiente...". Adicione uma nova variável de Usuário com o nome apropriado (por exemplo, </code>OPENAI_API_KEY<code>) e sua chave como valor.</li></p><p></ul><em>(Veja o FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Como configuro chaves de API?</a> para mais detalhes).</em></p><p>
<strong>3. Atualize o </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Normalmente ignorado ou pode ser deixado em branco quando is_local = False para a maioria das APIs
<h1>... outras configurações ...</h1>
<pre><code class="language-"><em>Atenção:</em> Certifique-se de que não há espaços no final dos valores no </code>config.ini<code>.</p><p><strong>Lista de Provedores de API</strong></p><p>| Provedor      | </code>provider_name<code> | Local? | Descrição                                          | Link da chave da API (Exemplos)              |
|---------------|-----------------|--------|----------------------------------------------------|----------------------------------------------|
| OpenAI        | </code>openai<code>        | Não    | Usa modelos ChatGPT via API da OpenAI.             | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini | </code>google<code>        | Não    | Usa modelos Google Gemini via Google AI Studio.     | <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek      | </code>deepseek<code>      | Não    | Usa modelos Deepseek via sua API.                   | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face  | </code>huggingface<code>   | Não    | Usa modelos da Hugging Face Inference API.          | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI    | </code>togetherAI<code>    | Não    | Usa diversos modelos open-source via TogetherAI API.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Nota:</em>
<ul><li>  Não recomendamos o uso de </code>gpt-4o<code> ou outros modelos da OpenAI para navegação web complexa e planejamento de tarefas, pois as otimizações atuais de prompt são voltadas para modelos como Deepseek.</li>
<li>  Tarefas de codificação/bash podem apresentar problemas com o Gemini, pois ele pode não seguir rigorosamente os prompts de formatação otimizados para o Deepseek.</li>
<li>  O campo </code>provider_server_address<code> no </code>config.ini<code> geralmente não é usado quando </code>is_local = False<code>, pois o endpoint da API normalmente é definido na biblioteca do provedor.</li></p><p></ul>Próxima etapa: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Inicie os serviços e rode o AgenticSeek</a></p><p><em>Consulte a seção <strong>Problemas conhecidos</strong> caso esteja enfrentando problemas</em></p><p><em>Consulte a seção <strong>Config</strong> para uma explicação detalhada do arquivo de configuração.</em></p><hr></p><h2>Inicie os serviços e execute</h2></p><p>Por padrão, o AgenticSeek é executado totalmente em docker.</p><p>Inicie os serviços necessários. Isso iniciará todos os serviços do docker-compose.yml, incluindo:
    <ul><li>searxng</li>
    <li>redis (necessário pelo searxng)</li>
    <li>frontend</li>
    <li>backend (se usar </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
<pre><code class="language-">
<strong>Atenção:</strong> Esta etapa irá baixar e carregar todas as imagens do Docker, o que pode levar até 30 minutos. Após iniciar os serviços, aguarde até que o serviço backend esteja totalmente rodando (você deve ver <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> no log) antes de enviar qualquer mensagem. Os serviços backend podem levar até 5 minutos para iniciar na primeira execução.</p><p>Acesse </code>http://localhost:3000/<code> e você deverá ver a interface web.</p><p><em>Resolução de problemas ao iniciar o serviço:</em> Se esses scripts falharem, certifique-se de que o Docker Engine está em execução e que o Docker Compose (V2, </code>docker compose<code>) está instalado corretamente. Verifique a saída no terminal para mensagens de erro. Veja <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Help! I get an error when running AgenticSeek or its scripts.</a></p><p><strong>Opcional:</strong> Rodar na máquina local (modo CLI):</p><p>Para rodar com interface CLI você precisará instalar o pacote na máquina local:
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Inicie os serviços:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
<pre><code class="language-">
Use o CLI: </code>python3 cli.py<code></p><hr></p><h2>Uso</h2></p><p>Certifique-se de que os serviços estão ativos e em execução com </code>./start_services.sh full<code> e acesse </code>localhost:3000<code> para a interface web.</p><p>Você também pode usar reconhecimento de voz (speech to text) configurando </code>listen = True<code> no config. Apenas para o modo CLI.</p><p>Para sair, simplesmente diga/digite </code>goodbye<code>.</p><p>Aqui estão alguns exemplos de uso:</p><blockquote><em>Crie um jogo da cobrinha em python!</em></blockquote></p><blockquote><em>Pesquise na web pelos melhores cafés em Rennes, França, e salve uma lista de três com seus endereços em rennes_cafes.txt.</em></blockquote></p><blockquote><em>Escreva um programa em Go para calcular o fatorial de um número, salve como factorial.go em sua área de trabalho</em></blockquote></p><blockquote><em>Procure na pasta summer_pictures todos os arquivos JPG, renomeie-os com a data de hoje e salve uma lista dos arquivos renomeados em photos_list.txt</em></blockquote></p><blockquote><em>Pesquise online por filmes de ficção científica populares de 2024 e escolha três para assistir hoje à noite. Salve a lista em movie_night.txt.</em></blockquote></p><blockquote><em>Pesquise na web os artigos mais recentes sobre IA de 2025, selecione três e escreva um script Python para capturar seus títulos e resumos. Salve o script como news_scraper.py e os resumos em ai_news.txt em /home/projects</em></blockquote></p><blockquote><em>Sexta-feira, pesquise na web por uma API gratuita de cotação de ações, registre-se com supersuper7434567@gmail.com, depois escreva um script Python para buscar, usando a API, os preços diários da Tesla, e salve os resultados em stock_prices.csv</em></blockquote></p><p><em>Observe que as capacidades de preenchimento de formulários ainda são experimentais e podem falhar.</em></p><p>Após digitar sua consulta, o AgenticSeek irá alocar o melhor agente para a tarefa.</p><p>Como este é um protótipo inicial, o sistema de roteamento de agentes pode não alocar sempre o agente correto com base na sua consulta.</p><p>Portanto, seja muito explícito no que deseja e em como a IA deve proceder. Por exemplo, se quiser que ela faça uma busca na web, não diga:</p><p></code>Você conhece alguns bons países para viajar sozinho?<code></p><p>Em vez disso, peça:</p><p></code>Faça uma busca na web e descubra quais são os melhores países para viajar sozinho<code></p><hr></p><h2><strong>Configuração para rodar o LLM em seu próprio servidor</strong>  </h2></p><p>Se você tem um computador potente ou um servidor disponível, mas deseja usá-lo a partir do seu laptop, você pode rodar o LLM em um servidor remoto usando nosso servidor LLM personalizado.</p><p>No seu "servidor" que irá rodar o modelo de IA, obtenha o endereço IP
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip local
curl https://ipinfo.io/ip # ip público
<pre><code class="language-">
Nota: Para Windows ou macOS, use ipconfig ou ifconfig respectivamente para encontrar o endereço IP.</p><p>Clone o repositório e entre na pasta </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Instale os requisitos específicos do servidor:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Execute o script do servidor.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Você pode escolher entre usar </code>ollama<code> e </code>llamacpp<code> como serviço LLM.</p><p>
Agora no seu computador pessoal:</p><p>Altere o arquivo </code>config.ini<code> para definir </code>provider_name<code> como </code>server<code> e </code>provider_model<code> como </code>deepseek-r1:xxb<code>.
Defina o </code>provider_server_address<code> para o endereço IP da máquina que irá rodar o modelo.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>Próxima etapa: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Inicie os serviços e rode o AgenticSeek</a>  </p><hr></p><h2>Reconhecimento de fala (Speech to Text)</h2></p><p>Atenção: o reconhecimento de fala só funciona no modo CLI no momento.</p><p>Por favor, note que atualmente o reconhecimento de fala só funciona em inglês.</p><p>A funcionalidade de reconhecimento de fala vem desabilitada por padrão. Para habilitá-la, defina a opção listen como True no arquivo config.ini:
</code></pre>
listen = True
<pre><code class="language-">
Quando habilitada, a função de reconhecimento de fala escuta por uma palavra-chave de ativação, que é o nome do agente, antes de começar a processar sua entrada. Você pode personalizar o nome do agente atualizando o valor </code>agent_name<code> no arquivo <em>config.ini</em>:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Para reconhecimento ideal, recomendamos o uso de um nome comum em inglês como "John" ou "Emma" como nome do agente</p><p>Assim que você ver a transcrição começar a aparecer, diga o nome do agente em voz alta para acordá-lo (por exemplo, "Friday").</p><p>Fale sua pergunta claramente.</p><p>Encerre seu pedido com uma frase de confirmação para sinalizar ao sistema que deve prosseguir. Exemplos de frases de confirmação incluem:</code></pre>
"faça isso", "vai em frente", "execute", "rodar", "iniciar", "obrigado", "pode ser?", "por favor", "ok?", "prossiga", "continue", "vá em frente", "faça isso", "entendeu?"
<pre><code class="language-">
<h2>Config</h2></p><p>Exemplo de configuração:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemplo para Ollama; use http://127.0.0.1:1234 para LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Lista de idiomas para TTS e potencialmente roteamento.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Explicação das configurações do </code>config.ini<code></strong>:</p><ul><li>  <strong>Seção </code>[MAIN]<code>:</strong></li>
    <li>  </code>is_local<code>: </code>True<code> se estiver usando um provedor LLM local (Ollama, LM-Studio, servidor local compatível com OpenAI) ou a opção de servidor auto-hospedado. </code>False<code> se estiver usando uma API em nuvem (OpenAI, Google, etc.).</li>
    <li>  </code>provider_name<code>: Especifica o provedor LLM.</li>
        <li>  Opções locais: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (para servidores locais compatíveis com OpenAI), </code>server<code> (para configuração de servidor auto-hospedado).</li>
        <li>  Opções de API: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: O nome ou ID do modelo específico para o provedor escolhido (por exemplo, </code>deepseekcoder:6.7b<code> para Ollama, </code>gpt-3.5-turbo<code> para API OpenAI, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> para TogetherAI).</li>
    <li>  </code>provider_server_address<code>: O endereço do seu provedor LLM.</li>
        <li>  Para provedores locais: por exemplo, </code>http://127.0.0.1:11434<code> para Ollama, </code>http://127.0.0.1:1234<code> para LM-Studio.</li>
        <li>  Para o tipo de provedor </code>server<code>: o endereço do seu servidor LLM auto-hospedado (por exemplo, </code>http://seu_ip_servidor:3333<code>).</li>
        <li>  Para APIs em nuvem (</code>is_local = False<code>): geralmente é ignorado ou pode ser deixado em branco, pois o endpoint da API é normalmente gerenciado pela biblioteca cliente.</li>
    <li>  </code>agent_name<code>: Nome do assistente de IA (por exemplo, Friday). Usado como palavra de ativação para reconhecimento de fala, se ativado.</li>
    <li>  </code>recover_last_session<code>: </code>True<code> para tentar restaurar o estado da sessão anterior, </code>False<code> para iniciar do zero.</li>
    <li>  </code>save_session<code>: </code>True<code> para salvar o estado da sessão atual para possível recuperação, </code>False<code> caso contrário.</li>
    <li>  </code>speak<code>: </code>True<code> para ativar saída de voz via texto para fala, </code>False<code> para desativar.</li>
    <li>  </code>listen<code>: </code>True<code> para ativar entrada de voz via reconhecimento de fala (somente modo CLI), </code>False<code> para desativar.</li>
    <li>  </code>work_dir<code>: <strong>Crucial:</strong> O diretório onde o AgenticSeek irá ler/gravar arquivos. <strong>Certifique-se de que este caminho é válido e acessível em seu sistema.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> para usar um prompt de sistema mais "estilo Jarvis" (experimental), </code>False<code> para o prompt padrão.</li>
    <li>  </code>languages<code>: Uma lista de idiomas separada por vírgulas (por exemplo, </code>en, zh, fr<code>). Usada para seleção de voz TTS (padrão é o primeiro) e pode auxiliar o roteador LLM. Evite muitos idiomas ou idiomas muito semelhantes para eficiência do roteador.</li>
<li>  <strong>Seção </code>[BROWSER]<code>:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code> para rodar o navegador automatizado sem janela visível (recomendado para interface web ou uso não interativo). </code>False<code> para exibir a janela do navegador (útil para modo CLI ou depuração).</li>
    <li>  </code>stealth_mode<code>: </code>True<code> para ativar medidas que dificultam a detecção da automação do navegador. Pode exigir instalação manual de extensões como anticaptcha.</li></p><p></ul>Esta seção resume os tipos de provedores LLM suportados. Configure-os no </code>config.ini<code>.</p><p><strong>Provedores Locais (Executados no Seu Próprio Hardware):</strong></p><p>| Nome do Provedor no </code>config.ini<code> | </code>is_local<code> | Descrição                                                                 | Seção de Configuração                                               |
|-------------------------------|------------|-------------------------------------------------------------------------|---------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Usa o Ollama para servir LLMs locais.                                   | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuração para rodar LLM localmente</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Usa o LM-Studio para servir LLMs locais.                                | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuração para rodar LLM localmente</a> |
| </code>openai<code> (para servidor local)| </code>True<code>     | Conecta a um servidor local que expõe uma API compatível com OpenAI (ex.: llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuração para rodar LLM localmente</a> |
| </code>server<code>                      | </code>False<code>    | Conecta ao servidor LLM auto-hospedado do AgenticSeek rodando em outra máquina. | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Configuração para rodar o LLM no seu próprio servidor</a> |</p><p><strong>Provedores de API (Baseados em Nuvem):</strong></p><p>| Nome do Provedor no </code>config.ini<code> | </code>is_local<code> | Descrição                                       | Seção de Configuração                               |
|-------------------------------|------------|------------------------------------------------|-----------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Usa a API oficial da OpenAI (ex.: GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a> |
| </code>google<code>                      | </code>False<code>    | Usa modelos Gemini do Google via API.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a> |
| </code>deepseek<code>                    | </code>False<code>    | Usa a API oficial Deepseek.                     | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a> |
| </code>huggingface<code>                 | </code>False<code>    | Usa a API de Inferência do Hugging Face.        | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a> |
| </code>togetherAI<code>                  | </code>False<code>    | Usa a API TogetherAI para vários modelos abertos.| <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuração para rodar com uma API</a> |</p><hr>
<h2>Solução de Problemas</h2></p><p>Se você encontrar problemas, esta seção traz orientações.</p><h1>Problemas Conhecidos</h1></p><h2>Problemas com o ChromeDriver</h2></p><p><strong>Exemplo de Erro:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Causa:</strong> A versão do ChromeDriver instalada é incompatível com a versão do seu navegador Google Chrome.</li>
<li>  <strong>Solução:</strong></li>
    <li> <strong>Verifique a Versão do Chrome:</strong> Abra o Google Chrome, vá em </code>Configurações > Sobre o Chrome<code> para encontrar sua versão (exemplo: "Versão 120.0.6099.110").</li>
    <li> <strong>Baixe o ChromeDriver Correspondente:</strong></li>
        <li>  Para versões do Chrome 115 ou mais recentes: Acesse os <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Endpoints JSON do Chrome for Testing (CfT)</a>. Encontre o canal "stable" e baixe o ChromeDriver para seu SO que corresponda à versão principal do seu Chrome.</li>
        <li>  Para versões mais antigas (menos comuns): Você pode encontrá-las na página <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  A imagem abaixo mostra um exemplo da página CfT:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Baixe a versão específica do Chromedriver na página Chrome for Testing">
    <ul><li> <strong>Instale o ChromeDriver:</strong></li>
        <li>  Certifique-se de que o </code>chromedriver<code> baixado (ou </code>chromedriver.exe<code> no Windows) esteja em um diretório listado na variável de ambiente PATH do seu sistema (ex.: </code>/usr/local/bin<code> no Linux/macOS ou uma pasta de scripts personalizada adicionada ao PATH no Windows).</li>
        <li>  Alternativamente, coloque-o no diretório raiz do projeto </code>agenticSeek<code>.</li>
        <li>  Certifique-se de que o driver seja executável (ex.: </code>chmod +x chromedriver<code> no Linux/macOS).</li>
    <li> Consulte a seção <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">Instalação do ChromeDriver</a> no guia principal de Instalação para mais detalhes.</li></p><p></ul>Se esta seção estiver incompleta ou você encontrar outros problemas com o ChromeDriver, considere pesquisar nos <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">Issues do GitHub</a> existentes ou abrir um novo.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Isso ocorre se houver incompatibilidade entre a versão do navegador e do chromedriver.</p><p>Você precisa acessar e baixar a versão mais recente:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Se você estiver usando o Chrome versão 115 ou mais recente, acesse:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>E baixe a versão do chromedriver correspondente ao seu SO.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Se esta seção estiver incompleta, por favor, abra uma issue.</p><h2>Problemas com connection adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Nota: a porta pode variar)
<pre><code class="language-">
<ul><li>  <strong>Causa:</strong> O <code>provider_server_address</code> no <code>config.ini</code> para o <code>lm-studio</code> (ou outros servidores locais compatíveis com OpenAI) está sem o prefixo <code>http://</code> ou está apontando para a porta errada.</li>
<li>  <strong>Solução:</strong></li>
    <li>  Certifique-se de que o endereço inclua <code>http://</code>. O LM-Studio normalmente usa <code>http://127.0.0.1:1234</code>.</li>
    <li>  Corrija no <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (ou a porta real do seu servidor LM-Studio).</li></p><p></ul><h2>SearxNG Base URL Não Fornecido</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>Perguntas Frequentes (FAQ)</h2></p><p><strong>P: Que hardware eu preciso?</strong>  </p><p>| Tamanho do Modelo | GPU           | Comentário                                                                                  |
|-------------------|---------------|--------------------------------------------------------------------------------------------|
| 7B                | 8GB Vram      | ⚠️ Não recomendado. Baixo desempenho, alucinações frequentes, agentes planejadores devem falhar. |
| 14B               | 12 GB VRAM (ex.: RTX 3060) | ✅ Utilizável para tarefas simples. Pode ter dificuldades com navegação web e planejamento. |
| 32B               | 24+ GB VRAM (ex.: RTX 4090) | 🚀 Sucesso na maioria das tarefas, pode ainda ter dificuldades com planejamento de tarefas  |
| 70B+              | 48+ GB Vram   | 💪 Excelente. Recomendado para casos de uso avançados.                                      |</p><p><strong>P: Recebo um erro, o que faço?</strong>  </p><p>Certifique-se de que o local está rodando (</code>ollama serve<code>), que seu </code>config.ini` corresponde ao seu provedor e que as dependências estão instaladas. Se nada funcionar, sinta-se à vontade para abrir uma issue.</p><p><strong>P: Realmente pode rodar 100% localmente?</strong>  </p><p>Sim, com Ollama, lm-studio ou provedores de servidor, todo reconhecimento de fala, LLM e modelo de texto para fala rodam localmente. Opções não locais (OpenAI ou outras APIs) são opcionais.</p><p><strong>P: Por que devo usar o AgenticSeek se já tenho o Manus?</strong></p><p>Ao contrário do Manus, o AgenticSeek prioriza independência de sistemas externos, oferecendo mais controle, privacidade e evitando custos de API.</p><p><strong>P: Quem está por trás do projeto?</strong></p><p>O projeto foi criado por mim, junto com dois amigos que atuam como mantenedores e colaboradores da comunidade open source no GitHub. Somos apenas um grupo de pessoas apaixonadas, não uma startup ou afiliados a qualquer organização.</p><p>Qualquer conta do AgenticSeek no X além da minha pessoal (https://x.com/Martin993886460) é uma falsificação.</p><h2>Contribua</h2></p><p>Estamos procurando desenvolvedores para melhorar o AgenticSeek! Veja as issues abertas ou discussões.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Guia de contribuição</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Mantenedores:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Horário de Paris </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Horário de Taipei</p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Horário de Taipei</p><h2>Agradecimentos Especiais:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> e <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> pela ajuda com dockerização do backend</p><h2>Patrocinadores:</h2></p><p>Patrocinadores mensais de 5 dólares ou mais aparecem aqui:
<ul><li><strong>tatra-labs</strong></li></p><p></ul>I'm sorry, but you haven't provided the content of Part 4 of 4 to translate. Please provide the text you'd like translated.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>