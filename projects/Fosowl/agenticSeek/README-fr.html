<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in French. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in French. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, French, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in French. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-fr.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">French</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek : Alternative privée et locale à Manus.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>Une <strong>alternative 100% locale à Manus AI</strong>, cet assistant IA activé par la voix navigue de façon autonome sur le web, écrit du code et planifie des tâches tout en gardant toutes les données sur votre appareil. Conçu pour les modèles de raisonnement locaux, il fonctionne entièrement sur votre matériel, garantissant une confidentialité totale et aucune dépendance au cloud.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visiter AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Pourquoi AgenticSeek ?</h3></p><ul><li>🔒 Entièrement local & privé - Tout s'exécute sur votre machine — pas de cloud, pas de partage de données. Vos fichiers, conversations et recherches restent privés.</li></p><p><li>🌐 Navigation web intelligente - AgenticSeek peut naviguer sur Internet de façon autonome — rechercher, lire, extraire des informations, remplir des formulaires web — tout cela en mode mains libres.</li></p><p><li>💻 Assistant de codage autonome - Besoin de code ? Il peut écrire, déboguer et exécuter des programmes en Python, C, Go, Java, et plus encore — sans supervision.</li></p><p><li>🧠 Sélection intelligente d’agents - Vous demandez, il choisit automatiquement le meilleur agent pour la tâche. Comme si vous aviez une équipe d’experts à disposition.</li></p><p><li>📋 Planifie & exécute des tâches complexes - De la planification de voyage à la gestion de projets complexes — il peut découper de grandes tâches en étapes et les réaliser en utilisant plusieurs agents IA.</li></p><p><li>🎙️ Activation vocale - Voix propre, rapide et futuriste, et transcription vocale permettant de lui parler comme à votre IA personnelle d’un film de science-fiction. (En cours de développement)</li></p><p></ul><h3><strong>Démo</strong></h3></p><blockquote><em>Peux-tu rechercher le projet agenticSeek, apprendre quelles compétences sont requises, puis ouvrir le CV_candidates.zip et ensuite me dire lesquelles correspondent le mieux au projet</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Avertissement : Cette démo, y compris tous les fichiers qui apparaissent (ex : CV_candidates.zip), est entièrement fictive. Nous ne sommes pas une entreprise, nous recherchons des contributeurs open-source, pas des candidats.</p><blockquote>🛠⚠️️ <strong>Travail en cours actif</strong></blockquote></p><blockquote>🙏 Ce projet a commencé comme un projet annexe, sans feuille de route ni financement. Il a largement dépassé mes attentes en finissant dans GitHub Trending. Les contributions, retours et votre patience sont grandement appréciés.</blockquote></p><h2>Prérequis</h2></p><p>Avant de commencer, assurez-vous d’avoir les logiciels suivants installés :</p><ul><li>  <strong>Git :</strong> Pour cloner le dépôt. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Télécharger Git</a></li>
<li>  <strong>Python 3.10.x :</strong> Nous recommandons fortement d’utiliser Python version 3.10.x. L’utilisation d’autres versions peut entraîner des erreurs de dépendances. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Télécharger Python 3.10</a> (choisissez une version 3.10.x).</li>
<li>  <strong>Docker Engine & Docker Compose :</strong> Pour exécuter les services groupés comme SearxNG.</li>
    <li>  Installer Docker Desktop (qui inclut Docker Compose V2) : <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Alternativement, installez Docker Engine et Docker Compose séparément sur Linux : <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (assurez-vous d’installer Compose V2, ex : <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Cloner le dépôt et configurer</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Modifier le contenu du fichier .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>Mettez à jour le fichier <code>.env</code> avec vos propres valeurs si besoin :</p><ul><li><strong>SEARXNG_BASE_URL</strong> : Laisser inchangé </li>
<li><strong>REDIS_BASE_URL</strong> : Laisser inchangé </li>
<li><strong>WORK_DIR</strong> : Chemin vers votre dossier de travail local. AgenticSeek pourra lire et interagir avec ces fichiers.</li>
<li><strong>OLLAMA_PORT</strong> : Numéro de port pour le service Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong> : Numéro de port pour le service LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong> : Port pour tout service LLM personnalisé supplémentaire.</li></p><p></ul><strong>Les clés API sont totalement optionnelles pour les utilisateurs qui choisissent de faire tourner les LLM localement. C’est le but principal de ce projet. Laissez vide si vous avez un matériel suffisant</strong></p><h3>3. <strong>Démarrer Docker</strong></h3></p><p>Assurez-vous que Docker est installé et en cours d’exécution sur votre système. Vous pouvez démarrer Docker avec les commandes suivantes :</p><ul><li><strong>Sur Linux/macOS :</strong>  </li>
    </ul>Ouvrez un terminal et lancez :
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Ou lancez Docker Desktop depuis votre menu d’applications si installé.</p><ul><li><strong>Sur Windows :</strong>  </li>
    </ul>Lancez Docker Desktop depuis le menu Démarrer.</p><p>Vous pouvez vérifier que Docker fonctionne en exécutant :</code></pre>sh
docker info
<pre><code class="language-">Si vous voyez des informations sur votre installation Docker, cela fonctionne correctement.</p><p>Voir le tableau des <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Fournisseurs locaux</a> ci-dessous pour un résumé.</p><p>Prochaine étape : <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Lancer AgenticSeek en local</a></p><p><em>Voir la section <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Dépannage</a> si vous rencontrez des problèmes.</em>
<em>Si votre matériel ne peut pas exécuter les LLM localement, voir <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utilisation via API</a>.</em>
<em>Pour une explication détaillée de </code>config.ini<code>, voir la <a href="#config" target="_blank" rel="noopener noreferrer">Section Config</a>.</em></p><hr></p><h2>Configuration pour faire tourner un LLM localement sur votre machine</h2></p><p><strong>Configuration matérielle requise :</strong></p><p>Pour faire tourner les LLM localement, vous aurez besoin d’un matériel suffisant. Au minimum, un GPU capable d’exécuter Magistral, Qwen ou Deepseek 14B est requis. Voir la FAQ pour des recommandations détaillées sur les modèles/performances.</p><p><strong>Démarrez votre fournisseur local</strong>  </p><p>Lancez votre fournisseur local, par exemple avec ollama :
</code></pre>sh
ollama serve
<pre><code class="language-">
Voir ci-dessous la liste des fournisseurs locaux pris en charge.</p><p><strong>Mettre à jour le config.ini</strong></p><p>Modifiez le fichier config.ini pour définir provider_name sur un fournisseur pris en charge et provider_model sur un LLM supporté par votre fournisseur. Nous recommandons un modèle de raisonnement tel que <em>Magistral</em> ou <em>Deepseek</em>.</p><p>Voir la <strong>FAQ</strong> à la fin du README pour le matériel requis.
</code></pre>sh
[MAIN]
is_local = True # Si vous exécutez localement ou avec un fournisseur distant.
provider_name = ollama # ou lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choisissez un modèle adapté à votre matériel
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nom de votre IA
recover_last_session = True # pour récupérer la session précédente
save_session = True # pour mémoriser la session courante
speak = False # texte vers parole
listen = False # reconnaissance vocale, uniquement pour CLI, expérimental
jarvis_personality = False # Pour un comportement plus "Jarvis" (expérimental)
languages = en zh # Liste des langues, la synthèse vocale utilisera la première langue de la liste
[BROWSER]
headless_browser = True # laisser inchangé sauf utilisation CLI sur hôte.
stealth_mode = True # Utilise selenium indétectable pour réduire la détection par les sites
<pre><code class="language-">
<strong>Attention</strong> :</p><ul><li>Le format du fichier </code>config.ini<code> ne supporte pas les commentaires. </li>
</ul>Ne copiez/collez pas la configuration d’exemple directement, car les commentaires provoqueront des erreurs. Modifiez plutôt manuellement le fichier </code>config.ini<code> selon vos besoins, sans commentaires.</p><ul><li>Ne mettez <em>PAS</em> provider_name à </code>openai<code> si vous utilisez LM-studio pour exécuter les LLM. Utilisez </code>lm-studio<code>.</li></p><p><li>Certains fournisseurs (ex : lm-studio) exigent </code>http://<code> devant l’IP. Par exemple </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Liste des fournisseurs locaux</strong></p><p>| Fournisseur  | Local ? | Description                                               |
|--------------|---------|-----------------------------------------------------------|
| ollama       | Oui     | Exécutez des LLM localement facilement avec ollama        |
| lm-studio    | Oui     | Exécutez un LLM localement avec LM studio (utiliser </code>provider_name<code> à </code>lm-studio<code>)|
| openai       | Oui     | Utilisez une API compatible openai (ex : serveur llama.cpp)  |</p><p>Prochaine étape : <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Démarrer les services et lancer AgenticSeek</a>  </p><p><em>Voir la section <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Dépannage</a> si vous rencontrez des problèmes.</em>
<em>Si votre matériel ne peut pas exécuter les LLM localement, voir <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utilisation via API</a>.</em>
<em>Pour une explication détaillée de </code>config.ini<code>, voir la <a href="#config" target="_blank" rel="noopener noreferrer">Section Config</a>.</em></p><h2>Configuration pour utilisation via API</h2></p><p>Cette configuration utilise des fournisseurs LLM externes, basés sur le cloud. Vous aurez besoin d’une clé API provenant du service choisi.</p><p><strong>1. Choisissez un fournisseur API et obtenez une clé API :</strong></p><p>Consultez la <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Liste des fournisseurs API</a> ci-dessous. Rendez-vous sur leur site web pour vous inscrire et obtenir une clé API.</p><p><strong>2. Définissez votre clé API comme variable d’environnement :</strong></p><ul><li>  <strong>Linux/macOS :</strong></li>
    </ul>Ouvrez votre terminal et utilisez la commande </code>export<code>. Il est conseillé de l’ajouter à votre fichier de profil de shell (ex : </code>~/.bashrc<code>, </code>~/.zshrc<code>) pour la persistance.
    </code>`<code>sh
    export PROVIDER_API_KEY="votre_cle_api_ici" 
    # Remplacez PROVIDER_API_KEY par le nom de variable spécifique, ex : OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Exemple pour TogetherAI :
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows :</strong></li>
<li>  <strong>Invite de commandes (Temporaire pour la session en cours) :</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (Temporaire pour la session en cours) :</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>De façon permanente :</strong> Recherchez "variables d'environnement" dans la barre de recherche Windows, cliquez sur "Modifier les variables d'environnement système", puis cliquez sur le bouton "Variables d'environnement...". Ajoutez une nouvelle variable utilisateur avec le nom approprié (par exemple, </code>OPENAI_API_KEY<code>) et votre clé comme valeur.</li></p><p></ul><em>(Voir la FAQ : <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Comment définir les clés API ?</a> pour plus de détails).</em></p><p><strong>3. Mettre à jour </code>config.ini<code> :</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Ou google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Ou gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Généralement ignoré ou peut rester vide lorsque is_local = False pour la plupart des APIs
<h1>... autres paramètres ...</h1>
<pre><code class="language-"><em>Attention :</em> Assurez-vous qu'il n'y a pas d'espaces à la fin des valeurs dans le fichier </code>config.ini<code>.</p><p><strong>Liste des fournisseurs d'API</strong></p><p>| Fournisseur   | </code>provider_name<code> | Local ? | Description                                           | Lien de clé API (Exemples)                     |
|---------------|-----------------|---------|------------------------------------------------------|-----------------------------------------------|
| OpenAI        | </code>openai<code>        | Non     | Utilise les modèles ChatGPT via l'API d'OpenAI.      | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini | </code>google<code>        | Non     | Utilise les modèles Google Gemini via Google AI Studio.| <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek      | </code>deepseek<code>      | Non     | Utilise les modèles Deepseek via leur API.            | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face  | </code>huggingface<code>   | Non     | Utilise les modèles de l'API Hugging Face Inference.  | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI    | </code>togetherAI<code>    | Non     | Utilise divers modèles open source via l'API TogetherAI.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Remarque :</em>
<ul><li>  Nous déconseillons l'utilisation de </code>gpt-4o<code> ou d'autres modèles OpenAI pour la navigation web complexe et la planification de tâches, car les optimisations actuelles des prompts sont adaptées à des modèles comme Deepseek.</li>
<li>  Les tâches de codage/bash peuvent rencontrer des problèmes avec Gemini, car il ne suit pas toujours strictement le formatage des prompts optimisés pour Deepseek.</li>
<li>  Le champ </code>provider_server_address<code> dans </code>config.ini<code> n'est généralement pas utilisé lorsque </code>is_local = False<code> car le point d'accès API est généralement codé en dur dans la bibliothèque du fournisseur concerné.</li></p><p></ul>Étape suivante : <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Démarrer les services et lancer AgenticSeek</a></p><p><em>Voir la section <strong>Problèmes connus</strong> si vous rencontrez des difficultés</em></p><p><em>Voir la section <strong>Config</strong> pour une explication détaillée du fichier de configuration.</em></p><hr></p><h2>Démarrer les services et lancer AgenticSeek</h2></p><p>Par défaut, AgenticSeek fonctionne entièrement dans Docker.</p><p>Démarrez les services requis. Cela démarrera tous les services du fichier docker-compose.yml, y compris :
    <ul><li>searxng</li>
    <li>redis (requis par searxng)</li>
    <li>frontend</li>
    <li>backend (si vous utilisez </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
<pre><code class="language-">
<strong>Attention :</strong> Cette étape téléchargera et chargera toutes les images Docker, ce qui peut prendre jusqu'à 30 minutes. Après le démarrage des services, veuillez attendre que le service backend soit complètement opérationnel (vous devriez voir <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> dans le journal) avant d'envoyer des messages. Les services backend peuvent prendre 5 minutes à démarrer lors du premier lancement.</p><p>Accédez à </code>http://localhost:3000/<code> et vous devriez voir l'interface web.</p><p><em>Dépannage du démarrage des services :</em> Si ces scripts échouent, assurez-vous que Docker Engine est en cours d'exécution et que Docker Compose (V2, </code>docker compose<code>) est correctement installé. Vérifiez les messages d'erreur dans le terminal. Voir <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ : Aide ! J'obtiens une erreur lors de l'exécution d'AgenticSeek ou de ses scripts.</a></p><p><strong>Optionnel :</strong> Exécuter sur l'hôte (mode CLI) :</p><p>Pour utiliser l'interface CLI, vous devez installer le paquet sur l'hôte :
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Démarrez les services :
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
<pre><code class="language-">
Utilisez la CLI : </code>python3 cli.py<code></p><hr></p><h2>Utilisation</h2></p><p>Assurez-vous que les services sont actifs avec </code>./start_services.sh full<code> et rendez-vous sur </code>localhost:3000<code> pour l'interface web.</p><p>Vous pouvez également utiliser la reconnaissance vocale en définissant </code>listen = True<code> dans la configuration. Uniquement en mode CLI.</p><p>Pour quitter, dites/écrivez simplement </code>goodbye<code>.</p><p>Voici quelques exemples d'utilisation :</p><blockquote><em>Créer un jeu du serpent en python !</em></blockquote></p><blockquote><em>Cherche sur le web les meilleurs cafés à Rennes, France, et sauvegarde une liste de trois avec leurs adresses dans rennes_cafes.txt.</em></blockquote></p><blockquote><em>Écris un programme Go pour calculer la factorielle d'un nombre, sauvegarde-le sous factorial.go dans ton espace de travail</em></blockquote></p><blockquote><em>Recherche dans mon dossier summer_pictures tous les fichiers JPG, renomme-les avec la date d’aujourd’hui, et sauvegarde la liste des fichiers renommés dans photos_list.txt</em></blockquote></p><blockquote><em>Recherche en ligne les films de science-fiction populaires de 2024 et choisis-en trois à regarder ce soir. Sauvegarde la liste dans movie_night.txt.</em></blockquote></p><blockquote><em>Cherche sur le web les derniers articles d’actualité IA de 2025, sélectionne-en trois, et écris un script Python pour extraire leurs titres et résumés. Sauvegarde le script sous news_scraper.py et les résumés dans ai_news.txt dans /home/projects</em></blockquote></p><blockquote><em>Vendredi, cherche sur le web une API gratuite de prix d’actions, inscris-toi avec supersuper7434567@gmail.com puis écris un script Python pour récupérer les prix quotidiens de Tesla via l’API et sauvegarde les résultats dans stock_prices.csv</em></blockquote></p><p><em>Notez que les capacités de remplissage de formulaire sont encore expérimentales et peuvent échouer.</em></p><p>Après avoir saisi votre requête, AgenticSeek allouera le meilleur agent pour la tâche.</p><p>Comme il s'agit d'un prototype précoce, le système de routage des agents peut ne pas toujours choisir le bon agent en fonction de votre requête.</p><p>Par conséquent, vous devez être très explicite sur ce que vous voulez et comment l’IA doit procéder ; par exemple, si vous souhaitez qu’elle effectue une recherche web, ne dites pas :</p><p></code>Connais-tu de bons pays pour voyager en solo ?<code></p><p>Demandez plutôt :</p><p></code>Fais une recherche web et trouve quels sont les meilleurs pays pour voyager en solo<code></p><hr></p><h2><strong>Configuration pour exécuter le LLM sur votre propre serveur</strong>  </h2></p><p>Si vous disposez d'un ordinateur puissant ou d'un serveur que vous pouvez utiliser, mais que vous souhaitez l'utiliser à distance depuis votre ordinateur portable, vous avez la possibilité d'exécuter le LLM sur un serveur distant en utilisant notre serveur llm personnalisé.</p><p>Sur votre "serveur" qui exécutera le modèle IA, récupérez l'adresse IP
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip publique
<pre><code class="language-">
Remarque : Pour Windows ou macOS, utilisez respectivement ipconfig ou ifconfig pour trouver l'adresse IP.</p><p>Clonez le dépôt et entrez dans le dossier </code>server/<code>.
</code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Installez les dépendances spécifiques au serveur :
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Exécutez le script serveur.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Vous avez le choix entre utiliser </code>ollama<code> ou </code>llamacpp<code> comme service LLM.</p><p>Maintenant sur votre ordinateur personnel :</p><p>Modifiez le fichier </code>config.ini<code> pour définir </code>provider_name<code> sur </code>server<code> et </code>provider_model<code> sur </code>deepseek-r1:xxb<code>.
Définissez </code>provider_server_address<code> sur l'adresse IP de la machine qui exécutera le modèle.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-">
Étape suivante : <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Démarrer les services et lancer AgenticSeek</a>  </p><hr></p><h2>Reconnaissance vocale (Speech to Text)</h2></p><p>Attention : la reconnaissance vocale ne fonctionne actuellement qu’en mode CLI.</p><p>Veuillez noter qu’actuellement la reconnaissance vocale ne fonctionne qu’en anglais.</p><p>La fonctionnalité de reconnaissance vocale est désactivée par défaut. Pour l’activer, définissez l’option listen sur True dans le fichier config.ini :
</code></pre>
listen = True
<pre><code class="language-">
Lorsqu’elle est activée, la reconnaissance vocale attend un mot-clé déclencheur, qui est le nom de l’agent, avant de commencer à traiter votre entrée. Vous pouvez personnaliser le nom de l’agent en modifiant la valeur </code>agent_name<code> dans le fichier <em>config.ini</em> :
</code></pre>
agent_name = Friday
<pre><code class="language-">
Pour une reconnaissance optimale, nous recommandons d'utiliser un prénom anglais courant comme "John" ou "Emma" comme nom d'agent.</p><p>Une fois que vous voyez la transcription apparaître, dites le nom de l'agent à voix haute pour le réveiller (ex. : "Friday").</p><p>Énoncez clairement votre requête.</p><p>Terminez votre demande par une phrase de confirmation pour indiquer au système de procéder. Exemples de phrases de confirmation :</code></pre>
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
<pre><code class="language-">
<h2>Config</h2></p><p>Exemple de configuration :</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Exemple pour Ollama ; utilisez http://127.0.0.1:1234 pour LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Liste des langues pour la synthèse vocale et potentiellement le routage.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Explication des paramètres </code>config.ini<code></strong> :</p><ul><li>  <strong>Section </strong></code>[MAIN]<code><strong> :</strong></li>
    <li>  </code>is_local<code> : </code>True<code> si vous utilisez un fournisseur LLM local (Ollama, LM-Studio, serveur compatible OpenAI local) ou l'option serveur auto-hébergé. </code>False<code> si vous utilisez une API cloud (OpenAI, Google, etc.).</li>
    <li>  </code>provider_name<code> : Spécifie le fournisseur LLM.</li>
        <li>  Options locales : </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (pour les serveurs compatibles OpenAI locaux), </code>server<code> (pour l'installation en auto-hébergement).</li>
        <li>  Options API : </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code> : Le nom ou ID du modèle spécifique pour le fournisseur choisi (ex. : </code>deepseekcoder:6.7b<code> pour Ollama, </code>gpt-3.5-turbo<code> pour l'API OpenAI, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> pour TogetherAI).</li>
    <li>  </code>provider_server_address<code> : L'adresse de votre fournisseur LLM.</li>
        <li>  Pour les fournisseurs locaux : ex. </code>http://127.0.0.1:11434<code> pour Ollama, </code>http://127.0.0.1:1234<code> pour LM-Studio.</li>
        <li>  Pour le type de fournisseur </code>server<code> : l'adresse de votre serveur LLM auto-hébergé (ex. : </code>http://votre_ip_serveur:3333<code>).</li>
        <li>  Pour les API cloud (</code>is_local = False<code>) : cela est souvent ignoré ou peut rester vide, car le point de terminaison de l'API est généralement géré par la bibliothèque cliente.</li>
    <li>  </code>agent_name<code> : Nom de l'assistant IA (ex. : Friday). Utilisé comme mot déclencheur pour la reconnaissance vocale si activée.</li>
    <li>  </code>recover_last_session<code> : </code>True<code> pour tenter de restaurer l'état de la session précédente, </code>False<code> pour recommencer à zéro.</li>
    <li>  </code>save_session<code> : </code>True<code> pour sauvegarder l'état actuel de la session pour une éventuelle récupération, </code>False<code> sinon.</li>
    <li>  </code>speak<code> : </code>True<code> pour activer la synthèse vocale, </code>False<code> pour désactiver.</li>
    <li>  </code>listen<code> : </code>True<code> pour activer la reconnaissance vocale (mode CLI uniquement), </code>False<code> pour désactiver.</li>
    <li>  </code>work_dir<code> : <strong>Crucial :</strong> Le répertoire dans lequel AgenticSeek lira/écrira les fichiers. <strong>Assurez-vous que ce chemin est valide et accessible sur votre système.</strong></li>
    <li>  </code>jarvis_personality<code> : </code>True<code> pour utiliser un prompt système de type "Jarvis" (expérimental), </code>False<code> pour le prompt standard.</li>
    <li>  </code>languages<code> : Liste de langues séparées par des virgules (ex. : </code>en, zh, fr<code>). Utilisé pour la sélection de la voix TTS (par défaut la première) et peut aider le routeur LLM. Évitez d’indiquer trop de langues ou des langues très similaires pour l’efficacité du routeur.</li>
<li>  <strong>Section </strong></code>[BROWSER]<code><strong> :</strong></li>
    <li>  </code>headless_browser<code> : </code>True<code> pour exécuter le navigateur automatisé sans fenêtre visible (recommandé pour l’interface web ou une utilisation non interactive). </code>False<code> pour afficher la fenêtre du navigateur (utile pour le mode CLI ou le débogage).</li>
    <li>  </code>stealth_mode<code> : </code>True<code> pour activer des mesures rendant l’automatisation du navigateur plus difficile à détecter. Peut nécessiter l’installation manuelle d’extensions comme anticaptcha.</li></p><p>
</ul>Cette section résume les types de fournisseurs LLM supportés. Configurez-les dans </code>config.ini<code>.</p><p><strong>Fournisseurs locaux (exécutés sur votre matériel) :</strong></p><p>| Nom du fournisseur dans </code>config.ini<code> | </code>is_local<code> | Description                                                                 | Section de configuration                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Utilise Ollama pour servir des LLM locaux.                                  | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuration pour exécuter un LLM localement</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Utilise LM-Studio pour servir des LLM locaux.                               | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuration pour exécuter un LLM localement</a> |
| </code>openai<code> (pour serveur local) | </code>True<code>     | Connexion à un serveur local exposant une API compatible OpenAI (ex : llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Configuration pour exécuter un LLM localement</a> |
| </code>server<code>                      | </code>False<code>    | Connexion au serveur LLM AgenticSeek auto-hébergé sur une autre machine.    | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Configuration pour exécuter le LLM sur votre propre serveur</a> |</p><p><strong>Fournisseurs API (basés sur le cloud) :</strong></p><p>| Nom du fournisseur dans </code>config.ini<code> | </code>is_local<code> | Description                                        | Section de configuration                                 |
|-------------------------------|------------|----------------------------------------------------|----------------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Utilise l’API officielle d’OpenAI (ex. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utiliser une API</a> |
| </code>google<code>                      | </code>False<code>    | Utilise les modèles Gemini de Google via API.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utiliser une API</a> |
| </code>deepseek<code>                    | </code>False<code>    | Utilise l’API officielle de Deepseek.                  | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utiliser une API</a> |
| </code>huggingface<code>                 | </code>False<code>    | Utilise l’API d’inférence de Hugging Face.             | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utiliser une API</a> |
| </code>togetherAI<code>                  | </code>False<code>    | Utilise l’API TogetherAI pour divers modèles open source. | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configuration pour utiliser une API</a> |</p><hr>
<h2>Dépannage</h2></p><p>Si vous rencontrez des problèmes, cette section vous guide.</p><h1>Problèmes connus</h1></p><h2>Problèmes de ChromeDriver</h2></p><p><strong>Exemple d’erreur :</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Cause :</strong> La version de ChromeDriver installée est incompatible avec la version de votre navigateur Google Chrome.</li>
<li>  <strong>Solution :</strong></li>
    <li> <strong>Vérifiez la version de Chrome :</strong> Ouvrez Google Chrome, allez dans </code>Paramètres > À propos de Chrome<code> pour trouver votre version (ex. : "Version 120.0.6099.110").</li>
    <li> <strong>Téléchargez le ChromeDriver correspondant :</strong></li>
        <li>  Pour Chrome version 115 et supérieure : Rendez-vous sur les <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Trouvez le canal "stable" et téléchargez le ChromeDriver pour votre OS correspondant à la version majeure de votre Chrome.</li>
        <li>  Pour les versions plus anciennes (plus rare) : Vous pouvez les trouver sur la page <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  L'image ci-dessous montre un exemple depuis la page CfT :</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Télécharger une version spécifique de Chromedriver sur la page Chrome for Testing">
    <ul><li> <strong>Installez ChromeDriver :</strong></li>
        <li>  Assurez-vous que le </code>chromedriver<code> téléchargé (ou </code>chromedriver.exe<code> sur Windows) est placé dans un dossier référencé par la variable d’environnement PATH de votre système (ex. : </code>/usr/local/bin<code> sous Linux/macOS, ou un dossier de scripts ajouté au PATH sous Windows).</li>
        <li>  Alternativement, placez-le dans le répertoire racine du projet </code>agenticSeek<code>.</li>
        <li>  Assurez-vous que le pilote est exécutable (ex. : </code>chmod +x chromedriver<code> sous Linux/macOS).</li>
    <li> Consultez la section <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">Installation de ChromeDriver</a> dans le guide principal d'installation pour plus de détails.</li></p><p></ul>Si cette section est incomplète ou si vous rencontrez d’autres problèmes avec ChromeDriver, veuillez consulter les <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">Issues GitHub existants</a> ou en ouvrir un nouveau.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Cela arrive s'il y a un décalage entre la version de votre navigateur et celle de chromedriver.</p><p>Vous devez télécharger la dernière version :</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Si vous utilisez Chrome version 115 ou supérieure, allez sur :</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>Et téléchargez la version de chromedriver correspondant à votre système d’exploitation.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Si cette section est incomplète, veuillez ouvrir une issue.</p><h2> Problèmes d’adaptateurs de connexion</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Note : le port peut varier)
<pre><code class="language-">
<ul><li>  <strong>Cause :</strong> Le paramètre <code>provider_server_address</code> dans <code>config.ini</code> pour <code>lm-studio</code> (ou tout autre serveur local compatible OpenAI) ne comporte pas le préfixe <code>http://</code> ou pointe vers le mauvais port.</li>
<li>  <strong>Solution :</strong></li>
    <li>  Assurez-vous que l’adresse commence par <code>http://</code>. LM-Studio utilise généralement <code>http://127.0.0.1:1234</code> par défaut.</li>
    <li>  Correction dans <code>config.ini</code> : <code>provider_server_address = http://127.0.0.1:1234</code> (ou le port réel de votre serveur LM-Studio).</li></p><p></ul><h2>URL de base SearxNG non fournie</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>Q : De quel matériel ai-je besoin ?</strong>  </p><p>| Taille du modèle  | GPU  | Remarques                                               |
|-------------------|------|--------------------------------------------------------|
| 7B                | 8GB Vram | ⚠️ Non recommandé. Performances faibles, hallucinations fréquentes, les agents planificateurs échoueront probablement. |
| 14B               | 12 GB VRAM (ex. RTX 3060) | ✅ Utilisable pour des tâches simples. Peut peiner sur la navigation web et la planification. |
| 32B               | 24+ GB VRAM (ex. RTX 4090) | 🚀 Réussite sur la plupart des tâches, peut encore avoir des difficultés pour la planification. |
| 70B+              | 48+ GB Vram | 💪 Excellent. Recommandé pour des usages avancés. |</p><p><strong>Q : J’ai une erreur, que faire ?</strong>  </p><p>Assurez-vous que le local est lancé (</code>ollama serve<code>), que votre </code>config.ini<code> correspond à votre fournisseur, et que les dépendances sont installées. Si rien ne fonctionne, n’hésitez pas à ouvrir une issue.</p><p><strong>Q : Peut-on vraiment tout faire tourner en local à 100% ?</strong>  </p><p>Oui, avec Ollama, lm-studio ou les fournisseurs </code>server`, toute la reconnaissance vocale, LLM et la synthèse vocale tournent en local. Les options non-locales (OpenAI ou autres API) sont facultatives.</p><p><strong>Q : Pourquoi utiliser AgenticSeek alors que j’ai Manus ?</strong></p><p>Contrairement à Manus, AgenticSeek privilégie l’indépendance vis-à-vis des systèmes externes, vous donnant plus de contrôle, de confidentialité et évitant les coûts d’API.</p><p><strong>Q : Qui est à l’origine du projet ?</strong></p><p>Le projet a été créé par moi-même, avec deux amis qui sont mainteneurs et contributeurs issus de la communauté open source sur GitHub. Nous ne sommes qu’un groupe de passionnés, pas une startup ni affiliés à une organisation.</p><p>Tout compte AgenticSeek sur X autre que mon compte personnel (https://x.com/Martin993886460) est une usurpation.</p><h2>Contribuer</h2></p><p>Nous recherchons des développeurs pour améliorer AgenticSeek ! Consultez les issues ouvertes ou les discussions.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Guide de contribution</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Mainteneurs :</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Fuseau horaire de Paris </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Fuseau horaire de Taipei </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Fuseau horaire de Taipei </p><h2>Remerciements particuliers :</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> et <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> pour leur aide à la dockerisation du backend</p><h2>Sponsors :</h2></p><p>Les sponsors mensuels de 5 $ ou plus apparaissent ici :
<ul><li><strong>tatra-labs</strong></li></p><p></ul>Certainly! However, you have not provided the content of "Part 4 of 4" to be translated. Please provide the text of the technical document you want translated into French, and I will proceed accordingly.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>