<!DOCTYPE html>
<html lang="nl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Dutch. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Dutch. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Dutch, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Dutch. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-nl.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Dutch</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Privé, Lokaal Manus-alternatief.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  Engels | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Portugees (Brazilië)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Spaans</a></p><p><em>Een <strong>100% lokaal alternatief voor Manus AI</strong>, deze stemgestuurde AI-assistent doorzoekt autonoom het web, schrijft code en plant taken, terwijl alle data op je apparaat blijft. Speciaal ontworpen voor lokale redeneermodellen, draait volledig op jouw hardware, wat volledige privacy en nul cloud-afhankelijkheid garandeert.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Bezoek AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="Licentie"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Waarom AgenticSeek?</h3></p><ul><li>🔒 Volledig Lokaal & Privé – Alles draait op jouw machine — geen cloud, geen datadeling. Jouw bestanden, gesprekken en zoekopdrachten blijven privé.</li></p><p><li>🌐 Slimme Webbrowser – AgenticSeek kan zelfstandig internetten — zoeken, lezen, info extraheren, webformulieren invullen — volledig handsfree.</li></p><p><li>💻 Autonome Code-assistent – Code nodig? Schrijft, debugt en voert programma's uit in Python, C, Go, Java en meer — geheel zonder toezicht.</li></p><p><li>🧠 Slimme Agentselectie – Jij vraagt, het kiest automatisch de beste agent voor de taak. Net alsof je een team van experts tot je beschikking hebt.</li></p><p><li>📋 Plant & Voert Complexe Taken Uit – Van reisplanning tot complexe projecten — het splitst grote taken op in stappen en voert deze uit met meerdere AI-agents.</li></p><p><li>🎙️ Stemgestuurd – Schone, snelle, futuristische spraak-naar-tekst en tekst-naar-spraak, zodat je kunt praten alsof het je persoonlijke AI uit een sciencefictionfilm is. (In ontwikkeling)</li></p><p></ul><h3><strong>Demo</strong></h3></p><blockquote><em>Kun je zoeken naar het agenticSeek-project, uitzoeken welke vaardigheden vereist zijn, vervolgens het bestand CV_candidates.zip openen en me vertellen welke kandidaten het beste bij het project passen?</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Disclaimer: Deze demo, inclusief alle getoonde bestanden (bijv: CV_candidates.zip), is volledig fictief. Wij zijn geen bedrijf, we zoeken open-source bijdragers, geen kandidaten.</p><blockquote>🛠⚠️️ <strong>Actief Werk in Uitvoering</strong></blockquote></p><blockquote>🙏 Dit project begon als een zijproject en heeft geen roadmap of financiering. Het is uitgegroeid tot meer dan verwacht en eindigde zelfs in GitHub Trending. Bijdragen, feedback en geduld worden zeer gewaardeerd.</blockquote></p><h2>Vereisten</h2></p><p>Voordat je begint, zorg ervoor dat je de volgende software hebt geïnstalleerd:</p><ul><li>  <strong>Git:</strong> Voor het klonen van de repository. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Download Git</a></li>
<li>  <strong>Python 3.10.x:</strong> We raden sterk aan om Python versie 3.10.x te gebruiken. Andere versies kunnen afhankelijkheidsfouten veroorzaken. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Download Python 3.10</a> (kies een 3.10.x versie).</li>
<li>  <strong>Docker Engine & Docker Compose:</strong> Voor het draaien van gebundelde diensten zoals SearxNG.</li>
    <li>  Installeer Docker Desktop (inclusief Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Of installeer Docker Engine en Docker Compose apart op Linux: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (zorg dat je Compose V2 installeert, bijv. <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Kloon de repository en stel in</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Wijzig de inhoud van het .env-bestand</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optioneel'
DEEPSEEK_API_KEY='optioneel'
OPENROUTER_API_KEY='optioneel'
TOGETHER_API_KEY='optioneel'
GOOGLE_API_KEY='optioneel'
ANTHROPIC_API_KEY='optioneel'</code></pre></p><p>Werk het <code>.env</code>-bestand bij met je eigen waarden waar nodig:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Ongewijzigd laten </li>
<li><strong>REDIS_BASE_URL</strong>: Ongewijzigd laten </li>
<li><strong>WORK_DIR</strong>: Pad naar je werkmap op je lokale machine. AgenticSeek kan deze bestanden lezen en ermee werken.</li>
<li><strong>OLLAMA_PORT</strong>: Poortnummer voor de Ollama-service.</li>
<li><strong>LM_STUDIO_PORT</strong>: Poortnummer voor de LM Studio-service.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Poort voor elke extra custom LLM-service.</li></p><p></ul><strong>API-sleutels zijn volledig optioneel voor gebruikers die ervoor kiezen om LLM lokaal uit te voeren. Dit is het primaire doel van dit project. Laat leeg als je over voldoende hardware beschikt</strong></p><h3>3. <strong>Start Docker</strong></h3></p><p>Zorg dat Docker is geïnstalleerd en actief op je systeem. Je kunt Docker starten met de volgende commando's:</p><ul><li><strong>Op Linux/macOS:</strong>  </li>
    </ul>Open een terminal en voer uit:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Of start Docker Desktop vanuit je applicatiemenu als deze is geïnstalleerd.</p><ul><li><strong>Op Windows:</strong>  </li>
    </ul>Start Docker Desktop via het Startmenu.</p><p>Je kunt controleren of Docker draait door het volgende uit te voeren:</code></pre>sh
docker info
<pre><code class="language-">Zie je informatie over je Docker-installatie, dan werkt Docker correct.</p><p>Bekijk de tabel met <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Lokale Providers</a> hieronder voor een overzicht.</p><p>Volgende stap: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Voer AgenticSeek lokaal uit</a></p><p><em>Kijk in de sectie <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Probleemoplossing</a> als je problemen ondervindt.</em>
<em>Kan jouw hardware LLM's niet lokaal draaien, zie <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Installatie voor gebruik met een API</a>.</em>
<em>Voor gedetailleerde uitleg over </code>config.ini<code>, zie <a href="#config" target="_blank" rel="noopener noreferrer">Config Sectie</a>.</em></p><hr></p><h2>Installatie voor lokaal draaien van LLM op je eigen machine</h2></p><p><strong>Hardwarevereisten:</strong></p><p>Om LLM's lokaal te draaien heb je voldoende hardware nodig. Minimaal is een GPU vereist die Magistral, Qwen of Deepseek 14B kan draaien. Zie de FAQ voor gedetailleerde model- en prestatieaanbevelingen.</p><p><strong>Stel je lokale provider in</strong>  </p><p>Start je lokale provider, bijvoorbeeld met ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
Zie hieronder voor een lijst van lokaal ondersteunde providers.</p><p><strong>Werk de config.ini bij</strong></p><p>Wijzig het config.ini-bestand om provider_name in te stellen op een ondersteunde provider en provider_model op een LLM die door jouw provider wordt ondersteund. We raden redeneermodellen aan zoals <em>Magistral</em> of <em>Deepseek</em>.</p><p>Zie de <strong>FAQ</strong> aan het einde van de README voor benodigde hardware.
</code></pre>sh
[MAIN]
is_local = True # Of je lokaal of met een externe provider draait.
provider_name = ollama # of lm-studio, openai, etc..
provider_model = deepseek-r1:14b # kies een model dat past bij je hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # naam van je AI
recover_last_session = True # of je de vorige sessie herstelt
save_session = True # of je de huidige sessie onthoudt
speak = False # tekst-naar-spraak
listen = False # spraak-naar-tekst, alleen voor CLI, experimenteel
jarvis_personality = False # Of je een meer "Jarvis"-achtige persoonlijkheid gebruikt (experimenteel)
languages = en zh # De lijst van talen, tekst-naar-spraak gebruikt standaard de eerste taal in de lijst
[BROWSER]
headless_browser = True # laat ongewijzigd tenzij je CLI op host gebruikt.
stealth_mode = True # Gebruik undetected selenium om browserdetectie te verminderen
<pre><code class="language-">
<strong>Waarschuwing</strong>:</p><ul><li>Het </code>config.ini<code>-bestand ondersteunt geen opmerkingen.</li>
</ul>Kopieer en plak de voorbeeldconfiguratie niet direct, omdat opmerkingen fouten zullen veroorzaken. Wijzig het </code>config.ini<code>-bestand handmatig met je gewenste instellingen, zonder opmerkingen.</p><ul><li>Stel <em>NIET</em> provider_name in op </code>openai<code> als je LM-studio gebruikt voor het draaien van LLM's. Stel in op </code>lm-studio<code>.</li></p><p><li>Sommige providers (bijv. lm-studio) vereisen dat je </code>http://<code> voor het IP-adres plaatst. Bijvoorbeeld </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Lijst van lokale providers</strong></p><p>| Provider  | Lokaal? | Beschrijving                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Ja     | Draai LLM's lokaal met ollama als LLM-provider            |
| lm-studio | Ja     | Draai LLM lokaal met LM studio (zet </code>provider_name<code> op </code>lm-studio<code>)|
| openai    | Ja     | Gebruik openai-compatibele API (bijv: llama.cpp server)   |</p><p>Volgende stap: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Start services en voer AgenticSeek uit</a>  </p><p><em>Kijk in de sectie <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Probleemoplossing</a> als je problemen ondervindt.</em>
<em>Kan jouw hardware LLM's niet lokaal draaien, zie <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Installatie voor gebruik met een API</a>.</em>
<em>Voor gedetailleerde uitleg over </code>config.ini<code>, zie <a href="#config" target="_blank" rel="noopener noreferrer">Config Sectie</a>.</em></p><h2>Installatie voor gebruik met een API</h2></p><p>Deze installatie gebruikt externe, cloudgebaseerde LLM-providers. Je hebt een API-sleutel nodig van de gekozen dienst.</p><p><strong>1. Kies een API-provider en verkrijg een API-sleutel:</strong></p><p>Raadpleeg de <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Lijst van API-providers</a> hieronder. Bezoek hun websites om je aan te melden en een API-sleutel te verkrijgen.</p><p><strong>2. Stel je API-sleutel in als omgevingsvariabele:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Open je terminal en gebruik het </code>export<code>-commando. Voeg dit het beste toe aan je shell-profielbestand (bijv. </code>~/.bashrc<code>, </code>~/.zshrc<code>) voor blijvende werking.
    </code>`<code>sh
    export PROVIDER_API_KEY="jouw_api_key_hier" 
    # Vervang PROVIDER_API_KEY door de specifieke variabelenaam, bijv. OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Voorbeeld voor TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Command Prompt (Tijdelijk voor huidige sessie):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (Tijdelijk voor huidige sessie):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>Permanent:</strong> Zoek naar "omgevingsvariabelen" in de Windows zoekbalk, klik op "Systeemomgevingsvariabelen bewerken," en klik vervolgens op de knop "Omgevingsvariabelen...". Voeg een nieuwe gebruikersvariabele toe met de juiste naam (bijv. </code>OPENAI_API_KEY<code>) en je sleutel als waarde.</li></p><p></ul><em>(Zie FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Hoe stel ik API-sleutels in?</a> voor meer details).</em></p><p>
<strong>3. Werk </code>config.ini<code> bij:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Of google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Of gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Wordt meestal genegeerd of kan leeg worden gelaten wanneer is_local = False voor de meeste APIs
<h1>... overige instellingen ...</h1>
<pre><code class="language-"><em>Waarschuwing:</em> Zorg ervoor dat er geen spaties aan het einde van de waarden in </code>config.ini<code> staan.</p><p><strong>Lijst van API-aanbieders</strong></p><p>| Provider     | </code>provider_name<code> | Lokaal? | Beschrijving                                        | API Key Link (Voorbeelden)                       |
|--------------|-----------------|---------|-----------------------------------------------------|--------------------------------------------------|
| OpenAI       | </code>openai<code>        | Nee     | Gebruik ChatGPT-modellen via de API van OpenAI.     | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini| </code>google<code>        | Nee     | Gebruik Google Gemini-modellen via Google AI Studio.| <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek     | </code>deepseek<code>      | Nee     | Gebruik Deepseek-modellen via hun API.              | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face | </code>huggingface<code>   | Nee     | Gebruik modellen van de Hugging Face Inference API. | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI   | </code>togetherAI<code>    | Nee     | Gebruik diverse open-source modellen via TogetherAI API.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Opmerking:</em>
<ul><li>  We raden af om </code>gpt-4o<code> of andere OpenAI-modellen te gebruiken voor complexe web-browsing en taakplanning, omdat de huidige prompt-optimalisaties zijn afgestemd op modellen zoals Deepseek.</li>
<li>  Programmeer/bash-taken kunnen problemen ondervinden met Gemini, omdat deze mogelijk niet strikt de prompt-formattering volgt die is geoptimaliseerd voor Deepseek.</li>
<li>  Het veld </code>provider_server_address<code> in </code>config.ini<code> wordt meestal niet gebruikt als </code>is_local = False<code>, omdat het API-eindpunt doorgaans is vastgelegd in de bibliotheek van de betreffende provider.</li></p><p></ul>Volgende stap: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Start services en voer AgenticSeek uit</a></p><p><em>Zie de sectie <strong>Bekende problemen</strong> als je problemen ondervindt</em></p><p><em>Zie de sectie <strong>Config</strong> voor een gedetailleerde uitleg van het configuratiebestand.</em></p><hr></p><h2>Start services en voer uit</h2></p><p>Standaard wordt AgenticSeek volledig in Docker uitgevoerd.</p><p>Start de vereiste services. Dit start alle services uit het docker-compose.yml bestand, waaronder:
    <ul><li>searxng</li>
    <li>redis (vereist door searxng)</li>
    <li>frontend</li>
    <li>backend (indien </code>full<code> gebruikt)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
<pre><code class="language-">
<strong>Waarschuwing:</strong> Deze stap downloadt en laadt alle Docker-images, wat tot 30 minuten kan duren. Na het starten van de services, wacht tot de backend service volledig draait (je zou <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> in de log moeten zien) voordat je berichten verstuurt. De backend services kunnen bij de eerste keer starten tot 5 minuten nodig hebben.</p><p>Ga naar </code>http://localhost:3000/<code> en je zou de webinterface moeten zien.</p><p><em>Problemen bij het starten van services:</em> Als deze scripts falen, controleer dan of Docker Engine draait en Docker Compose (V2, </code>docker compose<code>) correct is geïnstalleerd. Controleer de uitvoer in de terminal op foutmeldingen. Zie <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Help! Ik krijg een foutmelding bij het uitvoeren van AgenticSeek of de scripts.</a></p><p><strong>Optioneel:</strong> Uitvoeren op host (CLI modus):</p><p>Om de CLI-interface te gebruiken moet je het pakket op de host installeren:
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Start de services:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
<pre><code class="language-">
Gebruik de CLI: </code>python3 cli.py<code></p><hr></p><h2>Gebruik</h2></p><p>Zorg ervoor dat de services draaien met </code>./start_services.sh full<code> en ga naar </code>localhost:3000<code> voor de webinterface.</p><p>Je kunt ook spraak naar tekst gebruiken door </code>listen = True<code> in te stellen in de config. Alleen voor CLI-modus.</p><p>Om af te sluiten, zeg of typ je gewoon </code>goodbye<code>.</p><p>Hier zijn enkele voorbeelden van gebruik:</p><blockquote><em>Maak een slangenspel in python!</em></blockquote></p><blockquote><em>Zoek op internet naar de beste cafés in Rennes, Frankrijk, en sla een lijst van drie op met hun adressen in rennes_cafes.txt.</em></blockquote></p><blockquote><em>Schrijf een Go-programma om de faculteit van een getal te berekenen, sla het op als factorial.go in je werkmap</em></blockquote></p><blockquote><em>Zoek in mijn map summer_pictures naar alle JPG-bestanden, hernoem ze met de datum van vandaag, en sla een lijst van hernoemde bestanden op in photos_list.txt</em></blockquote></p><blockquote><em>Zoek online naar populaire sci-fi films uit 2024 en kies er drie om vanavond te kijken. Sla de lijst op in movie_night.txt.</em></blockquote></p><blockquote><em>Zoek op internet naar de laatste AI-nieuwsartikelen uit 2025, selecteer er drie, en schrijf een Python-script om hun titels en samenvattingen te scrapen. Sla het script op als news_scraper.py en de samenvattingen in ai_news.txt in /home/projects</em></blockquote></p><blockquote><em>Vrijdag, zoek op internet naar een gratis API voor aandelenkoersen, registreer met supersuper7434567@gmail.com, schrijf dan een Python-script om met de API dagelijks prijzen voor Tesla op te halen, en sla de resultaten op in stock_prices.csv</em></blockquote></p><p><em>Let op dat formuliervul-mogelijkheden nog experimenteel zijn en kunnen falen.</em></p><p>Nadat je je vraag hebt ingevoerd, zal AgenticSeek de beste agent voor de taak toewijzen.</p><p>Omdat dit een vroeg prototype is, zal het routeringssysteem van de agent mogelijk niet altijd de juiste agent toewijzen op basis van je vraag.</p><p>Wees daarom heel expliciet in wat je wilt en hoe de AI te werk zou moeten gaan; wil je bijvoorbeeld dat het een webzoekopdracht uitvoert, zeg dan niet:</p><p></code>Ken je goede landen voor solo-reizen?<code></p><p>Vraag in plaats daarvan:</p><p></code>Doe een webzoekopdracht en zoek uit wat de beste landen zijn voor solo-reizen<code></p><hr></p><h2><strong>Setup om de LLM op je eigen server te draaien</strong>  </h2></p><p>Als je een krachtige computer of server hebt die je kunt gebruiken, maar je wilt deze vanaf je laptop gebruiken, heb je de mogelijkheid om de LLM op een externe server te draaien met onze aangepaste llm-server.</p><p>Op je "server" die het AI-model zal draaien, haal het ip-adres op
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokaal ip
curl https://ipinfo.io/ip # publiek ip
<pre><code class="language-">
Opmerking: Voor Windows of macOS gebruik respectievelijk ipconfig of ifconfig om het IP-adres te vinden.</p><p>Kloon de repository en ga naar de map </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Installeer de server-specifieke vereisten:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Voer het serverscript uit.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Je kunt kiezen tussen </code>ollama<code> en </code>llamacpp<code> als LLM-service.</p><p>
Nu op je persoonlijke computer:</p><p>Wijzig het </code>config.ini<code>-bestand door </code>provider_name<code> op </code>server<code> te zetten en </code>provider_model<code> op </code>deepseek-r1:xxb<code>.
Stel </code>provider_server_address<code> in op het ip-adres van de machine die het model draait.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>Volgende stap: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Start services en voer AgenticSeek uit</a>  </p><hr></p><h2>Spraak naar Tekst</h2></p><p>Waarschuwing: spraak naar tekst werkt momenteel alleen in CLI-modus.</p><p>Let op: spraak naar tekst werkt momenteel alleen in het Engels.</p><p>De spraak-naar-tekst functionaliteit is standaard uitgeschakeld. Om deze in te schakelen, stel je de optie listen in op True in het config.ini-bestand:
</code></pre>
listen = True
<pre><code class="language-">
Wanneer ingeschakeld, luistert de spraak-naar-tekst functie naar een triggerwoord, namelijk de naam van de agent, voordat je invoer wordt verwerkt. Je kunt de naam van de agent aanpassen door de waarde van </code>agent_name<code> bij te werken in het <em>config.ini</em> bestand:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Voor optimale herkenning raden we aan een gangbare Engelse naam te gebruiken zoals "John" of "Emma" als agentnaam.</p><p>Zodra u het transcript ziet verschijnen, zeg de naam van de agent hardop om deze te activeren (bijv. "Friday").</p><p>Spreek uw vraag duidelijk uit.</p><p>Beëindig uw verzoek met een bevestigingszin om het systeem te laten doorgaan. Voorbeelden van bevestigingszinnen zijn:</code></pre>
"doe het", "ga je gang", "uitvoeren", "start", "begin", "dank je", "zou je", "alstublieft", "oké?", "doorgaan", "verder", "doe dat", "doe het", "begrijp je?"
<pre><code class="language-">
<h2>Config</h2></p><p>Voorbeeldconfiguratie:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Voorbeeld voor Ollama; gebruik http://127.0.0.1:1234 voor LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Lijst van talen voor TTS en eventueel routering.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Uitleg van </code>config.ini<code> Instellingen</strong>:</p><ul><li>  <strong></code>[MAIN]<code> Sectie:</strong></li>
    <li>  </code>is_local<code>: </code>True<code> als u een lokale LLM-provider gebruikt (Ollama, LM-Studio, lokale OpenAI-compatibele server) of de self-hosted serveroptie. </code>False<code> als u een cloudgebaseerde API gebruikt (OpenAI, Google, enz.).</li>
    <li>  </code>provider_name<code>: Specificeert de LLM-provider.</li>
        <li>  Lokale opties: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (voor lokale OpenAI-compatibele servers), </code>server<code> (voor de zelfgehoste server).</li>
        <li>  API-opties: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: De specifieke modelnaam of ID voor de gekozen provider (bijv. </code>deepseekcoder:6.7b<code> voor Ollama, </code>gpt-3.5-turbo<code> voor OpenAI API, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> voor TogetherAI).</li>
    <li>  </code>provider_server_address<code>: Het adres van uw LLM-provider.</li>
        <li>  Voor lokale providers: bijv. </code>http://127.0.0.1:11434<code> voor Ollama, </code>http://127.0.0.1:1234<code> voor LM-Studio.</li>
        <li>  Voor de </code>server<code>-provider: Het adres van uw zelfgehoste LLM-server (bijv. </code>http://your_server_ip:3333<code>).</li>
        <li>  Voor cloud-API's (</code>is_local = False<code>): Dit wordt vaak genegeerd of kan leeg gelaten worden, omdat het API-eindpunt meestal door de clientbibliotheek wordt afgehandeld.</li>
    <li>  </code>agent_name<code>: Naam van de AI-assistent (bijv. Friday). Wordt gebruikt als triggerwoord voor spraak-naar-tekst als dit is ingeschakeld.</li>
    <li>  </code>recover_last_session<code>: </code>True<code> om te proberen de vorige sessiestatus te herstellen, </code>False<code> om opnieuw te beginnen.</li>
    <li>  </code>save_session<code>: </code>True<code> om de huidige sessiestatus op te slaan voor mogelijk herstel, </code>False<code> anders.</li>
    <li>  </code>speak<code>: </code>True<code> om spraakuitvoer (text-to-speech) in te schakelen, </code>False<code> om uit te schakelen.</li>
    <li>  </code>listen<code>: </code>True<code> om spraak-naar-tekst-invoer (alleen CLI-modus) in te schakelen, </code>False<code> om uit te schakelen.</li>
    <li>  </code>work_dir<code>: <strong>Cruciaal:</strong> De map waarin AgenticSeek bestanden zal lezen/schrijven. <strong>Zorg ervoor dat dit pad geldig en toegankelijk is op uw systeem.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> voor een meer "Jarvis-achtige" systeemprompt (experimenteel), </code>False<code> voor de standaardprompt.</li>
    <li>  </code>languages<code>: Een komma-gescheiden lijst met talen (bijv. </code>en, zh, fr<code>). Wordt gebruikt voor TTS-stemselectie (standaard de eerste) en kan de LLM-router helpen. Vermijd te veel of zeer vergelijkbare talen voor routerefficiëntie.</li>
<li>  <strong></code>[BROWSER]<code> Sectie:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code> om de geautomatiseerde browser zonder zichtbaar venster uit te voeren (aanbevolen voor webinterface of niet-interactief gebruik). </code>False<code> om het browservenster te tonen (handig voor CLI-modus of debugging).</li>
    <li>  </code>stealth_mode<code>: </code>True<code> om maatregelen te activeren die browserautomatisering moeilijker te detecteren maken. Kan vereisen dat u handmatig browserextensies zoals anticaptcha installeert.</li></p><p>
</ul>Deze sectie geeft een overzicht van de ondersteunde LLM-providertypes. Stel ze in via </code>config.ini<code>.</p><p><strong>Lokale Providers (Draaiend op eigen hardware):</strong></p><p>| Providernaam in </code>config.ini<code> | </code>is_local<code> | Beschrijving                                                                 | Setup Sectie                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Gebruik Ollama om lokale LLM's te draaien.                                  | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup voor lokaal draaien van LLM</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Gebruik LM-Studio om lokale LLM's te draaien.                               | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup voor lokaal draaien van LLM</a> |
| </code>openai<code> (voor lokale server) | </code>True<code>     | Verbind met een lokale server die een OpenAI-compatibele API aanbiedt (bijv. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup voor lokaal draaien van LLM</a> |
| </code>server<code>                      | </code>False<code>    | Verbind met de zelfgehoste AgenticSeek LLM-server op een andere machine.    | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Setup om de LLM op uw eigen server te draaien</a> |</p><p><strong>API Providers (Cloud-Based):</strong></p><p>| Providernaam in </code>config.ini<code> | </code>is_local<code> | Beschrijving                                     | Setup Sectie                                        |
|-------------------------------|------------|--------------------------------------------------|-----------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Gebruik de officiële API van OpenAI (bijv. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup om te draaien met een API</a> |
| </code>google<code>                      | </code>False<code>    | Gebruik Google's Gemini-modellen via API.        | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup om te draaien met een API</a> |
| </code>deepseek<code>                    | </code>False<code>    | Gebruik de officiële API van Deepseek.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup om te draaien met een API</a> |
| </code>huggingface<code>                 | </code>False<code>    | Gebruik de Hugging Face Inference API.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup om te draaien met een API</a> |
| </code>togetherAI<code>                  | </code>False<code>    | Gebruik TogetherAI's API voor diverse open modellen. | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup om te draaien met een API</a> |</p><hr>
<h2>Problemen oplossen</h2></p><p>Als u problemen ondervindt, biedt deze sectie hulp.</p><h1>Bekende Problemen</h1></p><h2>ChromeDriver Problemen</h2></p><p><strong>Foutvoorbeeld:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Oorzaak:</strong> De geïnstalleerde ChromeDriver-versie is niet compatibel met de versie van uw Google Chrome-browser.</li>
<li>  <strong>Oplossing:</strong></li>
    <li> <strong>Controleer Chrome-versie:</strong> Open Google Chrome, ga naar </code>Instellingen > Over Chrome<code> om uw versie te vinden (bijv. "Versie 120.0.6099.110").</li>
    <li> <strong>Download de bijpassende ChromeDriver:</strong></li>
        <li>  Voor Chrome versies 115 en nieuwer: Ga naar de <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Zoek het "stable" kanaal en download de ChromeDriver voor uw besturingssysteem die overeenkomt met uw Chrome-hoofdversie.</li>
        <li>  Voor oudere versies (minder voorkomend): Mogelijk vindt u deze op de <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a> pagina.</li>
        <li>  De onderstaande afbeelding toont een voorbeeld van de CfT-pagina:</li>
            </ul><img src="./media/chromedriver_readme.png" alt="Download Chromedriver specifieke versie van de Chrome for Testing pagina">
    <ul><li> <strong>Installeer ChromeDriver:</strong></li>
        <li>  Zorg ervoor dat de gedownloade </code>chromedriver<code> (of </code>chromedriver.exe<code> op Windows) in een map staat die is opgenomen in het PATH-omgevingsvariabele van uw systeem (bijv. </code>/usr/local/bin<code> op Linux/macOS, of een aangepaste scriptsmap toegevoegd aan PATH op Windows).</li>
        <li>  Of plaats hem in de hoofdmap van het </code>agenticSeek<code>-project.</li>
        <li>  Zorg ervoor dat de driver uitvoerbaar is (bijv. </code>chmod +x chromedriver<code> op Linux/macOS).</li>
    <li> Raadpleeg de sectie <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">ChromeDriver Installatie</a> in de hoofdinstallatiehandleiding voor meer details.</li></p><p></ul>Als deze sectie onvolledig is of u andere ChromeDriver-problemen ondervindt, zoek dan bestaande <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a> of maak een nieuw issue aan.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Dit gebeurt als er een mismatch is tussen uw browser- en chromedriver-versie.</p><p>U dient de nieuwste versie te downloaden:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Als u Chrome versie 115 of nieuwer gebruikt, ga naar:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>En download de chromedriver-versie die overeenkomt met uw OS.</p><p><img src="./media/chromedriver_readme.png" alt="alt text"></p><p>Als deze sectie onvolledig is, maak dan een issue aan.</p><h2> connection adapters Problemen</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Let op: poort kan verschillen)
<pre><code class="language-">
<ul><li>  <strong>Oorzaak:</strong> Het <code>provider_server_address</code> in <code>config.ini</code> voor <code>lm-studio</code> (of andere vergelijkbare lokale OpenAI-compatibele servers) mist het <code>http://</code>-voorvoegsel of wijst naar de verkeerde poort.</li>
<li>  <strong>Oplossing:</strong></li>
    <li>  Zorg ervoor dat het adres <code>http://</code> bevat. LM-Studio gebruikt typisch <code>http://127.0.0.1:1234</code>.</li>
    <li>  Corrigeer <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (of uw daadwerkelijke LM-Studio serverpoort).</li></p><p></ul><h2>SearxNG Base URL Niet Opgegeven</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>Q: Welke hardware heb ik nodig?</strong>  </p><p>| Modelgrootte  | GPU  | Opmerking                                               |
|-----------|--------|-------------------------------------------------------------|
| 7B        | 8GB Vram | ⚠️ Niet aanbevolen. Slechte prestaties, frequente hallucinaties en planner agents zullen waarschijnlijk falen. |
| 14B        | 12 GB VRAM (bijv. RTX 3060) | ✅ Bruikbaar voor eenvoudige taken. Kan moeite hebben met web browsing en planningstaken. |
| 32B        | 24+ GB VRAM (bijv. RTX 4090) | 🚀 Succesvol bij de meeste taken, kan nog steeds moeite hebben met taakplanning |
| 70B+        | 48+ GB Vram | 💪 Uitstekend. Aanbevolen voor geavanceerde toepassingen. |</p><p><strong>Q: Ik krijg een foutmelding, wat moet ik doen?</strong>  </p><p>Controleer of lokaal draait (</code>ollama serve<code>), uw </code>config.ini` overeenkomt met uw provider en alle afhankelijkheden geïnstalleerd zijn. Werkt het dan nog niet, maak gerust een issue aan.</p><p><strong>Q: Kan het echt 100% lokaal draaien?</strong>  </p><p>Ja, met Ollama, lm-studio of serverproviders draaien alle spraak-naar-tekst, LLM en tekst-naar-spraak-modellen lokaal. Niet-lokale opties (OpenAI of andere API’s) zijn optioneel.</p><p><strong>Q: Waarom zou ik AgenticSeek gebruiken als ik Manus heb?</strong></p><p>In tegenstelling tot Manus, hecht AgenticSeek veel waarde aan onafhankelijkheid van externe systemen, wat u meer controle, privacy en geen API-kosten biedt.</p><p><strong>Q: Wie zit er achter het project?</strong></p><p>Het project is opgezet door mij samen met twee vrienden die als maintainers en bijdragers uit de open-source gemeenschap op GitHub fungeren. We zijn gewoon een groep gepassioneerde individuen, geen startup of onderdeel van een organisatie.</p><p>Elk AgenticSeek-account op X behalve mijn persoonlijke account (https://x.com/Martin993886460) is een imitatie.</p><h2>Bijdragen</h2></p><p>We zoeken ontwikkelaars om AgenticSeek te verbeteren! Bekijk open issues of discussies.</p><p><a href="./docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Bijdragegids</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Onderhouders:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Parijs Tijd </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Taipei Tijd </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Taipei Tijd </p><h2>Speciale Dank:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> en <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> Voor hulp bij backend-dockerization</p><h2>Sponsors:</h2></p><p>5$ of meer Maandelijkse sponsor verschijnt hier:
<ul><li><strong>tatra-labs</strong></li></p><p></ul>Sorry, but you didn't provide the content you want translated. Please provide the text of Part 4 of 4, and I'll translate it into Dutch for you, preserving the formatting and Markdown as requested.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>