<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in German. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in German. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, German, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in German. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-de.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">German</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Private, lokale Manus-Alternative.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>Eine <strong>100% lokale Alternative zu Manus AI</strong>, dieser sprachgesteuerte KI-Assistent durchsucht autonom das Web, schreibt Code und plant Aufgaben, während alle Daten auf Ihrem Gerät bleiben. Entwickelt für lokale Reasoning-Modelle läuft er vollständig auf Ihrer eigenen Hardware, gewährleistet vollständige Privatsphäre und keinerlei Cloud-Abhängigkeit.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visit AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Warum AgenticSeek?</h3></p><ul><li>🔒 Vollständig lokal & privat – Alles läuft auf Ihrem Computer — keine Cloud, keine Datenweitergabe. Ihre Dateien, Unterhaltungen und Suchanfragen bleiben privat.</li></p><p><li>🌐 Intelligentes Web-Browsing – AgenticSeek kann das Internet selbstständig durchsuchen — suchen, lesen, Informationen extrahieren, Webformulare ausfüllen — komplett freihändig.</li></p><p><li>💻 Autonomer Coding-Assistent – Brauchen Sie Code? Er kann Programme in Python, C, Go, Java und mehr schreiben, debuggen und ausführen — ganz ohne Aufsicht.</li></p><p><li>🧠 Intelligente Agenten-Auswahl – Sie fragen, er findet automatisch den besten Agenten für die Aufgabe. Wie ein Expertenteam, das bereit ist zu helfen.</li></p><p><li>📋 Plant & führt komplexe Aufgaben aus – Von Reiseplanung bis zu komplexen Projekten — er kann große Aufgaben in Schritte unterteilen und mit mehreren KI-Agenten erledigen.</li></p><p><li>🎙️ Sprachsteuerung – Klare, schnelle und futuristische Sprach-zu-Text- und Text-zu-Sprache-Funktionen, die es ermöglichen, mit ihm zu sprechen, als wäre er Ihr persönlicher KI-Assistent aus einem Sci-Fi-Film. (In Entwicklung)</li></p><p></ul><h3><strong>Demo</strong></h3></p><blockquote><em>Kannst du nach dem agenticSeek-Projekt suchen, herausfinden, welche Fähigkeiten benötigt werden, dann die Datei CV_candidates.zip öffnen und mir anschließend mitteilen, welche am besten zum Projekt passen?</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Hinweis: Diese Demo sowie alle darin erscheinenden Dateien (z. B. CV_candidates.zip) sind vollständig fiktiv. Wir sind kein Unternehmen, sondern suchen Open-Source-Mitwirkende, keine Bewerber.</p><blockquote>🛠⚠️️ <strong>Aktive Entwicklung</strong></blockquote></p><blockquote>🙏 Dieses Projekt begann als Nebenprojekt und hat weder eine Roadmap noch Finanzierung. Es ist viel größer geworden als erwartet und landete in GitHub Trending. Beiträge, Feedback und Geduld sind sehr willkommen.</blockquote></p><h2>Voraussetzungen</h2></p><p>Bevor Sie beginnen, stellen Sie sicher, dass folgende Software installiert ist:</p><ul><li>  <strong>Git:</strong> Zum Klonen des Repositories. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Git herunterladen</a></li>
<li>  <strong>Python 3.10.x:</strong> Wir empfehlen nachdrücklich die Python-Version 3.10.x zu verwenden. Andere Versionen können zu Abhängigkeitsfehlern führen. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Python 3.10 herunterladen</a> (Wählen Sie eine 3.10.x-Version).</li>
<li>  <strong>Docker Engine & Docker Compose:</strong> Zum Ausführen gebündelter Dienste wie SearxNG.</li>
    <li>  Installieren Sie Docker Desktop (enthält Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  Alternativ können Sie Docker Engine und Docker Compose unter Linux separat installieren: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (Stellen Sie sicher, dass Compose V2 installiert ist, z. B. <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Repository klonen und Setup</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Inhalt der .env-Datei anpassen</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>Aktualisieren Sie die <code>.env</code>-Datei nach Bedarf mit Ihren eigenen Werten:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Unverändert lassen</li>
<li><strong>REDIS_BASE_URL</strong>: Unverändert lassen</li>
<li><strong>WORK_DIR</strong>: Pfad zu Ihrem Arbeitsverzeichnis auf Ihrem lokalen Rechner. AgenticSeek kann diese Dateien lesen und mit ihnen interagieren.</li>
<li><strong>OLLAMA_PORT</strong>: Portnummer für den Ollama-Dienst.</li>
<li><strong>LM_STUDIO_PORT</strong>: Portnummer für den LM Studio-Dienst.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Port für einen zusätzlichen benutzerdefinierten LLM-Dienst.</li></p><p></ul><strong>API-Keys sind völlig optional für Nutzer, die LLM lokal ausführen. Das ist der Hauptzweck dieses Projekts. Lassen Sie die Felder leer, wenn Sie über ausreichend Hardware verfügen.</strong></p><h3>3. <strong>Docker starten</strong></h3></p><p>Stellen Sie sicher, dass Docker auf Ihrem System installiert und gestartet ist. Sie können Docker mit folgenden Befehlen starten:</p><ul><li><strong>Unter Linux/macOS:</strong>  </li>
    </ul>Öffnen Sie ein Terminal und führen Sie aus:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Oder starten Sie Docker Desktop über das Anwendungsmenü, falls installiert.</p><ul><li><strong>Unter Windows:</strong>  </li>
    </ul>Starten Sie Docker Desktop über das Startmenü.</p><p>Sie können überprüfen, ob Docker läuft, indem Sie folgenden Befehl ausführen:</code></pre>sh
docker info
<pre><code class="language-">Wenn Sie Informationen zu Ihrer Docker-Installation sehen, läuft Docker korrekt.</p><p>Eine Zusammenfassung finden Sie in der Tabelle der <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">lokalen Anbieter</a> unten.</p><p>Nächster Schritt: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">AgenticSeek lokal ausführen</a></p><p><em>Siehe den Abschnitt <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Fehlerbehebung</a>, falls Sie Probleme haben.</em>
<em>Wenn Ihre Hardware keine lokalen LLMs ausführen kann, siehe <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup zur Nutzung mit einer API</a>.</em>
<em>Für detaillierte Erklärungen zu </code>config.ini<code>, siehe <a href="#config" target="_blank" rel="noopener noreferrer">Config-Abschnitt</a>.</em></p><hr></p><h2>Setup zum lokalen Ausführen von LLM auf Ihrem Rechner</h2></p><p><strong>Hardware-Anforderungen:</strong></p><p>Um LLMs lokal auszuführen, benötigen Sie entsprechende Hardware. Mindestens eine GPU, die Magistral, Qwen oder Deepseek 14B ausführen kann, ist erforderlich. Siehe FAQ für detaillierte Modell-/Performance-Empfehlungen.</p><p><strong>Lokalen Anbieter einrichten</strong>  </p><p>Starten Sie Ihren lokalen Anbieter, zum Beispiel mit ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
Eine Liste der unterstützten lokalen Anbieter finden Sie unten.</p><p><strong>config.ini aktualisieren</strong></p><p>Passen Sie die config.ini-Datei an, um provider_name auf einen unterstützten Anbieter und provider_model auf ein von Ihrem Anbieter unterstütztes LLM zu setzen. Wir empfehlen Reasoning-Modelle wie <em>Magistral</em> oder <em>Deepseek</em>.</p><p>Siehe <strong>FAQ</strong> am Ende des README für benötigte Hardware.
</code></pre>sh
[MAIN]
is_local = True # Ob Sie lokal oder mit Remote-Anbieter arbeiten.
provider_name = ollama # oder lm-studio, openai, etc..
provider_model = deepseek-r1:14b # Wählen Sie ein Modell, das zu Ihrer Hardware passt
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # Name Ihrer KI
recover_last_session = True # Ob die letzte Sitzung wiederhergestellt werden soll
save_session = True # Ob die aktuelle Sitzung gespeichert werden soll
speak = False # Text zu Sprache
listen = False # Sprache zu Text, nur für CLI, experimentell
jarvis_personality = False # Ob eine „Jarvis“-ähnliche Persönlichkeit genutzt werden soll (experimentell)
languages = en zh # Liste der Sprachen, Text zu Sprache nutzt standardmäßig die erste auf der Liste
[BROWSER]
headless_browser = True # Unverändert lassen, außer bei Nutzung von CLI auf dem Host.
stealth_mode = True # Nutze undetected selenium zur Reduzierung der Browser-Erkennung
<pre><code class="language-">
<strong>Warnung</strong>:</p><ul><li>Das Format der </code>config.ini<code>-Datei unterstützt keine Kommentare. </li>
</ul>Kopieren Sie NICHT die Beispiel-Konfiguration direkt, da Kommentare zu Fehlern führen. Passen Sie die </code>config.ini<code> manuell mit Ihren gewünschten Einstellungen an – ohne Kommentare.</p><ul><li>Setzen Sie <em>NICHT</em> provider_name auf </code>openai<code>, wenn Sie LM-studio zum Ausführen von LLMs verwenden. Setzen Sie es stattdessen auf </code>lm-studio<code>.</li></p><p><li>Einige Anbieter (z. B. lm-studio) erfordern, dass Sie </code>http://<code> vor die IP-Adresse setzen. Zum Beispiel </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Liste lokaler Anbieter</strong></p><p>| Anbieter   | Lokal? | Beschreibung                                               |
|------------|--------|------------------------------------------------------------|
| ollama     | Ja     | Führen Sie LLMs lokal einfach mit ollama als LLM-Anbieter aus |
| lm-studio  | Ja     | Führen Sie LLM lokal mit LM Studio aus (setze </code>provider_name<code> auf </code>lm-studio<code>)|
| openai     | Ja     |  Nutzung einer OpenAI-kompatiblen API (z. B. llama.cpp server)  |</p><p>Nächster Schritt: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Dienste starten und AgenticSeek ausführen</a>  </p><p><em>Siehe den Abschnitt <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Fehlerbehebung</a>, falls Sie Probleme haben.</em>
<em>Wenn Ihre Hardware keine lokalen LLMs ausführen kann, siehe <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup zur Nutzung mit einer API</a>.</em>
<em>Für detaillierte Erklärungen zu </code>config.ini<code>, siehe <a href="#config" target="_blank" rel="noopener noreferrer">Config-Abschnitt</a>.</em></p><h2>Setup zur Nutzung mit einer API</h2></p><p>Dieses Setup verwendet externe, cloudbasierte LLM-Anbieter. Sie benötigen einen API-Key von Ihrem gewählten Dienst.</p><p><strong>1. Wählen Sie einen API-Anbieter und erhalten Sie einen API-Key:</strong></p><p>Siehe <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Liste der API-Anbieter</a> unten. Besuchen Sie deren Websites, um sich zu registrieren und einen API-Key zu erhalten.</p><p><strong>2. Setzen Sie Ihren API-Key als Umgebungsvariable:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Öffnen Sie Ihr Terminal und verwenden Sie den Befehl </code>export<code>. Am besten fügen Sie diesen Befehl in die Profil-Datei Ihrer Shell ein (z. B. </code>~/.bashrc<code>, </code>~/.zshrc<code>) für dauerhafte Speicherung.
    </code>`<code>sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Ersetzen Sie PROVIDER_API_KEY durch den spezifischen Variablennamen, z. B. OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Beispiel für TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Eingabeaufforderung (temporär für aktuelle Sitzung):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=dein_api_schlüssel_hier
    </code>`<code>
<ul><li>  <strong>PowerShell (temporär für aktuelle Sitzung):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="dein_api_schlüssel_hier"
    </code>`<code>
<ul><li>  <strong>Permanent:</strong> Suche nach „Umgebungsvariablen“ in der Windows-Suchleiste, klicke auf „Systemumgebungsvariablen bearbeiten“ und dann auf die Schaltfläche „Umgebungsvariablen...“. Füge eine neue Benutzervariable mit dem passenden Namen (z. B. </code>OPENAI_API_KEY<code>) und deinem Schlüssel als Wert hinzu.</li></p><p></ul><em>(Siehe FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Wie setze ich API-Schlüssel?</a> für weitere Details).</em></p><p>
<strong>3. Aktualisiere </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Oder google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Oder gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Wird typischerweise ignoriert oder kann leer bleiben, wenn is_local = False für die meisten APIs
<h1>... andere Einstellungen ...</h1>
<pre><code class="language-"><em>Warnung:</em> Stelle sicher, dass keine Leerzeichen am Ende der Werte in der </code>config.ini<code> stehen.</p><p><strong>Liste der API-Anbieter</strong></p><p>| Anbieter     | </code>provider_name<code> | Lokal? | Beschreibung                                      | API-Schlüssel-Link (Beispiele)                       |
|--------------|-----------------|--------|---------------------------------------------------|------------------------------------------------------|
| OpenAI       | </code>openai<code>        | Nein   | Nutzung der ChatGPT-Modelle über die OpenAI-API.  | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini| </code>google<code>        | Nein   | Nutzung der Google Gemini-Modelle via Google AI Studio. | <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek     | </code>deepseek<code>      | Nein   | Nutzung von Deepseek-Modellen über deren API.      | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face | </code>huggingface<code>   | Nein   | Nutzung von Modellen der Hugging Face Inference API. | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI   | </code>togetherAI<code>    | Nein   | Nutzung verschiedener Open-Source-Modelle über TogetherAI API. | <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Hinweis:</em>
<ul><li>  Wir raten davon ab, </code>gpt-4o<code> oder andere OpenAI-Modelle für komplexes Web-Browsing und Aufgabenplanung zu verwenden, da die aktuellen Prompt-Optimierungen auf Modelle wie Deepseek ausgerichtet sind.</li>
<li>  Coding-/Bash-Aufgaben könnten mit Gemini auf Probleme stoßen, da das Modell eventuell Formatierungsanweisungen, die für Deepseek optimiert sind, nicht strikt befolgt.</li>
<li>  Die </code>provider_server_address<code> in </code>config.ini<code> wird generell nicht verwendet, wenn </code>is_local = False<code>, da der API-Endpunkt in der Regel in der jeweiligen Provider-Bibliothek fest hinterlegt ist.</li></p><p></ul>Nächster Schritt: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Dienste starten und AgenticSeek ausführen</a></p><p><em>Siehe den Abschnitt <strong>Bekannte Probleme</strong>, falls du auf Schwierigkeiten stößt.</em></p><p><em>Siehe den Abschnitt <strong>Config</strong> für eine ausführliche Erklärung der Konfigurationsdatei.</em></p><hr></p><h2>Dienste starten und AgenticSeek ausführen</h2></p><p>Standardmäßig läuft AgenticSeek vollständig in Docker.</p><p>Starte die benötigten Dienste. Dies startet alle Dienste aus der docker-compose.yml, einschließlich:
    <ul><li>searxng</li>
    <li>redis (benötigt von searxng)</li>
    <li>frontend</li>
    <li>backend (bei Nutzung von </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
<pre><code class="language-">
<strong>Warnung:</strong> Dieser Schritt lädt und startet alle Docker-Images, was bis zu 30 Minuten dauern kann. Warte nach dem Starten der Dienste, bis der Backend-Dienst vollständig läuft (du solltest <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> im Log sehen), bevor du Nachrichten sendest. Das Backend kann beim ersten Start bis zu 5 Minuten benötigen.</p><p>Gehe zu </code>http://localhost:3000/<code> – dort solltest du die Weboberfläche sehen.</p><p><em>Fehlerbehebung beim Dienststart:</em> Wenn diese Skripte fehlschlagen, stelle sicher, dass die Docker Engine läuft und Docker Compose (V2, </code>docker compose<code>) korrekt installiert ist. Überprüfe die Ausgaben im Terminal auf Fehlermeldungen. Siehe <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Hilfe! Ich erhalte einen Fehler beim Ausführen von AgenticSeek oder dessen Skripten.</a></p><p><strong>Optional:</strong> Auf dem Host ausführen (CLI-Modus):</p><p>Um die CLI-Oberfläche zu nutzen, musst du das Paket auf dem Host installieren:
</code></pre>sh
./install.sh
./install.bat # Windows
<pre><code class="language-">
Dienste starten:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
<pre><code class="language-">
Nutze die CLI: </code>python3 cli.py<code></p><hr></p><h2>Nutzung</h2></p><p>Stelle sicher, dass die Dienste mit </code>./start_services.sh full<code> laufen und öffne </code>localhost:3000<code> für die Weboberfläche.</p><p>Du kannst auch Sprache-zu-Text nutzen, indem du </code>listen = True<code> in der config aktivierst. Nur im CLI-Modus verfügbar.</p><p>Um zu beenden, sage oder tippe einfach </code>goodbye<code>.</p><p>Hier einige Anwendungsbeispiele:</p><blockquote><em>Erstelle ein Snake-Spiel in Python!</em></blockquote></p><blockquote><em>Durchsuche das Web nach den besten Cafés in Rennes, Frankreich, und speichere drei mit Adresse in rennes_cafes.txt.</em></blockquote></p><blockquote><em>Schreibe ein Go-Programm, das die Fakultät einer Zahl berechnet, und speichere es als factorial.go in deinem Arbeitsbereich.</em></blockquote></p><blockquote><em>Durchsuche meinen summer_pictures-Ordner nach allen JPG-Dateien, benenne sie mit dem heutigen Datum um und speichere eine Liste der umbenannten Dateien in photos_list.txt.</em></blockquote></p><blockquote><em>Suche online nach beliebten Sci-Fi-Filmen aus 2024 und wähle drei für heute Abend aus. Speichere die Liste in movie_night.txt.</em></blockquote></p><blockquote><em>Durchsuche das Web nach den neuesten KI-Nachrichtenartikeln aus 2025, wähle drei aus und schreibe ein Python-Skript, das deren Titel und Zusammenfassungen extrahiert. Speichere das Skript als news_scraper.py und die Zusammenfassungen in ai_news.txt im Verzeichnis /home/projects.</em></blockquote></p><blockquote><em>Suche am Freitag im Web nach einer kostenlosen Aktienkurs-API, registriere dich mit supersuper7434567@gmail.com und schreibe dann ein Python-Skript, das täglich die Preise von Tesla abruft und die Ergebnisse in stock_prices.csv speichert.</em></blockquote></p><p><em>Beachte, dass das Ausfüllen von Formularen noch experimentell ist und fehlschlagen kann.</em></p><p>Nachdem du deine Anfrage eingegeben hast, weist AgenticSeek den am besten geeigneten Agenten für die Aufgabe zu.</p><p>Da dies ein frühes Prototypstadium ist, kann es sein, dass das Agenten-Routing-System nicht immer den passenden Agenten basierend auf deiner Anfrage auswählt.</p><p>Daher solltest du sehr explizit formulieren, was du möchtest und wie die KI vorgehen soll. Wenn du beispielsweise eine Websuche möchtest, sage nicht:</p><p></code>Kennst du gute Länder für Solo-Reisen?<code></p><p>Sondern frage:</p><p></code>Führe eine Websuche durch und finde heraus, welche Länder sich am besten für Solo-Reisen eignen<code></p><hr></p><h2><strong>Setup zur Ausführung des LLM auf deinem eigenen Server</strong>  </h2></p><p>Wenn du einen leistungsfähigen Computer oder Server hast, den du nutzen möchtest, aber von deinem Laptop aus darauf zugreifen willst, kannst du das LLM auf einem entfernten Server mit unserem eigenen LLM-Server ausführen.</p><p>Auf deinem „Server“, der das KI-Modell ausführen soll, ermittle die IP-Adresse:
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # lokale IP
curl https://ipinfo.io/ip # öffentliche IP
<pre><code class="language-">
Hinweis: Für Windows oder macOS nutze jeweils ipconfig oder ifconfig, um die IP-Adresse zu ermitteln.</p><p>Klonen das Repository und wechsle in den Ordner </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Installiere die spezifischen Anforderungen für den Server:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Starte das Server-Skript.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Du hast die Wahl zwischen der Nutzung von </code>ollama<code> und </code>llamacpp<code> als LLM-Service.</p><p>
Nun auf deinem persönlichen Computer:</p><p>Ändere die Datei </code>config.ini<code>, setze </code>provider_name<code> auf </code>server<code> und </code>provider_model<code> auf </code>deepseek-r1:xxb<code>.
Setze </code>provider_server_address<code> auf die IP-Adresse des Rechners, der das Modell ausführt.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>Nächster Schritt: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Dienste starten und AgenticSeek ausführen</a>  </p><hr></p><h2>Sprache-zu-Text</h2></p><p>Achtung: Sprache-zu-Text funktioniert derzeit nur im CLI-Modus.</p><p>Beachte, dass Sprache-zu-Text aktuell nur auf Englisch funktioniert.</p><p>Die Sprache-zu-Text-Funktion ist standardmäßig deaktiviert. Um sie zu aktivieren, setze die Option listen in der config.ini-Datei auf True:
</code></pre>
listen = True
<pre><code class="language-">
Wenn aktiviert, wartet die Sprache-zu-Text-Funktion auf ein Auslöse-Schlüsselwort, das der Name des Agenten ist, bevor deine Eingabe verarbeitet wird. Du kannst den Namen des Agenten anpassen, indem du den Wert </code>agent_name<code> in der <em>config.ini</em> aktualisierst:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Für eine optimale Erkennung empfehlen wir, einen geläufigen englischen Namen wie „John“ oder „Emma“ als Agentennamen zu verwenden.</p><p>Sobald das Transkript angezeigt wird, sagen Sie den Namen des Agenten laut, um ihn zu aktivieren (z.B. „Friday“).</p><p>Sprechen Sie Ihre Anfrage deutlich aus.</p><p>Beenden Sie Ihre Anfrage mit einer Bestätigungsphrase, um das System zum Fortfahren zu veranlassen. Beispiele für Bestätigungsphrasen sind:</code></pre>
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
<pre><code class="language-">
<h2>Config</h2></p><p>Beispiel-Konfiguration:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Beispiel für Ollama; benutze http://127.0.0.1:1234 für LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Liste der Sprachen für TTS und ggf. Routing.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Erklärung der </code>config.ini<code>-Einstellungen</strong>:</p><ul><li>  <strong></code>[MAIN]<code> Abschnitt:</strong></li>
    <li>  </code>is_local<code>: </code>True<code>, wenn ein lokaler LLM-Provider verwendet wird (Ollama, LM-Studio, lokaler OpenAI-kompatibler Server) oder die selbst gehostete Server-Option. </code>False<code>, wenn eine Cloud-basierte API (OpenAI, Google, etc.) verwendet wird.</li>
    <li>  </code>provider_name<code>: Gibt den LLM-Provider an.</li>
        <li>  Lokale Optionen: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (für lokale OpenAI-kompatible Server), </code>server<code> (für das selbst gehostete Server-Setup).</li>
        <li>  API-Optionen: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: Der spezifische Modellname oder die ID für den gewählten Provider (z.B. </code>deepseekcoder:6.7b<code> für Ollama, </code>gpt-3.5-turbo<code> für die OpenAI-API, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> für TogetherAI).</li>
    <li>  </code>provider_server_address<code>: Die Adresse Ihres LLM-Providers.</li>
        <li>  Für lokale Provider: z.B. </code>http://127.0.0.1:11434<code> für Ollama, </code>http://127.0.0.1:1234<code> für LM-Studio.</li>
        <li>  Für den </code>server<code>-Provider-Typ: Die Adresse Ihres selbst gehosteten LLM-Servers (z.B. </code>http://your_server_ip:3333<code>).</li>
        <li>  Für Cloud-APIs (</code>is_local = False<code>): Wird meist ignoriert oder kann leer gelassen werden, da der API-Endpunkt i.d.R. durch die Client-Bibliothek verarbeitet wird.</li>
    <li>  </code>agent_name<code>: Name des KI-Assistenten (z.B. Friday). Wird als Triggerwort für Sprache-zu-Text verwendet, falls aktiviert.</li>
    <li>  </code>recover_last_session<code>: </code>True<code>, um den Zustand der vorherigen Sitzung wiederherzustellen, </code>False<code> für einen Neustart.</li>
    <li>  </code>save_session<code>: </code>True<code>, um den aktuellen Sitzungsstatus für eine mögliche Wiederherstellung zu speichern, sonst </code>False<code>.</li>
    <li>  </code>speak<code>: </code>True<code>, um Text-zu-Sprache-Sprachausgabe zu aktivieren, </code>False<code> zum Deaktivieren.</li>
    <li>  </code>listen<code>: </code>True<code>, um Sprache-zu-Text-Eingabe zu aktivieren (nur CLI-Modus), </code>False<code> zum Deaktivieren.</li>
    <li>  </code>work_dir<code>: <strong>Wichtig:</strong> Das Verzeichnis, in dem AgenticSeek Dateien liest/schreibt. <strong>Stellen Sie sicher, dass dieser Pfad auf Ihrem System gültig und zugänglich ist.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> für einen experimentellen, „Jarvis-ähnlichen“ Systemprompt, </code>False<code> für den Standardprompt.</li>
    <li>  </code>languages<code>: Kommagetrennte Liste von Sprachen (z.B. </code>en, zh, fr<code>). Wird für die Auswahl der TTS-Stimme verwendet (Standard ist die erste) und kann dem LLM-Router helfen. Vermeiden Sie zu viele oder sehr ähnliche Sprachen für die Effizienz des Routers.</li>
<li>  <strong></code>[BROWSER]<code> Abschnitt:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code>, um den automatisierten Browser ohne sichtbares Fenster auszuführen (empfohlen für Web-Oberfläche oder nicht-interaktive Nutzung). </code>False<code>, um das Browserfenster anzuzeigen (nützlich für CLI-Modus oder Debugging).</li>
    <li>  </code>stealth_mode<code>: </code>True<code>, um Maßnahmen zu aktivieren, die die Erkennung von Browserautomatisierung erschweren. Dies kann die manuelle Installation von Browser-Erweiterungen wie anticaptcha erfordern.</li></p><p></ul>Dieser Abschnitt fasst die unterstützten LLM-Provider-Typen zusammen. Konfigurieren Sie diese in der </code>config.ini<code>.</p><p><strong>Lokale Provider (laufen auf Ihrer eigenen Hardware):</strong></p><p>| Provider-Name in </code>config.ini<code> | </code>is_local<code> | Beschreibung                                                                 | Setup-Abschnitt                                                    |
|-------------------------------|------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Nutzt Ollama zum Bereitstellen lokaler LLMs.                                | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup für lokalen LLM-Betrieb</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Nutzt LM-Studio zum Bereitstellen lokaler LLMs.                             | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup für lokalen LLM-Betrieb</a> |
| </code>openai<code> (für lokalen Server) | </code>True<code>     | Verbindung zu lokalem Server mit OpenAI-kompatibler API (z.B. llama.cpp).   | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup für lokalen LLM-Betrieb</a> |
| </code>server<code>                      | </code>False<code>    | Verbindung zum AgenticSeek-LLM-Server auf einem anderen Rechner.             | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Setup für eigenen LLM-Server</a> |</p><p><strong>API-Provider (Cloud-basiert):</strong></p><p>| Provider-Name in </code>config.ini<code> | </code>is_local<code> | Beschreibung                                      | Setup-Abschnitt                                      |
|-------------------------------|------------|--------------------------------------------------|------------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Nutzung der offiziellen OpenAI-API (z.B. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup für API-Betrieb</a>         |
| </code>google<code>                      | </code>False<code>    | Nutzung der Gemini-Modelle von Google via API.    | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup für API-Betrieb</a>         |
| </code>deepseek<code>                    | </code>False<code>    | Nutzung der offiziellen Deepseek-API.             | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup für API-Betrieb</a>         |
| </code>huggingface<code>                 | </code>False<code>    | Nutzung der Hugging Face Inference API.           | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup für API-Betrieb</a>         |
| </code>togetherAI<code>                  | </code>False<code>    | Nutzung der TogetherAI-API für verschiedene Open-Modelle. | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup für API-Betrieb</a>         |</p><hr>
<h2>Fehlerbehebung</h2></p><p>Wenn Sie auf Probleme stoßen, finden Sie in diesem Abschnitt Hilfestellungen.</p><h1>Bekannte Probleme</h1></p><h2>ChromeDriver-Probleme</h2></p><p><strong>Fehlerbeispiel:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Ursache:</strong> Die installierte ChromeDriver-Version ist nicht kompatibel mit Ihrer Google Chrome-Version.</li>
<li>  <strong>Lösung:</strong></li>
    <li> <strong>Chrome-Version prüfen:</strong> Öffnen Sie Google Chrome, gehen Sie zu </code>Einstellungen > Über Chrome<code>, um Ihre Version zu finden (z.B. „Version 120.0.6099.110“).</li>
    <li> <strong>Passenden ChromeDriver herunterladen:</strong></li>
        <li>  Für Chrome-Versionen 115 und neuer: Gehen Sie zu den <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Finden Sie den "stable"-Kanal und laden Sie den ChromeDriver für Ihr Betriebssystem herunter, der zu Ihrer Chrome-Hauptversion passt.</li>
        <li>  Für ältere Versionen (seltener): Sie finden diese ggf. auf der Seite <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  Das untenstehende Bild zeigt ein Beispiel von der CfT-Seite:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download Chromedriver specific version from Chrome for Testing page">
    <ul><li> <strong>ChromeDriver installieren:</strong></li>
        <li>  Stellen Sie sicher, dass der heruntergeladene </code>chromedriver<code> (bzw. </code>chromedriver.exe<code> unter Windows) in einem Verzeichnis liegt, das in Ihrer PATH-Umgebungsvariablen aufgeführt ist (z.B. </code>/usr/local/bin<code> unter Linux/macOS oder einen eigenen Scripts-Ordner unter Windows).</li>
        <li>  Alternativ können Sie ihn im Hauptverzeichnis des </code>agenticSeek<code>-Projekts ablegen.</li>
        <li>  Machen Sie den Treiber ausführbar (z.B. </code>chmod +x chromedriver<code> unter Linux/macOS).</li>
    <li> Siehe auch den Abschnitt <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">ChromeDriver-Installation</a> im Haupt-Installationsleitfaden für weitere Details.</li></p><p></ul>Wenn dieser Abschnitt unvollständig ist oder Sie auf weitere ChromeDriver-Probleme stoßen, durchsuchen Sie bitte die bestehenden <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">GitHub Issues</a> oder erstellen Sie ein neues.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Dies passiert, wenn die Versionen Ihres Browsers und des Chromedrivers nicht übereinstimmen.</p><p>Sie müssen die neueste Version herunterladen:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Wenn Sie Chrome Version 115 oder neuer verwenden, gehen Sie zu:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>Und laden Sie die zum Betriebssystem passende Chromedriver-Version herunter.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Wenn dieser Abschnitt unvollständig ist, erstellen Sie bitte ein Issue.</p><h2> Probleme mit Connection Adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Hinweis: Port kann variieren)
<pre><code class="language-">
<ul><li>  <strong>Ursache:</strong> Die <code>provider_server_address</code> in der <code>config.ini</code> für <code>lm-studio</code> (oder andere lokale OpenAI-kompatible Server) fehlt das Präfix <code>http://</code> oder sie verweist auf den falschen Port.</li>
<li>  <strong>Lösung:</strong></li>
    <li>  Stellen Sie sicher, dass die Adresse mit <code>http://</code> beginnt. LM-Studio verwendet typischerweise <code>http://127.0.0.1:1234</code>.</li>
    <li>  Korrigierte <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (oder Ihr tatsächlicher LM-Studio-Server-Port).</li></p><p></ul><h2>SearxNG-Basis-URL nicht angegeben</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>F: Welche Hardware benötige ich?</strong>  </p><p>| Modellgröße | GPU      | Kommentar                                                                                         |
|-------------|----------|--------------------------------------------------------------------------------------------------|
| 7B          | 8 GB VRAM| ⚠️ Nicht empfohlen. Schlechte Performance, häufige Halluzinationen, und Planungsagenten schlagen fehl. |
| 14B         | 12 GB VRAM (z.B. RTX 3060) | ✅ Für einfache Aufgaben nutzbar. Kann bei Web-Browsing und Planung ins Straucheln geraten. |
| 32B         | 24+ GB VRAM (z.B. RTX 4090) | 🚀 Erfolgreich bei den meisten Aufgaben, könnte bei komplexer Planung an Grenzen stoßen.   |
| 70B+        | 48+ GB VRAM| 💪 Exzellent. Empfohlen für fortgeschrittene Anwendungsfälle.                                 |</p><p><strong>F: Ich erhalte einen Fehler – was tun?</strong>  </p><p>Stellen Sie sicher, dass lokal ausgeführt wird (</code>ollama serve<code>), Ihre </code>config.ini` zum Provider passt und alle Abhängigkeiten installiert sind. Wenn nichts hilft, gerne ein Issue erstellen.</p><p><strong>F: Kann es wirklich 100% lokal laufen?</strong>  </p><p>Ja, mit Ollama, lm-studio oder server-Providern laufen alle Sprachmodelle und TTS/STT-Modelle lokal. Nicht-lokale Optionen (OpenAI oder andere APIs) sind optional.</p><p><strong>F: Warum AgenticSeek nutzen, wenn ich Manus habe?</strong></p><p>Im Gegensatz zu Manus setzt AgenticSeek auf Unabhängigkeit von externen Systemen, bietet mehr Kontrolle, Privatsphäre und vermeidet API-Kosten.</p><p><strong>F: Wer steckt hinter dem Projekt?</strong></p><p>Das Projekt wurde von mir ins Leben gerufen, zusammen mit zwei Freunden, die als Maintainer und Mitwirkende aus der Open-Source-Community auf GitHub agieren. Wir sind einfach eine Gruppe engagierter Leute, kein Startup oder Unternehmen.</p><p>Jedes AgenticSeek-Konto auf X außer meinem persönlichen (https://x.com/Martin993886460) ist eine Nachahmung.</p><h2>Mitwirken</h2></p><p>Wir suchen Entwickler, die AgenticSeek verbessern! Schau dir offene Issues oder Diskussionen an.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Beitragsleitfaden</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Maintainer:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Pariser Zeit </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Taipeh Zeit </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Taipeh Zeit </p><h2>Besonderer Dank:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> und <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> für die Unterstützung bei der Backend-Dockerisierung</p><h2>Sponsoren:</h2></p><p>5$ oder mehr monatliche Sponsoren werden hier genannt:
<ul><li><strong>tatra-labs</strong></li>
</ul>Certainly! Please provide the content for Part 4 of 4 that you wish to have translated.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>