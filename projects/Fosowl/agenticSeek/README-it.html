<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Italian. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Italian. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Italian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Italian. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-it.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Italian</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: Alternativa privata e locale a Manus.</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>  English | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>Un'alternativa <strong>100% locale a Manus AI</strong>, questo assistente vocale abilitato all'IA naviga autonomamente sul web, scrive codice e pianifica attività mantenendo tutti i dati sul tuo dispositivo. Progettato per modelli di ragionamento locali, funziona interamente sull'hardware locale, garantendo completa privacy e zero dipendenza dal cloud.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visita AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>Perché AgenticSeek?</h3></p><ul><li>🔒 Completamente Locale e Privato - Tutto funziona sulla tua macchina — nessun cloud, nessuna condivisione dei dati. I tuoi file, conversazioni e ricerche rimangono privati.</li></p><p><li>🌐 Navigazione Web Intelligente - AgenticSeek può navigare autonomamente in internet — cercare, leggere, estrarre informazioni, compilare moduli web — tutto a mani libere.</li></p><p><li>💻 Assistente di Codifica Autonomo - Hai bisogno di codice? Può scrivere, effettuare il debug ed eseguire programmi in Python, C, Go, Java e altro — tutto senza supervisione.</li></p><p><li>🧠 Selezione Intelligente dell’Agente - Tu chiedi, lui individua automaticamente l'agente migliore per il compito. Come avere un team di esperti sempre pronti ad aiutarti.</li></p><p><li>📋 Pianifica ed Esegue Compiti Complessi - Dalla pianificazione di viaggi a progetti complessi — può suddividere grandi attività in step e portarle a termine usando molteplici agenti IA.</li></p><p><li>🎙️ Voce Abilitata - Voce pulita, veloce, futuristica e riconoscimento vocale che ti permette di parlarci come se fosse la tua IA personale da un film di fantascienza. (In sviluppo)</li></p><p></ul><h3><strong>Demo</strong></h3></p><blockquote><em>Puoi cercare il progetto agenticSeek, capire quali competenze sono richieste, poi aprire CV_candidates.zip e dirmi quali corrispondono meglio al progetto?</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>Disclaimer: Questa demo, inclusi tutti i file che compaiono (es: CV_candidates.zip), è completamente fittizia. Non siamo una società, cerchiamo contributori open-source, non candidati.</p><blockquote>🛠⚠️️ <strong>Lavoro attivo in corso</strong></blockquote></p><blockquote>🙏 Questo progetto è iniziato come un side-project e non ha roadmap né finanziamenti. È cresciuto oltre le aspettative finendo nei GitHub Trending. Contributi, feedback e pazienza sono profondamente apprezzati.</blockquote></p><h2>Prerequisiti</h2></p><p>Prima di iniziare, assicurati di avere installato il seguente software:</p><ul><li>  <strong>Git:</strong> Per clonare il repository. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">Scarica Git</a></li>
<li>  <strong>Python 3.10.x:</strong> Consigliamo vivamente Python versione 3.10.x. L’uso di altre versioni può causare errori di dipendenza. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">Scarica Python 3.10</a> (scegli una versione 3.10.x).</li>
<li>  <strong>Docker Engine & Docker Compose:</strong> Per l’esecuzione di servizi integrati come SearxNG.</li>
    <li>  Installa Docker Desktop (che include Docker Compose V2): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">Windows</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">Mac</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">Linux</a></li>
    <li>  In alternativa, installa Docker Engine e Docker Compose separatamente su Linux: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (assicurati di installare Compose V2, ad esempio, <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>Clona il repository e configura</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. Modifica il contenuto del file .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>Aggiorna il file <code>.env</code> con i tuoi valori ove necessario:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: Lascia invariato</li>
<li><strong>REDIS_BASE_URL</strong>: Lascia invariato</li>
<li><strong>WORK_DIR</strong>: Percorso della tua cartella di lavoro sulla tua macchina locale. AgenticSeek potrà leggere e interagire con questi file.</li>
<li><strong>OLLAMA_PORT</strong>: Porta per il servizio Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: Porta per il servizio LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: Porta per eventuali servizi LLM personalizzati aggiuntivi.</li></p><p></ul><strong>Le chiavi API sono totalmente opzionali per gli utenti che scelgono di eseguire LLM localmente, che è lo scopo principale di questo progetto. Lascia vuoto se hai hardware sufficiente</strong></p><h3>3. <strong>Avvia Docker</strong></h3></p><p>Assicurati che Docker sia installato e in esecuzione sul tuo sistema. Puoi avviare Docker con i seguenti comandi:</p><ul><li><strong>Su Linux/macOS:</strong>  </li>
    </ul>Apri un terminale ed esegui:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    Oppure lancia Docker Desktop dal menu delle applicazioni se installato.</p><ul><li><strong>Su Windows:</strong>  </li>
    </ul>Avvia Docker Desktop dal menu Start.</p><p>Puoi verificare che Docker sia in esecuzione eseguendo:</code></pre>sh
docker info
<pre><code class="language-">Se vedi informazioni sulla tua installazione di Docker, allora è attivo correttamente.</p><p>Consulta la tabella dei <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">Provider Locali</a> qui sotto per un riassunto.</p><p>Prossimo passo: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">Esegui AgenticSeek in locale</a></p><p><em>Consulta la sezione <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Risoluzione dei problemi</a> se hai problemi.</em>
<em>Se il tuo hardware non può eseguire LLM in locale, vedi <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configurazione per esecuzione con una API</a>.</em>
<em>Per dettagli su </code>config.ini<code>, vedi <a href="#config" target="_blank" rel="noopener noreferrer">Sezione Config</a>.</em></p><hr></p><h2>Configurazione per esecuzione LLM in locale sulla tua macchina</h2></p><p><strong>Requisiti hardware:</strong></p><p>Per eseguire LLM localmente, avrai bisogno di hardware sufficiente. Al minimo, è richiesta una GPU in grado di eseguire Magistral, Qwen o Deepseek 14B. Consulta le FAQ per raccomandazioni dettagliate su modelli/prestazioni.</p><p><strong>Configura il tuo provider locale</strong></p><p>Avvia il tuo provider locale, ad esempio con ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
Consulta sotto per un elenco di provider locali supportati.</p><p><strong>Aggiorna il config.ini</strong></p><p>Modifica il file config.ini per impostare provider_name su un provider supportato e provider_model su un LLM supportato dal tuo provider. Consigliamo modelli di ragionamento come <em>Magistral</em> o <em>Deepseek</em>.</p><p>Consulta le <strong>FAQ</strong> alla fine del README per l’hardware necessario.
</code></pre>sh
[MAIN]
is_local = True # Se stai eseguendo localmente o con un provider remoto.
provider_name = ollama # oppure lm-studio, openai, ecc.
provider_model = deepseek-r1:14b # scegli un modello compatibile col tuo hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nome della tua IA
recover_last_session = True # se recuperare la sessione precedente
save_session = True # se ricordare la sessione attuale
speak = False # sintesi vocale
listen = False # riconoscimento vocale, solo per CLI, sperimentale
jarvis_personality = False # se usare una personalità tipo "Jarvis" (sperimentale)
languages = en zh # Elenco lingue, la sintesi vocale userà la prima della lista
[BROWSER]
headless_browser = True # lascia invariato a meno che usi CLI sull’host.
stealth_mode = True # Usa selenium non rilevabile per ridurre la detection del browser
<pre><code class="language-">
<strong>Attenzione</strong>:</p><ul><li>Il formato del file </code>config.ini<code> non supporta commenti.</li>
</ul>Non copiare e incollare direttamente la configurazione di esempio, poiché i commenti causeranno errori. Modifica manualmente il file </code>config.ini<code> con le impostazioni desiderate, escludendo qualsiasi commento.</p><ul><li>NON impostare provider_name su </code>openai<code> se usi LM-studio per eseguire LLM. Impostalo su </code>lm-studio<code>.</li></p><p><li>Alcuni provider (es: lm-studio) richiedono che ci sia </code>http://<code> davanti all’IP. Ad esempio </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>Elenco dei provider locali</strong></p><p>| Provider  | Locale? | Descrizione                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Sì     | Esegui LLM localmente con ollama come provider LLM        |
| lm-studio | Sì     | Esegui LLM localmente con LM studio (imposta </code>provider_name<code> su </code>lm-studio<code>)|
| openai    | Sì     |  Usa API compatibili openai (es: server llama.cpp)        |</p><p>Prossimo passo: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Avvia i servizi ed esegui AgenticSeek</a>  </p><p><em>Consulta la sezione <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">Risoluzione dei problemi</a> se hai problemi.</em>
<em>Se il tuo hardware non può eseguire LLM in locale, vedi <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Configurazione per esecuzione con una API</a>.</em>
<em>Per dettagli su </code>config.ini<code>, vedi <a href="#config" target="_blank" rel="noopener noreferrer">Sezione Config</a>.</em></p><h2>Configurazione per esecuzione tramite API</h2></p><p>Questa configurazione utilizza provider LLM esterni basati su cloud. Avrai bisogno di una chiave API dal servizio scelto.</p><p><strong>1. Scegli un provider API e ottieni una chiave API:</strong></p><p>Consulta l’<a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">Elenco dei provider API</a> qui sotto. Visita i loro siti web per registrarti e ottenere una chiave API.</p><p><strong>2. Imposta la tua chiave API come variabile d’ambiente:</strong></p><ul><li>  <strong>Linux/macOS:</strong></li>
    </ul>Apri il terminale e usa il comando </code>export<code>. È meglio aggiungerlo al file di profilo della shell (es: </code>~/.bashrc<code>, </code>~/.zshrc<code>) per la persistenza.
    </code>`<code>sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # Sostituisci PROVIDER_API_KEY con il nome della variabile specifica, es: OPENAI_API_KEY, GOOGLE_API_KEY
    </code>`<code>
    Esempio per TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>Windows:</strong></li>
<li>  <strong>Prompt dei comandi (temporaneo per la sessione corrente):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (temporaneo per la sessione corrente):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>Permanente:</strong> Cerca "variabili d'ambiente" nella barra di ricerca di Windows, clicca su "Modifica le variabili d'ambiente di sistema," poi clicca sul pulsante "Variabili d'ambiente...". Aggiungi una nuova variabile utente con il nome appropriato (es. </code>OPENAI_API_KEY<code>) e la tua chiave come valore.</li></p><p></ul><em>(Vedi FAQ: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">Come imposto le chiavi API?</a> per maggiori dettagli).</em></p><p>
<strong>3. Aggiorna </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # Oppure google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Oppure gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 ecc.
provider_server_address = # Tipicamente ignorato o può essere lasciato vuoto quando is_local = False per la maggior parte delle API
<h1>... altre impostazioni ...</h1>
<pre><code class="language-"><em>Attenzione:</em> Assicurati che non ci siano spazi finali nei valori di </code>config.ini<code>.</p><p><strong>Elenco dei fornitori API</strong></p><p>| Fornitore     | </code>provider_name<code> | Locale? | Descrizione                                         | Link chiave API (Esempi)                              |
|---------------|-----------------|---------|-----------------------------------------------------|--------------------------------------------------------|
| OpenAI        | </code>openai<code>        | No      | Utilizza i modelli ChatGPT tramite l'API di OpenAI. | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini | </code>google<code>        | No      | Utilizza i modelli Gemini di Google tramite AI Studio.| <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek      | </code>deepseek<code>      | No      | Utilizza i modelli Deepseek tramite la loro API.    | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face  | </code>huggingface<code>   | No      | Utilizza modelli tramite Hugging Face Inference API.| <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI    | </code>togetherAI<code>    | No      | Utilizza vari modelli open-source tramite TogetherAI API.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>Nota:</em>
<ul><li>  Sconsigliamo l'uso di </code>gpt-4o<code> o altri modelli OpenAI per la navigazione web complessa e la pianificazione di task poiché le attuali ottimizzazioni dei prompt sono orientate verso modelli come Deepseek.</li>
<li>  Task di coding/bash potrebbero avere problemi con Gemini, in quanto potrebbe non seguire rigorosamente i prompt di formattazione ottimizzati per Deepseek.</li>
<li>  Il campo </code>provider_server_address<code> in </code>config.ini<code> generalmente non viene utilizzato quando </code>is_local = False<code> poiché l'endpoint API è solitamente hardcoded nella rispettiva libreria del fornitore.</li></p><p></ul>Prossimo passo: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Avvia servizi ed esegui AgenticSeek</a></p><p><em>Consulta la sezione <strong>Problemi noti</strong> se hai problemi</em></p><p><em>Consulta la sezione <strong>Config</strong> per una spiegazione dettagliata del file di configurazione.</em></p><hr></p><h2>Avvia servizi ed Esegui</h2></p><p>Per impostazione predefinita AgenticSeek viene eseguito completamente in docker.</p><p>Avvia i servizi richiesti. Questo avvierà tutti i servizi dal docker-compose.yml, inclusi:
   <ul><li>searxng</li>
   <li>redis (necessario per searxng)</li>
   <li>frontend</li>
   <li>backend (se si utilizza </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Window
<pre><code class="language-">
<strong>Attenzione:</strong> Questo passaggio scaricherà e caricherà tutte le immagini Docker, il che potrebbe richiedere fino a 30 minuti. Dopo l'avvio dei servizi, attendi che il servizio backend sia completamente operativo (dovresti vedere <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> nel log) prima di inviare qualsiasi messaggio. I servizi backend potrebbero impiegare 5 minuti per avviarsi al primo avvio.</p><p>Vai su </code>http://localhost:3000/<code> e dovresti vedere l'interfaccia web.</p><p><em>Risoluzione dei problemi di avvio servizio:</em> Se questi script falliscono, assicurati che Docker Engine sia in esecuzione e che Docker Compose (V2, </code>docker compose<code>) sia installato correttamente. Controlla l'output nel terminale per eventuali messaggi di errore. Vedi <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">FAQ: Aiuto! Ricevo un errore quando eseguo AgenticSeek o i suoi script.</a></p><p><strong>Opzionale:</strong> Esegui su host (modalità CLI):</p><p>Per eseguire con l'interfaccia CLI devi installare il pacchetto sull'host:
</code></pre>sh
./install.sh
./install.bat # windows
<pre><code class="language-">
Avvia i servizi:
</code></pre>sh
./start_services.sh # MacOS
start ./start_services.cmd # Window
<pre><code class="language-">
Usa la CLI: </code>python3 cli.py<code></p><hr></p><h2>Utilizzo</h2></p><p>Assicurati che i servizi siano attivi e funzionanti con </code>./start_services.sh full<code> e vai su </code>localhost:3000<code> per l'interfaccia web.</p><p>Puoi anche utilizzare la funzione speech to text impostando </code>listen = True<code> nel config. Solo in modalità CLI.</p><p>Per uscire, basta dire/digitare </code>goodbye<code>.</p><p>Ecco alcuni esempi di utilizzo:</p><blockquote><em>Crea un gioco snake in python!</em></blockquote></p><blockquote><em>Cerca sul web i migliori caffè a Rennes, Francia, e salva una lista di tre con i loro indirizzi in rennes_cafes.txt.</em></blockquote></p><blockquote><em>Scrivi un programma Go per calcolare il fattoriale di un numero, salvalo come factorial.go nella tua workspace</em></blockquote></p><blockquote><em>Cerca nella mia cartella summer_pictures tutti i file JPG, rinominali con la data odierna e salva la lista dei file rinominati in photos_list.txt</em></blockquote></p><blockquote><em>Cerca online film di fantascienza popolari del 2024 e scegline tre da guardare stasera. Salva la lista in movie_night.txt.</em></blockquote></p><blockquote><em>Cerca sul web gli ultimi articoli di notizie sull'IA dal 2025, seleziona tre e scrivi uno script Python per estrarre i titoli e i riassunti. Salva lo script come news_scraper.py e i riassunti in ai_news.txt in /home/projects</em></blockquote></p><blockquote><em>Venerdì, cerca sul web una API gratuita per i prezzi delle azioni, registrati con supersuper7434567@gmail.com, quindi scrivi uno script Python per recuperare tramite l'API i prezzi giornalieri di Tesla e salva i risultati in stock_prices.csv</em></blockquote></p><p><em>Nota che le capacità di compilazione moduli sono ancora sperimentali e potrebbero fallire.</em></p><p>Dopo aver digitato la tua richiesta, AgenticSeek assegnerà il miglior agente per il compito.</p><p>Poiché si tratta di un prototipo iniziale, il sistema di instradamento degli agenti potrebbe non sempre assegnare l'agente giusto in base alla tua richiesta.</p><p>Pertanto, dovresti essere molto esplicito su ciò che desideri e su come l'IA potrebbe procedere; ad esempio, se vuoi che effettui una ricerca web, non dire:</p><p></code>Conosci alcuni buoni paesi per viaggiare da soli?<code></p><p>Invece, chiedi:</p><p></code>Fai una ricerca web e scopri quali sono i migliori paesi per viaggiare da soli<code></p><hr></p><h2><strong>Configurazione per eseguire l'LLM sul tuo server</strong>  </h2></p><p>Se disponi di un computer potente o di un server che puoi utilizzare, ma vuoi usarlo dal tuo portatile, hai la possibilità di eseguire l'LLM su un server remoto utilizzando il nostro llm server personalizzato.</p><p>Sul tuo "server" che eseguirà il modello AI, ottieni l'indirizzo IP
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip pubblico
<pre><code class="language-">
Nota: Per Windows o macOS, utilizza rispettivamente ipconfig o ifconfig per trovare l'indirizzo IP.</p><p>Clona il repository ed entra nella cartella </code>server/<code>.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
Installa i requisiti specifici del server:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
Esegui lo script server.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
Puoi scegliere tra utilizzare </code>ollama<code> e </code>llamacpp<code> come servizio LLM.</p><p>
Ora sul tuo computer personale:</p><p>Modifica il file </code>config.ini<code> per impostare </code>provider_name<code> su </code>server<code> e </code>provider_model<code> su </code>deepseek-r1:xxb<code>.
Imposta </code>provider_server_address<code> sull'indirizzo IP della macchina che eseguirà il modello.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>Prossimo passo: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">Avvia servizi ed esegui AgenticSeek</a>  </p><hr></p><h2>Speech to Text</h2></p><p>Attenzione: la funzione speech to text funziona solo in modalità CLI al momento.</p><p>Nota che attualmente il riconoscimento vocale funziona solo in inglese.</p><p>La funzionalità di speech-to-text è disabilitata per impostazione predefinita. Per abilitarla, imposta l'opzione listen su True nel file config.ini:
</code></pre>
listen = True
<pre><code class="language-">
Quando abilitata, la funzione speech-to-text ascolta una parola chiave di attivazione, che è il nome dell'agente, prima di iniziare a elaborare il tuo input. Puoi personalizzare il nome dell'agente aggiornando il valore </code>agent_name<code> nel file <em>config.ini</em>:
</code></pre>
agent_name = Friday
<pre><code class="language-">
Per un riconoscimento ottimale, si consiglia di utilizzare un nome inglese comune come "John" o "Emma" come nome dell'agente</p><p>Una volta che vedrai iniziare ad apparire la trascrizione, pronuncia ad alta voce il nome dell’agente per attivarlo (es. "Friday").</p><p>Pronuncia chiaramente la tua richiesta.</p><p>Termina la richiesta con una frase di conferma per segnalare al sistema di procedere. Esempi di frasi di conferma includono:</code></pre>
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
<pre><code class="language-">
<h2>Config</h2></p><p>Esempio di configurazione:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Esempio per Ollama; usa http://127.0.0.1:1234 per LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # Elenco delle lingue per TTS e potenzialmente per l’instradamento.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>Spiegazione delle impostazioni di </code>config.ini<code>:</strong></p><ul><li>  <strong>Sezione </code>[MAIN]<code>:</strong></li>
    <li>  </code>is_local<code>: </code>True<code> se si utilizza un provider LLM locale (Ollama, LM-Studio, server locale compatibile OpenAI) o l’opzione server self-hosted. </code>False<code> se si utilizza un’API cloud (OpenAI, Google, ecc.).</li>
    <li>  </code>provider_name<code>: Specifica il provider LLM.</li>
        <li>  Opzioni locali: </code>ollama<code>, </code>lm-studio<code>, </code>openai<code> (per server locali compatibili OpenAI), </code>server<code> (per il setup self-hosted).</li>
        <li>  Opzioni API: </code>openai<code>, </code>google<code>, </code>deepseek<code>, </code>huggingface<code>, </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: Il nome o ID specifico del modello per il provider scelto (es. </code>deepseekcoder:6.7b<code> per Ollama, </code>gpt-3.5-turbo<code> per OpenAI API, </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> per TogetherAI).</li>
    <li>  </code>provider_server_address<code>: L’indirizzo del tuo provider LLM.</li>
        <li>  Per provider locali: es. </code>http://127.0.0.1:11434<code> per Ollama, </code>http://127.0.0.1:1234<code> per LM-Studio.</li>
        <li>  Per il tipo </code>server<code>: l’indirizzo del tuo server LLM self-hosted (es. </code>http://your_server_ip:3333<code>).</li>
        <li>  Per API cloud (</code>is_local = False<code>): spesso viene ignorato o può essere lasciato vuoto, dato che l’endpoint API è solitamente gestito dalla libreria client.</li>
    <li>  </code>agent_name<code>: Nome dell’assistente AI (es. Friday). Utilizzato come parola di attivazione per il riconoscimento vocale se abilitato.</li>
    <li>  </code>recover_last_session<code>: </code>True<code> per tentare di ripristinare lo stato della sessione precedente, </code>False<code> per iniziare da zero.</li>
    <li>  </code>save_session<code>: </code>True<code> per salvare lo stato della sessione corrente per un eventuale recupero, </code>False<code> altrimenti.</li>
    <li>  </code>speak<code>: </code>True<code> per abilitare l’output vocale tramite sintesi vocale, </code>False<code> per disabilitarlo.</li>
    <li>  </code>listen<code>: </code>True<code> per abilitare l’input vocale tramite riconoscimento vocale (solo modalità CLI), </code>False<code> per disabilitarlo.</li>
    <li>  </code>work_dir<code>: <strong>Cruciale:</strong> La directory dove AgenticSeek leggerà/scriverà i file. <strong>Assicurati che questo percorso sia valido e accessibile dal tuo sistema.</strong></li>
    <li>  </code>jarvis_personality<code>: </code>True<code> per utilizzare un prompt di sistema più in stile "Jarvis" (sperimentale), </code>False<code> per il prompt standard.</li>
    <li>  </code>languages<code>: Un elenco separato da virgole di lingue (es. </code>en, zh, fr<code>). Usato per la selezione della voce TTS (predefinita la prima) e può aiutare l’instradatore LLM. Evita troppe lingue o lingue molto simili per efficienza.</li>
<li>  <strong>Sezione </code>[BROWSER]<code>:</strong></li>
    <li>  </code>headless_browser<code>: </code>True<code> per eseguire il browser automatizzato senza finestra visibile (consigliato per interfaccia web o uso non interattivo). </code>False<code> per mostrare la finestra del browser (utile in modalità CLI o per debug).</li>
    <li>  </code>stealth_mode<code>: </code>True<code> per abilitare misure che rendono più difficile rilevare l’automazione del browser. Potrebbe richiedere l’installazione manuale di estensioni come anticaptcha.</li></p><p></ul>Questa sezione riassume i tipi di provider LLM supportati. Configurali in </code>config.ini<code>.</p><p><strong>Provider Locali (In esecuzione sull’hardware locale):</strong></p><p>| Nome provider in </code>config.ini<code> | </code>is_local<code> | Descrizione                                                             | Sezione Setup                                                                 |
|-------------------------------|------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | Usa Ollama per servire LLM locali.                                      | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup per eseguire LLM localmente</a> |
| </code>lm-studio<code>                   | </code>True<code>     | Usa LM-Studio per servire LLM locali.                                   | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup per eseguire LLM localmente</a> |
| </code>openai<code> (per server locale)  | </code>True<code>     | Connette a un server locale che espone una API compatibile OpenAI (es. llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">Setup per eseguire LLM localmente</a> |
| </code>server<code>                      | </code>False<code>    | Connette al server LLM self-hosted AgenticSeek in esecuzione su un’altra macchina. | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">Setup per eseguire LLM su proprio server</a> |</p><p><strong>Provider API (Cloud):</strong></p><p>| Nome provider in </code>config.ini<code> | </code>is_local<code> | Descrizione                                        | Sezione Setup                                      |
|-------------------------------|------------|----------------------------------------------------|----------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | Usa l’API ufficiale di OpenAI (es. GPT-3.5, GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup per l'uso con API</a> |
| </code>google<code>                      | </code>False<code>    | Usa i modelli Gemini di Google tramite API.         | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup per l'uso con API</a> |
| </code>deepseek<code>                    | </code>False<code>    | Usa l’API ufficiale Deepseek.                       | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup per l'uso con API</a> |
| </code>huggingface<code>                 | </code>False<code>    | Usa l’Hugging Face Inference API.                   | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup per l'uso con API</a> |
| </code>togetherAI<code>                  | </code>False<code>    | Usa l’API TogetherAI per vari modelli open.         | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">Setup per l'uso con API</a> |</p><hr>
<h2>Risoluzione dei problemi</h2></p><p>Se incontri problemi, questa sezione fornisce indicazioni.</p><h1>Problemi noti</h1></p><h2>Problemi con ChromeDriver</h2></p><p><strong>Esempio di errore:</strong> </code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>Causa:</strong> La versione di ChromeDriver installata non è compatibile con la versione del browser Google Chrome.</li>
<li>  <strong>Soluzione:</strong></li>
    <li> <strong>Verifica versione Chrome:</strong> Apri Google Chrome, vai su </code>Impostazioni > Informazioni su Chrome<code> per trovare la tua versione (es. "Versione 120.0.6099.110").</li>
    <li> <strong>Scarica la versione corrispondente di ChromeDriver:</strong></li>
        <li>  Per versioni Chrome 115 e successive: Vai ai <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a>. Trova il canale "stable" e scarica ChromeDriver per il tuo sistema operativo che corrisponde alla major version di Chrome.</li>
        <li>  Per versioni precedenti (meno comuni): Potresti trovarle sulla pagina <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a>.</li>
        <li>  L'immagine sotto mostra un esempio dalla pagina CfT:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Scarica una versione specifica di Chromedriver dalla pagina Chrome for Testing">
    <ul><li> <strong>Installa ChromeDriver:</strong></li>
        <li>  Assicurati che il file </code>chromedriver<code> scaricato (o </code>chromedriver.exe<code> su Windows) sia posizionato in una directory presente nella variabile di ambiente PATH del sistema (es. </code>/usr/local/bin<code> su Linux/macOS, o una cartella scripts personalizzata aggiunta a PATH su Windows).</li>
        <li>  In alternativa, posizionalo nella directory principale del progetto </code>agenticSeek<code>.</li>
        <li>  Assicurati che il driver sia eseguibile (es. </code>chmod +x chromedriver<code> su Linux/macOS).</li>
    <li> Consulta la sezione <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">Installazione di ChromeDriver</a> nella guida di Installazione principale per maggiori dettagli.</li></p><p></ul>Se questa sezione è incompleta o incontri altri problemi con ChromeDriver, valuta la possibilità di cercare tra le <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">Issue GitHub esistenti</a> o aprirne una nuova.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>Questo accade se c’è una discrepanza tra la versione del browser e quella di chromedriver.</p><p>Devi andare a scaricare l’ultima versione:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>Se usi Chrome versione 115 o superiore vai su:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>E scarica la versione di chromedriver corrispondente al tuo sistema operativo.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>Se questa sezione è incompleta ti preghiamo di aprire una issue.</p><h2>Problemi con connection adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (Nota: la porta può variare)
<pre><code class="language-">
<ul><li>  <strong>Causa:</strong> Il campo <code>provider_server_address</code> in <code>config.ini</code> per <code>lm-studio</code> (o altri server locali compatibili OpenAI) manca del prefisso <code>http://</code> o punta alla porta sbagliata.</li>
<li>  <strong>Soluzione:</strong></li>
    <li>  Assicurati che l’indirizzo includa <code>http://</code>. LM-Studio di solito usa <code>http://127.0.0.1:1234</code>.</li>
    <li>  Correggi <code>config.ini</code>: <code>provider_server_address = http://127.0.0.1:1234</code> (o la porta effettiva del tuo server LM-Studio).</li></p><p></ul><h2>Base URL di SearxNG non fornito</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>FAQ</h2></p><p><strong>D: Che hardware mi serve?</strong>  </p><p>| Dimensione modello | GPU       | Commento                                                                                          |
|--------------------|-----------|---------------------------------------------------------------------------------------------------|
| 7B                 | 8GB Vram  | ⚠️ Sconsigliato. Prestazioni scarse, allucinazioni frequenti, agenti planner probabilmente falliranno. |
| 14B                | 12 GB VRAM (es. RTX 3060) | ✅ Usabile per compiti semplici. Può avere difficoltà con navigazione web e pianificazione.      |
| 32B                | 24+ GB VRAM (es. RTX 4090) | 🚀 Successo nella maggior parte dei task, può ancora avere problemi nel task planning           |
| 70B+               | 48+ GB Vram | 💪 Eccellente. Raccomandato per casi d’uso avanzati.                                             |</p><p><strong>D: Ottengo un errore, cosa devo fare?</strong>  </p><p>Assicurati che il locale sia in esecuzione (</code>ollama serve<code>), che il tuo </code>config.ini` corrisponda al provider e che le dipendenze siano installate. Se nulla funziona sentiti libero di aprire una issue.</p><p><strong>D: Può davvero funzionare 100% in locale?</strong>  </p><p>Sì, con i provider Ollama, lm-studio o server, tutto il riconoscimento vocale, LLM e sintesi vocale avvengono in locale. Le opzioni non locali (OpenAI o altre API) sono opzionali.</p><p><strong>D: Perché dovrei usare AgenticSeek quando ho Manus?</strong></p><p>A differenza di Manus, AgenticSeek dà priorità all’indipendenza dai sistemi esterni, offrendoti più controllo, privacy e risparmiando sui costi API.</p><p><strong>D: Chi c’è dietro il progetto?</strong></p><p>Il progetto è stato creato da me, insieme a due amici che sono manutentori e contributor dalla community open-source su GitHub. Siamo solo un gruppo di appassionati, non una startup né affiliati ad alcuna organizzazione.</p><p>Qualsiasi account AgenticSeek su X diverso dal mio personale (https://x.com/Martin993886460) è un’impersonificazione.</p><h2>Contribuisci</h2></p><p>Cerchiamo sviluppatori per migliorare AgenticSeek! Dai un’occhiata alle issue aperte o alle discussioni.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">Guida al contributo</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>Manutentori:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | Fuso orario Parigi </p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | Fuso orario Taipei </p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | Fuso orario Taipei </p><h2>Ringraziamenti speciali:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> e <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> per l’aiuto con la dockerizzazione backend</p><h2>Sponsor:</h2></p><p>Sponsor mensili da 5$ o più qui:
<ul><li><strong>tatra-labs</strong></li>
</ul>Certainly! Please provide the content of Part 4 of 4 that you would like me to translate into Italian.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>