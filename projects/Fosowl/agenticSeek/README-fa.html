<!DOCTYPE html>
<html lang="fa">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agenticSeek - Read agenticSeek documentation in Persian. This project has 18325 stars on GitHub.</title>
    <meta name="description" content="Read agenticSeek documentation in Persian. This project has 18325 stars on GitHub.">
    <meta name="keywords" content="agenticSeek, Persian, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "agenticSeek",
  "description": "Read agenticSeek documentation in Persian. This project has 18325 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Fosowl"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 18325
  },
  "url": "https://OpenAiTx.github.io/projects/Fosowl/agenticSeek/README-fa.html",
  "sameAs": "https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md",
  "datePublished": "2025-07-24",
  "dateModified": "2025-07-24"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Fosowl/agenticSeek" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    agenticSeek
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 18325 stars</span>
                <span class="language">Persian</span>
                <span>by Fosowl</span>
            </div>
        </div>
        
        <div class="content">
            <h1>AgenticSeek: جایگزین خصوصی و محلی Manus</h1></p><p><p align="center">
<img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>انگلیسی | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md" target="_blank" rel="noopener noreferrer">中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md" target="_blank" rel="noopener noreferrer">繁體中文</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md" target="_blank" rel="noopener noreferrer">Français</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md" target="_blank" rel="noopener noreferrer">日本語</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md" target="_blank" rel="noopener noreferrer">Português (Brasil)</a> | <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md" target="_blank" rel="noopener noreferrer">Español</a></p><p><em>یک <strong>جایگزین ۱۰۰٪ محلی برای Manus AI</strong>، این دستیار هوشمند مبتنی بر صدا، به طور مستقل وب را مرور می‌کند، کد می‌نویسد و وظایف را برنامه‌ریزی می‌کند در حالی که همه داده‌ها را روی دستگاه شما نگه می‌دارد. این ابزار برای مدل‌های استدلال محلی طراحی شده و کاملاً روی سخت‌افزار شما اجرا می‌شود تا حریم خصوصی کامل و وابستگی صفر به فضای ابری را تضمین کند.</em></p><p><a href="https://fosowl.github.io/agenticSeek.html" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square" alt="Visit AgenticSeek"></a> <img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License"> <a href="https://discord.gg/8hGDaME3TC" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white" alt="Discord"></a> <a href="https://x.com/Martin993886460" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl" alt="Twitter"></a> <a href="https://github.com/Fosowl/agenticSeek/stargazers" target="_blank" rel="noopener noreferrer"><img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars"></a></p><h3>چرا AgenticSeek ؟</h3></p><ul><li>🔒 کاملاً محلی و خصوصی - همه چیز روی دستگاه شما اجرا می‌شود — بدون ابر، بدون به‌اشتراک‌گذاری داده. فایل‌ها، گفتگوها و جستجوهای شما خصوصی باقی می‌مانند.</li></p><p><li>🌐 مرور هوشمند وب - AgenticSeek می‌تواند به طور خودکار اینترنت را مرور کند — جستجو، مطالعه، استخراج اطلاعات، پرکردن فرم‌های وب — همه بدون دخالت دست.</li></p><p><li>💻 دستیار برنامه‌نویسی خودمختار - به کد نیاز دارید؟ می‌تواند برنامه‌ها را در پایتون، C، Go، جاوا و بیشتر بنویسد، رفع اشکال کند و اجرا نماید — بدون نیاز به نظارت.</li></p><p><li>🧠 انتخاب هوشمند عامل - شما درخواست می‌کنید، این ابزار بهترین عامل را برای انجام کار به طور خودکار انتخاب می‌کند. مانند داشتن تیمی از متخصصان آماده کمک.</li></p><p><li>📋 برنامه‌ریزی و اجرای وظایف پیچیده - از برنامه‌ریزی سفر تا پروژه‌های پیچیده — می‌تواند وظایف بزرگ را به مراحل تقسیم کند و با استفاده از چندین عامل هوش مصنوعی کار را انجام دهد.</li></p><p><li>🎙️ فعال‌سازی صوتی - صدای تمیز، سریع و آینده‌نگر و تبدیل گفتار به متن به شما اجازه می‌دهد مثل یک هوش مصنوعی شخصی علمی-تخیلی با آن صحبت کنید. (در حال توسعه)</li></p><p></ul><h3><strong>دمو</strong></h3></p><blockquote><em>آیا می‌توانی پروژه agenticSeek را جستجو کنی، مهارت‌های مورد نیاز را بیاموزی، سپس فایل CV_candidates.zip را باز کنی و بگویی کدام‌یک بهترین تطابق را با پروژه دارد؟</em></blockquote></p><p>https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316</p><p>توجه: این دمو، شامل تمام فایل‌های ظاهر شده (مثلاً: CV_candidates.zip)، کاملاً ساختگی است. ما یک شرکت نیستیم؛ به دنبال مشارکت‌کنندگان متن‌باز هستیم، نه داوطلبان استخدام.</p><blockquote>🛠⚠️️ <strong>پروژه فعال و در حال توسعه</strong></blockquote></p><blockquote>🙏 این پروژه به عنوان یک پروژه جانبی آغاز شد و هیچ نقشه راه یا بودجه‌ای ندارد. فراتر از انتظار من رشد کرده و به GitHub Trending رسیده است. کمک‌ها، بازخوردها و شکیبایی شما بسیار ارزشمند است.</blockquote></p><h2>پیش‌نیازها</h2></p><p>قبل از شروع، مطمئن شوید نرم‌افزارهای زیر را نصب کرده‌اید:</p><ul><li>  <strong>Git:</strong> برای کلون کردن مخزن. <a href="https://git-scm.com/downloads" target="_blank" rel="noopener noreferrer">دانلود Git</a></li>
<li>  <strong>Python 3.10.x:</strong> اکیداً توصیه می‌شود از نسخه 3.10.x پایتون استفاده کنید. استفاده از نسخه‌های دیگر ممکن است منجر به خطاهای وابستگی شود. <a href="https://www.python.org/downloads/release/python-3100/" target="_blank" rel="noopener noreferrer">دانلود Python 3.10</a> (یک نسخه 3.10.x را انتخاب کنید).</li>
<li>  <strong>Docker Engine و Docker Compose:</strong> برای اجرای سرویس‌های بسته‌بندی‌شده مانند SearxNG.</li>
    <li>  نصب Docker Desktop (که شامل Docker Compose V2 است): <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank" rel="noopener noreferrer">ویندوز</a> | <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank" rel="noopener noreferrer">مک</a> | <a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank" rel="noopener noreferrer">لینوکس</a></li>
    <li>  یا به صورت جداگانه Docker Engine و Docker Compose را روی لینوکس نصب کنید: <a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine</a> | <a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener noreferrer">Docker Compose</a> (اطمینان حاصل کنید Compose V2 را نصب می‌کنید، مثلاً: <code>sudo apt-get install docker-compose-plugin</code>).</li></p><p></ul><h3>1. <strong>کلون کردن مخزن و راه‌اندازی</strong></h3></p><pre><code class="language-sh">git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env</code></pre></p><h3>2. تغییر محتوای فایل .env</h3></p><pre><code class="language-sh">SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'</code></pre></p><p>فایل <code>.env</code> را با مقادیر دلخواه خود به‌روزرسانی کنید:</p><ul><li><strong>SEARXNG_BASE_URL</strong>: بدون تغییر باقی بماند</li>
<li><strong>REDIS_BASE_URL</strong>: بدون تغییر باقی بماند</li>
<li><strong>WORK_DIR</strong>: مسیر پوشه کاری شما در دستگاه محلی. AgenticSeek می‌تواند به این فایل‌ها دسترسی داشته باشد و با آن‌ها تعامل کند.</li>
<li><strong>OLLAMA_PORT</strong>: شماره پورت سرویس Ollama.</li>
<li><strong>LM_STUDIO_PORT</strong>: شماره پورت سرویس LM Studio.</li>
<li><strong>CUSTOM_ADDITIONAL_LLM_PORT</strong>: پورت سرویس LLM سفارشی اضافی.</li></p><p></ul><strong>کلیدهای API کاملاً اختیاری هستند برای کاربرانی که انتخاب می‌کنند LLM را به صورت محلی اجرا کنند. این هدف اصلی این پروژه است. اگر سخت‌افزار کافی دارید، خالی بگذارید</strong></p><h3>3. <strong>راه‌اندازی Docker</strong></h3></p><p>اطمینان حاصل کنید Docker روی سیستم شما نصب و در حال اجرا است. می‌توانید Docker را با دستورات زیر اجرا کنید:</p><ul><li><strong>در لینوکس/مک:</strong>  </li>
    </ul>ترمینال را باز کنید و اجرا کنید:
    <pre><code class="language-sh">    sudo systemctl start docker
    ``<code>
    یا Docker Desktop را از منوی برنامه‌ها اجرا کنید (اگر نصب شده است).</p><ul><li><strong>در ویندوز:</strong>  </li>
    </ul>Docker Desktop را از منوی Start اجرا کنید.</p><p>برای اطمینان از اجرای Docker، دستور زیر را وارد کنید:</code></pre>sh
docker info
<pre><code class="language-">اگر اطلاعاتی درباره نصب Docker مشاهده کردید، یعنی به درستی اجرا می‌شود.</p><p>جدول <a href="#list-of-local-providers" target="_blank" rel="noopener noreferrer">ارائه‌دهندگان محلی</a> را در پایین برای خلاصه مشاهده کنید.</p><p>گام بعدی: <a href="#start-services-and-run" target="_blank" rel="noopener noreferrer">اجرای AgenticSeek به صورت محلی</a></p><p><em>در صورت بروز مشکل به بخش <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">عیب‌یابی</a> مراجعه کنید.</em>
<em>اگر سخت‌افزار شما قادر به اجرای LLM به صورت محلی نیست، به <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی برای اجرا با API</a> مراجعه کنید.</em>
<em>برای توضیحات دقیق‌تر فایل </code>config.ini<code> به بخش <a href="#config" target="_blank" rel="noopener noreferrer">پیکربندی</a> مراجعه کنید.</em></p><hr></p><h2>راه‌اندازی برای اجرای LLM به صورت محلی روی دستگاه شما</h2></p><p><strong>نیازمندی‌های سخت‌افزاری:</strong></p><p>برای اجرای LLMها به صورت محلی، به سخت‌افزار کافی نیاز دارید. حداقل، یک GPU که قادر به اجرای Magistral، Qwen یا Deepseek 14B باشد لازم است. برای توصیه‌های دقیق مدل/کارایی به بخش FAQ مراجعه کنید.</p><p><strong>راه‌اندازی ارائه‌دهنده محلی</strong>  </p><p>ارائه‌دهنده محلی خود را راه‌اندازی کنید، مثلاً با ollama:
</code></pre>sh
ollama serve
<pre><code class="language-">
در ادامه فهرست ارائه‌دهندگان محلی پشتیبانی‌شده آمده است.</p><p><strong>به‌روزرسانی config.ini</strong></p><p>فایل config.ini را تغییر دهید تا provider_name را به یک ارائه‌دهنده پشتیبانی‌شده و provider_model را به یک LLM سازگار با ارائه‌دهنده خود تنظیم کنید. مدل‌های استدلال مانند <em>Magistral</em> یا <em>Deepseek</em> توصیه می‌شود.</p><p>در FAQ انتهای README نیازمندی‌های سخت‌افزاری آورده شده است.
</code></pre>sh
[MAIN]
is_local = True # اگر به صورت محلی یا با ارائه‌دهنده راه دور اجرا می‌کنید.
provider_name = ollama # یا lm-studio، openai، و غیره.
provider_model = deepseek-r1:14b # مدلی که مناسب سخت‌افزار شماست را انتخاب کنید
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # نام هوش مصنوعی شما
recover_last_session = True # بازیابی جلسه قبلی
save_session = True # ذخیره جلسه فعلی
speak = False # تبدیل متن به گفتار
listen = False # تبدیل گفتار به متن، فقط CLI، آزمایشی
jarvis_personality = False # استفاده از شخصیت مشابه "Jarvis" (آزمایشی)
languages = en zh # لیست زبان‌ها، تبدیل متن به گفتار به اولین زبان لیست پیش‌فرض می‌شود
[BROWSER]
headless_browser = True # بدون تغییر بگذارید مگر اینکه از CLI روی میزبان استفاده می‌کنید.
stealth_mode = True # استفاده از selenium مخفی برای کاهش تشخیص مرورگر
<pre><code class="language-">
<strong>هشدار</strong>:</p><ul><li>فرمت فایل </code>config.ini<code> از کامنت پشتیبانی نمی‌کند.</li>
</ul>هرگز پیکربندی نمونه را مستقیماً کپی و جایگذاری نکنید، زیرا وجود کامنت باعث خطا می‌شود. به جای آن، فایل config.ini را به صورت دستی با تنظیمات دلخواه و بدون کامنت ویرایش کنید.</p><ul><li>اگر از LM-studio برای اجرای LLM استفاده می‌کنید، provider_name را روی </code>openai<code> قرار ندهید. باید روی </code>lm-studio<code> باشد.</li></p><p><li>برخی ارائه‌دهندگان (مثلاً lm-studio) نیاز دارند که </code>http://<code> قبل از IP باشد. مثلاً </code>http://127.0.0.1:1234<code></li></p><p></ul><strong>فهرست ارائه‌دهندگان محلی</strong></p><p>| ارائه‌دهنده  | محلی؟ | توضیحات                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | بله    | اجرای LLMها به راحتی با ollama به عنوان ارائه‌دهنده LLM |
| lm-studio  | بله    | اجرای LLM به صورت محلی با LM studio (provider_name را روی </code>lm-studio<code> قرار دهید)|
| openai    | بله     |  استفاده از API سازگار با openai (مثلاً: llama.cpp server)  |</p><p>گام بعدی: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">راه‌اندازی سرویس‌ها و اجرای AgenticSeek</a>  </p><p><em>در صورت بروز مشکل به بخش <a href="#troubleshooting" target="_blank" rel="noopener noreferrer">عیب‌یابی</a> مراجعه کنید.</em>
<em>اگر سخت‌افزار شما قادر به اجرای LLM به صورت محلی نیست، به <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی برای اجرا با API</a> مراجعه کنید.</em>
<em>برای توضیحات دقیق‌تر فایل </code>config.ini<code> به بخش <a href="#config" target="_blank" rel="noopener noreferrer">پیکربندی</a> مراجعه کنید.</em></p><h2>راه‌اندازی برای اجرا با API</h2></p><p>این روش از ارائه‌دهندگان LLM مبتنی بر ابر خارجی استفاده می‌کند. به کلید API از سرویس انتخابی خود نیاز دارید.</p><p><strong>۱. انتخاب یک ارائه‌دهنده API و دریافت کلید API:</strong></p><p>به <a href="#list-of-api-providers" target="_blank" rel="noopener noreferrer">فهرست ارائه‌دهندگان API</a> مراجعه کنید. به وب‌سایت آن‌ها بروید، ثبت‌نام کنید و کلید API دریافت نمایید.</p><p><strong>۲. تنظیم کلید API به عنوان متغیر محیطی:</strong></p><ul><li>  <strong>لینوکس/مک:</strong></li>
    </ul>ترمینال خود را باز کنید و از دستور </code>export<code> استفاده کنید. بهتر است این دستور را به فایل پروفایل شل خود (مثلاً </code>~/.bashrc<code>، </code>~/.zshrc<code>) برای پایداری اضافه کنید.
    </code>`<code>sh
    export PROVIDER_API_KEY="your_api_key_here" 
    # PROVIDER_API_KEY را با نام متغیر خاص جایگزین کنید، مثلاً OPENAI_API_KEY، GOOGLE_API_KEY
    </code>`<code>
    نمونه برای TogetherAI:
    </code>`<code>sh
    export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
    </code>`<code>
<ul><li>  <strong>ویندوز:</strong></li>
<li>  <strong>Command Prompt (موقت برای جلسه فعلی):</strong></li>
    </ul></code>`<code>cmd
    set PROVIDER_API_KEY=your_api_key_here
    </code>`<code>
<ul><li>  <strong>PowerShell (موقت برای جلسه فعلی):</strong></li>
    </ul></code>`<code>powershell
    $env:PROVIDER_API_KEY="your_api_key_here"
    </code>`<code>
<ul><li>  <strong>دائمی:</strong> در نوار جستجوی ویندوز عبارت "environment variables" را جستجو کنید، روی "Edit the system environment variables" کلیک کنید، سپس دکمه "Environment Variables..." را بزنید. یک متغیر کاربری جدید با نام مناسب (مثلاً </code>OPENAI_API_KEY<code>) و مقدار کلید خود اضافه کنید.</li></p><p></ul><em>(برای جزئیات بیشتر به پرسش متداول: <a href="#how-do-i-set-api-keys" target="_blank" rel="noopener noreferrer">چگونه کلید API را تنظیم کنم؟</a> مراجعه کنید).</em></p><p><strong>3. به‌روزرسانی </code>config.ini<code>:</strong></code></pre>ini
[MAIN]
is_local = False
provider_name = openai # یا google، deepseek، togetherAI، huggingface
provider_model = gpt-3.5-turbo # یا gemini-1.5-flash، deepseek-chat، mistralai/Mixtral-8x7B-Instruct-v0.1 و غیره
provider_server_address = # معمولاً زمانی که is_local = False باشد برای اکثر APIها نادیده گرفته می‌شود یا می‌توانید خالی بگذارید
<h1>... سایر تنظیمات ...</h1>
<pre><code class="language-"><em>هشدار:</em> اطمینان حاصل کنید که مقادیر </code>config.ini<code> فاقد فاصله اضافی در انتها باشند.</p><p><strong>لیست ارائه‌دهندگان API</strong></p><p>| ارائه‌دهنده   | </code>provider_name<code> | محلی؟ | توضیحات                                               | لینک کلید API (مثال)                              |
|---------------|-----------------|--------|-------------------------------------------------------|--------------------------------------------------|
| OpenAI        | </code>openai<code>        | خیر    | استفاده از مدل‌های ChatGPT از طریق API اوپن‌اِی‌آی.     | <a href="https://platform.openai.com/signup" target="_blank" rel="noopener noreferrer">platform.openai.com/signup</a> |
| Google Gemini | </code>google<code>        | خیر    | استفاده از مدل‌های Gemini گوگل از طریق Google AI Studio. | <a href="https://aistudio.google.com/keys" target="_blank" rel="noopener noreferrer">aistudio.google.com/keys</a> |
| Deepseek      | </code>deepseek<code>      | خیر    | استفاده از مدل‌های Deepseek از طریق API آن‌ها.         | <a href="https://platform.deepseek.com" target="_blank" rel="noopener noreferrer">platform.deepseek.com</a> |
| Hugging Face  | </code>huggingface<code>   | خیر    | استفاده از مدل‌های Hugging Face Inference API.         | <a href="https://huggingface.co/settings/tokens" target="_blank" rel="noopener noreferrer">huggingface.co/settings/tokens</a> |
| TogetherAI    | </code>togetherAI<code>    | خیر    | استفاده از مدل‌های متن‌باز مختلف از طریق TogetherAI API.| <a href="https://api.together.ai/settings/api-keys" target="_blank" rel="noopener noreferrer">api.together.ai/settings/api-keys</a> |</p><p><em>توجه:</em>
<ul><li>  توصیه می‌شود برای مرور وب پیچیده و برنامه‌ریزی وظایف از مدل‌هایی مانند Deepseek به جای </code>gpt-4o<code> یا سایر مدل‌های OpenAI استفاده کنید، چراکه بهینه‌سازی‌های فعلی prompt برای این مدل‌ها انجام شده است.</li>
<li>  وظایف کدنویسی/bash ممکن است با Gemini به مشکل بخورند، زیرا ممکن است فرمت‌بندی prompt که برای Deepseek بهینه شده را دقیق رعایت نکند.</li>
<li>  معمولاً زمانی که </code>is_local = False<code> باشد، مقدار </code>provider_server_address<code> در </code>config.ini<code> استفاده نمی‌شود زیرا endpoint مربوطه در کتابخانه ارائه‌دهنده API به صورت hardcode شده است.</li></p><p></ul>گام بعدی: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">شروع سرویس‌ها و اجرای AgenticSeek</a></p><p><em>در صورت بروز مشکل به بخش <strong>Known issues</strong> مراجعه کنید.</em></p><p><em>برای توضیح کامل فایل تنظیمات به بخش <strong>Config</strong> مراجعه کنید.</em></p><hr></p><h2>شروع سرویس‌ها و اجرا</h2></p><p>به طور پیش‌فرض AgenticSeek به طور کامل در داکر اجرا می‌شود.</p><p>سرویس‌های مورد نیاز را اجرا کنید. این کار تمام سرویس‌های موجود در docker-compose.yml را راه‌اندازی می‌کند، از جمله:
    <ul><li>searxng</li>
    <li>redis (مورد نیاز توسط searxng)</li>
    <li>frontend</li>
    <li>backend (در صورت استفاده از </code>full<code>)</li>
</ul></code></pre>sh
./start_services.sh full # مک‌اواس
start ./start_services.cmd full # ویندوز
<pre><code class="language-">
<strong>هشدار:</strong> این مرحله تمام ایمیج‌های داکر را دانلود و بارگذاری می‌کند که ممکن است تا ۳۰ دقیقه طول بکشد. پس از راه‌اندازی سرویس‌ها، لطفاً تا اجرای کامل سرویس backend (باید در لاگ عبارت <strong>backend: "GET /health HTTP/1.1" 200 OK</strong> را مشاهده کنید) منتظر بمانید و سپس پیام ارسال کنید. اجرای اولیه سرویس‌های backend ممکن است تا ۵ دقیقه طول بکشد.</p><p>به آدرس </code>http://localhost:3000/<code> بروید و باید رابط وب را مشاهده کنید.</p><p><em>عیب‌یابی شروع سرویس:</em> اگر این اسکریپت‌ها با خطا مواجه شدند، مطمئن شوید Docker Engine اجرا می‌شود و Docker Compose (نسخه ۲، </code>docker compose<code>) به درستی نصب شده است. خروجی ترمینال را برای پیام‌های خطا بررسی کنید. به <a href="#faq-troubleshooting" target="_blank" rel="noopener noreferrer">پرسش متداول: در اجرای AgenticSeek یا اسکریپت‌های آن خطا دریافت می‌کنم.</a> مراجعه کنید.</p><p><strong>اختیاری:</strong> اجرا بر روی میزبان (حالت CLI):</p><p>برای اجرا با رابط خط فرمان باید پکیج را روی میزبان نصب کنید:
</code></pre>sh
./install.sh
./install.bat # ویندوز
<pre><code class="language-">
سرویس‌ها را راه‌اندازی کنید:
</code></pre>sh
./start_services.sh # مک‌اواس
start ./start_services.cmd # ویندوز
<pre><code class="language-">
استفاده از CLI: </code>python3 cli.py<code></p><hr></p><h2>استفاده</h2></p><p>مطمئن شوید که سرویس‌ها با اجرای </code>./start_services.sh full<code> فعال هستند و برای رابط وب به </code>localhost:3000<code> بروید.</p><p>همچنین می‌توانید با تنظیم </code>listen = True<code> در فایل config از قابلیت تبدیل گفتار به متن استفاده کنید (فقط در حالت CLI).</p><p>برای خروج، کافیست بگویید/بنویسید </code>goodbye<code>.</p><p>در اینجا چند نمونه استفاده آورده شده است:</p><blockquote><em>یک بازی مار با پایتون بساز!</em></blockquote></p><blockquote><em>در وب به دنبال بهترین کافه‌ها در رن، فرانسه بگرد و لیستی از سه مورد با آدرس‌شان را در rennes_cafes.txt ذخیره کن.</em></blockquote></p><blockquote><em>یک برنامه Go بنویس که فاکتوریل یک عدد را محاسبه کند، آن را به نام factorial.go در workspace خود ذخیره کن</em></blockquote></p><blockquote><em>در پوشه summer_pictures خود تمام فایل‌های JPG را پیدا کن، آن‌ها را با تاریخ امروز تغییر نام بده و لیست فایل‌های تغییر نام یافته را در photos_list.txt ذخیره کن</em></blockquote></p><blockquote><em>در اینترنت به دنبال فیلم‌های علمی‌تخیلی محبوب سال ۲۰۲۴ بگرد و سه تای آن‌ها را برای تماشای امشب انتخاب کن. لیست را در movie_night.txt ذخیره کن.</em></blockquote></p><blockquote><em>در وب به دنبال آخرین اخبار هوش مصنوعی سال ۲۰۲۵ بگرد، سه مورد را انتخاب کن و یک اسکریپت پایتون برای جمع‌آوری عنوان و خلاصه‌شان بنویس. اسکریپت را به نام news_scraper.py و خلاصه‌ها را در ai_news.txt در /home/projects ذخیره کن</em></blockquote></p><blockquote><em>جمعه، در وب به دنبال یک API رایگان قیمت سهام بگرد، با ایمیل supersuper7434567@gmail.com ثبت‌نام کن، سپس یک اسکریپت پایتون برای دریافت قیمت روزانه تسلا با استفاده از API بنویس و نتایج را در stock_prices.csv ذخیره کن</em></blockquote></p><p><em>توجه داشته باشید که قابلیت پرکردن فرم هنوز آزمایشی است و ممکن است با خطا مواجه شود.</em></p><p>پس از وارد کردن پرسش، AgenticSeek بهترین عامل را برای انجام وظیفه اختصاص می‌دهد.</p><p>از آنجا که این نسخه یک نمونه اولیه است، سیستم مسیریابی عامل ممکن است همیشه عامل مناسب را با توجه به پرسش شما انتخاب نکند.</p><p>بنابراین باید کاملاً صریح بیان کنید که چه می‌خواهید و AI چگونه باید پیش برود؛ مثلاً اگر می‌خواهید جستجوی وب انجام شود، نگویید:</p><p></code>Do you know some good countries for solo-travel?<code></p><p>بلکه بپرسید:</p><p></code>Do a web search and find out which are the best country for solo-travel<code></p><hr></p><h2><strong>راه‌اندازی اجرای LLM روی سرور شخصی</strong>  </h2></p><p>اگر یک کامپیوتر قوی یا سروری در اختیار دارید اما می‌خواهید از لپ‌تاپ خود به آن دسترسی داشته باشید، می‌توانید LLM را روی یک سرور راه دور با استفاده از llm server سفارشی ما اجرا کنید.</p><p>روی "سرور" خود که قرار است مدل هوش مصنوعی روی آن اجرا شود، آدرس IP را به دست آورید
</code></pre>sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # آی‌پی محلی
curl https://ipinfo.io/ip # آی‌پی عمومی
<pre><code class="language-">
توجه: در ویندوز یا macOS از ipconfig یا ifconfig برای پیدا کردن آدرس IP استفاده کنید.</p><p>مخزن را کلون کرده و وارد پوشه </code>server/<code> شوید.</p><p></code></pre>sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
<pre><code class="language-">
پیش‌نیازهای سرور را نصب کنید:
</code></pre>sh
pip3 install -r requirements.txt
<pre><code class="language-">
اسکریپت سرور را اجرا کنید.
</code></pre>sh
python3 app.py --provider ollama --port 3333
<pre><code class="language-">
می‌توانید از بین </code>ollama<code> و </code>llamacpp<code> به عنوان سرویس LLM انتخاب کنید.</p><p>
حالا روی کامپیوتر شخصی خود:</p><p>فایل </code>config.ini<code> را ویرایش کرده و مقدار </code>provider_name<code> را روی </code>server<code> و </code>provider_model<code> را روی </code>deepseek-r1:xxb<code> تنظیم کنید.
مقدار </code>provider_server_address<code> را به آدرس آی‌پی دستگاهی که مدل روی آن اجرا می‌شود قرار دهید.
</code></pre>sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
<pre><code class="language-"></p><p>گام بعدی: <a href="#Start-services-and-Run" target="_blank" rel="noopener noreferrer">شروع سرویس‌ها و اجرای AgenticSeek</a>  </p><hr></p><h2>تبدیل گفتار به متن</h2></p><p>هشدار: تبدیل گفتار به متن در حال حاضر فقط در حالت CLI فعال است.</p><p>لطفاً توجه داشته باشید که فعلاً تبدیل گفتار به متن تنها به زبان انگلیسی کار می‌کند.</p><p>قابلیت تبدیل گفتار به متن به طور پیش‌فرض غیرفعال است. برای فعال‌سازی، گزینه listen را در فایل config.ini به True تغییر دهید:
</code></pre>
listen = True
<pre><code class="language-">
پس از فعال‌سازی، ویژگی تبدیل گفتار به متن منتظر یک کلمه کلیدی (نام عامل) می‌ماند تا پیش از پردازش ورودی شما شروع به کار کند. می‌توانید نام عامل را با به‌روزرسانی مقدار </code>agent_name<code> در فایل <em>config.ini</em> شخصی‌سازی کنید:
</code></pre>
agent_name = Friday
<pre><code class="language-">
برای شناسایی بهینه، توصیه می‌کنیم از یک نام رایج انگلیسی مانند "John" یا "Emma" به عنوان نام ایجنت استفاده کنید.</p><p>پس از اینکه رونوشت شروع به ظاهر شدن کرد، نام ایجنت را با صدای بلند بگویید تا بیدار شود (مثلاً "Friday").</p><p>سؤال خود را واضح بیان کنید.</p><p>در پایان درخواست خود، یک عبارت تأییدی بگویید تا سیستم اقدام کند. نمونه‌هایی از عبارات تأییدی:</code></pre>
"انجام بده"، "ادامه بده"، "اجرا کن"، "شروع کن"، "ممنون"، "لطفاً"، "باشه؟"، "ادامه"، "برو"، "این کار رو انجام بده"، "متوجه شدی؟"
<pre><code class="language-">
<h2>تنظیمات (Config)</h2></p><p>نمونه تنظیمات:</code></pre>
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # نمونه برای Ollama؛ برای LM-Studio از http://127.0.0.1:1234 استفاده کنید
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False</p><p>jarvis_personality = False
languages = en zh # لیست زبان‌ها برای تبدیل متن به گفتار و مسیریابی احتمالی.
[BROWSER]
headless_browser = False
stealth_mode = False
<pre><code class="language-">
<strong>توضیح تنظیمات </code>config.ini<code>:</strong></p><ul><li>  <strong>بخش </code>[MAIN]<code>:</strong></li>
    <li>  </code>is_local<code>: اگر از ارائه‌دهنده LLM محلی (Ollama، LM-Studio، سرور OpenAI سازگار محلی) یا گزینه سرور خودمیزبان استفاده می‌کنید مقدار </code>True<code> باشد. اگر از API مبتنی بر ابر (OpenAI، گوگل و غیره) استفاده می‌کنید مقدار </code>False<code> باشد.</li>
    <li>  </code>provider_name<code>: مشخص‌کننده ارائه‌دهنده LLM.</li>
        <li>  گزینه‌های محلی: </code>ollama<code>، </code>lm-studio<code>، </code>openai<code> (برای سرور OpenAI سازگار محلی)، </code>server<code> (برای سرور خودمیزبان).</li>
        <li>  گزینه‌های API: </code>openai<code>، </code>google<code>، </code>deepseek<code>، </code>huggingface<code>، </code>togetherAI<code>.</li>
    <li>  </code>provider_model<code>: نام یا شناسه مدل موردنظر برای ارائه‌دهنده انتخابی (مثلاً </code>deepseekcoder:6.7b<code> برای Ollama، </code>gpt-3.5-turbo<code> برای OpenAI API، </code>mistralai/Mixtral-8x7B-Instruct-v0.1<code> برای TogetherAI).</li>
    <li>  </code>provider_server_address<code>: آدرس ارائه‌دهنده LLM شما.</li>
        <li>  برای ارائه‌دهندگان محلی: مثلاً </code>http://127.0.0.1:11434<code> برای Ollama، </code>http://127.0.0.1:1234<code> برای LM-Studio.</li>
        <li>  برای نوع ارائه‌دهنده </code>server<code>: آدرس سرور LLM خودمیزبان شما (مثلاً </code>http://your_server_ip:3333<code>).</li>
        <li>  برای APIهای ابری (</code>is_local = False<code>): اغلب نادیده گرفته می‌شود یا می‌تواند خالی باشد، زیرا نقطه پایانی API معمولاً توسط کتابخانه کلاینت مدیریت می‌شود.</li>
    <li>  </code>agent_name<code>: نام دستیار هوشمند (مثلاً Friday). به‌عنوان کلمه کلیدی فعال‌سازی برای تبدیل گفتار به متن در صورت فعال بودن استفاده می‌شود.</li>
    <li>  </code>recover_last_session<code>: اگر مقدار </code>True<code> باشد سعی در بازیابی وضعیت جلسه قبلی دارد؛ اگر </code>False<code> باشد یک جلسه جدید آغاز می‌کند.</li>
    <li>  </code>save_session<code>: اگر مقدار </code>True<code> باشد وضعیت جلسه فعلی برای بازیابی احتمالی ذخیره می‌شود؛ در غیر اینصورت </code>False<code>.</li>
    <li>  </code>speak<code>: اگر مقدار </code>True<code> باشد خروجی صوتی تبدیل متن به گفتار فعال می‌شود؛ در غیر اینصورت غیرفعال است.</li>
    <li>  </code>listen<code>: اگر مقدار </code>True<code> باشد ورودی صوتی تبدیل گفتار به متن (فقط حالت CLI) فعال می‌شود؛ در غیر اینصورت غیرفعال.</li>
    <li>  </code>work_dir<code>: <strong>مهم:</strong> مسیری که AgenticSeek فایل‌ها را از آن می‌خواند/می‌نویسد. <strong>اطمینان حاصل کنید که این مسیر معتبر و در دسترس باشد.</strong></li>
    <li>  </code>jarvis_personality<code>: اگر مقدار </code>True<code> باشد از پرامپت سیستمی مشابه "Jarvis" استفاده می‌کند (آزمایشی)، اگر </code>False<code> باشد از پرامپت استاندارد بهره می‌گیرد.</li>
    <li>  </code>languages<code>: لیست زبان‌ها به صورت جداشده با ویرگول (مثلاً </code>en, zh, fr<code>). برای انتخاب صدای TTS (پیش‌فرض اولین زبان) و کمک به مسیریاب LLM استفاده می‌شود. برای بازدهی بهتر مسیریاب، از انتخاب زبان‌های بسیار مشابه یا تعداد زیاد خودداری کنید.</li>
<li>  <strong>بخش </code>[BROWSER]<code>:</strong></li>
    <li>  </code>headless_browser<code>: اگر مقدار </code>True<code> باشد مرورگر خودکار بدون نمایش پنجره اجرا می‌شود (برای رابط وب یا استفاده غیرتعاملی توصیه می‌شود). مقدار </code>False<code> باعث نمایش پنجره مرورگر می‌شود (برای حالت CLI یا اشکال‌زدایی مفید است).</li>
    <li>  </code>stealth_mode<code>: اگر مقدار </code>True<code> باشد اقدامات لازم برای سخت‌تر شدن تشخیص اتوماسیون مرورگر فعال می‌شود. ممکن است نیاز به نصب دستی افزونه‌هایی مانند anticaptcha داشته باشد.</li></p><p>
</ul>این بخش انواع ارائه‌دهندگان LLM پشتیبانی‌شده را خلاصه می‌کند. آن‌ها را در </code>config.ini<code> پیکربندی کنید.</p><p><strong>ارائه‌دهندگان محلی (اجرا روی سخت‌افزار خودتان):</strong></p><p>| نام ارائه‌دهنده در </code>config.ini<code> | </code>is_local<code> | توضیحات                                                                  | بخش راه‌اندازی                                                       |
|-------------------------------|------------|--------------------------------------------------------------------------|---------------------------------------------------------------------|
| </code>ollama<code>                      | </code>True<code>     | استفاده از Ollama برای سرویس‌دهی LLMهای محلی.                           | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">راه‌اندازی اجرای LLM به صورت محلی</a> |
| </code>lm-studio<code>                   | </code>True<code>     | استفاده از LM-Studio برای سرویس‌دهی LLMهای محلی.                        | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">راه‌اندازی اجرای LLM به صورت محلی</a> |
| </code>openai<code> (برای سرور محلی)     | </code>True<code>     | اتصال به سروری که API سازگار با OpenAI را ارائه می‌دهد (مثلاً llama.cpp). | <a href="#setup-for-running-llm-locally-on-your-machine" target="_blank" rel="noopener noreferrer">راه‌اندازی اجرای LLM به صورت محلی</a> |
| </code>server<code>                      | </code>False<code>    | اتصال به سرور LLM خودمیزبان AgenticSeek که روی دستگاه دیگر اجرا می‌شود.   | <a href="#setup-to-run-the-llm-on-your-own-server" target="_blank" rel="noopener noreferrer">راه‌اندازی اجرای LLM روی سرور خودتان</a>   |</p><p><strong>ارائه‌دهندگان API (مبتنی بر ابر):</strong></p><p>| نام ارائه‌دهنده در </code>config.ini<code> | </code>is_local<code> | توضیحات                                            | بخش راه‌اندازی                                       |
|-------------------------------|------------|----------------------------------------------------|------------------------------------------------------|
| </code>openai<code>                      | </code>False<code>    | استفاده از API رسمی OpenAI (مثلاً GPT-3.5، GPT-4). | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی با API</a>       |
| </code>google<code>                      | </code>False<code>    | استفاده از مدل‌های Gemini گوگل از طریق API.         | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی با API</a>       |
| </code>deepseek<code>                    | </code>False<code>    | استفاده از API رسمی Deepseek.                      | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی با API</a>       |
| </code>huggingface<code>                 | </code>False<code>    | استفاده از API استنتاج Hugging Face.               | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی با API</a>       |
| </code>togetherAI<code>                  | </code>False<code>    | استفاده از API TogetherAI برای مدل‌های باز مختلف.   | <a href="#setup-to-run-with-an-api" target="_blank" rel="noopener noreferrer">راه‌اندازی با API</a>       |</p><hr>
<h2>عیب‌یابی</h2></p><p>اگر با مشکلی مواجه شدید، این بخش راهنمایی ارائه می‌دهد.</p><h1>مشکلات شناخته‌شده</h1></p><h2>مشکلات ChromeDriver</h2></p><p><strong>نمونه خطا:</strong>  
</code>SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX<code></p><ul><li>  <strong>دلیل:</strong> نسخه نصب‌شده ChromeDriver با نسخه مرورگر Google Chrome شما ناسازگار است.</li>
<li>  <strong>راه‌حل:</strong></li>
    <li> <strong>بررسی نسخه Chrome:</strong> مرورگر Google Chrome را باز کنید و به </code>تنظیمات > درباره Chrome<code> بروید تا نسخه را بیابید (مثلاً "Version 120.0.6099.110").</li>
    <li> <strong>دانلود ChromeDriver هماهنگ:</strong></li>
        <li>  برای نسخه‌های 115 و بالاتر: به <a href="https://googlechromelabs.github.io/chrome-for-testing/" target="_blank" rel="noopener noreferrer">Chrome for Testing (CfT) JSON Endpoints</a> بروید. کانال "stable" را پیدا کنید و ChromeDriver مناسب سیستم عامل و نسخه اصلی مرورگر خود را دانلود کنید.</li>
        <li>  برای نسخه‌های قدیمی‌تر (کمتر رایج): ممکن است آن‌ها را در صفحه <a href="https://chromedriver.chromium.org/downloads" target="_blank" rel="noopener noreferrer">ChromeDriver - WebDriver for Chrome</a> پیدا کنید.</li>
        <li>  تصویر زیر نمونه‌ای از صفحه CfT را نشان می‌دهد:</li>
            </ul><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="دانلود نسخه خاص Chromedriver از صفحه Chrome for Testing">
    <ul><li> <strong>نصب ChromeDriver:</strong></li>
        <li>  اطمینان حاصل کنید که فایل دانلودی </code>chromedriver<code> (یا </code>chromedriver.exe<code> در ویندوز) در مسیری قرار گیرد که در متغیر محیطی PATH سیستم شما باشد (مثلاً </code>/usr/local/bin<code> در لینوکس/مک، یا یک پوشه اسکریپت اختصاصی که به PATH ویندوز افزوده شده).</li>
        <li>  یا آن را در پوشه اصلی پروژه </code>agenticSeek<code> قرار دهید.</li>
        <li>  مطمئن شوید که فایل اجرایی است (مثلاً </code>chmod +x chromedriver<code> در لینوکس/مک).</li>
    <li> به بخش <a href="#chromedriver-installation" target="_blank" rel="noopener noreferrer">نصب ChromeDriver</a> در راهنمای نصب اصلی برای جزئیات بیشتر مراجعه کنید.</li></p><p></ul>اگر این بخش کامل نیست یا با مشکل دیگری در ChromeDriver مواجه شدید، لطفاً در <a href="https://github.com/Fosowl/agenticSeek/issues" target="_blank" rel="noopener noreferrer">Issues گیت‌هاب</a> جستجو کنید یا مورد جدیدی ثبت نمایید.</p><p></code>Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path<code></p><p>این خطا هنگامی رخ می‌دهد که نسخه مرورگر و chromedriver شما مطابقت نداشته باشد.</p><p>باید به آدرس زیر رفته و آخرین نسخه را دانلود کنید:</p><p>https://developer.chrome.com/docs/chromedriver/downloads</p><p>اگر از Chrome نسخه 115 یا بالاتر استفاده می‌کنید به:</p><p>https://googlechromelabs.github.io/chrome-for-testing/</p><p>رفته و نسخه chromedriver متناسب با سیستم عامل خود را دانلود کنید.</p><p><img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text"></p><p>اگر این بخش کامل نیست لطفاً یک issue ثبت کنید.</p><h2>مشکلات connection adapters</h2>
</code></pre>
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'</code> (توجه: شماره پورت ممکن است متفاوت باشد)
<pre><code class="language-">
<ul><li>  <strong>دلیل:</strong> در تنظیمات <code>config.ini</code> برای <code>lm-studio</code> (یا سایر سرورهای مشابه OpenAI محلی) پیشوند <code>http://</code> در آدرس <code>provider_server_address</code> وجود ندارد یا پورت اشتباه است.</li>
<li>  <strong>راه‌حل:</strong></li>
    <li>  مطمئن شوید آدرس شامل <code>http://</code> است. LM-Studio معمولاً به طور پیش‌فرض <code>http://127.0.0.1:1234</code> است.</li>
    <li>  تنظیم صحیح در <code>config.ini</code>:  </li>
        </ul><code>provider_server_address = http://127.0.0.1:1234</code> (یا پورت واقعی سرور LM-Studio شما).</p><h2>SearxNG Base URL ارائه نشده است</h2>
</code></pre>
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.<code>
</code>`<code></p><h2>سوالات متداول (FAQ)</h2></p><p><strong>س: چه سخت‌افزاری نیاز دارم؟</strong>  </p><p>| اندازه مدل  | GPU  | توضیحات                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB Vram | ⚠️ توصیه نمی‌شود. عملکرد ضعیف، هذیان‌گویی زیاد و احتمال شکست ایجنت‌های برنامه‌ریز. |
| 14B        | 12 GB VRAM (مثلاً RTX 3060) | ✅ مناسب برای کارهای ساده. ممکن است در وب‌گردی و برنامه‌ریزی مشکل داشته باشد. |
| 32B        | 24+ GB VRAM (مثلاً RTX 4090) | 🚀 موفقیت در اکثر کارها، شاید هنوز در برنامه‌ریزی وظایف کمی مشکل داشته باشد. |
| 70B+        | 48+ GB Vram | 💪 عالی. برای موارد پیشرفته توصیه می‌شود. |</p><p><strong>س: خطا دریافت می‌کنم، چه کنم؟</strong>  </p><p>مطمئن شوید حالت محلی (</code>ollama serve<code>) اجرا می‌شود، </code>config.ini` شما با ارائه‌دهنده هماهنگ است و وابستگی‌ها نصب شده‌اند. اگر هیچ‌کدام جواب نداد یک issue ثبت کنید.</p><p><strong>س: آیا واقعاً می‌تواند ۱۰۰٪ محلی اجرا شود؟</strong>  </p><p>بله، با ارائه‌دهندگان Ollama، lm-studio یا server، تمام تبدیل گفتار به متن، LLM و تبدیل متن به گفتار به طور کامل محلی اجرا می‌شوند. گزینه‌های غیرمحلی (OpenAI یا سایر APIها) اختیاری هستند.</p><p><strong>س: چرا باید از AgenticSeek استفاده کنم وقتی Manus را دارم؟</strong></p><p>برخلاف Manus، AgenticSeek بر استقلال از سیستم‌های بیرونی تأکید دارد و کنترل، حریم خصوصی بیشتر و عدم هزینه API را به شما می‌دهد.</p><p><strong>س: چه کسی پشت پروژه است؟</strong></p><p>پروژه توسط من و دو دوست دیگر که نگهدارنده و مشارکت‌کننده از جامعه متن‌باز گیت‌هاب هستند ساخته شده است. ما فقط یک گروه علاقه‌مند هستیم و هیچ استارت‌آپ یا سازمانی پشت آن نیست.</p><p>هر حساب AgenticSeek در X به‌جز حساب شخصی من (https://x.com/Martin993886460) جعل هویت است.</p><h2>مشارکت</h2></p><p>ما به دنبال توسعه‌دهندگانی برای بهبود AgenticSeek هستیم! مسائل باز یا بحث‌ها را بررسی کنید.</p><p><a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer">راهنمای مشارکت</a></p><p><a href="https://www.star-history.com/#Fosowl/agenticSeek&Date" target="_blank" rel="noopener noreferrer"><img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date" alt="Star History Chart"></a></p><h2>نگهدارندگان:</h2></p><p> > <a href="https://github.com/Fosowl" target="_blank" rel="noopener noreferrer">Fosowl</a> | ساعت پاریس</p><p> > <a href="https://github.com/antoineVIVIES" target="_blank" rel="noopener noreferrer">antoineVIVIES</a> | ساعت تایپه</p><p> > <a href="https://github.com/steveh8758" target="_blank" rel="noopener noreferrer">steveh8758</a> | ساعت تایپه</p><h2>تشکر ویژه:</h2></p><p> > <a href="https://github.com/tcsenpai" target="_blank" rel="noopener noreferrer">tcsenpai</a> و <a href="https://github.com/plitc" target="_blank" rel="noopener noreferrer">plitc</a> برای کمک به داکرایز کردن بک‌اند</p><h2>اسپانسرها:</h2></p><p>اسپانسرهای ماهیانه ۵ دلار یا بیشتر اینجا نمایش داده می‌شوند:
<ul><li><strong>tatra-labs</strong></li>
</ul>Sure! Please provide the content of Part 4 of 4 for translation.

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-06-16

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-07-24 
    </div>
    
</body>
</html>