<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PAM - Read PAM documentation in English. This project has 248 stars on GitHub.</title>
    <meta name="description" content="Read PAM documentation in English. This project has 248 stars on GitHub.">
    <meta name="keywords" content="PAM, English, documentation, GitHub, open source">
    <meta name="author" content="OpenAiTx">
    <meta name="robots" content="index, follow">
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "SoftwareApplication",
  "name": "PAM",
  "description": "Read PAM documentation in English. This project has 248 stars on GitHub.",
  "author": {
    "@type": "Person",
    "name": "Perceive-Anything"
  },
  "applicationCategory": "DeveloperApplication",
  "operatingSystem": "Any",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "5",
    "ratingCount": 248
  },
  "url": "https://OpenAiTx.github.io/projects/Perceive-Anything/PAM/README-en.html",
  "sameAs": "https://raw.githubusercontent.com/Perceive-Anything/PAM/main/README.md",
  "datePublished": "2025-09-16",
  "dateModified": "2025-09-16"
}
    </script>
    
    <!-- GitHub-style CSS -->
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #24292e;
            background-color: #ffffff;
            max-width: 980px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .container {
            background: #ffffff;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 24px;
            margin-bottom: 20px;
        }
        
        .header {
            border-bottom: 1px solid #e1e4e8;
            padding-bottom: 16px;
            margin-bottom: 24px;
        }
        
        .project-title {
            font-size: 2em;
            font-weight: 600;
            margin-bottom: 8px;
        }
        
        .project-title a {
            color: #24292e;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            transition: color 0.2s ease;
        }
        
        .project-title a:hover {
            color: #0366d6;
            text-decoration: none;
        }
        
        .project-title .github-icon {
            width: 1em;
            height: 1em;
            fill: currentColor;
            opacity: 0.7;
            transition: opacity 0.2s ease;
        }
        
        .project-title a:hover .github-icon {
            opacity: 1;
        }
        
        .project-meta {
            color: #586069;
            font-size: 14px;
            margin-bottom: 16px;
        }
        
        .stars {
            display: inline-block;
            margin-right: 16px;
        }
        
        .language {
            display: inline-block;
            background-color: #f1f8ff;
            color: #0366d6;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 12px;
            font-weight: 500;
        }
        
        .content {
            font-size: 16px;
            line-height: 1.6;
        }
        
        h1, h2, h3, h4, h5, h6 {
            margin-top: 24px;
            margin-bottom: 16px;
            font-weight: 600;
            line-height: 1.25;
            color: #24292e;
        }
        
        h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
        h3 { font-size: 1.25em; }
        
        p {
            margin-bottom: 16px;
        }
        
        code {
            background-color: #f6f8fa;
            border-radius: 3px;
            font-size: 85%;
            margin: 0;
            padding: 0.2em 0.4em;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
        }
        
        pre {
            background-color: #f6f8fa;
            border-radius: 6px;
            font-size: 85%;
            line-height: 1.45;
            overflow: auto;
            padding: 16px;
            margin-bottom: 16px;
        }
        
        pre code {
            background-color: transparent;
            border: 0;
            display: inline;
            line-height: inherit;
            margin: 0;
            overflow: visible;
            padding: 0;
            word-wrap: normal;
        }
        
        a {
            color: #0366d6;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        ul, ol {
            margin-bottom: 16px;
            padding-left: 2em;
        }
        
        li {
            margin-bottom: 0.25em;
        }
        
        blockquote {
            border-left: 4px solid #dfe2e5;
            color: #6a737d;
            margin: 0 0 16px 0;
            padding: 0 1em;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e1e4e8;
            color: #586069;
            font-size: 14px;
            text-align: center;
        }
        
        .footer a {
            color: #0366d6;
        }
        
        .original-link {
            margin-top: 16px;
            padding: 12px;
            background-color: #f6f8fa;
            border-radius: 6px;
            border: 1px solid #e1e4e8;
        }
        
        @media (max-width: 768px) {
            body {
                padding: 10px;
            }
            
            .container {
                padding: 16px;
            }
            
            .project-title {
                font-size: 1.5em;
            }
        }
    </style>
    
    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Bing Count -->
    <script type="text/javascript">
        (function(c,l,a,r,i,t,y){
            c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
            t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
            y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
        })(window, document, "clarity", "script", "sh95yd6uwt");
    </script>        

    <!-- Statcounter -->
    <script type="text/javascript">
        var sc_project = 13142514;
        var sc_invisible = 1;
        var sc_security = "d03a31d8"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/13142514/0/d03a31d8/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>    
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="project-title">
                <a href="https://github.com/Perceive-Anything/PAM" target="_blank" rel="noopener noreferrer">
                    <svg class="github-icon" viewBox="0 0 16 16" aria-hidden="true">
                        <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
                    </svg>
                    PAM
                </a>
            </h1>
            <div class="project-meta">
                <span class="stars">⭐ 248 stars</span>
                <span class="language">English</span>
                <span>by Perceive-Anything</span>
            </div>
        </div>
        
        <div class="content">
            <p><div align="center">
<h1>
Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos (PAM)
</h1></p><p></div></p><p><div align="center"></p><p>[Weifeng Lin](), [Xinyu Wei](), [Ruichuan An](), [Tianhe Ren](), [Tingwei Chen](), [Renrui Zhang](), [Ziyu Guo]() <br>
[Wentao Zhang](), [Lei Zhang](), [Hongsheng Li]() <br>
CUHK, HKU, PolyU, PekingU</p><p></div></p><p><p align="center">
  <a href="https://Perceive-Anything.github.io"><b>🌐 Project Website</b></a> |
  <a href="https://arxiv.org/abs/2506.05302"><b>📕 Paper</b></a> |
  <a href="https://huggingface.co/Perceive-Anything/PAM-3B"><b>📥 Model Download</b></a> |
  <a href="https://huggingface.co/datasets/Perceive-Anything/PAM-data"><b>🤗 Dataset</b></a> |
  <a href="#quick-start"><b>⚡Quick Start</b></a> <br>
  <a href="#license"><b>📜 License</b></a> |
  <a href="#citation"><b>📖 Citation (BibTeX)</b></a> <br>
</p></p><p><p align="center">
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/teaser_img.jpg" width="95%"> <br>
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/teaser_video.jpg" width="95%"> <br>
</p></p><h2>News</h2></p><p><!-- <strong>2025.06.20</strong>: Release Gradio demo ([online demo]() and <a href="#gradio-demo" target="_blank" rel="noopener noreferrer">local</a>) --></p><p><!-- <strong>2025.06.05</strong>: Evaluation code Please refer to [this link](). --></p><p><strong>2025.06.08</strong>: Model weights (1.5B / 3B) and training datasets are released. Please refer to <a href="https://huggingface.co/Perceive-Anything/PAM-1.5B" target="_blank" rel="noopener noreferrer">PAM-1.5B</a>, <a href="https://huggingface.co/Perceive-Anything/PAM-3B" target="_blank" rel="noopener noreferrer">PAM-3B</a> and <a href="https://huggingface.co/datasets/Perceive-Anything/PAM-data" target="_blank" rel="noopener noreferrer">Datasets</a>.</p><p><strong>2025.06.08</strong>: PAM is released, a simple end-to-end region-level VLM for object segmentation and understanding. See <a href="https://arxiv.org/abs/2506.05302" target="_blank" rel="noopener noreferrer">paper</a></p><h2>Introduction</h2></p><p><strong>Perceive Anything Model (PAM)</strong> is a conceptually simple and efficient framework for comprehensive region-level visual understanding in images and videos. Our approach extends SAM 2 by integrating Large Language Models (LLMs), enabling simultaneous object segmentation with the generation of diverse, region-specific semantic outputs, including categories, label definition, functional explanations, and detailed captions. We propose to efficiently transform SAM 2's rich visual features, which inherently carry general vision, localization, and semantic priors into multi-modal tokens for LLM comprehension. To support robust multi-granularity understanding, we develop a dedicated data refinement and augmentation pipeline, yielding a high-quality <a href="https://huggingface.co/datasets/Perceive-Anything/PAM-data" target="_blank" rel="noopener noreferrer"><strong>dataset</strong></a> of image and video region-semantic annotations, including novel region-level streaming video caption data.</p><p>
<p align="center">
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/PAM_comp.jpg" width="95%"> <br>
    <img src="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/assets/PAM_arch.jpg" width="95%"> <br>
</p></p><h2>Installation</h2></p><ul><li>Clone this repository and navigate to the base folder</li></p><p></ul><pre><code class="language-bash">git clone https://github.com/Afeng-x/PAM.git
cd PAM</code></pre>
<ul><li>Install packages</li></p><p></ul><pre><code class="language-bash">### packages for base
conda create -n PAM python=3.10 -y
conda activate PAM
pip install --upgrade pip
pip install -e ".[train]"
<h3>packages for sam2</h3>
cd sam2
pip install -e ".[notebooks]"</code></pre>
<ul><li>Install Flash-Attention</li></p><p></ul><pre><code class="language-bash">pip install flash-attn --no-build-isolation
<h3>(If the method mentioned above don’t work for you, try the following one)</h3>
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
python setup.py install</code></pre>
<ul><li>Download the SAM2.1-h-large checkpoint:</li></p><p></ul><pre><code class="language-bash">cd llava/model/multimodal_encoder
bash download_ckpts.sh</code></pre>
<h2>Quick Start</h2></p><ul><li>Image: Please refer to the examples in <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/image_infer_example.ipynb" target="_blank" rel="noopener noreferrer">image_infer_example.ipynb</a></li>
<li>Video: Please refer to the examples in <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/video_infer_example.ipynb" target="_blank" rel="noopener noreferrer">video_infer_example.ipynb</a></li>
<li>Video Stream: Please refer to the examples in <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/./notebooks/video_stream_infer_example.ipynb" target="_blank" rel="noopener noreferrer">video_stream_infer_example.ipynb</a></li></p><p></ul><h2>Dataset</h2></p><p>Please refer to <a href="https://huggingface.co/datasets/Perceive-Anything/PAM-data" target="_blank" rel="noopener noreferrer">this link</a> to download our refined and augmented data annotations.</p><p><strong>Note:</strong> We do not directly provide the source images. However, for each dataset, we will provide the relevant download links or official website addresses to guide users on how to download them. <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/data/README.md" target="_blank" rel="noopener noreferrer">DATA_README</a></p><p><!-- ## Training PAM</p><p>You can train or fine-tune PAM on custom datasets of images, videos, or both. Please check the training <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/training/README.md" target="_blank" rel="noopener noreferrer">README</a> on how to get started. --></p><h2>Local Gradio Demo for PAM</h2>
In progress ......
<!-- ### Simple Gradio Demo for Image</p><p><a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/pam_image.py" target="_blank" rel="noopener noreferrer"><code>pam_image.py</code></a> - Interactive Gradio web interface for drawing masks on images and getting semantics. <strong>This demo is tested with <code>gradio</code> 5.5.0.</strong></p><h3>Simple Gradio Demo for Video</h3></p><p><a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/pam_video.py" target="_blank" rel="noopener noreferrer"><code>pam_video.py</code></a> - Interactive Gradio web interface for drawing masks on videos and getting semantics. <strong>This demo is tested with <code>gradio</code> 5.5.0.</strong> --></p><h2>License</h2></p><p>This code repository is licensed under <a href="./LICENSE" target="_blank" rel="noopener noreferrer">Apache 2.0</a>.</p><h2>Acknowledgement</h2>
We would like to thank the following projects for their contributions to this work:</p><ul><li><a href="https://github.com/LLaVA-VL/LLaVA-NeXT" target="_blank" rel="noopener noreferrer">LLaVA-Next</a></li>
<li><a href="https://github.com/facebookresearch/segment-anything" target="_blank" rel="noopener noreferrer">SAM</a></li>
<li><a href="https://github.com/facebookresearch/sam2" target="_blank" rel="noopener noreferrer">SAM 2</a></li></p><p></ul><h2>Citation</h2></p><p>If you find PAM useful for your research and applications, or use our dataset in your research, please use the following BibTeX entry.</p><pre><code class="language-bibtex">@misc{lin2025perceiveanythingrecognizeexplain,
      title={Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos}, 
      author={Weifeng Lin and Xinyu Wei and Ruichuan An and Tianhe Ren and Tingwei Chen and Renrui Zhang and Ziyu Guo and Wentao Zhang and Lei Zhang and Hongsheng Li},
      year={2025},
      eprint={2506.05302},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05302}, 
}</code></pre></p><p>

---

Tranlated By <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">Open Ai Tx</a> | Last indexed: 2025-07-19

---</p>
        </div>
        
        <div class="original-link">
            <strong>Original README:</strong> <a href="https://raw.githubusercontent.com/Perceive-Anything/PAM/main/README.md" target="_blank" rel="noopener noreferrer">View on GitHub</a>
        </div>
    </div>
    
    <div class="footer">
        <p>Translated by <a href="https://github.com/OpenAiTx/OpenAiTx" target="_blank" rel="noopener noreferrer">OpenAiTx</a> | 
        Last updated: 2025-09-16 
    </div>
    
</body>
</html>